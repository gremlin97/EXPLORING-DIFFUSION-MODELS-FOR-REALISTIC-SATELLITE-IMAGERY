Accepted as a paper at ICLR 2023 Workshop on Machine Learning for Remote Sensing
EVALUATION CHALLENGES FOR GEOSPATIAL ML
Esther Rolf
Harvard University
ABSTRACT
As geospatial machine learning models and maps derived from their predictions
are increasingly used for downstream analyses in science and policy, it is im-
perative to evaluate their accuracy and applicability. Geospatial machine learn-
ing has key distinctions from other learning paradigms, and as such, the correct
way to measure performance of spatial machine learning outputs has been a topic
of debate. In this paper, I delineate unique challenges of model evaluation for
geospatial machine learning with global or remotely sensed datasets, culminating
in concrete takeaways to improve evaluations of geospatial model performance.
1 M OTIVATION
Geospatial machine learning (ML), for example with remotely sensed data, is being used across
consequential domains, including public health (Nilsen et al., 2021; Draidi Areed et al., 2022) con-
servation (Sofaer et al., 2019), food security (Nakalembe, 2018), and wealth estimation (Jean et al.,
2016; Chi et al., 2022). By both their use and their very nature, geospatial predictions have a purpose
beyond model benchmarking; mapped data are to be read, scrutinized, and acted upon. Thus, it is
critical to rigorously and comprehensively evaluate how well a predicted map represents the state of
the world it is meant to reﬂect, or how well a spatial ML model performs across the many conditions
in which it might be used.
Unique structures in remotely sensed and geospatial data complicate or even invalidate use of tradi-
tional ML evaluation procedures. Partially as a result of misunderstandings of these complications,
the stated performance of several geospatial models and predictive maps has come into question
(Fourcade et al., 2018; Ploton et al., 2020). This in turn has sparked disagreement on what the
“right” evaluation procedure is. With respect to a certain set of spatial evaluation methods (described
in §4.1), one is jointly presented with the arguments that “spatial cross-validation is essential in pre-
venting overoptimistic model performance” (Meyer et al., 2019) and “spatial cross-validation meth-
ods have no theoretical underpinning and should not be used for assessing map accuracy” (Wadoux
et al., 2021). That both statements can simultaneously hold reﬂects the importance of using a diverse
set of evaluation methods tailored to the many ways in which a geospatial ML model might be used.
In this paper, I situate the challenges of geopsatial model evaluation in the perspective of an ML
researcher, synthesizing prior work across ecology, geology, statistics, and machine learning. I aim
in part to disentangle key factors that complicate effective evaluation of model and map performance.
First and foremost, evaluation procedures should be designed to measure as closely as possible the
quantity or phenomena they are intended to assess (§2). After the relevant performance measures
are established, considerations can be made about what is feasible with the available data (§3). With
all of this in mind, possible evaluation procedures (§4) can be compared and tailored to the task at
hand. Recognizing the interaction of these distinct but related steps exposes opportunities to improve
geospatial performance assessment, both in individual studies and more broadly (§5).
2 M AP ACCURACY AND MODEL PERFORMANCE : CONTRASTING VIEWS
Estimating accuracy indices and corresponding uncertainties of geospatial predictions is essential
to reporting geospatial ML performance (§2.1), especially when prediction maps will be used for
downstream analyses or policy decisions. At the same time, the potential value of a geospatial ML
model likely extends beyond that of a single mapped output (§2.2). Delineating the (possibly many)
facets of desired model and map use is key to measuring geospatial ML performance (§2.3).
1arXiv:2303.18087v1  [cs.LG]  31 Mar 2023
Accepted as a paper at ICLR 2023 Workshop on Machine Learning for Remote Sensing
2.1 M AP ACCURACY AS A POPULATION PARAMETER TO BE ESTIMATED
Establishing notation we will use throughout, let ˆy(ℓ)denote a model’s predicted value at location
ℓ, andy(ℓ)the reference, or “ground truth” value (which we assume can be measured). To calculate
amap accuracy index as a population parameter for accuracy index Fis to calculate A(D) =
F({(ˆy(ℓ),y(ℓ))}ℓ∈D)whereDis the target population of map use (e.g. all (lat, lon) pairs in a
global grid, or all administrative units in a set of countries). Examples of common Finclude root
mean squared error, and area under the ROC curve, among many others (Maxwell et al., 2021).
Typically, one only has a limited set of values yfor locations in an evaluation set ℓ∈S evalfrom which
to compute a statistic ˆA(Seval)to estimateA(D). Wadoux et al. (2021) discuss the value of using a
design-independent probability sample for design-based estimation of A(in contrast, model-based
estimation makes statistical assumptions about the data (Brus, 2021)). Here a design-independent
sample is one collected independently of the model training process. A probability sample is one
for which every location in Dhas a positive probability of appearing in Seval, and these probabilities
are known for all ℓ∈S eval(see, e.g. Lohr (2021)). Wadoux et al. (2021) emphasize that when Seval
is a design independent probability sample from population D, design-based inference can be used
to estimateA(D)with ˆA(Seval),regardless of the prediction model or distribution of training data .
Computing statistically valid estimates of map accuracy indices is clearly a key component of re-
porting overall geospatial ML model performance. It is often important to understand how accuracy
and uncertainty in predictions vary across sub-populations Dr1,Dr2...⊂D (such as administrative
regions or climate zones (Meyer & Pebesma, 2022)). If local accuracy indexesA(Dr1),A(Dr2)...
are low in certain sub-regions, this could expose concerns about fairness or model applicability.
2.2 M ODEL PERFORMANCE EXTENDS BEYOND MAP ACCURACY
Increasingly, geospatial ML models are designed with the goal of being used outside of the regions
where training labels are available. Models trained with globally available remotely sensed data
might be used to “ﬁll in” spatial gaps common to other data modalities (§3.2). The goals of spatial
generalization ,spatial extrapolation orspatial domain adaption can take different forms: e.g.
applying a model trained with data from one region to a wholly new region, or using data from a few
clusters or subregions to extend predictions across the entire region. When spatial generalizability
is desired, performance should be assessed speciﬁcally with respect to this goal (§4).
While spatial generalization is a key component of performance for many geospatial models, it too
is just one facet of geospatial model performance. Proposed uses of geospatial ML models and
their outputs include estimation of natural or causal parameters (Proctor et al., 2023), and reducing
autocorrelation of prediction residuals in-sample (Song & Kim, 2022). Other important facets of
geospatial ML performance are model interpretability (Brenning, 2022) and usability, including the
resources required to train, deploy and maintain models (Rolf et al., 2021).
2.3 C ONTRASTING PERSPECTIVES ON PERFORMANCE ASSESSMENT
The differences between estimating map accuracy as a population parameter (§2.1) and assessing a
model’s performance in the conditions it is most likely to be used (§2.2) are central to one of the
discrepancies introduced in §1. Meyer et al. (2019); Ploton et al. (2020); Meyer & Pebesma (2022)
state concerns in light of numerous ecological studies applying non-spatial validation techniques
with the explicit purpose of spatial generalization. They rightly caution that when data exhibit spa-
tial correlation (§3.1), non-spatial validation methods will almost certainly over-estimate predictive
performance in these use cases. Wadoux et al. (2021), in turn, argue that performance metrics from
spatial validation methods will not necessarily tell you anything about Aas a population parameter.
A second discrepancy between these two perspectives hinges on what data is assumed to be avail-
able (or collectable). While there are some major instances of probability samples being collected
for evaluation of global-scale maps (Boschetti et al., 2016; Stehman et al., 2021), this is far from
standard standard in geospatial ML studies (Maxwell et al., 2021). More often, datasets are created
“by merging all data available from different sources” (Meyer & Pebesma, 2022). Whatever the
intended use of a geospatial model, the availability of and structures within geopsatial and remotely
sensed data must be contended with in order to reliably evaluate any sort of performance.
2
Accepted as a paper at ICLR 2023 Workshop on Machine Learning for Remote Sensing
3 S TRUCTURES AND PATTERNS IN SPATIAL AND REMOTELY SENSED DATA
Geospatial and remotely sensed data exhibit distinct structures. For example, the chosen extent and
scale of a spatial prediction unit ( ℓin §2) has important implications for the design, use, and evalua-
tion of geospatial ML models, evidenced by the phenomena of “modiﬁable areal unit problem” and
“ecological fallacy” (Haining, 2009; Nikparvar & Thill, 2021; Yuan & McKee, 2022). Here, I focus
on two key factors of geospatial data that affect the validity of geospatial ML evaluation methods.
3.1 S PATIAL STRUCTURES : (A UTO)CORRELATION AND COVARIATE SHIFT
One key phenomena exhibited by many geopsatial data is that values of a variable (e.g. tree canopy
height) are often correlated across locations. Formally, for random process Z, thespatial autocor-
relation function is deﬁned as RZZ(ℓi,ℓj) =E[Z(ℓi)Z(ℓj)]/σiσj, whereσi,σjare the standard
deviations associated with Z(ℓi),Z(ℓj). For geospatial variables, we might expect RZZ(ℓi,ℓj)>0
whenℓiandℓjare closer together, namely that values of Zat nearby points tend to be closer in
value. The degree of spatial autocorrelation in data can be assessed with statistics such as Moran’s
Iand Geary’s C, and semi-variogram analsyes (see, e.g. (Gaetan & Guyon, 2010)).
Spatial autocorrelations and correlations between predictor and label variables can be an important
source of structure to leverage in geospatial ML models (Rolf et al., 2020; Klemmer & Neill, 2021),
yet they also present challenges. Models can “over-rely” on spatial correlations in the data, leading
to over-estimated accuracy despite poor spatial generalization performance. Overﬁtting to spatial
relationships in training data is of particular concern in the when data distributions differ between
training regions and regions of use. Such covariate shifts are common in geospatial data, e.g. across
climate zones, or spectral shifts in satellite imagery (Tuia et al., 2016; Hofﬁmann et al., 2021).
Presence of spatial correlations or domain shift alone do not invalidate assessing map accuracy with
a probability sample (§2.1). However, when evaluation is limited to existing data, issues of data
availability and representivity can amplify the challenges of geospatial model evaluation.
3.2 A VAILABILITY ,QUALITY ,AND REPRESENTIVITY OF GEOSPATIAL EVALUATION DATA
Many geospatial datasets exhibit gaps in coverage or quality of data. Meyer & Pebesma (2022) evi-
dence trends of geographic clustering around research sites primarily in a small number of countries,
across three datasets used for global mapping in ecology. Oliver et al. (2021) ﬁnd geographical bias
in coverage of species distribution data aggregated from ﬁeld observation, sensors measurements,
and citizen science efforts. Burke et al. (2021) note that the frequency at which nationally represen-
tative data on agriculture, population, and economic factors are collected varies widely across the
globe. While earth observation data such as satellite imagery have comparatively much higher cov-
erage across time and space (Burke et al., 2021), coverage of commercial products has been shown
to be biased toward regions of high commercial value (Dowman & Reuter, 2017).
Filling in data gaps is goal for which geospatial ML can be transformative (§2.2), yet these same
gaps complicate model evaluation. When training data are clustered in small regions, this can affect
our ability to train a high-performing model. When evaluation data are clustered in small regions,
this affects our ability to evaluate geospatial ML model performance at all.
4 S PATIALLY -AWARE EVALUATION METHODS : A BRIEF OVERVIEW
In §3, we established that geospatial data generally exhibit spatial correlations and data gaps, even
when target use areas Dare small. It is well documented that calculating accuracy indices with
non-spatial validation methods (e.g. standard k-fold cross-validation) will generally over-estimate
performance in such settings. Spatially-aware evaluation methods can control the spatial distribution
of training and validation set points to better simulate conditions of intended model use.
4.1 S PATIAL CROSS -VALIDATION METHODS
Several spatial cross-validation methods have been proposed that reduce spatial dependencies be-
tween train set points ℓ∈S trainfrom evaluation set points ℓ∈S eval. Spatial cross-validation methods
3
Accepted as a paper at ICLR 2023 Workshop on Machine Learning for Remote Sensing
typically stratify training and evaluation instances by larger geographies (Roberts et al., 2017; Valavi
et al., 2018) e.g. existing boundaries, spatial blocks, or automatically generated clusters. Buffered
cross-validation methods (such as spatial leave-one-out (Le Rest et al., 2014), leave-pair out (Airola
et al., 2019) and k-fold cross validation (Pohjankukka et al., 2017)) control the minimum distance
from any training point to any evaluation point. In addition to evaluating model performance, spa-
tial cross-validation has also been suggested as a way to to improve model selection and parameter
estimation in geospatial ML (Meyer et al., 2019; Schratz et al., 2019; Roberts et al., 2017).
While separating ℓ∈S trainfromℓ∈S evalcan reduce the amount of correlation between training and
evaluation data, a spatial split also induces a higher degree of spatial extrapolation to the learning
setup and potentially reduces variation in the evaluation set labels. As a result, it is possible for
spatial validation methods to systematically under-report performance, especially in interpolation
regimes (Roberts et al., 2017; Wadoux et al., 2021). In a different ﬂavor from the evaluation methods
above, Mil `a et al. (2022) propose to match the distribution of nearest neighbor distances between
train and evaluation set points to the corresponding distances between train set and target use area.
4.2 O THER SPATIALLY -AWARE VALIDATION EVALUATION METHODS
When the intended use of a geospatial model is to generate predictions outside the training distribu-
tion, it is critical to test the model’s ability to generalize across different conditions. For example,
studies have varied the amount of spatial extrapolation required by changing parameters of the spa-
tial validation setups in §4.1 , e.g. with buffered leave one out (Ploton et al., 2020; Brenning, 2022)
and checkerboard designs (Roberts et al., 2017; Rolf et al., 2021). Jean et al. (2016) assess extrapola-
tion ability across pairs of countries by iteratitvely training in one region and evaluating performance
in another. Rolf et al. (2021) ﬁnd that the distances at which a geospatial model has extrapolation
power can differ substantially depending on the properties of the prediction variable.
It is always critical to put the reported performance of geospatial ML models in context. Visualizing
the spatial distributions of predictions and error residuals can help expose overreliance on spatially
correlated predictors (Meyer et al., 2019) and sub-regions with low local model performance. Com-
paring performance to that of a baseline model built entirely on spatial predictors can contextualize
the value-add of a new geospatial model (Fourcade et al., 2018; Rolf et al., 2021).
5 T AKING STOCK : CONSIDERATIONS AND OPPORTUNITIES
Comprehensive reporting of performance is critical for geospatial ML methods, especially as stated
gains in research progress make their way to maps and decisions of real-world consequence. Eval-
uating performance of geospatial models is especially challenging in the face of spatial correlations
and limited availability or representivity of data. This means non-spatial data splits are generally
unsuitable for geospatial model evaluation with most existing datasets. Spatially-aware validation
methods are an important indicator of model performance including spatial generalization; however,
they generally do not provide valid statistical estimates of prediction map accuracy. This brings us
to end with three key opportunities for improving the landscape of geospatial ML evaluation:
Opportunity 1: Invest in evaluation data to measure map accuracy and overall performance of
geospatial models. When remote annotations are appropriate, labeling tools (e.g. Robinson et al.
(2022)) can facilitate the creation of probability-sampled evaluation datasets. Data collection and
aggregation efforts can focus on ﬁlling existing geospatial data gaps (Paliyam et al., 2021) or simu-
lating real-world prediction conditions like covariate or domain shift (Koh et al., 2021).
Opportunity 2: Invest in evaluation frameworks to precisely and transparently and report perfor-
mance and valid uses of a geospatial ML model ( `a la “model cards” (Mitchell et al., 2019)). This
includes improving spatial baselines, expanding methods for reporting uncertainty over space, and
delineating “areas of applicability” for geospatial models, e.g. as in Meyer & Pebesma (2022).
Opportunity 3: If the available data and evaluation frameworks are insufﬁcient, explain the limita-
tions of what types of performance can be evaluated . Distinguish between performance measures
that estimate a statistical parameter and those that indicate potential skill for a possible use case.
4
Accepted as a paper at ICLR 2023 Workshop on Machine Learning for Remote Sensing
ACKNOWLEDGMENTS
Esther Rolf was supported by the Harvard Data Science Initiative (HDSI) and the Center for Re-
search on Computation and Society (CRCS). Thank you to Konstantin Klemmer, Caleb Robinson,
and Jessie Finocchiaro for feedback on earlier drafts of this work.
REFERENCES
Antti Airola, Jonne Pohjankukka, Johanna Torppa, Maarit Middleton, Vesa Nyk ¨anen, Jukka Heikko-
nen, and Tapio Pahikkala. The spatial leave-pair-out cross-validation method for reliable AUC
estimation of spatial classiﬁers. Data Mining and Knowledge Discovery , 33(3):730–747, 2019.
Luigi Boschetti, Stephen V Stehman, and David P Roy. A stratiﬁed random sampling design in
space and time for regional to global scale burned area product validation. Remote sensing of
environment , 186:465–478, 2016.
Alexander Brenning. Spatial machine-learning model diagnostics: A model-agnostic distance-based
approach. International Journal of Geographical Information Science , pp. 1–23, 2022.
Dick J Brus. Statistical approaches for spatial sample survey: Persistent misconceptions and new
developments. European Journal of Soil Science , 72(2):686–703, 2021.
Marshall Burke, Anne Driscoll, David B Lobell, and Stefano Ermon. Using satellite imagery to
understand and promote sustainable development. Science , 371(6535):eabe8628, 2021.
Guanghua Chi, Han Fang, Sourav Chatterjee, and Joshua E Blumenstock. Microestimates of wealth
for all low-and middle-income countries. Proceedings of the National Academy of Sciences , 119
(3):e2113658119, 2022.
Ian Dowman and Hannes I Reuter. Global geospatial data from earth observation: Status and issues.
International Journal of Digital Earth , 10(4):328–341, 2017.
Wala Draidi Areed, Aiden Price, Kathryn Arnett, and Kerrie Mengersen. Spatial statistical machine
learning models to assess the relationship between development vulnerabilities and educational
factors in children in queensland, australia. BMC Public Health , 22(1):1–12, 2022.
Yoan Fourcade, Aur ´elien G Besnard, and Jean Secondi. Paintings predict the distribution of species,
or the challenge of selecting environmental predictors and evaluation statistics. Global Ecology
and Biogeography , 27(2):245–256, 2018.
Carlo Gaetan and Xavier Guyon. Spatial statistics and modeling , volume 90. Springer, 2010.
Robert Haining. The special nature of spatial data. The SAGE Handbook of Spatial Analysis. Los
Angeles: SAGE Publications , pp. 5–23, 2009.
J´ulio Hofﬁmann, Maciel Zortea, Breno De Carvalho, and Bianca Zadrozny. Geostatistical learning:
Challenges and opportunities. Frontiers in Applied Mathematics and Statistics , 7:689393, 2021.
Neal Jean, Marshall Burke, Michael Xie, W Matthew Davis, David B Lobell, and Stefano Ermon.
Combining satellite imagery and machine learning to predict poverty. Science , 353(6301):790–
794, 2016.
Konstantin Klemmer and Daniel B Neill. Auxiliary-task learning for geographic data with au-
toregressive embeddings. In Proceedings of the 29th International Conference on Advances in
Geographic Information Systems , pp. 141–144, 2021.
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Bal-
subramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A
benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning ,
pp. 5637–5664. PMLR, 2021.
K´evin Le Rest, David Pinaud, Pascal Monestiez, Jo ¨el Chadoeuf, and Vincent Bretagnolle. Spatial
leave-one-out cross-validation for variable selection in the presence of spatial autocorrelation.
Global ecology and biogeography , 23(7):811–820, 2014.
5
Accepted as a paper at ICLR 2023 Workshop on Machine Learning for Remote Sensing
Sharon L Lohr. Sampling: design and analysis . Chapman and Hall/CRC, 2021.
Aaron E Maxwell, Timothy A Warner, and Luis Andr ´es Guill ´en. Accuracy assessment in convo-
lutional neural network-based deep learning remote sensing studies—part 1: Literature review.
Remote Sensing , 13(13):2450, 2021.
Hanna Meyer and Edzer Pebesma. Machine learning-based global maps of ecological variables and
the challenge of assessing them. Nature Communications , 13(1):1–4, 2022.
Hanna Meyer, Christoph Reudenbach, Stephan W ¨ollauer, and Thomas Nauss. Importance of spatial
predictor variable selection in machine learning applications–moving from data reproduction to
spatial prediction. Ecological Modelling , 411:108815, 2019.
Carles Mil `a, Jorge Mateu, Edzer Pebesma, and Hanna Meyer. Nearest neighbour distance matching
leave-one-out cross-validation for map validation. Methods in Ecology and Evolution , 2022.
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson,
Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In
Proceedings of the conference on fairness, accountability, and transparency , pp. 220–229, 2019.
Catherine Nakalembe. Characterizing agricultural drought in the Karamoja subregion of Uganda
with meteorological and satellite-based indices. Natural Hazards , 91(3):837–862, 2018.
Behnam Nikparvar and Jean-Claude Thill. Machine learning of spatial data. ISPRS International
Journal of Geo-Information , 10(9):600, 2021.
Kristine Nilsen, Natalia Tejedor-Garavito, Douglas R Leasure, C Edson Utazi, Corrine W Ruk-
tanonchai, Adelle S Wigley, Claire A Dooley, Zoe Matthews, and Andrew J Tatem. A review of
geospatial methods for population estimation and their use in constructing reproductive, mater-
nal, newborn, child and adolescent health service indicators. BMC health services research , 21
(1):1–10, 2021.
Ruth Y Oliver, Carsten Meyer, Ajay Ranipeta, Kevin Winner, and Walter Jetz. Global and national
trends, gaps, and opportunities in documenting and monitoring species distributions. PLoS Biol-
ogy, 19(8):e3001336, 2021.
Madhava Paliyam, Catherine Nakalembe, Kevin Liu, Richard Nyiawung, and Hannah Kerner.
Street2sat: A machine learning pipeline for generating ground-truth geo-referenced labeled
datasets from street-level images. In Tackling Climate Change with Machine Learning Workshop
at the International Conference on Machine Learning , 2021.
Pierre Ploton, Fr ´ed´eric Mortier, Maxime R ´ejou-M ´echain, Nicolas Barbier, Nicolas Picard, Vivien
Rossi, Carsten Dormann, Guillaume Cornu, Ga ¨elle Viennois, Nicolas Bayol, et al. Spatial val-
idation reveals poor predictive performance of large-scale ecological mapping models. Nature
communications , 11(1):1–11, 2020.
Jonne Pohjankukka, Tapio Pahikkala, Paavo Nevalainen, and Jukka Heikkonen. Estimating the
prediction performance of spatial models via spatial k-fold cross validation. International Journal
of Geographical Information Science , 31(10):2001–2019, 2017.
Jonathan Proctor, Tamma Carleton, and Sandy Sum. Parameter recovery using remotely sensed
variables. Working Paper 30861, National Bureau of Economic Research, January 2023. URL
http://www.nber.org/papers/w30861 .
David R Roberts, V olker Bahn, Simone Ciuti, Mark S Boyce, Jane Elith, Gurutzeta Guillera-Arroita,
Severin Hauenstein, Jos ´e J Lahoz-Monfort, Boris Schr ¨oder, Wilfried Thuiller, et al. Cross-
validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure. Ecog-
raphy , 40(8):913–929, 2017.
Caleb Robinson, Anthony Ortiz, Hogeun Park, Nancy Lozano, Jon Kher Kaw, Tina Sederholm,
Rahul Dodhia, and Juan M Lavista Ferres. Fast building segmentation from satellite imagery and
few local labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 1463–1471, 2022.
6
Accepted as a paper at ICLR 2023 Workshop on Machine Learning for Remote Sensing
Esther Rolf, Michael I Jordan, and Benjamin Recht. Post-estimation smoothing: A simple baseline
for learning with side information. In International Conference on Artiﬁcial Intelligence and
Statistics , pp. 1759–1769. PMLR, 2020.
Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara,
Benjamin Recht, and Solomon Hsiang. A generalizable and accessible approach to machine
learning with global satellite imagery. Nature communications , 12(1):1–11, 2021.
Patrick Schratz, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and Alexander Brenning. Hy-
perparameter tuning and performance assessment of statistical and machine-learning algorithms
using spatial data. Ecological Modelling , 406:109–120, 2019.
Helen R Sofaer, Catherine S Jarnevich, Ian S Pearse, Regan L Smyth, Stephanie Auer, Gericke L
Cook, Thomas C Edwards Jr, Gerald F Guala, Timothy G Howard, Jeffrey T Morisette, et al.
Development and delivery of species distribution models to inform decision-making. BioScience ,
69(7):544–557, 2019.
Insang Song and Daehyun Kim. Three common machine learning algorithms neither enhance pre-
diction accuracy nor reduce spatial autocorrelation in residuals: An analysis of twenty-ﬁve so-
cioeconomic data sets. Geographical Analysis , 2022.
Stephen V Stehman, Bruce W Pengra, Josephine A Horton, and Danika F Wellington. Validation of
the us geological survey’s land change monitoring, assessment and projection (lcmap) collection
1.0 annual land cover products 1985–2017. Remote Sensing of Environment , 265:112646, 2021.
Devis Tuia, Claudio Persello, and Lorenzo Bruzzone. Domain adaptation for the classiﬁcation of
remote sensing data: An overview of recent advances. IEEE geoscience and remote sensing
magazine , 4(2):41–57, 2016.
Roozbeh Valavi, Jane Elith, Jos ´e J Lahoz-Monfort, and Gurutzeta Guillera-Arroita. blockcv: An r
package for generating spatially or environmentally separated folds for k-fold cross-validation of
species distribution models. bioRxiv , pp. 357798, 2018.
Alexandre MJ-C Wadoux, Gerard BM Heuvelink, Sytze De Bruin, and Dick J Brus. Spatial cross-
validation is not the right way to evaluate map accuracy. Ecological Modelling , 457:109692,
2021.
May Yuan and Arlo McKee. Embedding scale: New thinking of scale in machine learning and
geographic representation. Journal of Geographical Systems , 24(3):501–524, 2022.
7
Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial
Representation Learning
Colorado J Reed1,2*, Ritwik Gupta1*, Shufan Li1*,
Sarah Brockman3, Christopher Funk3, Brian Clipp3,
Kurt Keutzer1, Salvatore Candido2, Matt Uyttendaele2, Trevor Darrell1
1Berkeley AI Research;2Meta AI, FAIR;3Kitware Inc.
correspondence to ritwikgupta@berkeley.edu
Abstract
Large, pretrained models are commonly finetuned with
imagery that is heavily augmented to mimic different condi-
tions and scales, with the resulting models used for various
tasks with imagery from a range of spatial scales. Such
models overlook scale-specific information in the data for
scale-dependent domains, such as remote sensing. In this
paper, we present Scale-MAE , a pretraining method that ex-
plicitly learns relationships between data at different, known
scales throughout the pretraining process. Scale-MAE pre-
trains a network by masking an input image at a known input
scale, where the area of the Earth covered by the image deter-
mines the scale of the ViT positional encoding, not the image
resolution. Scale-MAE encodes the masked image with a
standard ViT backbone, and then decodes the masked image
through a bandpass filter to reconstruct low/high frequency
images at lower/higher scales. We find that tasking the net-
work with reconstructing both low/high frequency images
leads to robust multiscale representations for remote sensing
imagery. Scale-MAE achieves an average of a 2.4−5.6%
non-parametric kNN classification improvement across eight
remote sensing datasets compared to current state-of-the-art
and obtains a 0.9mIoU to 1.7mIoU improvement on the
SpaceNet building segmentation transfer task for a range of
evaluation scales.
1. Introduction
Remote sensing data is captured from satellites and planes
through a mixture of sensors, processing pipelines, and view-
ing geometries. Depending on the composition and relative
geometry of the sensor to the Earth, each image’s Ground
Sample Distance (GSD - the physical distance between two
*Denotes co-first authorship. Co-first authors will prioritize their names
on their resumes/websites.
Ground Truth Input Image Scale-MAE Vanilla MAE
Correct Incorrect0.3m GSD0.3m GSD
3.0m GSD3.0m GSDFigure 1. Scale-MAE learns better representations for multiscale
tasks compared to vanilla MAE. (Column 1) The top image spans
an area at 0.3m GSD and the bottom image shows the same region
at a coarser GSD. (Columns 2-4) The following columns show
a ground truth building segmentation, Scale-MAE segmentation
from a finetuned UperNet, and segmentation from an analogously
finetuned UperNet from a vanilla MAE, respectively. Scale-MAE
demonstrates better performance across images at both scales. See
the supplementary material for more examples.
adjacent pixels in an image) can vary from 0.3m to 1km, so a
100x100 pixel image could span anywhere from an Olympic-
size swimming pool (900 m2) to almost the entire country of
Jamaica (10,000 km2). The data within each image, and the
corresponding objects and points of interest, can therefore
vary across wide spatial ranges. Data from these multiscale
sensors provide critical and complementary information for
various operational and research applications in areas such
as atmospheric, hydrologic, agricultural, and environmental
monitoring [45, 52].
Few modern computer vision methods have explicitly ad-
dressed multiscale remote sensing imagery [35]. Neverthe-
less, the remote sensing vision community has increasingly
used large, pretrained models [13, 20], where such appli-
cations finetune a pretrained model for a single source ofarXiv:2212.14532v4  [cs.CV]  22 Sep 2023
Patchify + Mask
Resampled I
224px,  .7m GSDResampled I
224px,  .7m GSD
Ground Truth Image Ihr
(448px, .3m GSD)resampleMAE Encoder
Transformers
14 14GSDPE
L1 loss
448224
Scale-MAE Decoder
L2 loss
Loss
Upsampling
Reconstruction
28
56
Demask
High freq.
Ground Truth
LB
Low freq.
Ground TruthDeconv
DeconvLB
Ground Truth Image I hr
(448px, .3m GSD)
DecodingFigure 2. Scale-MAE employs the Masked Autoencoder framework. An input image is patchified and masked before being passed into an
MAE encoder. A Ground Sample Distance Positional Encoding (GSDPE) is added to the encoder input, which scales the positional encodings
to the area of ground covered. The Scale-MAE decoders has three stages: (1) Decoding, which uses a smaller number of transformer layers
than MAE to decode the encoded values (2) Upsampling, which progressively deconvolves the decoded feature map to a larger size before
being passed through the Laplacian Blocks (abbreviated LB, see Section 3), (3) Reconstruction, which then reconstructs low and high
frequency features at different scales. These outputs are used to compute an aggregate loss with ground truth low and high frequency features,
where following super resolution literature [2], an L1 loss is used for high frequency output to better reconstruct edges and an L2 loss is used
for low frequency output to better reconstruct average values.
data at a specific scale [13, 20, 22, 32, 41]. In this paper we
present Scale-MAE , a masked reconstruction model that ex-
plicitly learns relationships between data at different, known
scales throughout the pretraining process. By leveraging this
information, Scale-MAE produces a pretrained model that
performs better across a wide range of GSDs and tasks.
Masked Autoencoders [26] offer self-supervised learn-
ing without explicit augmentations. A standard Masked
Autoencoder resizes/crops an image, masks the majority of
the transformed image, and then tasks a Vision Transformer
(ViT) based autoencoder with embedding the unmasked com-
ponents. A decoding ViT then decodes the full image from
these learned embeddings, where the decoder is later dis-
carded and the encoder is used to produce representations
for an unmasked input image.
Existing MAE-based pretraining approaches fail to gen-
eralize across domains with images at multiple scales.
Scale-MAE (Figure 1) overcomes this through a GSD-based
positional encoding derived from the land area covered in the
image. This informs the ViT of both the position and scale of
the input image. Scale-MAE also uses a Laplacian-pyramid
decoder to encourage the network to learn multiscale rep-
resentations. The embeddings are decoded to two images
containing low and residual high frequency information, re-
spectively – see Figure 2. As we discuss in Section 3, this
structure allows the ViT decoder to use fewer parameters
than MAE while still producing strong representations across
multiple scales.
We show that Scale-MAE leads to better performing,
more robust multiscale representations than both a stan-
dard MAE and a recently proposed, state-of-the-art MAEs
SatMAE [13] and ConvMAE [21] across remote sensing
datasets with a variety of scale and resolution characteristics.
To the best of our knowledge Scale-MAE is the first self-supervised MAE to include scale-aware positional encoding
and Laplacian pyramids. In our experiments, Scale-MAE
achieves an average of a 5.6%nonparametric kNN classifica-
tion improvement across eight remote sensing datasets com-
pared to current state-of-the-art in addition to a 0.9mIoU
to1.7mIoU improvement on the SpaceNet building seg-
mentation transfer task for a range of evaluation scales (see
Figure 1).
2. Related Work
Representation learning and the Masked Autoencoder.
Representation learning aims to extract meaningful, intrin-
sic features from data for downstream use [5]. In prac-
tice, this often entails pretraining a deep network so that
a lightweight learning routine can then finetune it for a par-
ticular downstream task, see [15,16,17,24,27,30,37,49,66].
The Masked Autoencoder (MAE) is a recent state-of-the-art
self-supervised representation learning method in computer
vision that pretrains a ViT encoder by masking an image,
feeding the unmasked portion into a transformer-based en-
coder, and then tasking the decoder with reconstructing the
input image [26]. MAEs fail to leverage scale information in
scale-dependent domains as they are often reliant on absolute
or relative positional encodings. To the best of our knowl-
edge, Scale-MAE is the first MAE-based self-supervised
learning method to incorporate a scale-variant positional
encoding.
Remote Sensing Representation Learning Neumann et
al. [46] were one of the first to exhaustively share results on
existing representation learning and semi-supervised learn-
ing techniques for remote sensing imagery. Gao et al. [22]
demonstrated the effectiveness of MAE pretraining for re-
mote sensing image classification. Ayush et al. [3] lever-
aged the metadata from remote sensing images via spatially
aligned but temporally separated images as positive pairs
for contrastive learning and predicted the latitude and longi-
tude as pretext tasks. Gupta et al. [25] demonstrated the use
of MAEs as a pretraining approach for passive and active
remote sensing imagery. Their method introduced flexible
“adapters” which could be used interchangeably with an en-
coder for a set of input imagery modes. Cong et al. [13]
introduced the SatMAE, which used temporal and spectral
metadata in a positional encoding to encode spatio-temporal
relationships in data. The temporal data contains the year,
month, and hour enabling understanding of long-term change
with the year, weather information from the month, and hour
information for the time of day. Further Liu et al. [41] and
Iba˜nezet al. [32] have shown that MAE architectures can
be used for band selection in hyperspectral remote sensing
images, significantly reducing data redundancy while main-
taining high classification accuracy. Scale-MAE leverages
inherent absolute scale information information present in
scale-dependent domains as a way to learn robust, multiscale
features that reduce data usage downstream.
Super-resolution Super-resolution has proven effective in
improving accuracy within remote sensing images due to
the extremely small size of objects within the image [51].
Previous works have aimed to learn continuous implicit rep-
resentations for images at arbitrary resolutions to aid the
super-resolution task. These representations are used to
upsample the images either to specific scales [38] or to ar-
bitrary resolutions [10, 31, 61]. Most super-resolution work
aims to increase the resolution of the input image, whereas
Scale-MAE produces both higher and lower resolution im-
ages. There is some work on super-resolution for satellite
imagery, but much of this work is focused on synthetically
creating high-resolution datasets for use with models trained
specifically for high-resolution data [28, 35]. Scale-MAE ,
however, utilizes super-resolution as a means to obtain mul-
tiscale representations during pretraining.
Multiscale Features Because images can contain objects
of many different pixel resolutions, the vision community has
proposed many methods to extract multiscale features. These
include spatial pyramids [6, 34, 36, 50] and dense sampling
of windows [33, 62, 63]. These approaches have been com-
bined by methods such as [19], in which dense histogram-
of-gradient features are computed for each feature pyramid
level. Rather than using classical computer vision techniques
to extract multiscale features, convolutional neural networks
have been used to build deep multiscale features. CNNs
with subsampling layers inherently build feature pyramids, a
property exploited explicitly by models such as the Feature
Pyramid Network and the Single-Shot Detector, amongstothers [23, 39, 40]. Recently, this multiscale idea has been
extended to vision transformers by [18], who show that this
architecture improves various video recognition and image
classification tasks, as well as in [21, 67] which proposes
various hybrid CNN-MAE architectures that yield multi-
scale features during MAE pretraining. Different from these
works, Scale-MAE uses a Laplacian pyramid decoder as a
way to force an encoder to learn multiscale features with the
ViT architecture.
3. Scale-MAE
This section describes the Scale-MAE pretraining frame-
work as illustrated in Figure 2. Scale-MAE is a self-
supervised pretraining framework based on the Masked Au-
toencoder (MAE) [26]. Scale-MAE makes two contribu-
tions to the MAE framework. Standard MAE-based methods
use absolute or relative positional encodings to inform the
ViT of the position of the unmasked components, where
an image at resolution rwill have the same positional en-
codings regardless of the image content. Scale-MAE in-
troduces the Ground Sample Distance (GSD) based posi-
tional encoding that scales in proportion to the area of land
in an image, regardless of the resolution of the image. In
addition, Scale-MAE introduces the Laplacian-pyramid de-
coder to the MAE framework to encourage the network to
learn multiscale representations. Embeddings from a ViT
encoder are decoded to a lower resolution image that cap-
tures the lower frequency information and a higher resolu-
tion image that captures the high-frequency information. We
formalize Scale-MAE in the following subsections by first
specifying the necessary MAE background, describing the
GSD-based positional encoding, and then explaining the
Laplacian-pyramid decoder.
Setup LetI∈RH×W×Crepresent an input image of
height H, width W, and Cchannels. The MAE patchifies
Iinto a sequence Sof independent patches of height and
width Ppixels, where each of the Nppatches, s∈Shas
dimension s∈RP2C. A fraction, m, of the patches are
then removed and the remaining patches are then passed
through a projection function (e.g., a linear layer) to project
the patches SintoDdimensions, fE:RP2C→RD, to
obtain embedded patches SE=fE(S). AnR2positional
encoding vector, is then added to the embedded patches with
vx(pos,2i) = sinpos
100002i
D(1)
vy(pos,2i+ 1) = cospos
100002i
D(2)
where posis the position of the patch along the given axis
andiis the feature index (visualized in Figure 3), exactly
as introduced in [54]. These positional encodings are then
concatenated and added to the embedded patches, which
are then fed into a ViT encoder. After the encoder, the
removed mpatches are then placed back into their original
location in the sequence of patches where a learned mask
token represents the masked patches that were not encoded.
Another positional encoding vector is added to all patches
and a sequence of transformer blocks decodes these patches
to form the original input image, which is used as the learning
target.
Input Scale-MAE performs a super resolution reconstruc-
tion, where the input image Iis downsampled from a higher
resolution image Ihrat the ground truth GSD. Instead of
targeting the input image, Scale-MAE targets high frequency
and low frequency components of Ihr, which is common in
Laplacian pyramid super resolution models [64], where the
high frequency component is at the same resolution as the
ground truth image Ihrand the low frequency component
is at the same resolution as the input image I, as shown in
Figure 2. Following many works in super resolution [64], the
low frequency target image is obtained by interpolating Ihr
to a much lower resolution, rlowand then interpolating to the
same resolution as the input image I. The high frequency tar-
get image is obtained by downsampling Ihrto another lower
resolution rhigh-low , and then upsampling to the same resolu-
tion as the ground truth image Ihrand subtracting this image
Ihf=Ihr−Ihigh-low . The supplementary material provide
more information on the upsampling/downsampling method-
ology. The key components for Scale-MAE are described
next.
GSD Positional Encoding Images from scale-dependent
domains have a metric which defines the absolute scale for
the image. This metric has different names across domains
and is referred to as the Ground Sample Distance (GSD) in
remote sensing. The GSD is critical to understanding, con-
ceptually, the kinds of features that will be available in an
image. An image with finer GSD (lower number) will have
higher frequency details than an image with coarser GSD
(high number). Models are generally unaware of absolute
scale when learning over a set of data. Specifically, even if
they implicitly learn that all images in a dataset share a vary-
ing resolution from input-space augmentations, then these
models do not explicitly condition on the GSDs encountered
in unseen data.
We extend the positional encoding from Equation (2) to
include GSD by scaling the positional encoding relative to
the land area covered in an image as depicted in Figure 3
and mathematically:
vgsd,x(pos,2i) = sing
Gpos
100002i
D(3)
vgsd,y(pos,2i+ 1) = cosg
Gpos
100002i
D(4)
Figure 3. Ground Sample Distance Positional Encoding (GS-
DPE). (Left) Input images at the same pixel resolution but different
GSDs are shown. The image on the bottom is a subset of the image
on the top. (Center) This overlap in location, albeit at a different
resolution, is reflected in the GSDPE. The finer image with smaller
spatial extent is represented by a corresponding subsection of the
overall sine wave on the bottom. (Right) A standard positional
encoding is strictly dependent on the image resolution and uses the
same embedding for both. The colors behind the sine waves show
the intensity and quantization of the encoding.
where gis the GSD of the image and Gis a reference GSD,
nominally set to 1m. Intuitively, an object imaged at a finer
resolution has more pixels representing it. When imaging the
same object at a coarser resolution, those pixels must map to
fewer pixels. In Equation (4), we interpolate the positional
encoding by a factor ofG
gto account for the ordering of the
coarser set of pixels. This simple idea underpins the GSD
Positional Encoding, visualized in Figure 3.
Scale-MAE decoder The standard MAE learns represen-
tations by tasking a network with reconstructing an image
after masking out most of its pixels. While the standard
MAE decoder reconstructs the input image at the same scale
as its input, the objective of Scale-MAE is to learn multi-
scale representations. We draw on works from progressive
super-resolution such as [56], that learn a high resolution,
high frequency image and a lower resolution low frequency
image, that when combined together, yield the input image
at a higher resolution.
The Scale-MAE introduces a novel decoder which de-
codes to multiple scales with a progressive Laplacian de-
coder architecture, replacing the traditional MAE “decoder”,
which is really a Transfomer encoder. This architecture
consists of three stages: decoding, upsampling, and recon-
struction, which are shown in Figure 2 and detailed below.
Decoding follows the standard MAE decoder where fol-
lowing the encoder, the removed mpatches are then placed
back into their original location in the sequence of patches
where a learned mask token represents the masked patches
that were not encoded, a positional encoding is added, and
then a series of transformer layers decode all patches. In
contrast to the standard MAE decoder, the Scale-MAE de-
coder uses fewer transformer layers (e.g. 3 layers instead of
8), which reduces the parameter complexity as quantified
in Section 5. The output of these layers is then fed into the
upsampling stage.
Upsampling The latent feature maps from the decoding
stage are progressively upsampled to 2x and 4x resolution
using deconvolution blocks, where the first deconvolution
is 2x2 with stride 2 that outputs a feature map at 2x the in-
put resolution (28 in Figure 2), followed by a LayerNorm
and GELU, and then another 2x2 deconvolution layer that
outputs a feature maps at 2x the previous resolution (56 in
Figure 2). See the supplementary material for a full architec-
tural diagram.
Reconstruction After having been upsampled, the lower
resolution and higher resolution feature maps are passed into
Laplacian Blocks (LBs in Figure 2) that reconstruct high
and low resolution images for the high and low frequency
reconstruction, respectively. Architecturally, the Laplacian
Blocks consist of a sequence of three sub-blocks: a Lapla-
cian Feature Mapping Block, a Laplacian Upsample Block,
and a Laplacian Pyramid Reconstruction Block. The Feature
Mapping Block is used to project features within a particular
layer of the Laplacian Pyramid back to the RGB space. The
Laplacian Upsample Block represents a learnable upsam-
ple function that maps latent features from one layer of the
Laplacian Pyramid to a higher level. Finally, the Laplacian
Pyramid Reconstruction Block is used to reconstruct infor-
mation at the different frequencies in RGB space. Following
super resolution literature [2], an L1 loss is used for high
frequency output to better reconstruct edges and an L2 loss
is used for low frequency output to better reconstruct aver-
age values. The supplementary material has architectural
diagrams for each block.
4. Experiments
We investigate the quality of representations learned from
Scale-MAE pretraining through a set of experiments that
explore their robustness to scale as well as their transfer
performance to additional tasks. First, we present our main
experiments in Section 4.1 and compare with SatMAE [13],
a current state-of-the-art MAE for remote sensing imagery,
Input Image Mask Low Frequency High Frequency ReconstructionFigure 4. Scale-MAE reconstruction. Examples from Functional
Map of the World are shown. From left to right, an input image
at 224x224 resolution is shown. Its corresponding mask is visual-
ized as well. Columns 3 and 4 show the low and high frequency
produced by the Scale-MAE decoder. The last column is the re-
construction obtained from summing the low and high frequency
features together.
ConvMAE [21], a state-of-the-art multiscale MAE, as well
as several other approaches detailed throughout. The exact
implementation of Scale-MAE for the main experiments was
determined through a set of ablation experiments presented
in Section 4.2.
We pretrain a ViT-Large model with Scale-MAE using the
Functional Map of the World (FMoW) [12] RGB training set,
which consists of 363.6k images of varying image resolution
and GSD, for 800 epochs. The initial higher resolution image
Ihris taken as a random 448px2crop of the input image, and
the input image Iis then a downsampled 224px2fromIhr.
The low frequency groundtruth is obtained by downscaling
Ihrto 14px2and then upscaling to 224px2, while the high
frequency groundtruth is obtained by downscaling Ihrto
56px2and then upscaling to 448px2and subtracting this
image from Ihr.
Figure 4 shows examples of the masked input, low resolu-
tion/frequency, high resolution/frequency, and combined re-
construction of FMoW images during training. The low res-
olution/frequency images capture color gradients and land-
scapes, while the residual high resolution/frequency images
capture object edges, roads, and building outlines.
4.1. Representation Quality
We evaluate the quality of representations from
Scale-MAE by freezing the encoder and performing a non-
parametric k-nearest-neighbor (kNN) classification with
eight different remote sensing imagery classification datasets
0 25% 50% 75% 100%0.50.60.70.80.91.0KNN acc.
RESISC
Scale-MAE
SatMAE
ConvMAE
0 25% 50% 75% 100%0.50.60.70.80.91.0
Optimal-31
0 25% 50% 75% 100%0.50.60.70.80.91.0
MLRSNet
0 25% 50% 75% 100%0.50.60.70.80.91.0
CV-BrCT
0 25% 50% 75% 100%
Relative GSD0.50.60.70.80.91.0KNN acc.
WHU-RS19
0 25% 50% 75% 100%
Relative GSD0.50.60.70.80.91.0
EuroSAT
0 25% 50% 75% 100%
Relative GSD0.50.60.70.80.91.0
AiRound
0 25% 50% 75% 100%
Relative GSD0.50.60.70.80.91.0
UC MercedFigure 5. Learning better representations at all scales. Scale-MAE (blue) features perform better than state-of-the-art. We evaluate kNN
accuracy on eight datasets with a large variance in GSD. Scale-MAE consistently produces better results at coarser resolutions. In addition
to using evaluation datasets at different GSDs, to further test the multiscale representations, we create multiple test sets for each dataset
in which we downsampled the full resolution validation set to coarser GSDs at fixed percentages: XG%
val, G∈ {12.5,25,50,100}, where
EuroSat does not include the 12.5% because the images are at a resolution of 64px, our patch size is 16px, and an input image of 8px is too
small.
with different GSDs, none of which were encountered dur-
ing pretraining. The kNN classifier operates by encoding
all train and validation instances, where each embedded in-
stance in the validation set computes the cosine distance
with every other embedded instance in the training set. The
instance is classified correctly if the majority of its k-nearest-
neighbors are in the same class as the validation instance,
and incorrectly if they are in any other.
The reasoning behind the kNN classifier evaluation is
that a strong pretrained network will output semantically
grouped representation for unseen data of the same class.
This evaluation for the quality of representations occurs in
other notable works [7, 9, 57]. In addition to using evalua-
tion datasets at different GSDs, to further test the multiscale
representations, we create multiple test sets for each dataset.
Since we cannot synthesize data at a finer GSD than the
provided ground truth, we only downsample the full reso-
lution validation set to coarser GSDs at fixed percentages:
XG%
val, G∈ {12.5,25,50,100}.
Our analysis uses eight different land-use classification
datasets: RESISC-45 [11], the UC Merced Land Use Dataset
[65], AiRound and CV-BrCT [43], MLRSNet [48], EuroSAT
[29], Optimal-31 [55], WHU-RS19 [14], SpaceNet v1 and
v2 [53], and Functional Map of the World [12]. The datasets
used span a wide range of GSDs, e.g., MLRSNet consists of
data captured from aerial platforms with 0.1m GSD, while
RESISC45 has imagery from medium-resolution satellites
at>30m GSD. In some cases, the datasets present imagery
at mixed GSDs which are not specified, in which case we as-
sume an approximate constant GSD: see the supplementary
material for all details. Furthermore, we provide an expanded
set of experiments with linear probing and finetuning in the
supplementary material.Average Accuracy (%)
Dataset Scale-MAE SatMAE ConvMAE
AiRound 63.2 57.8 59.7
CV-BrCT 69.7 66.2 68.4
EuroSAT 86.7 84.4 88.8
MLRSNet 81.7 75.0 79.5
OPTIMAL-31 65.5 55.7 61.7
RESISC 70.0 61.0 67.0
UC Merced 75.0 69.8 70.0
WHU-RS19 79.5 78.5 77.0
Table 1. Scale-MAE performs better, across all GSDs (as in Fig-
ure 5), for all datasets we experimented with compared to SatMAE.
The average improvement across all datasets for Scale-MAE com-
pared to SatMAE is 5.6% and 2.4% compared to ConvMAE with
ViT-Large backbones.
We run kNN classification with k= 20 . Figure 5 shows
thatScale-MAE outperforms SatMAE and ConvMAE across
GSD scales in the different evaluation datasets and across
relative GSD scales within individual datasets. For example,
the UC Merced has a GSD of 0.3m, but evaluating at scales
[12.5%,100%] provides an artificial GSD range of [0.3m,
2.4m]. On this example, we see that Scale-MAE provides
the largest performance gap at the 2.4m GSD, with similar
performance at 0.3m.
Across all other evaluation datasets and wider range of
GSDs, Scale-MAE outperforms SatMAE and ConvMAE,
where Scale-MAE outperforms both methods by a larger gap
as the GSD increasingly varies from the original GSD, indi-
cating that Scale-MAE learns representations that are more
robust to changes in scale for remote sensing imagery. We
outperform SatMAE by an average of 5.6% and ConvMAE
by an average of 2.4% across all resolutions and datasets (see
Table 1). UC Merced at 100% of the true GSD is the only
evaluation where SatMAE outperforms Scale-MAE . The
supplementary material contains an extensive table demon-
strating kNN classification results with varying k.
Linear probing and finetuning We perform linear classi-
fication on the RESISC-45 and FMoW-RGB datasets. We
fine-tune for 50 epochs using the same hyperparameter set-
tings as SatMAE [13]: a base learning rate of 5×10−3, a
weight decay of 5×10−3. We do not use temporal data for
classification. For RESISC-45, we fine-tune for 100 epochs
with a base learning rate of 4×10−3, a weight decay of
5×10−3, and a global batch size of 256 across 2 GPUs. The
learning rate on the backbone is multiplied by a factor of 0.1.
We use RandomResizedCrop for augmentation. We train on
224x224 images and evaluate 256x256 images because we
found evaluating at a higher scale improves the performance
of all models. We report both the performance of end-to-end
fine-tuning and linear probing with a frozen backbone. The
linear probing setup was the same as finetuning except the
learning rate was 0.1. The results are shown in Table 2 and
Table 3.
Model Backbone Frozen/Finetune
Scale-MAE Vit-Large 89.6/95.7
SatMAE [13] Vit-Large 88.3/94.8
ConvMAE [21] ConvVit-Large 81.2/95.0
MAE [26] Vit-Large 88.9/93.3
Table 2. Transfer classification results on RESISC-45. Frozen
indicates a linear probe and finetune is a full end-to-end finetuning
of the entire model.
Model Backbone Top-1/Top-5
Scale-MAE ViT-Large 77.9/94.3
SatMAE †[13] ViT-Large 72.4/91.9
MAE [26] ViT-Large 68.4/90.3
ConvMAE [21] ConvVit-Large 74.1/91.4
SatMAE ∗[13] ViT-Large 77.8/-
GASSL [4] ResNet-50 71.55/-
MoCo-V2 [27] ResNet-50 64.34/-
Table 3. Full finetuning results on FMoW-RGB. †: We repro-
duce SatMAE and ConvMAE by taking their publicly available
codebases and pretraining on FMoW dataset for 800 epochs. The
results differ from their reported results, but are evaluated consis-
tently with ours. * Reports the results from the SatMAE paper [13].
Semantic segmentation transfer We use the SpaceNet v1
building segmentation dataset [53] to evaluate semantic seg-
mentation results on contrastive and MAE-based pretrainingmethods. Prior methods relied on the PSANet [68] segmen-
tation architecture, while Scale-MAE uses the UperNet [58]
segmentation architecture which is more common for ViT
backbones. For even comparison, we test the current state-
of-the-art SatMAE and ConvMAE methods with UperNet
as well. Results are detailed in Table 4.
Method Backbone Model mIoU
Sup. (Scratch) ResNet50 PSANet 75.6
GASSL [3] ResNet50 PSANet 78.5
Sup. (Scratch) ViT-Large PSANet 74.7
SatMAE [13] ViT-Large PSANet 78.1
Sup. (Scratch) ViT-Large UperNet 71.6
Vanilla MAE ViT-Large UperNet 77.9
SatMAE ViT-Large UperNet 78.0
ConvMAE ViT-Large UperNet 77.6
Scale-MAE ViT-Large UperNet 78.9
Table 4. Semantic segmentation results on SpaceNet v1.
Scale-MAE outperforms other methods across backbone and seg-
mentation architectures, where Sup. (Scratch) indicates a super-
vised model trained from scratch (a randomly initialized network).
With the same pretraining settings, Scale-MAE outper-
forms SatMAE by 0.9 mIoU, ConvMAE by 1.3 mIoU, and
a vanilla MAE by 1.0 mIoU. Scale-MAE outperforms all
other prior work, including GASSL [3], which SatMAE did
not outperform on the mean Intersection over Union (mIoU)
metric for semantic segmentation. Particularly, Scale-MAE
increases the gap in performance as the resolution of input
imagery becomes coarser, highlighting the absolute scale-
invariance introduced by our method.
In Figure 6, we compare SpaceNet v1 evaluations across
downscaled images (at 50%, 75%, and 100% of the origi-
nal image size) for Scale-MAE , SatMAE, and ConvMAE.
Similar to the classification results, Scale-MAE maintains
higher semantic segmentation performance over both meth-
ods, even with images at a coarser GSD. In fact, the per-
formance gap grows at coarser GSDs. Compared to the
next-best-performing method at the input GSD, Scale-MAE
is 0.9 mIoU higher, at 75% GSD Scale-MAE is 1.2 mIoU
higher, and at 50% Scale-MAE is 1.7 mIoU higher.
In Table 5, we further evaluate Scale-MAE , SatMAE, and
ConvMAE across SpaceNet v1, SpaceNet v2 [53], INRIA
Aerial Image [44], and GID-15 [59] remote sensing datasets
at native resolution. Scale-MAE outperforms both compara-
ble methods across all benchmarks.
4.2. Ablations
We ablate the key components of the Scale-MAE pretrain-
ing framework. For these experiments, we use a lightweight
pretraining setting, where we pretrain for 300 epochs on
50% 75% 100%
Relative GSD7072747678mIoU
Scale-MAE
SatMAE
ConvMAEFigure 6. SpaceNet v1 evaluation across downscaled images for
both Scale-MAE and SatMAE. Scale-MAE maintains higher se-
mantic segmentation performance over SatMAE, even with images
of coarser GSD.
SN1 SN2 INR. G15
RI SH VE PA KH - -
Conv. 77.6 78.7 82.2 78.3 74.8 82.2 37.4
Sat. 78.0 81.9 86.6 80.3 76.1 83.0 44.3
Scale 78.9 82.2 87.4 81.1 77.1 84.2 46.2
Table 5. mIoU on semantic segmentation tasks. SN1/2 (SpaceNet
v1/2), RI: Rio, SH: Shanghai, VE: Vegas, PA: Paris, KH: Khar-
toum; INR: INRIA; G15: GID-15. Conv., Sat., and Scale. are
ConvMAE, SatMAE, and Scale-MAE.
Method GSDPE KNN 50% KNN 100%
Vanilla MAE 72.8 77.8
Vanilla MAE ! 75.4 78.5
MAE + LP 75.3 79.6
Scale-MAE ! 78.1 80.7
Table 6. Ablation results indicating the importance of GSDPE as
determined by a KNN classification on RESISC-45 at a relative
GSD of 50% and 100% of its native GSD. Using the GSDPE
leads to better performance for both Scale-MAE and the Vanilla
MAE. MAE + LP denotes the vanilla MAE with the addition of
our progressive Laplacian decoder.
FMoW (rather than 800) and use a ViT-Base encoder (rather
than ViT-Large), and evaluate using a kNN evaluation on
RESISC-45 at 100% and 50% of its native GSD. The key
contributions that we ablate are as follows: the GSD posi-
tional encoder in Table 6, in which we find that the GSD
postional encoder benefits both Scale-MAE and Vanilla MAE
across resolutions. In Table 8, we see that the number of
transformer layers can be reduced from 8 to 3 compared to a
Vanilla MAE, which results in a performance improvement.
The standard masking rate of 75% still appears optimal for
Scale-MAE according to the results in Table 7.
In Table 9 we ablate the necessity of the low and high res-
olution reconstructions. Specifically, we test reconstructing
the low resolution image only, the high resolution image, andMask Rate KNN 50% KNN 100%
70% 77.3 79.3
75% 78.1 80.7
80% 78.1 79.9
Table 7. Ablation results indicating that a 75% mask rate is optimal
as determined by a KNN classification on RESISC-45 at a relative
GSD of 50% and 100% of its native GSD.
a combined image (rather than independent low/high recon-
structions). In this case, when the high resolution component
is reconstructed, we do not use the low-resolution residual,
but rather, directly reconstruct the high resolution result. The
“Combined” entry combines the low and high resolution re-
sults instead of treating them as separate learning objectives.
The separate low/high resolution reconstructions obtain the
best performance and robustness to changes in scale.
5. Discussion
In this section, we share observations about Scale-MAE ,
sketch our vision for future work, and discuss high-level
questions about Scale-MAE .
Computational complexity. Scale-MAE requires a much
smaller decoder than vanilla MAE—instead of a decoder
depth of eight, Scale-MAE works well with a depth of three.
In fact, with 322.9M vs 329.5M parameters using ViT-
Large, Scale-MAE is smaller than vanilla MAE. However,
GPU memory usage for equal batch sizes are higher for
Scale-MAE since we reconstruct a higher resolution image
in the Scale-MAE Decoder.
Multi-spectrality and modality. Electro-optical (EO)
satellites, such as the ones comprising the datasets mentioned
in this work, capture light at different wavelengths. Each
wavelength has a different sensor, and each sensor can have a
different resolution. Scale-MAE requires input tensors to be
stacked to pass through the model. This means that we are
unable to use Scale-MAE when the input image’s bands are
all of different GSDs. Additionally, synthetic aperture radar
(SAR) imagery is another form of remote sensing where res-
olution varies across a single band. Extending Scale-MAE
to work with different resolution bands and modalities is
reserved for future work.
Can the Scale-MAE methodology be applied to other
backbones? Methods such as ConvNeXt [42] provide
competitive performance compared to Transformers. The
core components of our work can be integrated, with ad-
ditional work, into different architectures. The Laplacian
Decoder in Scale-MAE can be engineered to ingest convo-
Decoding Layers KNN 50% KNN 100%
1 76.0 78.4
2 77.9 80.4
3 78.1 80.7
4 77.5 80.0
8 77.7 78.9
Table 8. Ablation results indicating that fewer transformer layers in
the decoding stage tend to work better for Scale-MAE as determined
by a KNN classification on RESISC-45 at a relative GSD of 50%
and 100% of its native GSD.
Low Res High Res Combined KNN 50% KNN 100%
! 77.6 80.2
! 72.9 74.3
! 77.2 80.3
! ! 78.1 80.7
Table 9. These ablation results indicate that reconstructing both the
low resolution and high resolution components lead to robust perfor-
mance. Note: when the high resolution component is reconstructed,
the low-resolution residual is not used—the high resolution result is
directly reconstructed. The “Combined” entry merges the low and
high resolution results instead of treating them as separate losses.
The evaluations are a kNN classification ( k=20) on RESISC-45 at
relative GSDs 50% and 100% of its native GSD.
lutional feature maps. Existing work on scale-aware CNNs
can be extended to work with the Laplacian Decoder.
Evaluating on more remote sensing datasets. The field
of remote sensing has had a renaissance in the last five years
with the amount of available datasets. These can be generic,
like Functional Map of the World, to highly specific, such
as identifying illegal airstrips in Brazil [1, 8] or identifying
illegal fishing vessels [47]. In fact, there are so many small,
specific remote sensing datasets that entire review papers
are written to enumerate them [60]. We chose to focus
datasets with properties of remote sensing that are relevant
to multiscale representation learning.
6. Conclusion
Remote sensing imagery has accelerated the rate of scien-
tific discovery in a broad set of disciplines. With increasingly
precise methods to extract environmental indicators using
computer vision methods, automated understanding of re-
motely sensed sources has become a mainstay in scientific
literature. Remote sensing payloads are diverse and capture
data at a wide range of resolutions, a feature heavily utilized
by scientists. Current computer vision methods for remote
sensing necessitate the training of a new model per input
resolution. Not only is the training process expensive, but
the overhead of curating a dataset at multiples scales makesthis a daunting task.
We introduce Scale-MAE , a pretraining framework which
introduces scale invariance into encoders that are used
for a diverse set of downstream tasks. Our insights into
scale-inclusive positional encodings and progressive multi-
frequency feature extraction result in models that perform
significantly better than state-of-the-art pretraining methods
across (1) multiple scales and (2) many benchmarks.
Our goal is to take the extremely diverse and rich source
of information present in remote sensing imagery and make it
simple to use with minimal training iterations required. With
the introduction of Scale-MAE , we hope to further accelerate
the rate at which scientific disciplines create impact.
Acknowledgements
We deeply thank Kyle Michel from Meta for providing us
with his help during our time of need. Satellite imagery and
derived images used in this paper in are from datasets which
redistribute imagery from Google Earth, DigitalGlobe, and
Copernicus Sentinel 2022 data. Trevor Darrell’s group was
supported in part by funding from the Department of Defense
as well as BAIR’s industrial alliance programs. Ritwik Gupta
is supported by the National Science Foundation under Grant
No. DGE-2125913.
References
[1]Manuela Andreoni, Blacki Migliozzi, Pablo Robles, and
Denise Lu. The Illegal Airstrips Bringing Toxic Mining
to Brazil’s Indigenous Land. The New York Times , 2022.
[2]Saeed Anwar, Salman Khan, and Nick Barnes. A deep
journey into super-resolution: A survey. ACM Computing
Surveys (CSUR) , 53(3):1–34, 2020.
[3]Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tan-
may, Marshall Burke, David Lobell, and Stefano Ermon.
Geography-Aware Self-Supervised Learning. In 2021
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 10161–10170, Montreal, QC, Canada, Oct.
2021. IEEE.
[4]Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tan-
may, Marshall Burke, David Lobell, and Stefano Ermon.
Geography-aware self-supervised learning. In Proceedings of
the IEEE/CVF International Conference on Computer Vision ,
pages 10181–10190, 2021.
[5]Yoshua Bengio, Aaron Courville, and Pascal Vincent. Rep-
resentation learning: A review and new perspectives. IEEE
transactions on pattern analysis and machine intelligence ,
35(8):1798–1828, 2013.
[6]P. Burt and E. Adelson. The Laplacian Pyramid as a Com-
pact Image Code. IEEE Transactions on Communications ,
31(4):532–540, Apr. 1983.
[7]Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing Properties in Self-Supervised Vision Transformers. In
2021 IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 9630–9640, Montreal, QC, Canada, Oct.
2021. IEEE.
[8]Fen Chen, Ruilong Ren, Tim Van de V oorde, Wenbo Xu,
Guiyun Zhou, and Yan Zhou. Fast Automatic Airport Detec-
tion in Remote Sensing Images Using Convolutional Neural
Networks. Remote Sensing , 10(3):443, Mar. 2018.
[9]Xinlei Chen and Kaiming He. Exploring Simple Siamese
Representation Learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 15750–15758, 2021.
[10] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning contin-
uous image representation with local implicit image function.
InProceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 8628–8638, 2021.
[11] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sens-
ing Image Scene Classification: Benchmark and State of the
Art. Proceedings of the IEEE , 105(10):1865–1883, Oct.
2017.
[12] Gordon Christie, Neil Fendley, James Wilson, and Ryan
Mukherjee. Functional Map of the World. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 6172–6180, Salt Lake City, UT, June 2018. IEEE.
[13] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu,
Erik Rozi, Yutong He, Marshall Burke, David B. Lobell, and
Stefano Ermon. SatMAE: Pre-training Transformers for
Temporal and Multi-Spectral Satellite Imagery, Oct. 2022.
[14] Dengxin Dai and Wen Yang. Satellite Image Classification via
Two-Layer Sparse Coding With Biased Image Representation.
IEEE Geoscience and Remote Sensing Letters , 8(1):173–176,
Jan. 2011.
[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018.
[16] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman,
Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep
convolutional activation feature for generic visual recognition.
InInternational conference on machine learning , pages 647–
655. PMLR, 2014.
[17] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal
Vincent. Why does unsupervised pre-training help deep
learning? In Proceedings of the thirteenth international
conference on artificial intelligence and statistics , pages 201–
208. JMLR Workshop and Conference Proceedings, 2010.
[18] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,
Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.
Multiscale vision transformers. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 6824–6835, 2021.
[19] Pedro Felzenszwalb, David McAllester, and Deva Ramanan.
A discriminatively trained, multiscale, deformable part model.
In2008 IEEE Conference on Computer Vision and Pattern
Recognition , pages 1–8, June 2008.
[20] Anthony Fuller, Koreen Millard, and James R. Green. SatViT:
Pretraining Transformers for Earth Observation. IEEE Geo-
science and Remote Sensing Letters , 19:1–5, 2022.[21] Peng Gao, Teli Ma, Hongsheng Li, Ziyi Lin, Jifeng Dai, and
Yu Qiao. ConvMAE: Masked Convolution Meets Masked
Autoencoders, May 2022.
[22] Yuan Gao, Xiaojuan Sun, and Chao Liu. A general self-
supervised framework for remote sensing image classification.
Remote Sensing , 14(19):4824, 2022.
[23] Golnaz Ghiasi and Charless C. Fowlkes. Laplacian Pyramid
Reconstruction and Refinement for Semantic Segmentation.
In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling,
editors, Computer Vision – ECCV 2016 , Lecture Notes in
Computer Science, pages 519–534, Cham, 2016. Springer
International Publishing.
[24] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville.
Deep Learning . MIT Press, Cambridge, MA, USA, 2016.
http://www.deeplearningbook.org .
[25] Ritwik Gupta, Colorado Reed, Anja Rohrbach, and Trevor
Darrell. Accelerating Ukraine Intelligence Analysis
with Computer Vision on Synthetic Aperture Radar Im-
agery. http://bair.berkeley.edu/blog/2022/03/21/ukraine-sar-
maers/, Mar. 2022.
[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross Girshick. Masked Autoencoders Are Scal-
able Vision Learners, Dec. 2021.
[27] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
9729–9738, 2020.
[28] Yutong He, Dingjie Wang, Nicholas Lai, William Zhang,
Chenlin Meng, Marshall Burke, David Lobell, and Stefano
Ermon. Spatial-temporal super-resolution of satellite im-
agery via conditional pixel synthesis. Advances in Neural
Information Processing Systems , 34:27903–27915, 2021.
[29] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
Damian Borth. Introducing Eurosat: A Novel Dataset and
Deep Learning Benchmark for Land Use and Land Cover
Classification. In IGARSS 2018 - 2018 IEEE International
Geoscience and Remote Sensing Symposium , pages 204–207,
July 2018.
[30] Olivier Henaff. Data-efficient image recognition with con-
trastive predictive coding. In International conference on
machine learning , pages 4182–4192. PMLR, 2020.
[31] X Hu, H Mu, X Zhang, Z Wang, T Tan, and J Meta-SR Sun.
A magnification-arbitrary network for super-resolution. In
Proceedings of the 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), Long Beach, CA,
USA, pages 15–20, 2019.
[32] Damian Iba ˜nez, Ruben Fernandez-Beltran, Filiberto Pla, and
Naoto Yokoya. Masked auto-encoding spectral-spatial trans-
former for hyperspectral image classification. IEEE Transac-
tions on Geoscience and Remote Sensing , 2022.
[33] Pengpeng Ji, Shengye Yan, and Qingshan Liu. Region-Based
Spatial Sampling for Image Classification. In 2013 Seventh
International Conference on Image and Graphics , pages 874–
879, July 2013.
[34] Jan J. Koenderink. The structure of images. Biological
Cybernetics , 50(5):363–370, Aug. 1984.
[35] Pawel Kowaleczko, Tomasz Tarasiewicz, Maciej Ziaja, Daniel
Kostrzewa, Jakub Nalepa, Przemyslaw Rokita, and Michal
Kawulok. Mus2: A benchmark for sentinel-2 multi-image
super-resolution. arXiv preprint arXiv:2210.02745 , 2022.
[36] S. Lazebnik, C. Schmid, and J. Ponce. Beyond Bags of
Features: Spatial Pyramid Matching for Recognizing Natural
Scene Categories. In 2006 IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition (CVPR’06) ,
volume 2, pages 2169–2178, June 2006.
[37] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep
learning. nature , 521(7553):436–444, 2015.
[38] Christian Ledig, Lucas Theis, Ferenc Husz ´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 4681–4690,
2017.
[39] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature Pyramid
Networks for Object Detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 2117–2125, 2017.
[40] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian
Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C. Berg.
SSD: Single Shot MultiBox Detector. In Bastian Leibe, Jiri
Matas, Nicu Sebe, and Max Welling, editors, Computer Vi-
sion – ECCV 2016 , Lecture Notes in Computer Science, pages
21–37, Cham, 2016. Springer International Publishing.
[41] Yufei Liu, Xiaorun Li, Ziqiang Hua, Chaoqun Xia, and Liaoy-
ing Zhao. A band selection method with masked convolu-
tional autoencoder for hyperspectral image. IEEE Geoscience
and Remote Sensing Letters , 2022.
[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A ConvNet for the
2020s. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 11976–11986,
2022.
[43] Gabriel Machado, Edemir Ferreira, Keiller Nogueira, Hugo
Oliveira, Matheus Brito, Pedro Henrique Targino Gama, and
Jefersson Alex dos Santos. AiRound and CV-BrCT: Novel
Multiview Datasets for Scene Classification. IEEE Journal
of Selected Topics in Applied Earth Observations and Remote
Sensing , 14:488–503, 2021.
[44] Emmanuel Maggiori, Yuliya Tarabalka, Guillaume Charpiat,
and Pierre Alliez. Can semantic labeling methods generalize
to any city? the inria aerial image labeling benchmark. In
IEEE International Geoscience and Remote Sensing Sympo-
sium (IGARSS) . IEEE, 2017.
[45] Malachy Moran, Kayla Woputz, Derrick Hee, Manuela
Girotto, Paolo D’Odorico, Ritwik Gupta, Daniel Feldman,
Puya Vahabi, Alberto Todeschini, and Colorado J. Reed.
Snowpack Estimation in Key Mountainous Water Basins from
Openly-Available, Multimodal Data Sources, Aug. 2022.
[46] Maxim Neumann, Andr ´e Susano Pinto, Xiaohua Zhai, and
Neil Houlsby. In-domain representation learning for remote
sensing. ArXiv , abs/1911.06721, 2019.[47] Fernando Paolo, Tsu-ting Tim Lin, Ritwik Gupta, Bryce
Goodman, Nirav Patel, Daniel Kuster, David Kroodsma, and
Jared Dunnmon. xView3-SAR: Detecting Dark Fishing
Activity Using Synthetic Aperture Radar Imagery. In Thirty-
Sixth Conference on Neural Information Processing Systems
Datasets and Benchmarks Track , Oct. 2022.
[48] Xiaoman Qi, Panpan Zhu, Wang Yuebin, Liqiang Zhang, Jun-
huan Peng, Mengfan Wu, Jialong Chen, Xudong Zhao, Ning
Zang, and P. Takis Mathiopoulos. MLRSNet: A multi-label
high spatial resolution remote sensing dataset for semantic
scene understanding. ISPRS Journal of Photogrammetry and
Remote Sensing , 169:337–350, Nov. 2020.
[49] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. Improving language understanding by gener-
ative pre-training. Preprint , 2018.
[50] A. Rosenfeld and M. Thurston. Edge and Curve Detection
for Visual Scene Analysis. IEEE Transactions on Computers ,
C-20(5):562–569, May 1971.
[51] Jacob Shermeyer and Adam Van Etten. The effects of super-
resolution on object detection performance in satellite im-
agery. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition Workshops , pages 0–0,
2019.
[52] Beth Tellman, Jonathan A. Sullivan, and Colin S. Doyle.
Global Flood Observation with Multiple Satellites. In Global
Drought and Flood , chapter 5, pages 99–121. American
Geophysical Union (AGU), 2021.
[53] Adam Van Etten, Dave Lindenbaum, and Todd M. Bacastow.
SpaceNet: A Remote Sensing Dataset and Challenge Series,
July 2019.
[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is All you Need. In Advances in
Neural Information Processing Systems , volume 30. Curran
Associates, Inc., 2017.
[55] Qi Wang, Shaoteng Liu, Jocelyn Chanussot, and Xuelong
Li. Scene Classification With Recurrent Attention of VHR
Remote Sensing Images. IEEE Transactions on Geoscience
and Remote Sensing , 57(2):1155–1167, Feb. 2019.
[56] Yifan Wang, Federico Perazzi, Brian McWilliams, Alexander
Sorkine-Hornung, Olga Sorkine-Hornung, and Christopher
Schroers. A Fully Progressive Approach to Single-Image
Super-Resolution. In 2018 IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition Workshops (CVPRW) ,
pages 977–97709, Salt Lake City, UT, USA, June 2018.
IEEE.
[57] Zhirong Wu, Yuanjun Xiong, Stella X. Yu, and Dahua Lin.
Unsupervised Feature Learning via Non-Parametric Instance
Discrimination. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 3733–3742,
2018.
[58] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian
Sun. Unified Perceptual Parsing for Scene Understanding. In
Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and
Yair Weiss, editors, Computer Vision – ECCV 2018 , volume
11209, pages 432–448. Springer International Publishing,
Cham, 2018.
[59] Xin-Yi Tong, Gui-Song Xia, Qikai Lu, Huangfeng Shen,
Shengyang Li, Shucheng You, Liangpei Zhang. Land-cover
classification with high-resolution remote sensing images us-
ing transferable deep models. Remote Sensing of Environment,
doi: 10.1016/j.rse.2019.111322 , 2020.
[60] Zhitong Xiong, Fahong Zhang, Yi Wang, Yilei Shi, and
Xiao Xiang Zhu. EarthNets: Empowering AI in Earth Obser-
vation, Oct. 2022.
[61] Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Ul-
trasr: Spatial encoding is a missing key for implicit im-
age function-based arbitrary-scale super-resolution. arXiv
preprint arXiv:2103.12716 , 2021.
[62] Shengye Yan, Xinxing Xu, Dong Xu, Stephen Lin, and Xue-
long Li. Beyond Spatial Pyramids: A New Feature Extraction
Framework with Dense Spatial Sampling for Image Classifi-
cation. In European Conference on Computer Vision 2012 ,
pages 473–487, Aug. 2012.
[63] Shengye Yan, Xinxing Xu, Dong Xu, Stephen Lin, and Xue-
long Li. Image Classification With Densely Sampled Image
Windows and Generalized Adaptive Multiple Kernel Learn-
ing. IEEE Transactions on Cybernetics , 45(3):381–390, Mar.
2015.
[64] Wenming Yang, Xuechen Zhang, Yapeng Tian, Wei Wang,
Jing-Hao Xue, and Qingmin Liao. Deep learning for single
image super-resolution: A brief review. IEEE Transactions
on Multimedia , 21(12):3106–3121, 2019.
[65] Yi Yang and Shawn Newsam. Bag-of-visual-words and spa-
tial extensions for land-use classification. In Proceedings of
the 18th SIGSPATIAL International Conference on Advances
in Geographic Information Systems , GIS ’10, pages 270–279,
New York, NY , USA, Nov. 2010. Association for Computing
Machinery.
[66] Matthew D Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In European conference on
computer vision , pages 818–833. Springer, 2014.
[67] Renrui Zhang, Ziyu Guo, Rongyao Fang, Bin Zhao, Dong
Wang, Yu Qiao, Hongsheng Li, and Peng Gao. Point-
M2AE: Multi-scale Masked Autoencoders for Hierarchical
Point Cloud Pre-training, Oct. 2022.
[68] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi,
Chen Change Loy, Dahua Lin, and Jiaya Jia. PSANet: Point-
wise Spatial Attention Network for Scene Parsing. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV) , pages 267–283, 2018.
A. Datasets
In our experiments, we used a total of ten datasets (Table 10) for the tasks of land-use/land-cover classification and semantic
segmentation. There are a large amount of remote sensing datasets in existence. Many remote sensing datasets fundamentally
capture the same data with minor changes in location or distribution. We selected datasets with key, representative properties .
These properties include (1) a diversity in the amount of kinds of classes/objects represented, (2) a large spectrum of ground
sample distances from (ideally) known sensor configurations, and (3) pansharpened, othrorectified, and quality controlled
imagery and labels. We capture these properties in Table 10.
A.1. Diversity in classes
For both pretraining and downstream evaluations, it is a desirable property to include as much geographic and class diversity
as possible. In order to capture a wide amount of classes in remote sensing, it is necessary to include multiple localities and
environments. This property serves as a proxy for the amount of unique “features” available in the dataset.
Dataset Resolution (px) GSD (m) Number of Images Number of Classes Task Type
AiRound [43] 500 0.3 - 4800 11,753 11 C
CV-BrCT [43] 500 0.3 - 4800 24,000 9 C
EuroSAT [29] 64 10 27,000 10 C
MLRSNet [48] 256 0.1 - 10 109,161 46 C
Optimal-31 [55] 256 0.5 - 8 1,860 31 C
RESISC-45 [11] 256 0.2 - 30 31,500 45 C
UC Merced [65] 256 0.3 2,100 21 C
WHU-RS19 [14] 256 0.5 1050 19 C
fMoW [12] Various 0.3 1,047,691 62 C
SpaceNet v1 [53] Various 0.5 6,940 2 SS
Table 10. Statistics of all datasets used in our experiments. Task types are classification (C) and semantic segmentation (SS).
A.2. Spectrum of GSDs
Scale-MAE is built to be invariant to the input absolute scale of the dataset. Many datasets are collected from a single sensor
and processed in a uniform fashion. To validate that our method works with many resolutions, we included datasets which are
collected from a variety of sensors but then processed in a uniform fashion. This excludes differences in processing as a factor
affecting our experiments and narrowly targets resolution instead.
A.3. Quality control
It is hard to assess the quality of remote sensing datasets without manually verifying a majority of instances of the data.
We mandated that images used are pansharpened (and therefore the highest resolution possible to extract from the sensor),
orthorectified (and therefore well-aligned with the geodetic ellispoid), and projected to the same coordinate reference system.
This eliminates large differences in sensor-to-image processing.
B. Laplacian and Upsampling Block Architectures
Figure 7 illustrates the architecture of Laplacian and Upsampling block architectures described below.
B.1. Laplacian Block
Laplacian Blocks are used to reconstruct the target at a specific resolution and frequency. A Laplacian Block consists of
a chain of Feature Mapping Block, which distills information at a specific frequency, followed by one final Reconstruction
Block, which generates the final output. A Feature Mapping Block consists of a 3x3 depth-wise convolution layer with GELU
activation, followed by 1x1 convolution. A Reconstruction Block consists of a 4x4 transpose convolution layer followed by a
k= 20 k= 100 k= 5
Dataset Res Scale. Sat. Conv. Scale. Sat. Conv. Scale. Sat. Conv.
AiRound16 0.401 0.375 0.423 0.396 0.367 0.401 0.370 0.355 0.403
32 0.561 0.510 0.539 0.536 0.491 0.517 0.541 0.492 0.539
64 0.689 0.607 0.658 0.643 0.579 0.621 0.692 0.604 0.666
128 0.743 0.650 0.681 0.690 0.600 0.622 0.749 0.660 0.690
256 0.729 0.662 0.658 0.678 0.621 0.602 0.731 0.663 0.676
496 0.670 0.664 0.620 0.609 0.613 0.566 0.685 0.669 0.632
CV-BrCT16 0.522 0.478 0.567 0.485 0.443 0.513 0.524 0.475 0.585
32 0.653 0.615 0.656 0.588 0.560 0.592 0.695 0.644 0.699
64 0.744 0.701 0.711 0.674 0.635 0.644 0.780 0.727 0.754
128 0.763 0.725 0.732 0.710 0.662 0.667 0.805 0.758 0.782
256 0.761 0.725 0.727 0.694 0.666 0.664 0.802 0.770 0.771
496 0.737 0.727 0.709 0.656 0.657 0.631 0.792 0.771 0.765
EuroSAT16 0.744 0.727 0.826 0.699 0.695 0.788 0.751 0.729 0.835
32 0.901 0.876 0.898 0.869 0.854 0.863 0.912 0.871 0.909
64 0.956 0.931 0.940 0.935 0.913 0.914 0.960 0.934 0.947
MLRSNet16 0.563 0.491 0.607 0.535 0.461 0.549 0.551 0.479 0.617
32 0.772 0.677 0.744 0.726 0.625 0.688 0.772 0.684 0.762
64 0.893 0.815 0.851 0.849 0.754 0.792 0.911 0.839 0.876
128 0.936 0.875 0.894 0.892 0.814 0.834 0.950 0.899 0.918
256 0.918 0.892 0.882 0.862 0.840 0.817 0.940 0.913 0.910
OPTIMAL-3116 0.354 0.322 0.439 0.312 0.298 0.370 0.317 0.319 0.418
32 0.574 0.500 0.587 0.567 0.508 0.545 0.565 0.519 0.561
64 0.793 0.609 0.698 0.742 0.561 0.598 0.782 0.646 0.688
128 0.816 0.670 0.714 0.731 0.646 0.595 0.809 0.694 0.725
256 0.739 0.681 0.646 0.653 0.638 0.550 0.761 0.731 0.693
RESISC16 0.382 0.347 0.458 0.370 0.327 0.428 0.353 0.323 0.435
32 0.628 0.527 0.601 0.597 0.505 0.568 0.609 0.508 0.592
64 0.798 0.667 0.731 0.754 0.631 0.677 0.803 0.667 0.734
128 0.864 0.748 0.798 0.819 0.699 0.743 0.882 0.762 0.817
256 0.826 0.758 0.762 0.761 0.708 0.690 0.850 0.771 0.788
UC Merced16 0.524 0.472 0.598 0.400 0.370 0.462 0.512 0.488 0.617
32 0.767 0.670 0.683 0.605 0.535 0.593 0.828 0.682 0.726
64 0.842 0.795 0.771 0.719 0.729 0.652 0.884 0.842 0.845
128 0.858 0.788 0.750 0.662 0.738 0.655 0.884 0.847 0.838
256 0.762 0.802 0.700 0.595 0.757 0.590 0.851 0.842 0.817
WHU-RS1916 0.545 0.445 0.576 0.400 0.380 0.562 0.525 0.490 0.631
32 0.650 0.729 0.670 0.610 0.675 0.576 0.760 0.690 0.754
64 0.850 0.805 0.833 0.770 0.730 0.680 0.920 0.840 0.837
128 0.970 0.910 0.882 0.890 0.890 0.685 0.985 0.895 0.941
256 0.960 0.940 0.892 0.880 0.925 0.709 0.975 0.945 0.931
Table 11. Scale-MAE outperforms SatMAE and ConvMAE on kNN classification across a variety of k, across a variety of resolutions.
kNN Classification results for Scale-MAE , SatMAE and ConvMAE across a variety of k. Resolution is reported in pixels.
3x3 depth-wise convolution layer, a 1x1 convolution layer, and a 2x2 transpose convolution layer. In our experiments, we have
two Feature Mapping Blocks per Laplacian Block.
d3x3, 256512-d
1x1, 512+x N times
4x4T, 256
d3x3, 128
1x1, 256
2x2T, 3
GELU
Layer Norm
512-d
2x2T, 512
2x2T, 512x N timesLaplacian Block
Upsampling BlockFigure 7. (top) The Laplacian Block (LB) is a fully convolutional architecture consists of a chain of Feature Mapping Block followed by
one final Reconstruction Block. (bottom) The UpSampling Block (UB) consists of a series of transpose convolution layers separated by
LayerNorm and GELU activation.
B.2. Upsampling Block
Upsampling Blocks are used to upsample the feature map to a higher resolution. It consists of a series of 2x2 transpose
convolution layers with LayerNorm and GELU activation between them. The number of such transposed convolution layers
are a function of the output and input resolution. This is a progressive process in which we repetitively upsample the feature
map by a factor of 2 until we reach the desired target resolution. Figure 7 illustrates the architecture of these two blocks.
C. Evaluation Details
As discussed in the main experimental section, we investigated the quality of representations learned from Scale-MAE
pretraining through a set of experiments that explore their robustness to scale as well as their transfer performance to additional
tasks. We provide more information and details on these evaluations here. In order to compare with SatMAE [13] and
ConvMAE [21], for our main experiments, we pretrained Scale-MAE with a ViT-Large model using the Functional Map of
the World (FMoW) RGB training set, which consists of 363.6k images of varying image resolution and GSD. The initial
higher resolution image Ihris taken as a random 448px2crop of the input image, and the input image Iis then a downsampled
224px2fromIhr. The low frequency groundtruth is obtained by downscaling Ihrto 14px2and then upscaling to 224px2, while
the high frequency groundtruth is obtained by downscaling Ihrto 56px2and then upscaling to 448px2and subtracting this
image from Ihr. This is a common method for band pass filtering used in several super resolution works, where a high to low
to high resolution interpolation is used to obtain only low frequency results, and then high frequency results are obtained by
subtracting the low frequency image.
As further discussed in the main experimental section, we evaluate the quality of representations from Scale-MAE by
freezing the encoder and performing a nonparametric k-nearest-neighbor (kNN) classification with eight different remote
sensing imagery classification datasets with different GSDs, none of which were encountered during pretraining. All kNN
evaluations were conducted on 4 GPUs. Results are in Table 11. The kNN classifier operates by encoding all train and
validation instances, where each embedded instance in the validation set computes the cosine distance with each embedded
instance in the training set, where the instance is classified correctly if the majority of its k-nearest-neighbors are in the same
class as the validation instance. The justification for a kNN classifier evaluation is that a strong pretrained network will output
semantically grouped representation for unseen data of the same class. This evaluation for the quality of representations occurs
Scale-MAE
 Ground Truth Vanilla MAE
Correct Incorrect
Scale-MAE Ground Truth Vanilla MAE
Figure 8. Visualization of Segmentation Results on SpaceNet. The left, center, right columns are ground truth labels, Scale-MAE and
vanilla MAE, respectively. The top row shows a 0.3m GSD image and the bottom row shows a 3.0m GSD image. As shown in the figure,
Scale-MAE performs better at both higher and lower GSDs.
in other notable works [7, 9, 57].
D. Visualization of SpaceNet Segmentation
Figure 8 shows an additional set of segmentation examples comparing Scale-MAE and vanilla MAE pre-trained on FMoW
and finetuned on SpaceNet v1. The left, center, right columns are ground truth labels, Scale-MAE and vanilla MAE respectively.
The top row shows a 0.3m GSD image and the bottom row shows a 3.0m GSD image. As shown in the figure, Scale-MAE
performs better at both higher and lower GSDs.
E. Glossary
E.1. Ground sample distance
Ground sample distance (GSD) is the distance between the center of one pixel to the center of an adjacent pixel in a remote
sensing image. GSD is a function of sensor parameters (such as its dimensions and focal length), image parameters (the
target dimensions of the formed image), and the geometry of the sensor with respect to the object being imaged on the Earth.
Remote sensing platforms frequently have multiple sensors to capture different wavelengths of light. Each of these sensors
have varying parameters, resulting in different GSDs for an image of the same area. Additionally, the ground is not a uniform
surface with changes in elevation common across the swath of the sensor. In total, a remote sensing platform has a sense of
absolute scale that varies along two dimensions: (1) spectrally depending on the sensor used to capture light, and (2) spatially
depending on surface elevation.
Domain Adaptation 
for the Classification  
of Remote Sensing Data
An overview of recent advances
DEVIS TUIA, CLAUDIO PERSELLO, 
AND LORENZO BRUZZONEAdvances in Machine Learning for Remote Sensing and Geosciences
image licensed by ingram publishing
jUNE 2016    ieee Geoscience and remote sensin G ma Gazine                                                                             0274-6638/16©2016IEEE 41 The success of the supervised classification of remotely 
sensed images acquired over large geographical areas or 
at short time intervals strongly depends on the representa -
tivity of the samples used to train the classification algo -
rithm and to define the model. When training samples are 
collected from an image or a spatial region that is different 
from the one used for mapping, spectral shifts between the two distributions are likely to make the model fail. Such 
shifts are generally due to differences in acquisition and  atmospheric conditions or to changes in the nature of the 
object observed. To design classification methods that are 
robust to data set shifts, recent remote sensing literature has 
considered solutions based on domain adaptation (DA)  approaches. Inspired by machine-learning literature, several 
DA methods have been proposed to solve specific problems 
in remote sensing data classification. This  article provides a 
critical review of the recent advances in DA  approaches for 
remote sensing and presents an overview of DA methods 
divided into four categories: 1) invariant feature selection, 2) representation matching, 3) adaptation of classifiers, and 
4) selective  sampling. We provide an  overview of recent 
Digital Object Identifier 10.1 109/MGRS.2016.2548504 
Date of publication: 13 June 2016
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
                                           ieee Geoscience and remote sensin G ma Gazine    june 201642 
 methodologies,  examples of  applications of the considered 
techniques to real remote sensing images characterized by 
very high spatial and spectral resolution as well as possible 
guidelines for the selection of the method to use in real ap -
plication scenarios. 
Remote Sen Sing Facing n ew o ppoRtunitie S 
With the advent of the new generation of satellite mis -
sions, which are often made up of constellations of satel -
lites with short revisit time 
and very high-resolution sen -
sors, the amount of remote sensing images available has increased significantly. Now -
adays, the monitoring of dy-namic processes has become possible [ 1], [2 ], and biophy-
sical parameter estimation and classification problems can be addressed with the 
use of several data sources 
[3]–[6 ]. As a consequence, 
analysts have the opportuni -
ty to use multitemporal and multisource images for tasks such as repetitive  monitoring 
of the territory, change de -
tection, image mosaicking, 
and large-scale processing (i.e., processing involving many 
 image tiles) [ 7].
Remote sensing is therefore facing new opportuni -
ties. However, such opportunities cannot be seized un -
less they come with the capability to provide accurate 
products in a timely manner. A bottleneck of supervised image-processing-based pipelines is the need of training 
the model on reference points that are specific to every 
acquisition. To be accurate, most models need to be trained on the known samples coming from the image under study. 
Since obtaining new ground samples of high quality for 
each image acquisition is not realistic, to retrain or adapt an existing model without such ground samples becomes mandatory. Figure 1  
illustrates situations where  adaptive 
models might be of great use. In these situations (which correspond to those considered in this article), 
only one image, i.e., shown in red 
in the figure, has sufficient reference labels (e.g., obtained in an extensive 
ground campaign), whereas the oth -
ers have no labeled samples or have 
them only in an insufficient number. 
This setting is more realistic, since, 
on the one hand, extensive labeling cannot follow the pace of image ac -
quisitions, and, on the other hand, 
repetitive ground campaigns are simply not often an op -
tion, mainly for economic and manpower reasons. Indeed, 
gathering ground information is costly and cannot always 
be performed by photointerpretation. This is particularly true when the task concerns very large areas or considers 
quantities that cannot be photointerpreted by an analyst, 
such as chlorophyll concentrations [ 8], plant water stress 
[9], or tree species [ 10].
To address the cases described in Figure 1, a possible 
solution  would be to bypass the problem and assume that 
the model that is already available is robust enough to 
process the new images accurately. Despite the fact that 
this is possible only in cases where the new image is ac -
quired by the same sensor as the previous one, it is well 
known that the direct application of a pretrained model 
on a new data set often provides poor results because the spectra observed in the new scene, even though rep -
resenting the same types of objects, are different from those of the scene used for training. The differences can be related to a series of deformations (or shifts) related to 
a variety of effects, such as a biased sampling in the spa -
tial domain (typically if the ground sampling has been 
focused on a region nonrepresentative of the new scene), 
changes in the acquisition conditions (including the il -
lumination or acquisition angle), or seasonal changes. 
When the new data are acquired by a different sensor, the 
strategy explained previously is simply not applicable, as 
most models require that all images (or domains) provide samples of the same dimensionality (and where each di -
mension has the same meaning) at test time. In this case, fusion strategies exist but generally only apply to certain combinations of sensors, and they only use the bands 
that these sensors have in common or that are reason -
ably similar [ 3], which prevents reusing the models that 
are already trained on the first image of the region that becomes available (which can be crucial, for example, in 
postcatastrophe interventions) and exploiting sensors’ synergies in multisensorial schemes.
To process remote sensing images efficiently and accu -
rately, modern processing systems must be designed to be Model Extension on
Wide SurfacesMosaicking Model Extension on Wide
and Asynchronous Scenes
Figu Re 1. Examples of cases where DA is necessary to extend a model to new image acquisi -
tions. In all three cases, the images can be from different sensors, but only the images in red 
have extensive reference labels (i.e., can be used for training an accurate supervised model). 
to pRoceSS R emote  
SenSing  image S 
eFFicient LY an D 
accu RateLY, moDe Rn 
pRoceSSing  SYStemS 
muSt Be DeSigne D to B e 
RoBuSt in t He Face o F 
cHange S in acQuiSition  
conDition S an D 
tempo RaL SHi FtS anD, 
iDeaLLY, to  Be aDaptiV e 
to SenSoR DiFFeRence S.
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
june 2016    ieee Geoscience and remote sensin G ma Gazine                                                        43 
robust in the face of changes in acquisition conditions and 
temporal shifts and, ideally, to be adaptive to sensor dif -
ferences. The need for adapting existing models has been acknowledged for many years, as shown by the signature extension community [ 11], [12 ], but the change in the 
amount and nature of data (as well as their resolution) cre -
ated the need for a new research direction. In this article, we advocate that the solution can be found in DA strategies, a 
field that is deeply rooted in statistical and machine learn -
ing [13], [14 ].
In general, DA aims to adapt models trained to solve a 
specific task to a new, yet related, task, for which the knowl -
edge of the initial model is sufficient, although not perfect. 
As a traditional example in computer vision, DA methods 
have been deployed to take classifiers that recognize ob -
jects in pictures from commercial websites and adapt them 
to the recognition of objects photographed by simple web 
cameras [ 15]. In this example, the classifier is presented a 
problem with the same objective (i.e., classifying pictures 
into a limited set of object classes) and the same features 
but where the data relations are slightly different. For ex -
ample, at Amazon.com the pictures have no background 
and the object is mostly in the center of the image, while 
this is not the case in webcam images. DA is therefore used 
to adapt the classifier that is accurate on Amazon.com to the new data distribution. Of course, this is just one ex -
ample. In the DA literature, models are modified to adapt to new data spaces (multimodal), related tasks (multitask), or subtle changes in probability distributions (see a recent 
review in [ 16]). The connections to the multitemporal, 
multisensor, and multiresolution image classification tasks 
discussed previously are strong [ 7].
The aim of this review article is to provide an introduc -
tion to the DA field and to provide examples of applications of DA techniques in remote sensing. With this in mind, we 
draw a taxonomy of the DA strategies that have been pro -
posed in recent remote sensing literature and discuss their 
strengths and weaknesses. We also provide a series of prac -
tical examples about the use of DA in high- to very high-resolution image-processing tasks. However, we will not enter into the technical details of specific DA literature; for 
interested readers, we refer to the recent surveys published 
in [13], [14], [16], and [17 ].
tRanSFeR LeaRning  anD Domain  aDaptation
Transfer-learning problems arise when inferences have to be made on processes that are not stationary over time or 
space. As previously discussed, this is the case in the anal -
ysis of remote sensing images where different acquisitions 
are typically subject to different conditions (e.g., illumi -
nation, viewing angle, soil moisture, and topography). Such differences can affect the observed spectral signa -
tures of the land-cover types and, therefore, the distribu -
tion of the information classes in the feature space [ 18]. 
Different transfer-learning problems (and the techniques to tackle them) have been considered in the literature, including DA, multitask learning, domain generaliza -
tion, sample selection bias, and covariate shift [ 14]. In 
this article, we will focus on DA, which is a particular form of transfer learning.
Let us consider two domains, called source domain and 
target domain , that are associated with two images ac -
quired on different geographical areas (but with similar 
land-cover characteristics) or on the same area at differ -
ent time instants. Figure 2 shows the DA problem in the 
context of remote sensing image classification. The source 
and target domains are associated with the joint probabil -
ity distributions 
(, ) PX Ys and (, ), PX Yt respectively. The 
two joint probabilities define the classification problems 
on the two domains, where 
X is the input (vector) vari -
able (i.e., spectral bands of 
the source image with pos -
sible additional features used 
to characterize the contex -
tual information of the single pixel) and 
Y is the output 
variable associated with a set 
of classes (i.e., land-cover or 
land-use information). The aim of DA methods is to adapt a classifier trained on the source domain to make predictions 
on the target domain.
Supervised DA assumes that labeled samples are availa  ble 
for both domains. The labeled sets 
xx{( ,),,(, )} Ty ys
nn 11f =  
and xx{( ,),,(, )} Ty yt
mm 11f =  are the source- and target-
domain training sets, respectively. Supervised DA meth -
ods focus on challenging situations where labeled target-
domain samples are less numerous than those available in 
the source domain (i.e., ). mn11  In such conditions, the 
proper use of source-domain information is very important 
in solving the target problem. Most of the work in DA as -
sumes that source and target domains share the same set Source Image Target Image
DA
Ps(X,Y ) = Ps(X)Ps(Y|X) Pt(X,Y ) = Pt(X)Pt(Y|X) ≠
Figu Re 2. A graphical representation of the DA problem in the 
context of remote sensing image classification. Source and target images can be acquired on different geographical areas (but with 
similar land-cover characteristics) or on the same area at different times. The two images are associated with two different joint dis -
tributions, which characterize the two classification problems. The two distributions can differ due to different acquisition conditions (e.g., illumination, viewing angle, soil moisture, and topography). 
tRanSFeR-LeaRning  
pRoBLemS aRiSe wHen 
inFeRence S HaVe to B e 
maDe on p RoceSSeS tHat 
aRe not Stationa RY o VeR 
time o R Space .
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
                                           ieee Geoscience and remote sensin G ma Gazine    june 201644 
of classes. There are only a few papers that address DA con -
sidering differences in the set of classes between source and 
target [ 19]–[22 ].
Semisupervised DA methods assume that a training set 
is available only for the source domain, whereas target-do -
main information is limited to a set of unlabeled samples 
xx{, ,} . Ut
m 1f =  This DA setting is more challenging than 
the supervised case and requires important assumptions on 
the relationship between source and target domains to make 
the algorithm converge to a consistent solution on the target 
domain. All DA methods are based on the assumption that 
(, ) PX Ys and (, ) PX Yt are different but close enough to en -
sure that the source-domain 
information can be of help 
for solving the target-domain learning problem. On the one 
hand, if the source and target 
domain are arbitrarily differ -
ent, there is no hope that the 
source-domain information 
will provide an advantage in solving the task in the target 
domain. On the other hand, 
if 
(, )( ,) , PX YP XYst=  no ad -
aptation is necessary, and the 
model trained on the source 
can be readily applied to the 
target. Semisupervised DA methods are effective in situa -
tions that lie in between these two extreme cases.Unsupervised DA methods are the last family, and they 
assume that two unlabeled domains have to be matched. This is the most difficult case because label information is not available for any domain. In this situation, DA methods 
aim to match the marginal distributions of the two domains 
()PXs and ()PXt without knowledge on the learning task 
(classification or regression). Unsupervised methods can be used as preprocessing of any analysis task (e.g., clustering or 
density estimation), but they imperatively need to have data sets with similar structural properties before adaptation. 
Unsupervised DA models are generally feature extractors or 
matching algorithms that exploit the geometrical structure of the data.
Problems related to DA are the sample selection bias 
and covariate shift [ 23]–[25 ]. The sample selection bias 
originates when the available training samples are not in -
dependently and randomly selected from the underlying distribution (i.e., the training set is not a random sample of the population). This situation is very likely to occur in 
remote sensing problems where training points are usually 
selected and labeled through photointerpretation or field surveys. The covariate shift is a particular case of sample se -
lection bias where the bias depends only on the input vari -
able 
X (and not on .)Y Different operational conditions 
that result in biased training samples are discussed in [ 26]. 
Clearly, a training set xx{( ,),, (, )} Ty ynn 11f =  obtained un -
der a  sample selection bias leads to skewed estimation of 
the true underlying distribution of the classes, resulting in an estimated 
(, )( ,) . PXYP XY! t  For this reason, the effect 
of a sample selection bias is similar to the DA problem de -
scribed previously. The covariate shift problem is generally not as severe as the general sample selection bias and the 
DA problem. Let us consider that both training sets on the 
source and target are samples with bias (a bias depending on X only, i.e., covariate shift) from the same joint distribution. 
The joint probabilities on the two domains can be factorized 
as 
(, )( )( |) PX YP XP YXss s=  and (, )( )( |) . PX YP XP YXtt t=  
In this particular case of covariate shift, the estimated con -
ditional probabilities will be approximately equal, while 
the marginal distributions will generally be different, i.e., 
(| )( |) PY XP YXst. tt  and () () . PX PXst!tt
Figure 3 illustrates the two different issues of DA 
and the sample selection bias with two explanatory ex -
amples. The plots report the distributions of the labeled 
samples from two domains in a bidimensional feature space. A four-class classification problem is considered. 
In the case of DA, the class-conditional densities may 
change from the source to the target domain, possibly resulting in a significantly different classification prob -
lem. Note that the distribution of the classes in the target domain may overlap with different classes on the source domain (see the green circle in the target domain that 
represents the location of the green class in the source). 
In this  situation, training samples of the source domain 
can be misleading for the classification of the target 
domain [ 27], [28 ]. In the  case of sample selection bias, Source Domain
DA
Sample
Selection BiasTarget Domain
FiguRe 3. Explanatory examples of DA and sample selection bias 
problems. The plots represent labeled samples in a bidimensional feature space on the source and target domain. A four-class  
classification problem is considered. In the case of DA, the class-
conditional densities may vary from the source domain to the target 
domain, resulting in a significantly different classification problem 
(the green circles correspond to the location of the green classes in the source domain). In the case of sample selection bias, the 
aforementioned issue will not occur, resulting in a milder type of shift 
from the source domain.
tHe Samp Le SeLection 
BiaS o Riginate S wHen 
tHe aVaiLaBLe tRaining  
Samp LeS aRe not  
inDepen DentLY an D 
RanDomLY S eLecte D  
FRom tHe unDeRLYing 
DiStRiBution . 
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
june 2016    ieee Geoscience and remote sensin G ma Gazine                                                        45 
source- and target-domain samples are drawn (with bias) 
from the same underlying distribution, generally resulting 
in a milder type of shift with respect to DA. (The problem 
of cross-domain class overlap is not likely to occur in sam -
ple selection bias problems.)
In real remote sensing problems, it is expected that the 
two aforementioned issues may occur at the same time and can be profoundly entangled, meaning that 1) the two 
theoretical underlying distributions 
(, ) PX Ys and (, ) PX Yt 
associated with the source and target domains, respec -
tively, may differ because of changes in the image acquisi -
tion conditions (e.g., radiometric conditions), and 2) the 
available training samples could be not fully representative 
of the statistical populations in their respective domains 
and will, therefore, lead to biased estimations of the class probabilities and to inaccurate classification models. In 
the remote sensing literature, several techniques have been 
presented to solve the transfer-learning problem irrespec -
tive of the cause of the data set shift between source and 
target domains. The next section will present the different 
families of techniques that have been proposed in remote sensing image processing.
a taXonom Y oF aDaptation met HoDS
Adapting a model trained on one image to another (or a 
series of new images) can be performed in different ways. 
In this section, we detail recent approaches proposed in the remote sensing literature by grouping them into the 
following four categories:
 ◗The selection of invariant features: A set of the input 
features (i.e., the original bands or additional features 
extracted from the remote sensing image) that are not 
affected by shifting factors are identified and selected before training the classification algorithm. According -
ly, the features affected by the most severe data set shift are removed, and the classifier considers a feature space showing higher stability across domains. An alternative 
approach is to encode invariance by including additional 
synthetic labeled samples in the training set to extract features that better model the intraclass variability across 
the domains.
 ◗The adaptation of the data distributions:  The data 
distributions of the target and source domains are made as similar as possible to keep the classifier un -
changed. With respect to the previous family, these 
methods work on the original input spaces and try 
to extract a common space where all domains can be 
treated equally. This is generally achieved by means of joint feature extraction.
 ◗The adaptation of the classifier: Here, the classifica -
tion model defined by training on the source domain is 
adapted to the target domain by considering unlabeled 
samples of the target domain. In this case, the data distributions remain unchanged, and the classifier is 
adapted to the target data distribution using strategies 
based on semisupervised learning. ◗The adaptation of the classifier by active learning (AL):  This adaptation is performed by providing a lim -
ited amount of well-chosen labeled samples from the target domain. This is a special case of the previous fam -
ily, where we allow some new labeled examples to be 
sampled in the target domain to retrain the model itera -
tively. Due to their acquisition cost, these samples need 
to be selected well, according to their potential to lead 
the model toward the desired target classifier.
The rest of this section details recent advances in these 
four families. We will limit the discussion to approaches 
specific to the remote sensing literature and invite the in -
terested reader to consult the specialized machine-learning 
and computer vision literature in [ 13], [14], and [16 ].
SELECTING INVARIANT FEATURESThe first family of DA methods is based on the selec -
tion of invariant features that are usually a subset of the original set of features that are the most robust in  
the face of changes from the source to the target do -
main. The main idea of the approach is to select features to reduce the difference between 
(, ) PX Ys and (, ). PX Yt An 
alternative strategy for encoding the invariance is based on 
the inclusion of additional synthetic labeled samples in 
the training set, a procedure known in machine learning 
as data augmentation . A meth -
od adopting this strategy was 
studied in [ 29], where sam -
ple-selection bias problems are addressed by enriching the training set with artificial 
examples that correspond to 
physically consistent varia -
tions of the training samples 
(e.g., illumination, size, and 
rotation). To limit the num -
ber of additional examples to be used by the support vector 
machine (SVM), variations are generated only for the train -
ing samples considered as support vectors by the classifier 
trained on the source domain only. 
Let us consider the first strategy and focus on the analy-
sis of hyperspectral images as an application of particular interest. Hyperspectral sensors are capable of capturing 
hundreds of narrow spectral bands from a wide range of 
the electromagnetic spectrum, which is why they are par -
ticularly sensitive to subtle changes in image-acquisition 
conditions, leading to a nonstationary behavior of the spec -
tral signature of the classes and, therefore, to problems that 
should be solved by transfer learning and DA approaches. 
Figure 4 shows an example of a shift in the signature of a 
 hyperspectral image acquired by the Hyperion sensor over 
two areas of the Okavango Delta in Botswana.
In [30], the authors propose an approach for selecting 
subsets of features that are both discriminative of the land-cover classes and invariant between the source and the target tHe coVaRiate SH iFt iS  
a pa RticuLaR ca Se oF 
Samp Le SeLection  BiaS 
wHe Re tHe BiaS DepenDS 
onLY on t He input  
VaRiaBL e X (anD not   
on y).
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
                                           ieee Geoscience and remote sensin G ma Gazine    june 201646 
domain. The main idea of this 
approach is to explicitly con -
sider two distinct terms in the criterion function for evaluat -
ing both the discrimination 
capability 
D of the feature 
subset and the data set shift 
P of the features between the 
source and target domain. The first term is standard in filter 
methods for feature selection and provides high scores when 
the features selected show 
some kind of dependency with the desired output (e.g., the 
classes to be predicted). The 
second term has been introduced to evaluate the invariance 
of the feature subset between the two domains. The subset of 
features 
F is selected by jointly optimizing the two terms D 
and ,P i.e., by solving the following multiobjective optimiza -
tion problem:
 (( ),()), argmin FP F
||FlD-
= (1)where l is the size of the feature subset. Both D and P 
are treated as functions of the subset of considered fea -
tures .F The specific definitions of the terms D and P 
are reported in [ 30], considering their parametric esti -
mation (assuming Gaussian distribution of the classes) 
in both the supervised and semisupervised DA settings. In [31], the two terms are defined considering kernel-
based dependence estimators and kernel embedding of conditional distributions, resulting in a nonparamet -
ric approach that does not require the estimation of the 
class distributions as an intermediate step. Equation (1) 
is solved by adopting a genetic multiobjective optimiza -
tion algorithm. The solution results in features with high 
capability to discriminate classes (with a small value of 
)D- and high stability on the two domains (with a small 
data set shift ).P Adopting a multiobjective optimization 
approach instead of considering a linear combination of 
the two terms frees the user from specifying in advance 
the relative importance of the two terms D and .P The 
solution of the multiobjective problem allows one to find the solutions that represent the best tradeoffs of discrimi -
native and stable feature subsets for the specific transfer-
learning problem at hand.0 50 100 15001,0002,0003,0004,0005,0006,0007,000
Class 1
Class 2
Class 3
Class 4
Class 5
Class 6
Class 7Class 8
Class 9
Class 10
Class 11
Class 12
Class 13
Class 14
Class 1
Class 2
Class 3
Class 4
Class 5
Class 6
Class 7Class 8
Class 9
Class 10
Class 11
Class 12
Class 13
Class 14
01,0002,0003,0004,0005,0006,0007,000
Source Domain 
Target Domain (b)
05 0 100 150
(c) (a)
Figu Re 4. (a) A false color composition of a portion of the hyperspectral data set. (b) The mean spectral signature of the classes on the 
source domain. (c) The mean spectral signature of the classes on the target domain. 
in tHe Remote  SenSing 
LiteRatuRe, SeVeRaL 
tecHniQueS Ha Ve Been 
pReSente D to S oLVe tHe 
tRanSFeR-LeaRning  
pRoBLem  iRReSpecti Ve  
oF tHe cau Se oF tHe Data 
Set SHiFt Between 
SouRce an D taRget 
DomainS.
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
june 2016    ieee Geoscience and remote sensin G ma Gazine                                                        47 
In the following, we report the experimental results ob -
tained on a hyperspectral image acquired by the Hyperion 
sensor of the Earth Observation 1 satellite in an area of the 
Okavango Delta, Botswana [ 32] (see Figure 4 ). For more in -
formation about the experimental setting and the obtained 
results, see [ 30]. The labeled reference samples were col -
lected on two spatially disjoint areas with slightly different 
characteristics, thus representing two different domains. 
The samples taken on the first area, which was considered 
as the source domain, were partitioned into a training set 
Ts and a test set Ts by random sampling. Samples taken on 
the second area (i.e., the target domain) were used to derive 
a training set Tt and test set Tt according to the same pro -
cedure. The estimated Pareto front for the selection of six features is reported in Figure 5 . 
Each point on the two graphs corresponds to a different 
selected feature subset 
F, i.e, a feature set minimizing (1 ). 
In Figure 5(a), the color of the points indicates the overall accuracy (OA) obtained on the source-domain test set 
Ts 
using an SVM classifier trained using Ts (according to the 
reported color scale bar). In Figure 5(b), the color indicates the OA obtained by the same SVM classifier on the target-
domain test set 
.Tt 
The results show that the solutions with higher relevance 
D result in better classification accuracies on the source do -
main. However, relevance alone is not sufficient for select -
ing features that are stable for the classification on the target domain. We observe that the most accurate solutions on the target domain 
Tt are those that exhibit a good tradeoff be -
tween the relevance and invariance terms, which confirms 
the importance of the invariance term and shows that the 
P measure is able to capture the information of feature 
stability. To select the subset of features that leads to good generalization capabilities on different domains, tradeoff  
solutions between the two competing objectives should be 
identified. The selected subset of features results in an OA 
of 91% on the source domain and 80.7% on the target. The 
set of features selected according to the optimization of 
D 
results in an OA of 92.7% on the source but only 64.4% on the target, showing that the features selected by accounting for the data set shift between the domains can significantly improve the generalization capability on the target domain.
ADAPTING DATA DISTRIBUTIONS
The second family reviewed considers DA methods that 
aim to adapt the representation of the original data, regard -
less of the model that will process them afterward. A review 
of the methods proposed in computer vision and machine 
learning can be found in [ 16]. Here, we will focus on the ap -
proaches proposed in the remote sensing literature. This type 
of adaptation is often done by relative normalization methods, 
i.e., methods that do not provide physical units as an  output 
but that instead provide similarly distributed digital numbers. 
Their aim is to make the data 
distri  butions more similar 
across the domains to train a 
single model that can simul -
taneously  classify the source 
and target domains.
In general, a data represen -
tation transformation with the aim of making data sources more similar should have the 
following desirable properties:
 ◗The method should be able 
to align unpaired data (see 
the “Unpaired” column in Table 1), which allows the 
alignment of noncoregistered data (not even imaging the same location) or data with different spatial resolutions.
 ◗The method should be able to align data of different dimen -
sionality (see the “ D Dimensionality” column in Table 1) to 
allow multisource classification. 
 ◗The method should be able to align several domains at the same time (see the “Multisource” column in Table 1) 
to enhance multitemporal adaptation instead of pair -
wise adaptation.
 ◗The method should be able to align in a nonlinear 
way (see the “Nonlinear” column in Table 1), since the −0.65 −0.645 −0.64 −0.635 −0.630.090.0920.0940.0960.0980.1
−∆P
−0.65 −0.645 −0.64 −0.635 −0.630.090.0920.0940.0960.0980.1
−∆P
657075808590
556065707580
(a) (b)
Figu Re 5. The Pareto front estimated using a multiobjective genetic algorithm for the selection of six features. Each dot corresponds to a 
feature set minimizing (1). The color indicates the OA on (a) the source test set Ts and (b) the target test set Tt, according to the reported 
color scale bar (adapted from [30]).
tHe aim o F Da met HoDS 
iS to a Dapt a c LaSSiFieR 
tRaine D on t He SouRce 
Domain to ma Ke 
pReDiction S on  tHe 
taRget Domain .
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
                                           ieee Geoscience and remote sensin G ma Gazine    june 201648 
 transformation between domains can be nonlinear be -
cause of atmospheric or illumination effects.
 ◗The method should be able to use labeled information 
from the source domain when available (see the “Labels 
in s” column in Table 1). A discriminative transform tends 
to align the data sets better because it aligns the data ac -
cording to the semantic classes required by the user.
 ◗The method should avoid being forced to use labeled 
information in all domains (see the “No labels in t” 
column in Table 1), as labels might not be available in 
all domains or their acquisition might have a high cost, 
typically through terrestrial campaigns (see the “Adap -
tation of the Classifier by Active Learning” section).
Several methods have been proposed in recent remote sensing literature. We provide a brief review in this sec -
tion and a summary in Table 1 . Depending on the specific 
situation, the analyst can use this table to select the most suitable  approach.
Most of the recent literature focuses on feature-extrac -
tion strategies, where the extracted features align the data spaces with each other. In that space, the same classifier (or regressor) can be applied to all the domains. Beyond 
the works dealing with traditional or multidimensional 
histogram matching [ 33] or data alignment with principal 
component analysis (PCA) or kernel PCA (KPCA) [ 34], 
the authors of [35] propose to minimize the statistical distance between domains, which is assessed through a kernel-based dependence estimator, the maximum mean 
discrepancy (MMD) [ 18]. Other studies still focus on fea -
ture extraction, but based on multiview models. In [ 36], 
Nielsen aligns domains with canonical correlation analy-sis (CCA) and performs change detection therein. The ap -
proach is extended to a kernel and semisupervised version 
in [37], where the authors perform change detection with 
different sensors. In [ 38], the domains are matched in a la -
tent space defined through an eigenproblem aiming at pre -
serving label (dis)similarities and the geometric structure 
of the single manifolds. A nonlinear (kernelized) version of the algorithm is proposed in [ 39], where the approach 
is particularly appealing because it can align an arbitrary 
number of domains of different dimensionality, as do CCA and kernel CCA (KCCA), but without requiring paired 
examples.  However, it has the disadvantage of requiring 
labeled samples in all domains. 
In [40], the authors relax this requirement by working 
on semantic ties, i.e., samples issued from the same object 
but whose class is unknown. This last method therefore requires at least a partial overlap between the images to 
find the ties, either manually or by stereo matching, as in 
[41]. The authors in [ 42] regularize the manifold alignment 
(MA) solution with spatial information, leading to a more 
stable feature representation transfer. In [ 43], they propose 
a multiscale approach, considering the preservation of 
both local and global geometric characteristics and relying 
on clustering pairs rather than labeled correspondences. 
Other recent methods rely on eigendecompositions, 
such as those proposed in [ 44] and [45 ]. In [ 44], two PCA 
eigensystems (i.e., one for the source domain and another 
for the target domain) are aligned by minimizing their di -
vergence. In [ 45], the authors consider a sparse represen -
tation approach where they reduce the difference between domains again by minimizing the MMD. In both [ 44] 
and [45 ], the authors aim to transfer category models that 
are learned on landscape views to aerial views from very high-resolution remote sensing images. In [ 46], the authors 
propose a set of techniques based on sample reweighing 
and transformation to address different DA situations. The 
study also offers a causal interpretation of the different forms of domain shift. The adaptation strategies are devel -
oped on the basis of the embedding of sample distributions in the reproducing kernel Hilbert space.
Beyond classical feature extraction, the authors in [ 47] 
align multitemporal sequences based on a measure of simi -
larity between sequence barycenters, which corresponds to a global alignment of the spectra in a time series of images. 
In [48], the authors consider spatial shifts in large image TABLE 1. Th E REPRESENTATION ALIgN mENT m EThODS USED IN REmOTE SENSINg. 
mEThOD LABELS IN s NO LABELS IN t mULTISOURCE  UNPAIRED  D DImENSIONALITY NONLINEAR  
Pca # { # { # #
KPca [34] # { # { # {
(ss)tca [35] # { { # { # {
cca [ 36] # { # { { # { #
Kcca  [37] # { # { { # { {
ma [43] { { { { { #
ssma [ 38] { # { { { #
Kernel method for  
manifold ali Gnment [ 39] { # { { { {
Gm [50] # { # { # #
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
june 2016    ieee Geoscience and remote sensin G ma Gazine                                                        49 
acquisitions, in which the spectra are spatially detrended 
using Gaussian processes to avoid shifts related to local -
ized class variability. In [ 49], the authors perform anom -
aly detection by a sparse discriminative transform that maximizes the distance between the anomaly class and 
the background classes (defined as a set of endmembers) 
and minimizes the distance between the source and target distributions after reduction by PCA. In [ 50], the authors 
consider the domains as multidimensional graphs and propose to align the domains by solving a graph-matching problem. Finally, the authors in [ 51] find a multispectral 
mapping between source and target spectra to project the labeled pixels of the source domain into the target domain. Tie points are found between the labeled source pixels and 
the pixels in the target by registration, and then the map -
ping between the source and target is learned by regression 
between the corresponding pairs. Then the labeled pixels 
are projected into the target domain and are used to train 
a classifier therein. As for [ 40], partial overlap between the 
images is required.
As one can see in Table 1 , some methods will be more 
suitable than others, depending on the problem. For exam -
ple, canonical correlation-based methods can be used only 
for coregistered data, while nonmultiview methods such as 
KPCA and (semisupervised) transfer component analysis [(SS)TCA] cannot align more than two domains at a time. 
In the following, we compare a series of methods on 
the challenging problem of transferring a classifier over a multiangular sequence of images over Rio de Janeiro [ 52], 
illustrated in Figure 6 . More details on this example can be 
found in [ 38]. The images are not coregistered but are all 
acquired from a single pass of the WorldView2 sensor. For 
this reason, the only shifts observed are due to angular ef -
fects. The problem is an 11-classes problem, and a separate 
ground truth is provided per each image ( Table 2 ).
The adaptation experiment is designed by taking the 
nadir image (off-nadir angle 
.)609 i=% as the source im -
age and using all the others as target images. We apply 
the PCA, KPCA, graph matching (GM), and semisuper -
vised manifold alignment (SSMA) transforms and then 
train a classifier using 100 labeled pixels from the source domain and predict all of the target domains using that 
classifier, without further modifications. For PCA, KPCA, and GM, the adaptation is done for each target domain separately, while, for SSMA, a single adaptation projec -
tion is obtained for all domains at once. For SSMA, we 
also used 50 labeled pixels per class from each target do -
main. To be fair in the evaluation, the projections for the 
PCA, KPCA, and GM methods are obtained in an un -
supervised way, but then the classifier is trained using the original training points from the nadir acquisition, stacked to the transformed labeled pixels of the domain 
to be tested. We also add a best-case scenario, where we 
directly use labeled samples from the target domains for the classification.  The results are illustrated in Figure 7 .
The prediction in the off-nadir images using the origi -
nal training samples from the nadir image leads to poor results, especially for strong off-nadir angles. All of the 
methods considered leverage the decrease in performance 
and lead to a quasi-flat prediction surface (meaning that the model can predict correctly, regardless of the angu -
lar configuration) with particularly good performance by the SSMA method, which seems to best align the data TABLE 2. Th E NUmBER Of LABELED PIxELS AVAILABLE fOR  
EACh DATA SET IN Th E mULTIANgULAR ExPERI mENTS   
(i = Off-NADIR ANgLE).
CLASS   i38.79-%29.16-%6.09%26.76%39.5%
Water 83,260 79,937 66,084 63,492 54,769 
Grass 8,127 8,127 8,127 8,127 8,127
Pools 244 244 223 195 195 
trees 4,231 4,074 3,066 3,046 3,046
concrete 707 719 719 719 696 
Bare soil 790 790 790 790 811
asphalt 2,949 2,949 2,949 2,827 2,827 
Gray  
buildings 6,291 6,061 5,936 4,375 4,527 
red  
buildings 1,147 1,080 1,070 1,046 1,042 
White buildings 1,683 1,683 1,571 1,571 1,571 
shadows 1,829 1,056 705 512 525 
tarmac 5,179 5,179 5,179 2,166 2,758 
−38.79° −29.16° 6.09° 26.76° 39.5°
Figu Re 6. The five images of the Rio de Janeiro angular sequence [52]. 
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
                                           ieee Geoscience and remote sensin G ma Gazine    june 201650 
distributions. This is not surprising, since, among the test -
ed methods, SSMA is the only one with a clear discrimina -
tive component that uses labels in all domains to define 
the projections.
ADAPTING CLASSIFIERS WITH  
SEMISUPERVISED APPROACHES
A widely used approach to DA is based on adaptation of 
the model of the classifier derived on the source domain 
to the target domain. In the literature, the approach is de -
fined as semisupervised if this adaptation is based only on 
unlabeled samples of the target domain (no target train -
ing samples are used). The rationale of semisupervised ad -
aptation is to use the relations between the distributions of the source and target domains to infer a reliable solu -
tion to the problem described in the target domain. The 
common assumption of most of the methods is that the 
source and target domains share the same set of classes 
and features.
The first attempts to address semisupervised DA in re -
mote sensing image classification were presented in [ 53], 
where a DA technique is proposed that updates the parame -
ters of an already trained parametric maximum-likelihood 
(ML) classifier on the basis of the distribution of a new im -
age for which no labeled samples are available. In [ 54], the 
ML-based DA technique is extended to the framework of the Bayesian rule for cascade classification (i.e., the classifi -
cation process is performed by jointly considering informa -
tion contained in the source and the target domains). The basic idea in both methods is modeling the observed spaces 
by a mixture of distributions, whose components can be es -
timated by the use of unlabeled target data. This is achieved by using the expectation maximization (EM) algorithm with the finite Gaussian mixture model. In [ 55] and [56 ], 
DA approaches based on multiple-classifier and multiple-cascade–classifier architectures were defined. Gaussian ML classifiers, radial basis function neural networks, and 
hybrid cascade classifiers are used as base classifiers. The 
decision tree update and randomization are used for DA in [57]. A set of decision trees is made robust in the face of 
data set shift either by training it with the EM using den -
sity functions from the target domain (similar to [ 54]) or 
by randomizing the decision trees (but, in this case, with -
out control of the adaptation objective). The authors also propose a semisupervised extension where the classifiers performing poorly on the few labeled samples in the target 
domain are downweighted in the final decision. Finally, the 
DA technique proposed in [ 20] for multitemporal images 
addresses the challenging situation where the source and 
target domains have a different set of classes. The sets of 
classes of the target and the source domains are automati -
cally identified in the DA step by the joint use of unsuper -
vised change detection and the Jeffries–Matusita statistical distance measure. This process results in the detection of classes that appeared or disappeared between the domains.
The semisupervised problem has also been extensively 
studied in the framework of kernel methods with SVM classifiers, which has been done especially for addressing 
sample selection bias problems (see the “Transfer Learn -
ing and Domain Adaption” section). Most of the semisu -
pervised techniques proposed with SVM exploit the clus -
ter assumption, i.e., adapt the position of the hyperplane 
estimated on the source domain to the target domain, assuming that it should be located in low-density regions −38.79 −29.16 6.09 26.76 39.50.20.30.40.50.60.70.80.9
Acquisition AngleKappa
Best Case (Labels from All Images)
Training with Image at 6.09° OnlyPCA KPCA
GM SSMALDA−38.79 −29.16 6.09 26.76 39.50.20.30.40.50.60.70.80.9
Acquisition AngleKappa
SVM
Figu Re 7. The classification results over the five Rio de Janeiro acquisitions (adapted from [38]). There are 100 labeled samples per class 
from the nadir image used to train a classifier and then to test on the others. In the SSMA experiment, 50 labeled pixels per class are used 
from the other acquisitions. 
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
june 2016    ieee Geoscience and remote sensin G ma Gazine                                                        51 
of the feature space. In [ 58], the authors employ the trans -
ductive SVM, a method that iteratively moves the deci -
sion boundary of the SVM classifier toward low-density 
areas of the unlabeled target domain. Other semisuper -
vised approaches are imported in remote sensing in [ 59], 
where the SVM semisupervised learning is addressed in the primal formulation of the cost function. In [ 60], the 
authors regularize the SVM solution by adding a new 
term in the optimization accounting for the divergence 
between source and target domains (i.e., the MMD dis -
cussed in the “Adapting Data Distributions” section). By 
doing so, the decision function selected depends on a ker -
nel that both projects in a discriminative space and mini -
mizes the shift between training and test data. In [ 61], 
the authors cast the DA problem as a multitask learning 
problem, where each source–domain pair (i.e., each task) is solved by deforming the kernel by sharing information 
among the tasks. 
The Laplacian SVM technique applied to the classifica -
tion of multispectral remote sensing images is presented in [62], which exploits an additional regularization term on 
the geometry of both the labeled and the unlabeled sam -
ples by using the Laplacian graph. In [ 63], the authors also 
use a manifold-regularized classifier in a semisupervised 
setting, where the adaptation is performed by adding semi -
labeled examples from the target domain.
A specific semisupervised SVM that is defined for ad -
dressing DA problems is presented in [ 64]. The domain ad -
aptation SVM (DASVM) starts from a standard supervised 
learning on the training samples of the source domain, 
which is followed by an iterative procedure. At each itera -
tion, it includes in the learning cost function a subset of 
unlabeled samples of the target domain adequately select -
ed while gradually removing the training samples of the 
source domain. At convergence, the DASVM can accurately 
classify the samples of the target domain.ADAPTATION OF THE CLASSIFIER  
BY ACTIVE LEARNING In most of the previously mentioned approaches, it is as -
sumed that no label information can be obtained in the 
newly acquired target domains (i.e., semisupervised DA). 
This assumption may hinder the success of classification in 
the case of very strong deformations or when new classes that are unseen during training appear in the test data. A 
small amount of labeled data issued from the target do -
main may solve this problem efficiently. However, since 
the acquisition is timely and can be costly, it becomes man -
datory to choose the samples well. AL strategies have been 
proposed to tackle this challenging task and guide the DA process with the selection of the most informative target 
samples [19], [27], [28], [ 65]–[67 ].
AL is the name of a set of methodologies aiming at the 
interaction between a user and a prediction model, where the user provides labels based on knowledge of the task 
to be solved and the model performs the prediction and highlights samples for which it has the highest uncertainty 
[68]. By focusing on these samples, the user provides the 
labels where they help the most and, thus, allows the clas -
sifier to migrate in a fast way toward the optimal model. Surveys on AL methods applied to remote sensing can be 
found in [ 69]–[71 ]. In the case of DA, the user provides 
examples coming from the target domain alone, and the 
optimal classifier is the one that would have been obtained 
with several examples in the target domain. The process starts with a classifier that is optimal for the source do -
main and gradually evolves to model the data distribution in the target domain. Figure 8 summarizes the AL process 
for DA.
One could apply classical AL strategies in transfer-learn -
ing problems under the sample selection bias assumption with successful results, because classical AL will point out 
samples close to the current decision boundaries, and the 
00.511.522.53
Target Domain
xiyiUncer tain
ResponseProvides
LabelUserSource Domain
(XS, YS)
(XT)ClassifierClassification
Confidence
Classification Map
Figu Re 8. A flow chart of the AL paradigm for DA. 
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
                                           ieee Geoscience and remote sensin G ma Gazine    june 201652 
user, by the labels provided, will disclose the shifted areas 
where the next iterations will focus. However, depending 
on the degree of transformation between the domains, one 
can use more sophisticated strategies that take into account measures of the deformation between the domains. In this 
respect, the following problems have driven DA-related re -
search in AL:
 ◗When it is expected that new classes will appear in 
the target domain, AL can be used to highlight the 
areas of the feature space where these classes could 
be. In [ 19], by using the reasoning of sample selection 
bias, the feature space in the target domain is screened 
using clustering, and dense clusters with no labeled 
samples are presented to the user, who can then pro -
vide labels if new classes are present. In [ 72], the 
detection of new classes is set as a change-detection problem where an uncertainty of changes is assessed with an information theoretic criterion. Image time 
series are analyzed in [ 21].
 ◗When significant differences between source and target 
domains are expected (i.e., when the sample selection 
bias assumption does not hold), the presence of labeled 
source samples, although beneficial at the beginning of the process, can be harmful for the classification of the 
target domain [ 27], [28 ] (see the examples in the “Trans -
fer Learning and Domain Adaptation” section and 
Figure 3). If the class distributions in the target domain 
overlap with those in the source domain, relying on the 
labels from the source will lead to classifier errors. Ac -
cordingly, the approaches in [ 27], [28], and [67 ] consider 
reweighting of the samples in the training set enriched by AL. When samples from the source domain become less relevant or misleading for the correct classification of the target domain, they are downweighted in the adapted classifier or completely removed. Accordingly, 
the classifier specializes to the target domain through 
the inclusion of target samples and gradually forgets the initial source domain.
 ◗When the areas to be processed become very large, specific solutions must be designed to avoid too many 
iterations of the AL process. In this respect, solutions 
based on the selection of clusters [ 73], compressed sens -
ing [ 74], or geographically distributed search strategies 
[75]–[78 ] have been considered.
In the following, we focus on one example related to the 
second point above, i.e., the reweighting of source samples. 
This example is adapted from [ 67]. We study the feasibil -
ity of the migration of a model optimized for land-cover 
mapping in a geographical area to another spatially dis -
joint region. To do so, we consider the well-known Ken -
nedy Space Center (KSC) hyperspectral image acquired by the airborne visible/infrared imaging spectrometer 
[Figure 9(a) and (b)] and try to adapt the model learned 
therein to be accurate in a spatially disjoint section of the same flightline [Figure 9(c) and (d)]. We consider only the 
ten classes present in all images. The starting model is 
learned using a training set composed of 50 labeled pix -
els per class and is then enriched by new samples that are 
either added randomly or using the breaking ties AL strat -
egy [ 79]. The classifier is an SVM, either standard (when no 
other mention is done) or adaptive using the TrAdaBoost model, which is a DA method based on the reweighting 
of the SVM sample weights after the inclusion of the new  
labeled points from the target domain [ 66].
When using the source SVM without adaptation, we 
reach an OA lower than 65%, while the results obtained by an SVM that is trained directly on the target labeled sam -
ples (which are available for testing) would provide an ac -
curacy of 90% ( Figure 10 ). Here, the shift is clearly visible 
and relates to a loss in accuracy of 25%. Using a random 
sampling in the target domain, we get a constant increase 
in performance [shown in Figure 10(a) by the green line with * markers]; however, after 300 queries, we are still 5% 
away from the classifier learned using only 500 samples 
from the target domain. Moreover, the learning rate is slow and the gain is almost linear with the number of queries. 
We then assess different DA strategies. 
First, TrAdaBoost is applied to the set enriched by the 
random samples [shown in Figure 10(a) by the brown line with 
# markers]. By forgetting the source domain, i.e., by 
downweighting the source samples that are contradictory with respect to the new samples from the target domain, 
we already see a significant improvement that fills half of the gap between the best case and the random sampling. 
However, when using AL (shown by the blue line with dia -
mond markers) and even more when using it in conjunction 
with the TrAdaBoost model (shown by the black line with 
circle markers), the learning rate is much higher in the first 
iterations, which means the first queries are much more 
(a) (b)
(c) (d)
Figu Re 9. KSC data used in the AL DA experiment: (a) the source 
image, (b) the source GT, (c) the target image, and (d) the target GT 
(not available). 
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
june 2016    ieee Geoscience and remote sensin G ma Gazine                                                        53 
effective than in the random sampling experiments, and 
the model converges to the result obtained with 500 ran -
dom target queries (shown by the solid blue line) with only 250 active queries, corresponding to a total of 750 samples in the model since we still have the 500 initial samples from 
the source. Figure 10(b)  shows the percentage of the sup -
port vectors from each domain with nonzero weights. In 
the target domain, this share increases constantly [shown 
in Figure 10(b) by the solid blue line], while it stabilizes for 
the samples from the source to 40% of the original train -
ing samples (shown by the dashed red line). This means 
that the importance of the source in the model is strongly 
reduced in the first iterations and then remains constant, while each new sample from the target becomes immedi -
ately important and receives a strong weight from the boost -
ed SVM classifier.
guiDeLineS FoR cH ooSing  
tHe aDaptation  StRateg Y
In this section, we will first provide guidelines for the selec -
tion of the most appropriate adaptation strategy and then 
discuss the issue of the validation of the adapted models.
HOW TO CHOOSE 
In the previous sections, we presented different approach -
es to DA that were grouped into four families. Depending 
on the problem considered, an analyst can favor one or 
another. However, there are some guidelines that should be taken into account that will depend mainly on the data 
available (e.g., the sensors to be used) and on the effort 
already provided by the analyst (e.g., whether a classifier is already available or if labels in the target domain are 
available or can be acquired easily). The guidelines are 
as follows:  ◗If the data to be used are acquired by different sensors, they are associated with different feature spaces. In this 
case, only heterogeneous DA (i.e., methods that allow 
for aligning spaces of different dimensionality) should be considered. Accordingly, multiview feature-representation 
transfer methods such as CCA and KCCA or MA (see the 
“Adapting Data Distributions” section) are the recom -
mended choices. 
 ◗If a classifier trained on the source is already available and the effort of training is considerable, methods of 
the third and fourth families (i.e., the adaptation of 
classifiers and the adaptation by selective sampling, respectively) should be preferred. These methods 
build on the model that is already defined on the 
source domain, while those of the two other families imply the definition of a new classifier that is success -
ful in all domains.
 ◗Whenever it is possible to acquire new labeled samples in the target, it should be done. There is no better way 
to correct for a data set shift than by having examples of 
the class-conditional distribution in the target. The AL and MA methods are to be preferred in that case.
 ◗The level of data set shift the methods can cope with goes along with their level of flexibility. Representation 
methods relying on labeled samples from the target can 
cope with strong nonlinear deformations because they allow for a kind of feature registration between the do-
mains, while those that do not use target samples (e.g., 
PCA, TCA, and GM) are successful only if the data distri-butions are already prealigned and have not undergone 
drastic shifts, such as the cases where the signature of a 
target class becomes identical with one of the others in the source. Among the unlabeled methods, the differenc-
es in their flexibility should be considered, going from 500 550 600 650 700 750 800 8506065707580859095
Number of Pixels in Training SetOA (%)Source Labeled
Target Labeled
Random Sampling
TrAdaBoost
Traditional AL
TrAdaBoost + AL
0 5 10 15 20 25 30 350102030405060708090100
AL Iteration
(a) (b)Alphas (%)% Source Norm. Alphas > 0.02
% Target Norm. Alphas > 0.2
Figu Re 10. The AL results over the KSC data of Figure 9 (adapted from [67]). (a) The learning rates (OA) for different methods. (b) The 
percentage of source (dashed red line) and target (solid blue line) a weights larger than 0.02 (source) and 0.2 (target) along the iterative AL 
process in the TrAdaBoost + AL experiment, which is shown by the black line in (a).
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
                                           ieee Geoscience and remote sensin G ma Gazine    june 201654 
linear global methods (e.g., PCA representation transfer) 
to local methods (e.g., based on clustering, as with GM 
and MA). If the first can address only rotations, transla-
tions, and to some extent scalings of the data clouds, the others can model the per-sample transformation and al-
low more flexibility of the transform. The same type of 
reasoning holds for semisupervised methods, which will be able to correct for smaller shifts than methods based 
on AL. When deployed in a DA setting, AL methods can 
collect target labeled samples that provide evidence of the real target class distributions, while the semisuper-
vised method uses only unlabeled data in the target and, 
therefore, cannot easily discover drastic changes in the class distributions.
 ◗A combination of methods from different families is also possible. For example, selecting invariant features 
can be a preprocessing step to kernel MA, where the la -
bels in the target domain have been acquired by AL us -
ing the labels from the source domain. 
With these simple guidelines in mind, the analyst can select 
the most appropriate strategy (or combine a series of them) according to the considered data and application.
HOW TO VALIDATE
A typical bottleneck for the employment of an adaptation 
strategy is the validation of the adaptation process itself, 
since it is assumed that no (or only few) labeled data are available for the target domain. Nonetheless, one should 
assess whether the adaptation was successful in the pro -
cessing of the target image, even though no labeled sam -
ples are available for such validation. To address this 
crucial issue, a circular vali -
dation strategy is presented 
and applied to remote sens -
ing images in [ 64]. The strat -
egy is based on the idea that an intrinsic structure relates 
the solutions that are consis -
tent with the source and the 
target domains. A solution 
for the target domain, for which no prior information 
is available, is assumed to be 
consistent if the solution to the source-domain data is as -
sociated with an acceptable accuracy. The solution to the 
source-domain data should be obtained by applying the 
same DA algorithm in the reverse sense, i.e., by using the 
classification labels in place of missing prior knowledge for target-domain instances. The source-domain data is 
considered as unlabeled in the reverse DA learning, and 
the accuracy of the source-domain data can be evaluated due to the available true labels for source-domain sam -
ples. This strategy can be effective for both understanding if the adaptation is feasible in the considered data set and 
selecting the most effective strategy.
conc LuSionS
In this article, we reviewed the recent DA advances for re -
mote sensing image analysis. DA is a rising field of investi -
gation in remote sensing, as it answers the need for reusing 
available ground reference samples to classify or further 
process new image acquisitions that may be covering dif -
ferent areas, at different time instants, and possibly with 
different sensors. The increasing satellite-data availability 
trend observed in the last few years (in particular, thanks to satellite constellations such as the Sentinels or the NASA 
A-Train) and the commercialization of drone-mounted 
cameras have pushed these problems to the forefront of re -
searchers’ and analysts’ priorities.
We have reviewed the recent models proposed in the lit -
erature, which were grouped in four main families: 1) the approaches based on the selection of invariant features, 
2) those based on the matching of the data representation, 
3) those based on the adaptation of the classifier trained on the source domain, and 4) those based on limited but 
effective sampling of labeled samples in the target domain. 
With practical examples, we have provided the reader with a thorough introduction to the field and some guidelines 
for the selection of the approaches to use in real applica -
tion scenarios.
We believe that DA is of the highest importance to fu -
ture Earth observation since multimodality and repeated 
imaging have become unavoidable [ 7]. The data are already 
there, and new, challenging problems can now be tackled 
with remote sensing. The discipline has succeeded in enter -
ing many new sectors of society, and it is now time to pro -
vide the tools to the users to perform a trustable monitor -
ing that can be obtained in different sensor configurations 
or modalities. We think that DA and machine learning in general can contribute to providing an answer to this call.
autHoR in FoRmation
Devis t uia (devis.tuia@geo.uzh.ch) received a diploma in 
geography at the University of Lausanne (UNIL) in 2004, 
a master of advanced studies degree in environmental en-
gineering at the Federal Institute of Technology of Laus-anne (EPFL) in 2005, and a Ph.D. degree in environmental 
sciences at UNIL in 2009. Subsequently, he worked as a 
visiting postdoctoral researcher at the University of Valen-cia, Spain, and at the University of Colorado, Boulder. He 
then worked as a senior research associate at EPFL under a 
Swiss National Foundation (SNF) program. Since 2014, he has been an SNF assistant professor in the Department of 
Geography at the University of Zurich. His research inter-
ests include the development of algorithms for informa-tion extraction and data fusion of remote sensing images 
using machine-learning algorithms. He serves as chair of 
the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. He is Da iS a R iSing F ieLD o F 
inVeStigation  in Remote  
SenSing, aS it an SweRS 
tHe nee D FoR ReuSing 
aVaiLaBLe gRounD 
ReFeRence  Samp LeS to  
cLaSSiFY o R FuRtHeR 
pRoceSS new image  
acQuiSitionS t Hat  
maY Be coVeRing 
DiFFeRent aReaS.
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
june 2016    ieee Geoscience and remote sensin G ma Gazine                                                        55 
an associate editor of IEEE Journal of Selected Topics in Ap-
plied Earth Observation and Remote Sensing. He is a Senior 
Member of the IEEE. 
claudio persello  (c.persello@utwente.nl) received the 
Laurea (B.S.) and Laurea Specialistica (M.S.) degrees in telecommunications engineering and the Ph.D. degree in 
communication and information technologies from the University of Trento, Italy, in 2003, 2005, and 2010, respec -
tively. He is currently an assistant professor in the Faculty of Geo-Information Science and Earth Observation, Uni -
versity of Twente, The Netherlands. In 2011–2014, he was 
a Marie Curie research fellow, conducting research activity 
at the Max Planck Institute for Intelligent Systems and the Remote Sensing Laboratory of the University of Trento. His 
main research interests are the analysis of remote sensing 
data, machine learning, image classification, pattern recog -
nition, and unmanned aerial vehicles. He is a referee for 
multiple journals, including IEEE Transactions on Geoscience 
and Remote Sensing , Pattern Recognition Letters , and Image and 
Vision Computing Journal . He served on the Scientific Com -
mittee of the Sixth International Workshop on the Analysis of Multitemporal Remote Sensing Images. He is a Member of the IEEE. 
Lorenzo Bruzzone  (lorenzo.bruzzone@disi.unitn.it)  
received the Laurea (M.S.) degree in electronic engineering 
(summa cum laude) and the Ph.D. degree in telecommu -
nications from the University of Genoa, Italy, in 1993 
and 1998, respectively. He is currently a full professor of 
telecommunications at the University of Trento, Italy. His 
current research interests are in the areas of remote sens -
ing, radar and synthetic aperture radar, signal processing, 
and pattern recognition. He is the principal investigator of the Radar for Icy Moon Exploration instrument in the framework of the Jupiter Icy moons Explorer mission of 
the European Space Agency. He is the author (or coauthor) 
of 180 scientific publications in refereed international journals (129 in IEEE journals), more than 250 papers in 
conference proceedings, and 20 book chapters. He is the 
editor-in-chief of IEEE Geoscience and Remote Sensing Maga -
zine and is an associate editor of other journals. He is a 
Fellow of the IEEE.
ReFeRence S
[1] R. de Jong, S. de Bruin, A. de Wit, M. E. Schaepman, and D. L. 
Dent, “Analysis of monotonic greening and browning trends 
from global NDVI time-series,” Remote Sens. Environ. , vol. 115, 
no. 2, pp. 692–702, 2011. 
[2] F. Pacifici, N. Longbotham, and W. Emery, “The importance of 
physical quantities for the analysis of multitemporal and mul -
tiangular optical very high spatial resolution images,” IEEE 
Trans. Geosci. Remote Sens. , vol. 52, no. 10, pp. 6241–6256, 
2014. 
[3] J. J. Walker, K. M. de Beurs, R. H. Wynne, and F. Gao, “Evalu -
ation of Landsat and MODIS data fusion products for analysis 
of dryland forest phenology,” Remote Sens. Environ. , vol. 117, 
pp. 381–393, Feb. 2012.  [4] W. Liao, X. Huang, F. Van Collie, A. Gautama, W. Philips, H. Liu, 
T. Zhu, M. Shimoni, G. Moser, and D. Tuia, “Processing of ther -
mal hyperspectral and digital color cameras: Outcome of the 2014 data fusion contest,” 
IEEE J. Sel. Topics Appl. Earth  Observ. , 
vol. 8, no. 6, pp. 2984–2996, 2015. 
[5] J. Amorós-López, L. Gómez-Chova, L. Alonso, L. Guanter, R. Zurita-Milla, J. Moreno, and G. Camps-Valls, “Multitemporal 
fusion of Landsat/TM and ENVISAT/MERIS for crop monitor -
ing,” 
Int. J. Appl. Earth Obs. Geoinfo. , vol. 23, pp. 132–141, 
Aug. 2013. 
[6] I. Walde, S. Hese, C. Berger, and C. Schmullius, “From land cov -
er-graphs to urban structure types,” Int. J. Geogr. Info. Sci. , vol. 
28, no. 3, pp. 584–609, 2014. 
[7] L. Gómez-Chova, D. Tuia, G. Moser, and G. Camps-Valls, “Mul -
timodal classification of remote sensing images: A review and future directions,” 
Proc. IEEE , vol. 103, no. 9, pp. 1560–1584, 
2015.
[8] J. Verrelst, L. Alonso, J. P. R. Caicedo, J. Moreno, and G. Camps-
Valls, “Gaussian Process retrieval of chlorophyll content from 
imaging spectroscopy data,” IEEE J. Sel. Topics Appl. Earth 
 Observ. , vol. 6, no. 2, pp. 867–874, 2013.
[9] J. Behmann, J. Steinrücken, and L. Plümer, “Detection of early plant stress responses in hyperspectral images,” 
ISPRS J. Int. 
Soc. Photo. Remote Sens. , vol. 93, pp. 98–111, July 2014. 
[10] M. Dalponte, L. Bruzzone, and D. Gianelle, “Tree species clas -
sification in the Southern Alps based on the fusion of very high 
geometrical resolution multispectral/hyperspectral images and 
LiDAR data,” Remote Sens. Environ. , vol. 123, pp. 258–270, 
Aug. 2012. 
[11] M. D. Fleming, J. S. Berkebile, and R. M. Hoffer, “Computer-aid -
ed analysis of LANDSAT-I MSS data: A comparison of three ap -
proaches, including a ‘modified clustering’ approach,” Purdue 
University, West Lafayette, IN, Lab. Applicat. Remote Sensing  
Inform. Note 072475, 1975.
[12] I. Olthof, C. Butson, and R. Fraser, “Signature extension through 
space for northern landcover classification: A comparison of ra -
diometric correction methods,” Remote Sens. Environ. , vol. 95, 
no. 3, pp. 290–302, 2005. 
[13] J. Quiñonero-Candela, M. Sugiyama, A. Schwaighofer, and N. D. 
Lawrence, Eds., Dataset Shift in Machine Learning , Cambridge, 
MA: MIT Press, 2008.
[14] S. J. Pan and Q. Yang, “A survey on transfer learning,” IEEE 
Trans. Know. Data Eng. , vol. 22, no. 10, pp. 1345–1359, 2010.
[15] K. Saenko, B. Kulis, M. Fritz, and T. Darrell, “Adapting visual 
category models to new domains,” in Proc. 11th Europ. Conf. 
Comput. Vision , Heraklion, Greece, 2010, pp. 213–226.
[16] V. M. Patel, R. Gopalan, R. Li, and R. Chellappa, “Visual domain adaptation: A survey of recent advances,” 
IEEE Signal Processing 
Mag. , vol. 32, no. 3, pp. 53–69, 2015.
[17] M. Sugiyama and M. Kawanabe, Machine Learning in Non-sta -
tionary Environments , Cambridge, MA: MIT Press, 2012.
[18] G. Matasci, N. Longbotham, F. Pacifici, M. Kanevski, and D. Tuia, “Understanding angular effects in VHR imagery and their 
significance for urban land-cover model portability: A study of 
two multi-angle in-track image sequences,” 
ISPRS J. Int. Soc. 
Photo. Remote Sens. , vol. 107, pp. 99–111, Sept. 2015.
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
                                           ieee Geoscience and remote sensin G ma Gazine    june 201656 
[19] D. Tuia, E. Pasolli, and W. J. Emery, “Using active learning to 
adapt remote sensing image classifiers,” Remote Sens. Environ. , 
vol. 115, no. 9, pp. 2232–2242, 2011.
[20] K. Bahirat, F. Bovolo, L. Bruzzone, and S. Chaudhuri, “A novel  
domain adaptation Bayesian classifier for updating land- 
cover maps with class differences in source and target 
 domains,” IEEE Trans. Geosci. Remote Sens. , vol. 50, no. 7, 
pp. 2810–2826, 2012. 
[21] B. Demir, F. Bovolo, and L. Bruzzone, “Updating land-cover maps by classification of image time series: A novel change-  
detection-driven transfer learning approach,” 
IEEE Trans. Geosci. 
Remote Sens. , vol. 51, no. 1, pp. 300–312, 2013. 
[22] G. Jun and J. Ghosh, “Semisupervised learning of hyperspectral 
data with unknown land-cover classes,” IEEE Trans. Geosci. Re -
mote Sens. , vol. 51, no. 1, pp. 273–282, 2013. 
[23] J. J. Heckman, “Sample selection bias as a specification error,” 
Econometrica , vol. 47, no. 1, pp. 153–161, 1979.
[24] B. Zadrozny, “Learning and evaluating classifiers under sample selection bias,” in 
Proc. 21st Int. Conf. on Machine Learn. , 
Banff, Canada, 2004, p. 114.
[25] J. Huang, A. Gretton, B. Schölkopf, A. J. Smola, and K. M. Borg -
wardt, “Correcting sample selection bias by unlabeled data,” in 
Proc. 21st Annu. Conf. Neural Inform. Proc. Syst. . Vancouver, 
Canada, 2007, pp. 601–608. 
[26] C. Persello and L. Bruzzone, “Active and semisupervised learn -
ing for the classification of remote sensing images,” IEEE Trans. 
Geosci. Remote Sens. , vol. 52, no. 11, pp. 6937–6956, 2014. 
[27] C. Persello and L. Bruzzone, “Active learning for domain ad -
aptation in the supervised classification of remote sensing  
images,” IEEE Trans. Geosci. Remote Sens. , vol. 50, no. 11,  
pp. 4468–4483, 2012.
[28] C. Persello, “Interactive domain adaptation for the classification 
of remote sensing images using active learning,” IEEE Geosci. 
Remote Sens. Lett. , vol. 10, no. 4, pp. 736–740, 2013.
[29] E. Izquierdo-Verdiguier, V. Laparra, L. Gómez-Chova, and G. 
Camps-Valls, “Encoding invariances in remote sensing image  
classification with SVM,” IEEE Geosci. Remote Sens. Lett. ,  
vol. 10, no. 5, pp. 981–985, 2013.
[30] L. Bruzzone and C. Persello, “A novel approach to the 
 selection of spatially invariant features for the classifica -
tion of  hyperspectral images with improved generalization 
 capa bility,” IEEE Trans. Geosci. Remote Sens. , vol. 47, no. 9,  
pp. 3180–3191, 2009. 
[31] C. Persello and L. Bruzzone, “Kernel-based domain invariant feature selection in hyperspectral images for transfer learning,” 
IEEE Trans. Geosci. Remote Sens , vol. 54, no. 5, pp. 2615–2626, 
2016. 
[32] J. Ham, Y. Chen, M. M. Crawford, and J. Ghosh, “Investigation of the random forest framework for classification of hyperspec -
tral data,” 
IEEE Trans. Geosci. Remote Sens. , vol. 43, no. 3,  
pp. 492–501, 2005. 
[33] S. Inamdar, F. Bovolo, L. Bruzzone, and S. Chaudhuri, “Mul -
tidimensional probability density function matching for pre- 
processing of multitemporal remote sensing images,” IEEE 
Trans. Geosci. Remote Sens. , vol. 46, no. 4, pp. 1243–1252, 
2008. [34] A. Nielsen and M. J. Canty, “Kernel principal component and 
maximum autocorrelation factor analyses for change detection,” 
Proc. SPIE , vol. 7477, 74770T, 2009.  
[35] G. Matasci, M. Volpi, M. Kanevski, L. Bruzzone, and D. Tuia, “Semisupervised transfer component analysis for domain 
adaptation in remote sensing image classification,” 
IEEE 
Trans. Geosci. Remote Sens. , vol. 53, no. 7, pp. 3550–3564, 
2015.
[36] A. A. Nielsen, “The regularized iteratively reweighted MAD 
method for change detection in multi- and hyperspectral 
data,” IEEE Trans. Image Proc. , vol. 16, no. 2, pp. 463–478, 
2007. 
[37] M. Volpi, G. Camps-Valls, and D. Tuia, “Spectral alignment of 
cross-sensor images with automated kernel canonical correla -
tion analysis,” ISPRS J. Int. Soc. Photo. Remote Sens. , vol. 107, 
pp. 50–63, Sept. 2015. 
[38] D. Tuia, M. Volpi, M. Trolliet, and G. Camps-Valls, “Semisu -
pervised manifold alignment of multimodal remote sensing 
images,” IEEE Trans. Geosci. Remote Sens. , vol. 52, no. 12,  
pp. 7708–7720, 2014.
[39] D. Tuia and G. Camps-Valls, “Kernel manifold alignment  
for domain adaptation,” PLoS One , vol. 11, p. e0148655, 
Feb. 2016.
[40] D. Marcos Gonzalez, G. Camps-Valls, and D. Tuia, “Weakly su -
pervised alignment of multisensor images,” in Proc. IEEE Int. 
Geosci. and Remote Sens. Symp (IGARSS) , Milan, Italy, 2015, 
pp. 2588–2591.
[41] J. Montoya-Zegarra, C. Leistner, and K. Schindler, “Semantic 
tie points,” in Proc. IEEE Workshop Applic. Comput. Vision 
(WACV) , Clearwater Beach, FL, 2013, pp. 377–384.
[42] H. L. Yang and M. M. Crawford, “Spectral and spatial proximity-
based manifold alignment for multitemporal hyperspectral im -
age classification,” IEEE Trans. Geosci. Remote Sens. , vol. 54, no. 
1, pp. 51–64, 2016. 
[43] H. L. Yang and M. M. Crawford, “Domain adaptation with pres -
ervation of manifold geometry for hyperspectral image classi -
fication,” IEEE J. Sel. Topics Appl. Earth Observ. , vol. 9, no. 2,  
pp. 543–555, 2016. 
[44] H. Sun, S. Liu, S. Zhou, and H. Zou, “Unsupervised cross-view 
semantic transfer for remote sensing image classification,” IEEE 
Geosci. Remote Sens. Lett. , vol. 13, no. 1, pp. 13–17, 2016. 
[45] H. Sun, S. Liu, S. Zhou, and H. Zou, “Transfer sparse subspace 
analysis for unsupervised cross-view scene model adaptation,” 
IEEE J. Sel. Topics Appl. Earth Observ. , 2016. DOI: 10.1109/
JSTARS.2015.2500961
[46] K. Zhang, B. Schölkopf, K. Muandet, Z. Wang, Z. Zhou, and C. 
Persello, “Single-source domain adaptation with target and con -
ditional shift,” in Regularization, Optimization, Kernels, and 
Support Vector Machines , J. A. K. Suykens, M. Signoretto, and A. 
Argyriou, Eds. Boca Raton, FL: CRC, 2014.
[47] F. Petitjean, A. Ketterlin, and P. Gancarski, “A global averaging method for dynamic time warping, with applications to cluster -
ing,” 
Pattern Recogn. , vol. 44, no. 3, pp. 678–693, 2011. 
[48] G. Jun and J. Ghosh, “Spatially adaptive classification of land 
cover with remote sensing data,” IEEE Trans. Geosci. Remote 
Sens. , vol. 49, no. 7, pp. 2662–2673, 2011. 
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
june 2016    ieee Geoscience and remote sensin G ma Gazine                                                        57 
[49] L. Zhang, L. Zhang, D. Tao, and X. Huang, “Sparse transfer mani -
fold embedding for hyperspectral target detection,” IEEE Trans. 
Geosci. Remote Sens. , vol. 52, no. 2, pp. 1030–1043, 2014. 
[50] D. Tuia, J. Muñoz-Marí, L. Gómez-Chova, and J. Malo, “Graph 
matching for adaptation in remote sensing,” IEEE Trans. Geosci. 
Remote Sens. , vol. 51, no. 1, pp. 329–341, 2013. 
[51] E. Othman, Y. Bazi, N. Alajlan, H. AlHichri, and F. Melgani, “Three-layer convex network for domain adaptation in multitemporal  
VHR images,” 
IEEE Geosci. Remote Sens. Lett. , vol. 13, no. 3, 2016. 
[52] F. Pacifici, J. Chanussot, and Q. Du, “2011 GRSS Data Fusion Contest: Exploiting WorldView-2 multi-angular acquisitions,” in 
Proc. IEEE Int. Geosci. and Remote Sens. Symp (IGARSS) , Van -
couver, Canada, 2011, pp. 1163–1166.
[53] L. Bruzzone and D. F. Prieto, “Unsupervised retraining of a 
maximum-likelihood classifier for the analysis of multitemporal 
remote-sensing images,” IEEE Trans. Geosci. Remote Sens. , vol. 
39, no. 2, pp. 456–460, 2001. 
[54] L. Bruzzone and D. F. Prieto, “A partially unsupervised cascade 
classifier for the analysis of multitemporal remote-sensing im -
ages,” Pattern Recogn. Lett. , vol. 23, no. 9, pp. 1063–1071, 2002. 
[55] L. Bruzzone and R. Cossu, “A multiple cascade-classifier system 
for a robust a partially unsupervised updating of land-cover  
maps,” IEEE Trans. Geosci. Remote Sens. , vol. 40, no. 9,  
pp. 1984–1996, 2002. 
[56] L. Bruzzone, R. Cossu, and G. Vernazza, “Combining paramet -
ric and non-parametric algorithms for a partially unsupervised classification of multitemporal remote-sensing images,” 
Info. 
Fusion , vol. 3, no. 4, pp. 289–297, 2002. 
[57] S. Rajan, J. Ghosh, and M. M. Crawford, “Exploiting class hier -
archy for knowledge transfer in hyperspectral data,” IEEE Trans. 
Geosci. Remote Sens. , vol. 44, no. 11, pp. 3408–3417, 2006. 
[58] L. Bruzzone, M. Chi, and M. Marconcini, “A novel transduc -
tive SVM for semisupervised classification of remote-sensing 
images,” IEEE Trans. Geosci. Remote Sens. , vol. 44, no. 11,  
pp. 3363–3373, 2006. 
[59] M. Chi and L. Bruzzone, “Semi-supervised classification of hyperspectral images by SVMs optimized in the primal,” 
IEEE 
Trans. Geosci. Remote Sens. , vol. 45, no. 6, pp. 1870–1880, 
2007. 
[60] Z. Sun, C. Wang, H. Wang, and J. Li, “Learn multiple-kernel SVMs for domain adaptation in hyperspectral data,” 
IEEE Geos -
ci. Remote Sens. Lett. , vol. 10, no. 5, pp. 1224–1228, 2013. 
[61] J. M. Leiva-Murillo, L. Gómez-Chova, and G. Camps-Valls, “Mul -
titask remote sensing data classification,” IEEE Trans. Geosci. 
Remote Sens. , vol. 51, no. 1, pp. 151–161, 2013. 
[62] L. Gómez-Chova, G. Camps-Valls, J. Muñoz-Marí, and J. Calpe-
Maravilla, Semi-supervised image classification with Laplacian 
support vector machines,” IEEE Geosci. Remote Sens. Lett. ,  
vol. 5, no. 3, pp. 336–340, 2008. 
[63] W. Kim and M. M. Crawford, Adaptive classification for hyperspec-tral image data using manifold regularization kernel machines,” 
IEEE 
Trans. Geosci. Remote Sens , vol. 48, no. 1 1, pp. 41 10–4121, 2010. 
[64] L. Bruzzone and M. Marconcini, “Domain adaptation problems: 
A DASVM classification technique and a circular validation 
strategy,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 32, no. 5,  
pp. 770–787, 2010.[65] G. Jun and J. Ghosh, “An efficient active learning algorithm with knowledge transfer for hyperspectral data analysis,” in 
Proc. 
IEEE Int. Geosci. and Remote Sens. Symp (IGARSS) , Boston, 
MA, 2008, pp. I-52–I-55.
[66] S. Rajan, J. Ghosh, and M. M. Crawford, “An active learning  
approach to hyperspectral data classification,” IEEE Trans. 
Geosci. Remote Sens. , vol. 46, no. 4, pp. 1231–1242, 2008. 
[67] G. Matasci, D. Tuia, and M. Kanevski, “SVM-based boosting of ac -
tive learning strategies for efficient domain adaptation,” IEEE J. 
Sel. Topics Appl. Earth Observ. , vol. 5, no. 5, pp. 1335–1343, 2012. 
[68] B. Settles, Active Learning . San Rafael, CA: Morgan & Claypool, 
2012.
[69] D. Tuia, M. Volpi, L. Copa, M. Kanevski, and J. Muñoz-Marí, “A survey of active learning algorithms for supervised remote sens -
ing image classification,” 
IEEE J. Select Topics Signal Processing,  
vol. 5, no. 3, pp. 606–617, 2011. 
[70] M. M. Crawford, D. Tuia, and H. L. Yang, “Active learning: Any value for classification of remotely sensed data?” 
Proc. IEEE , vol. 
101, no. 3, pp. 593–608, 2013.
[71] L. Bruzzone, C. Persello, and B. Demir, “Active learning methods in classification of remote sensing images,” in 
Signal and Im -
age Processing for Remote Sensing , C. Chen, Ed. Boca Raton, FL: 
CRC, 2012.
[72] B. Demir, F. Bovolo, and L. Bruzzone, “Detection of land-cover transitions in multitemporal remote sensing images with active 
learning based compound classification,” 
IEEE Trans. Geosci. 
Remote Sens. , vol. 50, no. 5, pp. 1930–1941, 2012.
[73] A. Stumpf, N. Lachiche, J. P. Malet, N. Kerle, and A. Puissant, “Active learning in the spatial-domain for landslide mapping in 
remote sensing images,” in 
Proc. Euro. Conf. Machine Learn. 
(ECML) , Active Learning in Real-World Applications Work -
shop, Bristol, U.K., 2012.
[74] M. Roy, F. Melgani, A. Ghosh, E. Blanzieri, and S. Ghosh, “Land-cover classification of remotely sensed images using compressive 
sensing having severe scarcity of labeled patterns,” 
IEEE Geosci. 
Remote Sens. Lett. , vol. 12, no. 6, pp. 1257–1261, 2015.
[75] N. Alajlan, E. Pasolli, F. Melgani, and A. Franzoso, “Large-scale image classification using active learning,” 
IEEE. Geosci. Remote 
Sens. Lett. , vol. 11, no. 1, 259–263, 2014. 
[76] P. Blanchart, M. Ferecatu, S. Cui, and M. Datcu, “Pattern retrieval in large image databases using multiscale coarse-to-fine cascad -
ed active learning,” 
IEEE J. Sel. Topics Appl. Earth Observ. , vol. 
7, no. 4, 1127–1141, 2014.
[77] B. Demir, L. Minello, and L. Bruzzone, “Definition of effec -
tive training sets for supervised classification of remote sensing images by a novel cost-sensitive active learning method,” 
IEEE 
Trans. Geosci. Remote Sens. , vol. 52, pp. 1272–1284, Feb. 2014. 
[78] C. Persello, A. Boularis, M. Dalponte, T. Gobakken, E. Naesset,  
and B. Schölkopf, “Cost-sensitive active learning with lookahead: 
Optimizing field surveys for remote sensing data classification,” IEEE 
Trans. Geosci. Remote Sens. , vol. 52, no. 10, pp. 6652–6664, 2014.
[79] T. Luo, K. Kramer, D. B. Goldgof, L. O. Hall, S. Samson, A. 
Remsen, and T. Hopkins, “Active learning to recognize mul -
tiple types of plankton,” J. Mach. Learn. Res. , vol. 6, 589–613, 
Apr. 2005. 
 grs
Authorized licensed use limited to: ASU Library. Downloaded on March 08,2024 at 03:13:37 UTC from IEEE Xplore.  Restrictions apply. 
Towards Geospatial Foundation Models via Continual Pretraining
Mat´ıas Mendieta1*Boran Han2Xingjian Shi3Yi Zhu3Chen Chen1
1Center for Research in Computer Vision, University of Central Florida
2Amazon Web Services3Boson AI
matias.mendieta@ucf.edu boranhan@amazon.com xshiab@connect.ust.hk
yi@boson.ai chen.chen@crcv.ucf.edu
Abstract
Geospatial technologies are becoming increasingly es-
sential in our world for a wide range of applications,
including agriculture, urban planning, and disaster re-
sponse. To help improve the applicability and performance
of deep learning models on these geospatial tasks, vari-
ous works have begun investigating foundation models for
this domain. Researchers have explored two prominent ap-
proaches for introducing such models in geospatial appli-
cations, but both have drawbacks in terms of limited per-
formance benefit or prohibitive training cost. Therefore,
in this work, we propose a novel paradigm for building
highly effective geospatial foundation models with minimal
resource cost and carbon impact. We first construct a com-
pact yet diverse dataset from multiple sources to promote
feature diversity, which we term GeoPile. Then, we in-
vestigate the potential of continual pretraining from large-
scale ImageNet-22k models and propose a multi-objective
continual pretraining paradigm, which leverages the strong
representations of ImageNet while simultaneously provid-
ing the freedom to learn valuable in-domain features. Our
approach outperforms previous state-of-the-art geospatial
pretraining methods in an extensive evaluation on seven
downstream datasets covering various tasks such as change
detection, classification, multi-label classification, seman-
tic segmentation, and super-resolution. Code is available
athttps://github.com/mmendiet/GFM .
1. Introduction
The significance of geospatial technologies has pro-
gressively increased for various applications worldwide.
Progress in this domain can substantially improve our abil-
ity to understand the earth and how we interact with it. With
the rising popularity of foundation models in vision and nat-
ural language, researchers have begun to investigate apply-
*Work done as an intern at Amazon Web Services
92.5
94.1
95.8
97.4
99.0
30.537.945.252.660.0
43.050.257.564.872.0
81.083.586.088.591.0
68.0
69.9
71.8
73.6
75.50.59
0.6
0.62
0.63
0.6440.0
50.5
61.0
71.5
82.0DSFIN
(Change Detection)
WHU Aerial
(Segmentation)
Vaihingen
(Segmentation)
SpaceNet2
(Super-resolution)BigEarthNet
(Multi-label
Calssification)UC Merced
(Classification)OSCD
(Change Detection)
N/AFigure 1. Our geospatial foundation model (GFM) achieves favor-
able performance on a broad set of tasks in comparison to other
state-of-the-art geospatial pretraining methods (SeCo [28], Sat-
MAE [9]) and ImageNet supervised pretraining baselines. Leg-
end is as follows. Cyan: ImageNet-1k Supervised (ResNet50),
Blue: SeCo [28], Purple: ImageNet-22k Supervised (ViT), Or-
ange: SatMAE [9], Gray: ImageNet-22k Supervised (Swin),
Green: GFM (ours).
ing such principles to the geospatial domain in order to en-
hance the suitability of deep learning models in downstream
tasks [29, 28, 9, 2]. In the literature, various works have ex-
plored two prominent approaches for introducing pretrained
foundation models in geospatial applications. The first ob-
vious approach is to leverage existing foundation models
from the natural image domain, like those trained on the
large-scale ImageNet-22k dataset [11]. In practice, this
is done by directly finetuning publicly-available ImageNet
pretrained models on the downstream tasks . This approach
has the advantage of being straight-forward, as ImageNet
models can be simply downloaded from many open-source
model zoos, and has been shown to be effective [29, 30].
However, due to the domain gap between natural images
and remote sensing, this approach is not optimal for geospa-
tial data, and still leaves performance gains on the table.
In recent years, a second approach has gained significant
traction, where researchers aim to pretrained models spe-
cific to the geospatial domain [28, 2, 9, 38]. These methods
typically train a network from scratch on a large corpus
of remote sensing imagery to learn in-domain representa-
tions transferable to downstream tasks. Unfortunately, this
can require a significant amount of data and training time
to achieve good performance, especially when employing
large state-of-the-art (SOTA) transformer models. For in-
stance, the current SOTA in geospatial foundation models,
SatMAE [9], requires 768 hours on a V100 GPU for train-
ing a vision transformer [14]. This has substantial cost
associated with producing the model, not just in terms of
time and computation but also environmentally, with a to-
tal estimated carbon footprint of 109.44 kg CO 2equivalent.
Additionally, the final performance of such models are not
consistently better across various tasks than simply utilizing
publicly-available ImageNet pretrained models (Section 4),
despite the high resource expense.
In this work, we propose to investigate a different
paradigm for producing more effective geospatial founda-
tion models with substantially less resource costs. First, we
begin with a discussion on pretraining data selection, and
ultimately construct a concise yet diverse collection of data
from various sources to promote feature diversity and ef-
fective pretraining. Second, rather than following the afore-
mentioned typical approaches, we investigate the potential
ofcontinual pretraining for the geospatial domain from
readily-available ImageNet models. Continual pretraining
has been practiced in the NLP domain with success in var-
ious works [16, 17, 26]. In this paradigm, existing founda-
tion models are further improved for a specific domain or
task through a secondary pretraining stage. This new single
model can now be fine-tuned on the various downstream
tasks in that domain. In principle, we reason that such a
paradigm has the potential to boost performance by utiliz-
ing large-scale ImageNet representations as a base on which
stronger geospatial foundation models can be built. Further-
more, such natural image models are constantly being im-
proved and released by the general computer vision commu-
nity, providing a consistent source of better baseline mod-
els. Therefore, an approach that could enable the geospa-
tial domain to leverage these improvements with minimal
resource needs and carbon footprint paves the way for con-
tinual, sustainable benefits for the geospatial community.
However, when we initially experiment with the standard
continual pretraining formulation, we find it provides only
marginal benefits (Section 3.2). Instead, we discover that
utilizing ImageNet representations as an auxiliary distilla-
tion objective during pretraining leads to a stronger geospa-
tial foundation model. Building upon this principle, we pro-pose a multi-objective continual pretraining paradigm that
significantly enhances performance while requiring mini-
mal resources. Our approach leverages ImageNet’s power-
ful representations to facilitate and expedite learning, while
also enabling the acquisition of valuable in-domain features
via self-supervised learning on geospatial data. Further-
more, our proposed Geospatial Foundation Model (GFM)
exhibits strong performance, surpassing previous state-of-
the-art (SOTA) methods across a diverse range of down-
stream tasks (Section 4). Our contributions are as follows:
• We investigate a novel paradigm for creating highly ef-
fective geospatial models with minimal resource costs.
Our methodology begins with data selection and con-
struction of a compact yet diverse dataset from multi-
ple sources to promote feature diversity and enhance
pretraining effectiveness, which we term GeoPile. We
further explore the potential of continual pretraining
from ImageNet models, but find it is not satisfactory in
its standard formulation.
• Therefore, to achieve better performance with minimal
resource needs, we propose a multi-objective contin-
ual pretraining paradigm. Our design is surprisingly
simple yet effective, constructed as a teacher-student
strategy with both a distillation objective and self-
supervised masked image modeling. This approach
allows GFM to leverage the strong representations of
ImageNet to guide and quicken learning, while simul-
taneously providing the freedom to learn valuable in-
domain features.
• We evaluate our GFM approach, as well as several
baseline and SOTA methods, on 7 datasets covering
important geospatial applications such as change de-
tection, classification, multi-label classification, se-
mantic segmentation, and super-resolution. Overall,
our GFM performs favorably over previous methods
(as shown in Figure 1).
2. Related Work
Geospatial Pretraining . Various works have experi-
mented with employing supervised or self-supervised pre-
training paradigms in the geospatial domain. The clas-
sical work of [29], and more recent paper [38], investi-
gate supervised pretraining on individual datasets of various
sizes. Interestingly, these still often found the ImageNet
pretrained models to perform very well, particularly with
vision transformers [14, 25]. Other works have explored
self-supervised learning paradigms for remote sensing, pri-
marily focused on contrastive methods. [28] and [2] employ
a MoCo [7] style objective using spatially aligned but tem-
porally different images as the positive pairs. [23] and [20]
also utilize a MoCo-inspired objective, but specify a crop-
ping procedure to generate positives and negatives within
and across images. [37] employs a colorization objective
on Sentinel-2 imagery utilizing the various spectral bands.
Most recently, SatMAE [9] explores the use of masked im-
age modeling to train a large ViT model. This work is sim-
ilar in some respect to ours, as we also train a transformer
model with an MIM objective. However, we find that Sat-
MAE often does not perform better than the off-the-shelf
ImageNet-22k pretrained ViT (Section 4). This indicates
both the difficulty of building strong geospatial pretrained
models from scratch and highlights the potential usefulness
of leveraging continual pretraining instead, as we investi-
gate in this work.
Masked Image Modeling . Masked image modeling
(MIM) has been proposed in various forms in recent years,
and has recently been found to be particularly effective
in the natural image domain, surpassing many contrastive
works and being shown to be friendlier to downstream op-
timization [41, 18, 44, 3, 40] In general, the goal is to learn
from data in a self-supervised manner by asking the model
to generate pixel values for intentionally-withheld regions
in an image. [32] is an early work with an aim of learning
strong visual representations through inpainting masked re-
gions. In [6], Chen et. al train a large transformer to pre-
dict pixels autoregressively. After the introduction of vi-
sion transformers (ViT) [14], many works continued to im-
prove various MIM variants. [3] and [44] take inspiration
from BERT [12] in natural language processing, and tok-
enize the image patches with either a pretrained model or
jointly trained online tokenizer, with the objective being to
reconstruct at a token-level rather than raw pixels. Recently,
[41] and [18] show that a masked image modeling task of
simply regressing directly on the image pixels is sufficient
and effective. In this work, we leverage the framework from
[41], as it is compatible with hierarchical transformer archi-
tectures [25].
In this work, we develop our pretraining objective based
on a masked image modeling approach like [41, 18]. Explo-
ration of the masked image modeling framework in geospa-
tial applications is still in its early stages, and could help
alleviate some concerns with contrastive approaches in this
domain. Particularly, the choice of augmentations with con-
trastive methods can be quite difficult, as common selec-
tions such as greyscale, color jitter and others that heav-
ily affect the intensity of the image can instill undesirable
invariances [29]. On the other hand, MIM objectives like
[41, 18] rely only on simple spatial augmentations such
as flipping and cropping. Furthermore, a common remote
sensing application is that of change detection, which re-
quires a model to detect changes in two images from the
same location but at different times. In order to still be ef-
fective on this task, works that use contrastive approacheson temporal positives introduce various design choices. For
instance, SeCo [28] creates multiple feature subspaces dur-
ing pretraining, each one invariant to a separate form of aug-
mentation. [1] also employs temporal positives, but instead
chooses the sampling locations for the pretraining data to
ensure that image pairs contain primarily natural illumina-
tion and viewing angle variant, without major changes such
as new urban developments.
Continual Pretraining . Continual pretraining has been
primarily introduced in the natural language domain [16,
17, 26], in order to improve large language models (LLM).
[16] illustrates the viability of two additional stages of
pretraining, using in-domain data (domain-adaptive), and
then even further using task-specific data (task-adaptive).
[17] proposes a continual training paradigm for enabling
temporal reasoning abilities to pretrained language mod-
els. [26] focus on using continual pretraining to enable
mixed language neural machine translation. In the vision
domain, [22] employs a BYOL [15] style continual pre-
training paradigm for 2D medical image segmentation. [34]
explores a hierarchical pretraining approach for task adap-
tation. However, they primarily focus on adapting to a spe-
cific downstream task at a time, employing three training
stages on top of an existing pretrained model for each task
individually. In contrast, we employ one efficient in-domain
pretraining setting that can generalize to many downstream
tasks, as illustrated in Section 4. Furthermore, rather than
directly loading the pretrained weights from existing mod-
els as initialization, we find instead that leveraging the rep-
resentations as an auxiliary distillation objective during the
pretraining process enables learning stronger representa-
tions.
3. Methodology
In the following sections, we discuss the pretraining data
selection (Sec, 3.1), investigate vanilla continual pretraining
(Sec. 3.2), and present our GFM method (Sec. 3.3).
3.1. Pre-training Data Selection
A particularly common choice of source data among
geospatial contrastive pretraining works is Sentinel-2 im-
agery [28, 1, 37] due to its large corpus of available data
and ease of access. Therefore, to begin our study, we first
gather a pretraining dataset of 1.3 million Sentinel-2 im-
ages using the sampling technique from [28]. After gather-
ing the Sentinel-2 data, we employ it to pretrain a Swin-B
[25] model with the masked image modeling (MIM) objec-
tive from [41]. We then finetune and evaluate this model
on a wide variety of downstream datasets to get a broad un-
derstanding of its performance potential in many tasks (see
Section 4 for task details). For a comparison, we finetune
the ImageNet-22k pretrained Swin-B from the official Swin
Transformer repository [25] on all downstream tasks as a
Figure 2. We visualize some example images from the pretraining
datasets with Sentinel-2 (left) and GeoPile (right). Sentinel-2 has
noticeably much lower feature diversity within a single image and
across images than that of our GeoPile pretraining dataset.
baseline. In order to compare these models across all tasks,
we introduce an average relative performance metric (ARP)
in which we take the relative difference on each task with
respect to the ImageNet-22k baseline, and then average that
difference:
ARP (M) =1
NNX
i=1score (M,taski)−score (baseline ,taski)
score (baseline ,taski).
(1)
Here “baseline” is the Swin-B model pretrained on
ImageNet-22k, as mentioned above. Mdenotes the model
for performance evaluation, and N is the number of tasks.
There are 7tasks used in Section 4 covering important
geospatial applications such as classification, multi-label
classification, semantic segmentation, change detection,
and super-resolution. The reported ARP value is scaled by
100 to show as a percentage.
We compare these two models in Table 1. Interest-
ingly, we find that the Sentinel-2 model performs poorly
on downstream tasks compared to the ImageNet-22k base-
line. To investigate further, we visualize multiple samples
from Sentinel-2 in the left columns of Figure 2. Upon in-
spection, we note that the feature diversity within a single
image and across images of Sentinel-2 is perceivably low.
To further quantify this suspicion, we calculate the average
image entropy over a randomly sampled set of 3000 im-
ages from the collected Sentinel-2 data as well as the typ-
ical ImageNet dataset as a baseline. Overall, the Sentinel
images have an average entropy of 3.9 compared to 5.1 of
ImageNet. Such an evaluation provides insights into the
potential pitfalls of Sentinel-2 data in pretraining transform-
ers. For MIM objectives, training data with a substantially
lower entropy can make for an easier reconstruction task,
since masked regions may be more similar to their neigh-
bors. Therefore, the network does not have to work as hard
to fill in the blanks, limiting the learning potential. Overall,
these result indicate that the noticeably narrow scope of fea-Table 1. Dataset Analysis. To evaluate each method, we finetune
the pretrained model on seven different tasks, outlined in Section
4 and report the ARP metric defined in Equation 1. We also report
the training time in hours on a V100 GPU, as well as the car-
bon impact estimations2in kg CO 2equivalent [24]. Overall, our
collected GeoPile pretraining dataset significantly improves down-
stream performance. †indicates the vanilla continual pretrain-
ing approach of initializing the model with ImageNet-22k weights
prior to conducting MIM training on GeoPile. To further improve
the performance in an efficient manner, we introduce our continu-
ous pretraining paradigm GFM.
Method # Images Epochs ARP ↑Time↓CO2↓
ImageNet-22k Sup. 14M - 0.0 - -
Sentinel-2 [28] 1.3M 100 -5.83 155.6 22.2
GeoPile 600k 200 0.92 133.3 19.0
GeoPile†600k 200 1.24 133.3 19.0
GeoPile†600k 800 1.45 533.2 76.0
GFM 600k 100 3.31 93.3 13.3
Table 2. Breakdown of datasets in the GeoPile. We gather approxi-
mately 600k samples from a combination of labeled and unlabeled
satellite imagery with various ground sample distances and scenes.
Dataset # Images GSD # Classes
NAIP [31] 300,000 1m n/a
RSD46-WHU [27] 116,893 0.5m - 2m 46
MLRSNet [33] 109,161 0.1m - 10m 60
RESISC45 [8] 31,500 0.2m - 30m 45
PatternNet [45] 30,400 0.1m - 0.8m 38
tures and limited per-sample information in Sentinel-2 data
may be limiting the potential of the pretrained model.
Therefore, we set out to collect a diverse geospatial pre-
training dataset. Sourcing from both labeled and unlabelled
data, we form a new pretraining dataset which we term
GeoPile. The breakdown of GeoPile is shown in Table 2.
For textural detail, we ensure a variety of ground sample
distances (GSD), including images with much higher reso-
lution than Sentinel-2 (which has a GSD of 10m). Further-
more, the selected labeled datasets encompass a wide vari-
ety of classes from general remote sensing scenes, ensuring
visual diversity across samples. We calculate the average
entropy of our GeoPile dataset, and find it to be 4.6, much
higher than that of Sentinel-2. Furthermore, the textural and
visual diversity is qualitatively evident in Figure 2. In Table
1, the enhancing effect of the data selection is clearly shown
by the substantial performance increase.
3.2. Vanilla Continual Pretraining
Next, after establishing our pretraining data selection, we
investigate an alternate pretraining paradigm that bridges
the gap between the two common approaches mentioned
2CO2estimations were completed with mlco2.github.io from [24].
ℱ𝑇
ℱ𝑆𝒫…
…… 𝒟ℒ𝑓𝑒𝑎𝑡
GeoPile
𝑓𝐿𝑇𝑃(𝑓𝐿𝑆)
ℒ𝑀𝐼𝑀
……
Foundation 
ModelModel Zoo
Large -scale 
Dataset…
Figure 3. Our GFM continual pretraining pipeline, which leverages publicly-available large-scale models in concert with our compiled
geospatial dataset and pretraining objective. First, we select a concise set of data from various sources, which we term GeoPile (Section
3.1). Next, we train GFM with our multi-objective continual pretraining approach. Our GFM framework is constructed as a teacher-student
paradigm, with two parallel model branches. The teacher FTis initialized with ImageNet-22k weights (top) and frozen during training.
The student FSis initialized from random initialization (bottom), and is trained to serve as the final geospatial foundation model. In a
continual pretraining fashion, we leverage the intermediate features of an ImageNet-22k pretrained model to guide and quicken learning.
Furthermore, we build in an MIM objective on the student branch to learn valuable in-domain features directly from the geospatial data.
in Section 1. Specifically, we investigate the potential of
continual pretraining in the context of geospatial pretrained
models. To do so, we first employ the vanilla continual pre-
training approach; that is, using the ImageNet-22k weights
as initialization prior to beginning the pretraining step with
GeoPile. We find this to be helpful in improving perfor-
mance over starting from scratch. This validates the pos-
sibility of continual pretraining as a beneficial paradigm to
provide performance gain without additional resource costs.
Nonetheless, the improvement is still limited, with ∼0.3%
ARP increase over starting from scratch and ∼1.24% ARP
over the baseline.
To further improve the performance of our pretrained
model in comparison to the ImageNet-22k baseline, we in-
crease the number of pretraining epochs in the next row of
Table 1. While we are able to make improvements, this
comes at the cost of substantially more computational cost
and carbon footprint for marginal gain. Therefore, we ask
the question: how can we significantly improve the per-
formance further while maintaining minimal compute and
carbon footprint overhead? To this end, we propose a sim-
ple and efficient approach for building geospatial pretrained
models capable of strong downstream performance.
3.3. GFM Pretraining
A significant number of geospatial foundation model
studies disregard the existing large-scale model represen-
tations. This is far from ideal, particularly for large trans-
former models known to require a vast amount of data and
compute power to train. Instead, we reason that the valu-
able knowledge available in models like those trained on
ImageNet-22k should be leveraged to produce strong per-formance with minimized overhead. To this end, we pro-
pose an unsupervised multi-objective training paradigm for
effective and efficient pretraining of geospatial models, il-
lustrated in Figure 3.
There are two main components in our framework. First,
we randomly initialize an encoder FSand decoder Dset up
for MIM as in [41]. During training, the input is randomly
masked, and the network attempts to reconstruct the image
at the output. This MIM objective is enforced with an L1
loss [41]:
LMIM =∥Oκ−Gκ∥1
N, (2)
whereOκare the original pixel values from κmasked re-
gions,Gκare the generated reconstructions for those re-
gions, and Nis the total number of masked pixels.
For the continual pretraining of our framework, we ini-
tialize a second encoder branch FTup to a chosen stage L
and load the ImageNet-22k pretrained weights. This branch
behaves as a form of teacher during the training process
to the student branch ( FS), which will serve as our final
model. For the ImageNet teacher, we freeze the weights,
to both ensure that the structured representations are main-
tained during the training process, and also reduce the com-
putation required during optimization.
Rather than using the masked input as in the student
branch, the teacher receives the unmasked image as input,
and provides a feature output fT
Lat stage L. This feature
has access to the full context of the input, enabling it to
capture informative representations. We utilize this feature
to guide the representations of the student, and form a sec-
ondary objective with the cosine similarity between branch
features:
Lfeat =−P(fS
L)

P(fS
L)

2·fT
L

fT
L

2, (3)
where fS
LandfT
Lare the intermediate features of the stu-
dent and teacher branches at stage L, andPis an linear
projection layer. Therefore, the final loss during training is
simply the summation of these objectives:
L=LMIM +Lfeat. (4)
This training paradigm enables an ideal two-fold optimiza-
tion. Distillation from the intermediate features of the
teacher ensure that the student can benefit from the teacher’s
diverse knowledge, learning more in less time. Further-
more, the student is simultaneously given freedom to adapt
to in-domain data through its own pretraining objective,
gathering new features to improve performance.
We analyze the ARP and resource cost of this approach
in Table 1. Notably, our GFM is able to achieve better over-
all performance with substantially less computation and
emissions impact compared to vanilla continual pretraining
with the same dataset, illustrating that our multi-objective
continual pretraining paradigm is an effective method for
training these models. Comparatively, the SOTA geospa-
tial pretrained method SatMAE [9] requires 768 hours on a
V100 GPU and 109.44 kg equivalent CO 2according to their
reported results. Therefore, GFM enables more than 8×
reduction in total training time and carbon impact. More-
over, we find that the performance of SatMAE is often not
superior to the off-the-shelf ImageNet-22k pretrained ViT
(Section 4). This implies that building powerful geospatial
pretrained models from scratch is challenging and further
underscores the benefits of utilizing continual pretraining
instead. We show these results in the following section.
4. Experiments
To verify the effectiveness of our model in detail, we
conduct experiments on seven geospatial datasets of vari-
ous tasks including change detection (Section 4.1), classifi-
cation (Section 4.2), segmentation (Section 4.3), and super-
resolution (Section 4.4).
For pretraining, we employ 8 NVIDIA V100 GPUs with
a batch size of 2048 (128 per GPU) and the image size
of 192×192. All pretraining settings are the same as in
[41]. For downstream tasks, 4 NVIDIA A10G GPUs are
employed. During the pretraining stage, we utilize RGB
bands as they are most commonly available among data
sources and tasks. For downstream tasks with additional
band inputs, we initialize the RGB patch embeddings with
the pretrained weights and randomly initialize the remain-
ing channels. Potentially improving performance even fur-
ther though the employment of additional data modalitiesTable 3. Onera Satellite Change Detection Results
Method Precision ↑Recall ↑F1↑
ResNet50 (ImageNet-1k) [19] 70.42 25.12 36.20
SeCo [28] 65.47 38.06 46.94
MATTER [1] 61.80 57.13 59.37
ViT (ImageNet-22k) [14] 48.34 22.52 30.73
SatMAE [9] 48.19 42.24 45.02
Swin (random)[25] 51.80 47.69 49.66
Swin (ImageNet-22k)[25] 46.88 59.28 52.35
GFM 58.07 61.67 59.82
Figure 4. Qualitative results of downstream performance on OSCD
comparing our GFM with ImageNet-22k and randomly initialized
baselines. White, green, red colors show true positive, false posi-
tive, and false negative respectively.
Table 4. DSFIN Change Detection Results
Method Precision ↑Recall ↑F1↑
ResNet50 (ImageNet-1k) [19] 28.74 92.07 43.80
SeCo [28] 39.68 81.02 53.27
ViT (ImageNet-22k) [14] 70.77 66.34 68.49
SatMAE [9] 70.45 60.29 64.98
Swin (random)[25] 57.97 62.06 59.94
Swin (ImageNet-22k)[25] 67.11 72.33 69.62
GFM 74.83 67.98 71.24
will be an intriguing avenue for future research. Additional
training details for these tasks are provided in the supple-
mentary material .
4.1. Change Detection
Change detection is a particularly important remote sens-
ing task, helping us understand how humans interact with
our planet over time, and natural phenomena that change
our planet’s landscape. We conduct experiments on both
the Onera Satellite Change Detection (OSCD [5]) in Table
3 and DSIFN [43] in Table 4.
OSCD consists of 14 image pairs extracted from various
regions around the world within a three year period of 2015
to 2018. The images are taken from Sentinel-2 with GSDs
ranging from 10m to 60m, and split into 14 images for train-
ing and 10 for evaluation. The annotations indicate whether
the change has occurred on a pixel level, and focus primarily
on urban developments. Similarly, we also test our method
on DSIFN dataset. This dataset contains high-resolution
imagery, such as WorldView-3 and GeoEys-1 [43]. This
dataset contains 3490 high resolution samples for training
and 48 images for evaluation respectively. Every pair of im-
ages from a given location at two different timestamps will
be fed into the swin encoder [25] for feature extraction. The
difference between the features from each pair is computed
and fed into an UPerNet [39] to generate the final binary
segmentation masks [28, 4]. The encoder is initialized with
the pretrained weights.
For both datasets, we report the precision, recall, and
F1 score on the “change” class. As the results presented
from OSCD (Table 3 and Figure 4) and DSIFN (Table 4),
GFM shows a consistent improvement over the ImageNet-
22k baseline across both datasets. Notably, SatMAE is able
to improve over its ImageNet-22k baseline on OSCD, but
lags behind on DSIFN. This further highlights the difficulty
of training large vision transformers from scratch that can
perform consistently across different GSDs.
4.2. Classification
Another common remote sensing application is that of
classification. We evaluate two datasets common in the
literature [28, 1]: UC Merced Land Use Dataset [42] and
BigEarthNet [36]. The UC Merced Land Use Dataset is a
classic dataset in the remote sensing field. It contains 21
classes, each with 100 images at 256x256 pixels and an ap-
proximate GSD of 1 foot. We split the data into train and
validation according to [13]. BigEarthNet [36] (BEN) is a
large-scale remote sensing dataset for multi-label classifi-
cation. The data consist of 12-band Sentinel-2 images with
sizes of 120x120, 60x60, and 20x20 pixels for the bands at
10m, 20m, and 60m GSDs, respectively. We employ the
data split and 19 class evaluation as common in the litera-
ture [29, 28, 9].
In Table 5, we report the classification accuracy on
UC Merced (UCM) and mean average precision results
on BigEarthNet (BEN) for all methods. On UC Merced,
we note the SeCo [28] pretrained model performs signif-
icantly worse than its ImageNet-1k pretrained counterpart
with ResNet-50. These two datasets are very different in
both classes, satellite source, and GSDs, and therefore hav-
ing a diverse feature knowledge is imperative to maintain-
ing performance despite these distinctions. Our model can
provide robust performance in both cases by leveraging Im-
ageNet representations and remote sensing data in its learn-
ing. Furthermore, one key motivation for training a geospa-
tial foundation model is to improve the sample efficiency forTable 5. UC Merced classification accuracy and BigEarthNet
multi-label classification mean average precision results.
Method UCM BEN 10% BEN 1%
ResNet50 (ImageNet-1k) [19] 98.8 80.0 41.3
SeCo [28] 97.1 82.6 63.6
ViT (ImageNet-22k)[14] 93.1 84.7 73.6
SatMAE [9] 92.6 81.8 68.9
Swin (random)[25] 66.9 80.6 65.7
Swin (ImageNet-22k) [25] 99.0 85.7 79.5
GFM 99.0 86.3 80.7
downstream tasks. Notably, we find that our model main-
tains strong performance on BigEarthNet, even when only
given 1% of the training data.
4.3. Segmentation
Segmentation is a popular remote sensing application for
enabling automated extraction of building footprints or land
cover mappings over wide regions. We therefore conduct
experiments on this task on two different datasets. Vai-
hingen [35] is an urban semantic segmentation dataset col-
lected over Vaihingen, Germany at a GSD of 0.9m. We
employ the data split implemented in the MMSegmenta-
tion library [10] for our experiments, with 344 training and
398 for validation, all with an image size of 512x512 pix-
els. The WHU Aerial building [21] dataset is sampled over
Christchurch, New Zealand at a GSD of 0.3m. Image tiles
are provided at 512×512pixels, split into 4736 for training
and 2416 for evaluation.
We report the intersect of union (IoU) segmentation re-
sults for all methods in Table 6. ImageNet pretrained mod-
els are notably strong performers in all cases. On both
datasets, SeCo lags substantially behind its ImageNet coun-
terpart. Interestingly, SatMAE is able to bring improvement
over ImageNet-22k on WHU, but fails to do so to a larger
degree on Vaihingen. However, our approach is able to
leverage the already strong ImageNet-22k representations
and guide them towards the geospatial domain, resulting in
overall improvement.
4.4. Super-resolution
In the previous experiments, we evaluated several com-
mon high-level tasks. Nonetheless, the low-level task of
super-resolution is also important in the geospatial domain.
For this task, we re-purpose the SpaceNet2 dataset, which
contains 10,593 8-band images from four cities: Las Ve-
gas, Paris, Shanghai, and Khartoum. The data is provided
at both a GSD of 1.24m (multi-spectral, 162x162 pixels)
and 0.3m (pan-sharpened multispectral, 650x650 pixels).
We formulate a super-resolution task, taking as input the
1.24m multi-spectral images and generating the 0.3m pan-
sharpened equivalent. We evaluate the super-resolution per-
Table 6. Results on the WHU Aerial and Vaihingen segmentation
datasets. We finetune all methods for 40k iterations, and report the
IoU for the building class on WHU and mean IoU (mIoU) across
the 6 classes (impervious surface, building, low vegetation, tree,
car, clutter) of Vaihingen.
Method WHU Aerial Vaihingen
ResNet50 (ImageNet-1k) [19] 88.5 74.0
SeCo [28] 86.7 68.9
ViT (ImageNet-22k) [14] 81.6 72.6
SatMAE [9] 82.5 70.6
Swin (random) [25] 88.2 67.0
Swin (ImageNet-22k) [25] 90.4 74.7
GFM 90.7 75.3
Table 7. SpaceNet2 Super-resolution Results. Notably, while
SatMAE fails to enhance its baseline (ViT ImageNet-22k), our
method exhibits substantial improvement over its respective base-
line (Swin ImageNet-22k) in both PSNR and SSIM.
Method PSNR ↑SSIM↑
ViT (ImageNet-22k)[14] 23.279 0.619
SatMAE [9] 22.742 0.621
Swin (random) [25] 21.825 0.594
Swin (ImageNet-22k) [25] 21.655 0.612
GFM 22.599 0.638
formance of our model and several baselines with the peak
signal-to-noise ratio (PSNR) and structural similarity in-
dex measure (SSIM) in Table 7. The ViT-L ImageNet-22k
model and our model are among the best in terms of PSNR
and SSIM, respectively. Interestingly, SatMAE is not able
to improve over its baseline. On the other hand, our method
improves considerably over its ImageNet-22k baseline.
5. Ablation Studies
We perform multiple ablation studies on the choice of
distillation stage, student initialization, training objectives,
the pretraining dataset components. Further detailed results
and discussions are provided in the supplementary material .
5.1. Distillation Stage
When implementing our feature map distillation objec-
tive, a natural question is at which point should the map-
ping take place. We experiment with different locations
by stage in the Swin transformer and calculate the corre-
sponding ARP in Figure 5. Overall, performing the dis-
tillation after Stage 3 yields the highest ARP. Hence, we
employ this scheme for all downstream experiments. This
result is also intuitively expected; distilling at Stage 3 gives
a large portion of the model the supervisory signal from the
teacher, while still allowing for purely domain-specific fea-
ture learning in the final layers.
(a) (b)
ARPFigure 5. a) Distillation stage ablation results. b) Student initializa-
tion ablation results. “Both” indicates that the teacher and student
branches are initialized with ImageNet weights prior to geospatial
pretraining. “Teacher” indicates that just the teacher branch is ini-
tialized, as described in Section 3.3.
Table 8. GeoPile pretraining dataset ablation. We remove each
dataset individually from GeoPile and report the number of im-
ages remaining and resulting ARP. The row “w/o curated datasets”
removes all data other than NAIP imagery.
Data # Images ARP ↑
w/o WHU-RSD46 444,061 1.77
w/o MLRSNet 451,793 2.17
w/o Resisc45 529,454 1.57
w/o PatternNet 557,554 1.79
w/o curated datasets 300,000 0.53
w/o NAIP 260,954 1.50
5.2. Student Initialization
In our proposed framework, we maintain the teacher
model frozen with ImageNet pretrained weights, and ran-
domly initialize the student. Another alternative is to initial-
ize the student also with ImageNet weights prior to begin-
ning the geospatial pretraining process. However, as shown
in Figure 5, this is not the most optimal option. Such ini-
tialization is unnecessary in our framework, since it already
allows for seamless integration of ImageNet representations
with valuable in-domain features. Forcibly doing so likely
introduces too much bias towards the natural image repre-
sentations. Therefore an unbiased student is most ideal and
effective.
5.3. GeoPile Pretraining Dataset
To ablate components of the GeoPile, we remove each
dataset individually to see its relative importance. Also,
we compare using just the labeled data portion and using
just the unlabeled NAIP imagery portion. As expected, us-
ing just data from labeled datasets gives better performance
with less images than using just images gathered from just
NAIP. The human-curated samples in these datasets are
more likely to contain relevant objects and features, as they
each correspond to a particular class of interest. Still, unla-
beled data like NAIP can be sourced easily and with scale.
Further scaling of both labeled and unlabeled portions could
further improve performance; however, it will also increase
the training time and sustainability impact. Therefore, we
maintain GeoPile at approximately 600,000 images.
Table 9. Ablation results for the training objectives in GFM. For w/o teacher, we only conduct MIM with GeoPile. For w/o MIM, we
simply perform the distillation objective from the ImageNet-22k model to our student model with GeoPile. We abbreviate the following
for horizontal space: UC Merced (UCM), BigEarthNet (BEN), WHU Aerial (WHU), Vaihingen (Vai), SpaceNet2 (SN2).
Method OSCD (F1) DSFIN (F1) UCM BEN 10% BEN 1% WHU Vai. SN2 (PSNR) SN2 (SSIM)
w/o teacher 57.3 67.65 98.8 86.5 80.0 90.5 74.0 22.509 0.631
w/o MIM 59.58 71.86 98.8 86.1 80.2 90.2 72.6 22.069 0.608
GFM 59.82 71.24 99.0 86.3 80.7 90.7 75.3 22.599 0.638
Table 10. Results for employing temporal pairs and datasets from SeCo [28] in our multi-objective pretraining framework. TP indicates
that the teacher receives one image from a temporal pair, and the student receives the other. SI indicates that the same image is inputted to
the teacher and student.
Dataset Inputs OSCD (F1) DSFIN (F1) UCM BEN 10% BEN 1% WHU Vai. SN2 (PSNR) SN2 (SSIM)
SeCo 100k [28] TP 57.03 62.48 80.0 80.6 68.6 88.3 66.3 22.078 0.572
SeCo 100k [28] SI 58.41 67.92 92.1 83.9 76.5 88.8 68.1 22.439 0.602
SeCo 1M [28] SI 58.87 69.41 95.7 86.2 77.1 89.6 71.0 22.281 0.626
GeoPile SI 59.82 71.24 99.0 86.3 80.7 90.7 75.3 22.599 0.638
5.4. Multi-objective Ablation.
To delve deeper into the evaluation of GFM’s perfor-
mance, we extend our analysis by conducting experiments
in which we exclude the teacher component and MIM com-
ponent individually, as detailed in Table 9. We find that
training with the multi-objective approach is the best per-
former overall. This shows that the integrated distillation
and MIM objectives within the GFM framework both con-
tribute to producing a well-balanced mode for downstream
tasks, and are important aspects of efficient and effective
geospatial learning.
5.5. Temporal Pairs Experiment
Some works employ temporal pairs in the pretraining
procedure [28, 2, 1], meaning two satellite images from the
same spatial region but taken at different times. We also ex-
periment with the use of temporal positives in our training
paradigm using the dataset proposed in SeCo [28]. In this
case, the teacher receives one image from a temporal pair,
and the student receives the other. The temporal changes
can possibly serve as a form of natural augmentation for the
distillation objective. However, as shown in Table 10, we
find that using temporal positives (TP) is worse than simply
using the same image (SI) for both branches. Therefore, we
simply use the same image for both branches for other ex-
periments. We further scale up the data by employing the
1M sample Sentinel-based dataset from SeCo. Nonethe-
less, GeoPile proves to be more effective as a pretraining
data source for our GFM.
6. Conclusion
In summary, this paper investigates an alternative
paradigm from previous work towards producing better
geospatial foundation models with substantially less re-source cost. To this end, we first construct a concise
yet diverse collection of data from various remote sens-
ing sources for pretraining. Second, we propose a sur-
prisingly simply yet effective multi-objective continual pre-
training paradigm, in which we leverage the strong repre-
sentations of ImageNet-22k to guide and quicken learning,
while simultaneously providing the freedom to learn valu-
able in-domain features through self-supervised learning on
geospatial data. We hope that our GFM approach will serve
as an example to inspire other works in investigating ef-
ficient and sustainable methods for developing geospatial
foundation models.
Broader Impact and Limitations. As the geospa-
tial community continues to innovate, the resulting impact
promises to positively benefit both the earth and society.
Automating the process of extracting useful information
from geospatial data can aid scientists, engineers, and others
to make data-informed decisions on infrastructure advance-
ment, food supply improvements, and natural disaster re-
sponse. A potential limitation of our GFM approach is that
it may still be somewhat constrained by the performance
of the ImageNet-22k model. If perhaps a model was trained
from scratch on an extremely large corpus of remote sensing
data, the performance may eventually also lead to improved
performance over ImageNet baselines. However, this would
incur a substantial amount of training time and CO 2impact.
Furthermore, as mentioned in Section 1, natural image mod-
els are constantly being improved and released by the gen-
eral computer vision community. Therefore, our approach
enables the geospatial domain to effectively leverage these
improvements for better in-domain performance with mini-
mal carbon impact. We believe this is a sustainable way for
the geospatial community to continually benefit from the
most recent progress in computer vision, enabling a smarter,
safer, and healthier planet.
References
[1] Peri Akiva, Matthew Purri, and Matthew J. Leotta. Self-
supervised material and texture representation learning for
remote sensing tasks. CoRR , abs/2112.01715, 2021. 3, 6, 7,
9
[2] Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tan-
may, Marshall Burke, David B. Lobell, and Stefano Er-
mon. Geography-aware self-supervised learning. CoRR ,
abs/2011.09980, 2020. 1, 2, 9
[3] Hangbo Bao, Li Dong, and Furu Wei. Beit: BERT pre-
training of image transformers. CoRR , abs/2106.08254,
2021. 3
[4] Rodrigo Caye Daudt, Bertr Le Saux, and Alexandre Boulch.
Fully convolutional siamese networks for change detection.
In2018 25th IEEE International Conference on Image Pro-
cessing (ICIP) , pages 4063–4067, 2018. 7
[5] R. Caye Daudt, B. Le Saux, A. Boulch, and Y . Gousseau.
Urban change detection for multispectral earth observation
using convolutional neural networks. In IEEE International
Geoscience and Remote Sensing Symposium (IGARSS) , July
2018. 6
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In Hal Daum ´e III and Aarti Singh,
editors, Proceedings of the 37th International Conference
on Machine Learning , volume 119 of Proceedings of Ma-
chine Learning Research , pages 1691–1703. PMLR, 13–18
Jul 2020. 3
[7] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming
He. Improved baselines with momentum contrastive learn-
ing. CoRR , abs/2003.04297, 2020. 2
[8] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-
ing image scene classification: Benchmark and state of the
art.Proceedings of the IEEE , 105(10):1865–1883, Oct 2017.
4
[9] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu,
Erik Rozi, Yutong He, Marshall Burke, David B Lobell, and
Stefano Ermon. Satmae: Pre-training transformers for tem-
poral and multi-spectral satellite imagery. arXiv preprint
arXiv:2207.08051 , 2022. 1, 2, 3, 6, 7, 8
[10] MMSegmentation Contributors. MMSegmentation:
Openmmlab semantic segmentation toolbox and bench-
mark. https://github.com/open-mmlab/
mmsegmentation , 2020. 7
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 1
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 3
[13] Ivica Dimitrovski, Ivan Kitanovski, Dragi Kocev, and Nikola
Simidjievski. Current trends in deep learning for earth obser-
vation: An open-source benchmark arena for image classifi-
cation, 2022. 7[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. CoRR , abs/2010.11929, 2020. 2, 3, 6, 7, 8
[15] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin
Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Do-
ersch, Bernardo ´Avila Pires, Zhaohan Daniel Guo, Mo-
hammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu,
R´emi Munos, and Michal Valko. Bootstrap your own la-
tent: A new approach to self-supervised learning. CoRR ,
abs/2006.07733, 2020. 3
[16] Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta,
Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith.
Don’t stop pretraining: Adapt language models to domains
and tasks. CoRR , abs/2004.10964, 2020. 2, 3
[17] Rujun Han, Xiang Ren, and Nanyun Peng. DEER: A data ef-
ficient language model for event temporal reasoning. CoRR ,
abs/2012.15283, 2020. 2, 3
[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Doll´ar, and Ross B. Girshick. Masked autoencoders are scal-
able vision learners. CoRR , abs/2111.06377, 2021. 3
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. arXiv preprint
arXiv:1512.03385 , 2015. 6, 7, 8
[20] Neal Jean, Sherrie Wang, Anshul Samar, George Azzari,
David B. Lobell, and Stefano Ermon. Tile2vec: Unsuper-
vised representation learning for spatially distributed data.
CoRR , abs/1805.02855, 2018. 2
[21] Shunping Ji, Shiqing Wei, and Meng Lu. Fully convolutional
networks for multisource building extraction from an open
aerial and satellite imagery data set. IEEE Transactions on
Geoscience and Remote Sensing , 57(1):574–586, 2019. 7
[22] Andr ´as Kalapos and B ´alint Gyires-T ´oth. Self-supervised
pretraining for 2d medical image segmentation. arXiv
preprint arXiv:2209.00314 , 2022. 3
[23] Jian Kang, Ruben Fernandez-Beltran, Puhong Duan, Sicong
Liu, and Antonio J. Plaza. Deep unsupervised embedding
for remotely sensed images based on spatially augmented
momentum contrast. IEEE Transactions on Geoscience and
Remote Sensing , 59(3):2598–2610, 2021. 2
[24] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt,
and Thomas Dandres. Quantifying the carbon emissions of
machine learning. arXiv preprint arXiv:1910.09700 , 2019.
4
[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,
Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans-
former: Hierarchical vision transformer using shifted win-
dows. CoRR , abs/2103.14030, 2021. 2, 3, 6, 7, 8
[26] Zihan Liu, Genta Indra Winata, and Pascale Fung. Contin-
ual mixed-language pre-training for extremely low-resource
neural machine translation. CoRR , abs/2105.03953, 2021. 2,
3
[27] Yang Long, Yiping Gong, Zhifeng Xiao, and Qing Liu. Ac-
curate object localization in remote sensing images based on
convolutional neural networks. IEEE Transactions on Geo-
science and Remote Sensing , 55(5):2486–2498, 2017. 4
[28] Oscar Ma ˜nas, Alexandre Lacoste, Xavier Gir ´o-i-Nieto,
David V ´azquez, and Pau Rodr ´ıguez. Seasonal contrast: Un-
supervised pre-training from uncurated remote sensing data.
CoRR , abs/2103.16607, 2021. 1, 2, 3, 4, 6, 7, 8, 9
[29] Maxim Neumann, Andr ´e Susano Pinto, Xiaohua Zhai, and
Neil Houlsby. In-domain representation learning for remote
sensing. CoRR , abs/1911.06721, 2019. 1, 2, 3, 7
[30] Keiller Nogueira, Ot ´avio Augusto Bizetto Penatti, and Je-
fersson Alex dos Santos. Towards better exploiting convo-
lutional neural networks for remote sensing scene classifica-
tion. CoRR , abs/1602.01517, 2016. 1
[31] U.S. Department of Agriculture. National agriculture im-
agery program (NAIP). 4
[32] Deepak Pathak, Philipp Kr ¨ahenb ¨uhl, Jeff Donahue, Trevor
Darrell, and Alexei A. Efros. Context encoders: Feature
learning by inpainting. CoRR , abs/1604.07379, 2016. 3
[33] Xiaoman Qi, Panpan Zhu, Yuebin Wang, Liqiang Zhang,
Junhuan Peng, Mengfan Wu, Jialong Chen, Xudong Zhao,
Ning Zang, and P. Takis Mathiopoulos. Mlrsnet: A multi-
label high spatial resolution remote sensing dataset for se-
mantic scene understanding. ISPRS Journal of Photogram-
metry and Remote Sensing , 169:337–350, 2020. 4
[34] Colorado J Reed, Xiangyu Yue, Ani Nrusimha, Sayna
Ebrahimi, Vivek Vijaykumar, Richard Mao, Bo Li, Shang-
hang Zhang, Devin Guillory, Sean Metzger, et al. Self-
supervised pretraining improves self-supervised pretraining.
InProceedings of the IEEE/CVF Winter Conference on Ap-
plications of Computer Vision , pages 2584–2594, 2022. 3
[35] Franz Rottensteiner, Gunho Sohn, Jaewook Jung, Markus
Gerke, Caroline Baillard, Sebastien Benitez, and Uwe Bre-
itkopf. The isprs benchmark on urban object classification
and 3d building reconstruction. ISPRS Annals of the Pho-
togrammetry, Remote Sensing and Spatial Information Sci-
ences I-3 (2012), Nr. 1 , 1(1):293–298, 2012. 7
[36] Gencer Sumbul, Marcela Charfuelan, Beg ¨um Demir, and
V olker Markl. Bigearthnet: A large-scale benchmark archive
for remote sensing image understanding. In IGARSS 2019-
2019 IEEE International Geoscience and Remote Sensing
Symposium , pages 5901–5904. IEEE, 2019. 7
[37] Stefano Vincenzi, Angelo Porrello, Pietro Buzzega, Marco
Cipriano, Pietro Fronte, Roberto Cuccu, Carla Ippoliti, An-
namaria Conte, and Simone Calderara. The color out of
space: learning self-supervised representations for earth ob-
servation imagery. CoRR , abs/2006.12119, 2020. 3
[38] Di Wang, Jing Zhang, Bo Du, Gui-Song Xia, and Dacheng
Tao. An empirical study of remote sensing pretraining. IEEE
Transactions on Geoscience and Remote Sensing , 2022. 2
[39] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
Jian Sun. Unified perceptual parsing for scene understand-
ing. In European Conference on Computer Vision . Springer,
2018. 7
[40] Zhenda Xie, Zigang Geng, Jingcheng Hu, Zheng Zhang, Han
Hu, and Yue Cao. Revealing the dark secrets of masked im-
age modeling. arXiv preprint arXiv:2205.13543 , 2022. 3
[41] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jian-
min Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim:
A simple framework for masked image modeling. CoRR ,
abs/2111.09886, 2021. 3, 5, 6[42] Yi Yang and Shawn Newsam. Bag-of-visual-words and spa-
tial extensions for land-use classification. In Proceedings of
the 18th SIGSPATIAL international conference on advances
in geographic information systems , pages 270–279, 2010. 7
[43] Chenxiao Zhang, Peng Yue, Deodato Tapete, Liangcun
Jiang, Boyi Shangguan, Li Huang, and Guangchao Liu.
A deeply supervised image fusion network for change de-
tection in high resolution bi-temporal remote sensing im-
ages. ISPRS Journal of Photogrammetry and Remote Sens-
ing, 166:183–200, 2020. 6, 7
[44] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang
Xie, Alan L. Yuille, and Tao Kong. ibot: Image BERT pre-
training with online tokenizer. CoRR , abs/2111.07832, 2021.
3
[45] Weixun Zhou, Shawn Newsam, Congmin Li, and Zhenfeng
Shao. Patternnet: A benchmark dataset for performance
evaluation of remote sensing image retrieval. ISPRS Jour-
nal of Photogrammetry and Remote Sensing , 145:197–209,
2018. Deep Learning RS Data. 4
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1
RSVQA: Visual Question Answering for Remote
Sensing Data
Sylvain Lobry, Member, IEEE, Diego Marcos, Jesse Murray, Devis Tuia, Senior Member, IEEE
This is the pre-acceptance version, to read the ﬁnal version pub-
lished in the journal IEEE Transactions on Geoscience and Remote
Sensing, please go to: https://doi.org/10.1109/TGRS.2020.2988782.
Abstract —This paper introduces the task of visual question
answering for remote sensing data (RSVQA). Remote sensing
images contain a wealth of information which can be useful
for a wide range of tasks including land cover classiﬁcation,
object counting or detection. However, most of the available
methodologies are task-speciﬁc, thus inhibiting generic and easy
access to the information contained in remote sensing data. As
a consequence, accurate remote sensing product generation still
requires expert knowledge. With RSVQA, we propose a system to
extract information from remote sensing data that is accessible to
every user: we use questions formulated in natural language and
use them to interact with the images. With the system, images
can be queried to obtain high level information speciﬁc to the
image content or relational dependencies between objects visible
in the images. Using an automatic method introduced in this
article, we built two datasets (using low and high resolution data)
of image/question/answer triplets. The information required to
build the questions and answers is queried from OpenStreetMap
(OSM). The datasets can be used to train (when using supervised
methods) and evaluate models to solve the RSVQA task. We
report the results obtained by applying a model based on
Convolutional Neural Networks (CNNs) for the visual part and on
a Recurrent Neural Network (RNN) for the natural language part
to this task. The model is trained on the two datasets, yielding
promising results in both cases.
Index Terms —Visual Question Answering, Deep learning,
Dataset, Natural Language, Convolution Neural Networks, Re-
current Neural Networks, Very High Resolution, OpenStreetMap
I. I NTRODUCTION
REMOTE sensing data is widely used as an indirect
source of information. From land cover/land use to
crowd estimation, environmental or urban area monitoring,
remote sensing images are used in a wide range of tasks
of high societal relevance. For instance, remote sensing data
can be used as a source of information for 6 of the 17 sus-
tainable development goals as deﬁned by the United Nations
[1]. Due to the critical nature of the problems that can be
addressed using remote sensing data, signiﬁcant effort has
been made to increase its availability in the last decade. For
instance, Sentinel-2 satellites provide multispectral data with
a relatively short revisiting time, in open-access. However,
while substantial effort has been dedicated to improving the
means of direct information extraction from Sentinel-2 data
Sylvain Lobry, Diego Marcos, Jesse Murray and Devis Tuia are with
Laboratory of Geo-Information Science and Remote Sensing, Wageningen
University, The Netherlands email: work@sylvainlobry.com
Classification
Is it a rural or urban area?Regression
How many buildings are there?
Detection
Is there a road?Classical tasks
Regression
What is the area covered by small
buildings?Detection
Is there a road at the top of the image?Specific tasks
Regression / Detection
What is the number of roads next to a
park?Detection
Is there a building next to a parking?Mix of tasksFig. 1. Example of tasks achievable by a visual question answering model
for remote sensing data.
in the framework of a given task (e.g. classiﬁcation [2], [3]),
the ability to use remote sensing data as a direct source of
information is currently limited to experts within the remote
sensing and computer vision communities. This constraint,
imposed by the technical nature of the task, reduces both the
scale and variety of the problems that could be addressed with
such information as well as the number of potential end-users.
This is particularly true when targeting speciﬁc applications
(detecting particular objects, e.g.thatched roofs or buildings in
a developing country [4]) which would today call for important
research efforts. The targeted tasks are often multiple and
changing in the scope of a project calls for strong expert
knowledge, limiting the information which can be extracted
from remote sensing data. To address these constraints, we
introduce the problem of visual question answering (VQA)
for remote sensing data.arXiv:2003.07333v2  [cs.CV]  14 May 2020
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 2
VQA is a new task in computer vision, introduced in its
current form by [5]. The objective of VQA is to answer a
free-form and open-ended question about a given image. As
the questions can be unconstrained, a VQA model applied
to remote sensing data could serve as a generic solution
to classical problems involving remote sensing data (e.g.
”Is there a thatched roof in this image?” for thatched roof
detection), but also very speciﬁc tasks involving relations
between objects of different nature (e.g. ”Is there a thatched
roof on the right of the river?”). Examples of potential
questions are shown in Figure 1.
To the best of our knowledge, this is the ﬁrst time (after
the ﬁrst exploration in [6]) that VQA has been applied to
extract information from remote sensing data. It builds on the
task of generating descriptions of images through combining
image and natural language processing to provide the user
with easily accessible, high-level semantic information. These
descriptions are then used for image retrieval and intelligence
generation [7]. As seen in this introduction, VQA systems rely
on the recent advances in deep learning. Deep learning based
methods, thanks to their ability to extract high-level features,
have been successfully developed for remote sensing data as
reviewed in [8]. Nowadays, this family of methods is used to
tackle a variety of tasks; for scene classiﬁcation, an early work
by [9] evaluated the possibility to adapt networks pre-trained
on large natural image databases (such as ImageNet [10]) to
classify hyperspectral remote sensing images. More recently,
[11] used an intermediate high level representation using recur-
rent attention maps to classify images. Object detection is also
often approached using deep learning methods. To this effect,
[12] introduced an object detection dataset and evaluated clas-
sical deep learning approaches. Methods taking into account
the speciﬁcity of remote sensing data have been developed,
such as [13] which proposed to modify the classical approach
by generating rotatable region proposal which are particularly
relevant for top-view imagery. Deep learning methods have
also been developed for semantic segmentation. In [14], the
authors evaluated different strategies for segmenting remote
sensing data. More recently, a contest organized on the dataset
of building segmentation created by [15] has motivated the
development of a number of new methods to improve results
on this task [16]. Similarly, [17] introduced a contest including
three tasks: road extraction, building detection and land cover
classiﬁcation. Best results for each challenge were obtained
using deep neural networks: [18], [19], [20].
Natural language processing has also been used in remote
sensing. For instance, [21] used a convolutional neural
network (CNN) to generate classiﬁcation probabilities for a
given image, and used a recurrent neural network (RNN) to
generate its description. In a similar fashion, [7] used a CNN
to obtain a multi semantic level representation of an image
(object, land class, landscape) and generate a description
using a simple static model. More recently, [22] uses an
encoder/decoder type of architecture where a CNN encodes
the image and a RNN decodes it to a textual representation,
while [23] projects the textual representation and the image to
a common space. While these works are use cases of naturallanguage processing, they do not enable interactions with the
user as we propose with VQA.
A VQA model is generally made of 4 distinct components:
1) a visual feature extractor, 2) a language feature extractor, 3)
a fusion step between the two modalities and 4) a prediction
component. Since VQA is a relatively new task, an important
number of methodological developments have been published
in both the computer vision and natural language processing
communities during the past 5 years, reviewed in [24]. VQA
models are able to beneﬁt from advances in the computer
vision and automatic language processing communities for
the features extraction components. However, the multi-modal
fusion has been less explored and therefore, an important
amount of work has been dedicated to this step. First VQA
models relied on a non-spatial fusion method, i.e.a point-wise
multiplication between the visual and language feature vectors
[5]. Being straightforward, this method does not allow every
component from both feature vectors to interact with each
other. This interaction would ideally be achieved by multiply-
ing the ﬁrst feature vector by the transpose of the other, but
this operation would be computationally intractable in practice.
Instead, [25] proposed a fusion method which ﬁrst selects
relevant visual features based on the textual feature (attention
step) and then, combines them with the textual feature. In [26],
the authors used Tucker decomposition to achieve a similar
purpose in one step. While these attention mechanisms are
interesting for ﬁnding visual elements aligned with the words
within the question, they require the image to be divided in a
regular grid for the computation of the attention, and this is not
suitable to objects of varying size. A solution is presented in
[27], which learns an object detector to select relevant parts of
the image. In this research, we use a non-spatial fusion step to
keep the model part relatively simple. Most traditional VQA
works are designed for a speciﬁc dataset, either composed
of natural images (with questions covering an unconstrained
range of topics) or synthetic images. While interesting for
the methodological developments that they have facilitated,
these datasets limit the potential applications of such systems
to other problems. Indeed, it has been shown in [28] that
VQA models trained on a speciﬁc dataset do not generalize
well to other datasets. This generalization gap raises questions
concerning the applicability of such models to speciﬁc tasks.
A notable use-case of VQA is helping visually impaired
people through natural language interactions [29]. Images
acquired by visually impaired people represent an important
domain shift, and as such a challenge for the applicability
of VQA models. In [30], the authors conﬁrm that networks
trained on generic datasets do not generalize to their speciﬁc
one. However, they manage to obtain much better results
by ﬁne-tuning or training models from scratch on their
task-speciﬁc dataset.
In this study, we propose a new application for VQA,
speciﬁcally for the interaction with remote sensing images.
To this effect, we propose the ﬁrst remote sensing-oriented
VQA datasets, and evaluate the applicability of this task on
remote sensing images. We propose a method to automatically
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 3
Elements catalog
Road
Water area
Commercial building
Industrial building
Residential area
Retail
Religious areaLand usages
...Attributes catalog
Relations catalog
PositionalSizeShape Count
PresenceComparison
Area
Rural/UrbanQuestions catalog
(a) Question construction procedure. Dash lines represent optional paths
Road"How many roads are present in the image?"
"Is there a small retail place?"
"Is there more buildings at the top of a circular religious place than roads in the image?"Road Count Base question
Retail Size Presence Base question
Building
Religious ShapePositional
Comparison Base question
(b) Construction path for sample questions.
Fig. 2. Illustration of the question construction procedure.
generate remote sensing-oriented VQA datasets from already
available human annotations in section II and generate two
datasets. We then use this newly-generated data to train
our proposed RSVQA model with a non-spatial fusion step
described in section III. Finally, the results are evaluated and
discussed in section IV.
Our contribution are the following:
•a method to generate remote sensing-oriented VQA
datasets;
•2 datasets;
•the proposed RSVQA model.
This work extends the preliminary study of [6] by considering
and disclosing a second larger dataset consisting of very high
resolution images. This second dataset helps testing the spatial
generalization capability of VQA and provides an extensive
discussion highlighting remaining challenges. The method to
generate the dataset, the RSVQA model and the two datasets
are available on https://rsvqa.sylvainlobry.com/.
II. D ATASETS
A. Method
As seen in the introduction, a main limiting factor for
VQA is the availability of task-speciﬁc datasets. As such,
we aim at providing a collection of remote sensing images
with questions and answers associated to them. To do so,
we took inspiration from [31], in which the authors build
a dataset of question/answer pairs about synthetic images
following an automated procedure. However, in this study
we are interested in real data (discussed in subsection II-B).
Therefore, we use the openly accessible OpenStreetMap data
containing geo-localized information provided by volunteers.
By leveraging this data, we can automatically extract theinformation required to obtain question/answer pairs relevant
to real remotely sensed data and create a dataset made of
(image, question, answer) triplets.
The ﬁrst step of the database construction is to create
the questions. The second step is to compute the answers
to the questions, using the OSM features belonging to the
image footprint. Note that multiple question/answer pairs are
extracted for each image.
1) Question contruction: Our method to construct the
questions is illustrated in Figure 2. It consists of four main
components:
1) choice of an element category (highlighted in red in
Figure 2(a));
2) application of attributes to the element (highlighted in
green in Figure 2(a));
3) selection based on the relative location to another ele-
ment (highlighted in green in Figure 2(a))
4) construction of the question (highlighted in blue in
Figure 2(a)).
Examples of question constructions are shown in Figure 2(b).
These four components are detailed in the following.
Element category selection : First, an element category is ran-
domly selected from the element catalog. This catalog is built
by extracting the elements from one of the following OSM
layers: road,water area ,building andland use . While roads
and water areas are directly treated as elements, buildings
and land use related objects are deﬁned based on their ”type”
ﬁeld, as deﬁned in the OSM data speciﬁcation. Examples of
land use objects include residential area, construction area,
religious places, . . . Buildings are divided in two categories:
commercial (e.g. retail, supermarket, ...) and residential (e.g.
house, apartments, . . . ).
Attributes application : The second (optional) step is to
reﬁne the previously selected element category. To do so,
we randomly select from one of the two possible attribute
categories:
•Shape : each element can be either square, rectangular
or circular. Whether an element belongs to one of these
shape types is decided based on basic geometrical prop-
erties (i.e. hard thresholds on area-to-perimeter ratio and
area-to-circumscribed circle area ratio).
•Size: using hard thresholds on the surface area, elements
can be considered ”small”, ”medium” or ”large”. As we
are interested in information at different scales in the
two datasets, we use different threshold values, which
are described in Table I.
Relative position : Another possibility to reﬁne the element is
to look at its relative position compared to another element.
We deﬁne 5 relations: ”left of”, ”top of”, ”right of”, ”bottom
of”, ”next to”. Note that these relative positions are understood
in the image space (i.e. geographically). The special case of
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 4
Scale Small Medium Large
Low resolution <3000m2<10000m2≥10000m2
High resolution <100m2<500m2≥500m2
TABLE I
THRESHOLDS FOR SIZE ATTRIBUTES ACCORDING TO THE DATASET
SCALE . W HEN DEALING WITH LOW RESOLUTION DATA ,VISIBLE OBJECTS
OF INTEREST ARE LARGER . TO DEAL WITH THIS DISPARITY ,WE ADAPT
THE SIZE THRESHOLDS TO THE RESOLUTION OF THE IMAGES .
”next to” is handled as a hard threshold on the relative distance
between the two objects (less than 1000m). When looking at
relative positions, we select the second element following the
procedure previously deﬁned.
Question construction : At this point of the procedure, we
have an element (e.g. road), with an optional attribute (e.g.
small road) and an optional relative position (e.g. small road
on the left of a water area). The ﬁnal step is to generate
a ”base question” about this element. We deﬁne 5 types of
questions of interest (”Question catalog” in Figure 2(a)), from
which a speciﬁc type is randomly selected to obtain a base
question. For instance, in the case of comparison questions, we
randomly choose among ”less than”, ”equals to” and ”more
than” and construct a second element.
This base question is then turned into a natural language
question using pre-deﬁned templates for each question type
and object. For some question types (e.g. count), more than
one template is deﬁned (e.g. ’How many are there?’,
’What is the number of ?’ or ’What is the amount of ?’).
In this case, the template to be used is randomly selected. The
stochastic process ensures the diversity, both in the question
types and the question templates used.
2) Answer construction: : To obtain the answer to the
constructed question, we extract the objects from the OSM
database corresponding to the image footprint. The objects b
corresponding to the element category and its attributes are
then selected and used depending on the question type:
•Count : In the case of counting, the answer is simply the
number of objects b.
•Presence : A presence question is answered by comparing
the number of objects bto 0.
•Area : The answer to a question about the area is the sum
of the areas of the objects b.
•Comparison : Comparison is a speciﬁc case for which
a second element and the relative position statement is
needed. This question is then answered by comparing the
number of objects bto the ones of the second element.
•Rural/Urban : The case of rural/urban questions is han-
dled in a speciﬁc way. In this case, we do not cre-
ate a speciﬁc element, but rather count the number of
buildings (both commercial or residential). This number
of buildings is then thresholded to a predeﬁned number
depending on the resolution of the input data (to obtain
a density) to answer the question. Note that we are using
a generic deﬁnition of rural and urban areas but this can
be easily adapted using the precise deﬁnition of each
Fig. 3. Images selected for the LR dataset over the Netherlands. Each point
represent one Sentinel-2 image which was later split into tiles. Red points
represent training samples, green pentagon represents the validation image,
and blue triangle is for the test image. Note that one training image is not
visible (as it overlaps with the left-most image).
country.
B. Data
Following the method presented in subsection II-A, we
construct two datasets with different characteristics.
Low resolution (LR) : this dataset is based on Sentinel-2
images acquired over the Netherlands. Sentinel-2 satellites
provide 10m resolution (for the visible bands used in this
dataset) images with frequent updates (around 5 days) at a
global scale. These images are openly available through ESA’s
Copernicus Open Access Hub1.
To generate the dataset, we selected 9 Sentinel-2 tiles cover-
ing the Netherlands with a low cloud cover (selected tiles are
shown in Figure 3). These tiles were divided in 772images of
size 256×256(covering 6.55km2) retaining the RGB bands.
From these, we constructed 77′232 questions and answers
following the methodology presented in subsection II-A. We
split the data in a training set ( 77.8%of the original tiles), a
validation set ( 11.1%) and a test set ( 11.1%) at the tile level
(the spatial split is shown in Figure 3). This allows to limit
spatial correlation between the different splits.
High resolution (HR) : this dataset uses 15cm resolution aerial
RGB images extracted from the High Resolution Orthoim-
agery (HRO) data collection of the USGS. This collection
1https://scihub.copernicus.eu/
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 5
Fig. 4. Extent of the HR dataset with a zoom on the Portland, Manhattan (New
York City) and Philadelphia areas. Each point represent one image (generally
of size 5000×5000 ) which was later split into tiles. The images cover the
New York City/Long Island region, Philadelphia and Portland. Red points
represent training samples, green pentagons represent validation samples, and
blue indicators are for the test sets (blue triangles for test set 1, blue stars for
test set 2).
covers most urban areas of the USA, along with a few areas
of interest (e.g. national parks). For most areas covered by the
dataset, only one tile is available with acquisition dates ranging
from year 2000 to 2016, with various sensors. The tiles are
openly accessible through USGS’ EarthExplorer tool2.
From this collection, we extracted 161 tiles belonging to
the North-East coast of the USA (see Figure 4) that were split
into10′659images of size 512×512(each covering 5898m2).
We constructed 1′066′316questions and answers following the
methodology presented in subsection II-A. We split the data
in a training set ( 61.5%of the tiles), a validation set ( 11.2%),
and test sets ( 20.5%for test set 1, 6.8%for test set 2). As it
can be seen in Figure 4, test set 1 covers similar regions as
the training and validation sets, while test set 2 covers the city
of Philadelphia, which is not seen during the training. Note
that this second test set also uses another sensor (marked as
unknown on the USGS data catalog), not seen during training.
Differences between the two datasets :
Due to their characteristics, the two datasets represent two
different possible use cases of VQA:
•The LR dataset allows for large spatial and temporal
coverage thanks to the frequent acquisitions made by
Sentinel-2. This characteristic could be of interest for
future applications of VQA such as large scale queries
(e.g. rural/urban questions) or temporal (which is out of
the scope of this study). However, due to the relatively
low resolution (10m), some objects can not be seen on
such images (such as small houses, roads, trees, . . . ).
This fact severely limits the questions to which the model
2https://earthexplorer.usgs.gov/could give an accurate answer.
•Thanks to the much ﬁner resolution of the HR dataset,
a quantity of information of interest to answer typical
questions is present. Therefore, in contrast to the LR
dataset, questions concerning objects’ coverage or count-
ing relatively small objects can possibly be answered
from such data. However, data of such resolution is
generally less frequently updated and more expensive to
acquire.
Based on these differences, we constructed different types
of questions for the two datasets. Questions concerning the
area of objects are only asked in the HR dataset. On the other
hand, questions about urban/rural area classiﬁcation are only
asked in the LR dataset, as the level of zoom of images from
the HR dataset would prevent a meaningful answer from being
provided.
To account for the data distributions and error margins we
also quantize different answers in both datasets:
•Counting in LR: as the coverage is relatively large
(6.55km2), the number of small objects contained in one
tile can be high, giving a heavy tailed distribution for the
numerical answers, as shown in Figure 6. More precisely,
while 26.7% of the numerical answers are ’0’ and 50%
of the answers are less than ’7’, the highest numerical
answer goes up to ’17139’. In addition to making the
problem complex, we can argue that allowing such a
range of numerical answer does not make sense on data
of this resolution. Indeed, it would be in most cases
impossible to distinguish 17139 objects on an image of
65536 pixels. Therefore, numerical answers are quantized
into the following categories:
–’0’;
–’between 1 and 10’;
–’between 11 and 100’;
–’between 101 and 1000’;
–’more than 1000’.
•In a similar manner, we quantize questions regarding
the area in the HR dataset. A great majority (60.9%) of
the answer of this type are ’0m2’, while the distribution
also presents a heavy tail. Therefore, we use the same
quantization as the one proposed for counts for the LR
dataset. Note that we do not quantize purely numerical
answers (i.e. answers to questions of type ’count’) as
the maximum number of objects is 89 in our dataset.
Counting answers therefore correspond to 89 classes in
the model in this case (see section III).
C. Discussion
Questions/Answers distributions :
We show the ﬁnal distribution of answers per question type
for both datasets in Figure 5. We can see that most question
types (with the exception of ’rural/urban’ questions in the
LR dataset, asked only once per image) are close to evenly
distributed by construction. The answer ’no’ is dominating
the answers’ distribution for the HR dataset with a frequency
of 37.7%. In the LR dataset, the answer ’yes’ occurs 34.9%
of the time while the ’no’ frequency is 34.3%. The strongest
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 6
(a) Distribution of answers for the LR dataset
yesno
yes
no0m2between 10m2 and 100m2more than 1000m2presencecount
comparea (b) Distribution of answers for the HR dataset (numerical answers are ordered,
and 0 is the most frequent)
Fig. 5. Distributions of answers in the Low resolution (LR) and High resolution (HR) datasets.
Fig. 6. Frequencies of exact counting answers in the LR dataset. Only the
left part of the histogram is shown (until 200 objects), the largest (single)
count being 17139. 50% of the answers are less than 7 objects in the tile.
imbalance occurs for the answer ’0’ in the HR dataset
(with a frequency of 60.9% for the numerical answer). This
imbalance is greatly reduced by the quantization process
described in the previous paragraph.
Limitations of the proposed method :
While the proposed method for image/question/answer triplets
generation has the advantage of being automatic and easily
scalable while using data annotated by humans, a few lim-
itations have been observed. First, it can happen that some
annotations are missing or badly registered [4]. Furthermore,
it was not possible to match the acquisition date of the imagery
to the one of OSM. The main reason being that it is impossible
to know if a newly added element appeared at the same time
in reality or if it was just entered for the ﬁrst time in OSM.
As OSM is the main source of data for our process, errors in
OSM will negatively impact the accuracy of our databases.
Furthermore, due to the templates used to automatically
construct questions and provide answers, the set of questions
and answers is more limited than what it is in traditional VQA
datasets (9 possible answers for the LR dataset, 98 for the HRdataset).
III. VQA M ODEL
We investigate the difﬁculty of the VQA task for remote
sensing using a basic VQA model based on deep learning. An
illustration of the proposed network is shown in Figure 7. In
their simple form, VQA models are composed of three parts
[24]:
A. feature extraction;
B. fusion of these features to obtain a single feature vector
representing both the visual information and the question;
C. prediction based on this vector.
As the model shown in Figure 7 is learned end-to-end, the
vector obtained after the fusion (in green in Figure 7) can be
seen as a joint embedding of both the image and the question
which is used as an input for the prediction step. We detail
each of these 3 parts in the following.
A. Feature extraction
The ﬁrst component of our VQA model is the feature extrac-
tion. Its purpose is to obtain a low-dimensional representation
of the information contained in the image and the question.
1) Visual part: To extract information from a 2D image,
a common choice is to use a Convolutional Neural Network
(CNN). Speciﬁcally, we use a Resnet-152 model [32] pre-
trained on ImageNet [10]. The principal motivation for this
choice is that this architecture manages to avoid the un-
desirable degradation problem (decreasing performance with
deeper networks) by using residual mappings of the layers’
inputs which are easier to learn than the common choice of
direct mappings. This architecture has been succesfully used
in a wide range of work in the remote sensing community
(e.g. [8], [17], [33]). The last average pooling layer and fully
connected layer are replaced by a 1×12D convolution which
outputs a total of 2048 features which are vectorized. A ﬁnal
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 7
⊙B. Fusion A. Features extraction
CNN
ResNet-152
"Is there a building?" RNN
skip-thoughts
2048 1200
1200 2400
1200 256Output
vector
Yes
No
89 ...
A. 1) Visual part
 A. 2) Language partC. Prediction
⊙
Fully connected layer
Point wise multiplication
Legend
Fig. 7. Framework of the proposed Visual Question Answering model.
fully connected layer is learned to obtain a 1200 dimension
vector.
2) Language part: The feature vector is obtained using the
skip-thoughts model [34] trained on the BookCorpus dataset
[35]. This model is a recurrent neural network, which aims at
producing a vector representing a sequence of words (in our
case, a question). To make this vector informative, the model
is trained in the following way: it encodes a sentence from a
book in a latent space, and tries to decode it to obtain the two
adjacent sentences in the book. By doing so, it ensures that
the latent space embeds semantic information. Note that this
semantic information is not remote sensing speciﬁc due to the
BookCorpus dataset it has been trained on. However, several
works, including [36], have successfully applied non-domain
speciﬁc NLP models to remote sensing. In our model, we use
the encoder which is then followed by a fully-connected layer
(from size 2400 elements to 1200).
B. Fusion
At this step, we have two feature vectors (one representing
the image, one representing the question) of the same size. To
merge them into a single vector, we use a simple strategy: a
point-wise multiplication after applying the hyperbolic tangent
function to the vectors’ elements. While being a ﬁxed (i.e.
not learnt) operation, the end-to-end training of our model
encourages both feature vectors to be comparable with respect
to this operation.
C. Prediction
Finally, we project this 1200 dimensional vector to the
answer space by using a MLP with one hidden layer of 256
elements. We formulate the problem as a classiﬁcation task, in
which each possible answer is a class. Therefore, the size of
the output vector depends on the number of possible answers.D. Training procedure
We train the model using the Adam optimizer [37] with a
learning rate of 10−5until convergence (150 epochs in the
case of the LR dataset, and 35 epochs in the case of the HR
dataset). We use a dropout of 0.5 for every fully connected
layer. Due to the difference of input size between the two
datasets (HR images are 4 times larger), we use batches of
70 instances for the HR dataset and 280 for the LR dataset.
Furthermore, when the questions do not contain a positional
component relative to the image space (i.e. ”left of”, ”top of”,
”right of” or ”bottom of”, see subsection II-A), we augment the
image space by randomly applying vertical and/or horizontal
ﬂipping
IV. R ESULTS AND DISCUSSION
We report the results obtained by our model on the test sets
of the LR and HR datasets. In both cases, 3 model runs have
been trained and we report both the average and the standard
deviation of our results to limit the variability coming from
the stochastic nature of the optimization.
The numerical evaluation is achieved using the accuracy,
deﬁned in our case as the ratio of correct answers. We report
the accuracy per question type (see subsection II-A), the
average of these accuracies (AA) and the overall accuracy
(OA).
We show some predictions of the model on the different
test sets in Figure 8 and Figure 9 to qualitatively assess the
results. Numerical performance of the proposed model on the
LR dataset is reported in Table II and the confusion matrix
is shown in Figure 10. The performance on both tests sets of
the HR dataset are reported in Table III and the confusion
matrices are shown in Figure 11.
General accuracy assessment:
The proposed model achieves an overall accuracy of 79%
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 8
What is the area covered by residential buildings?
0 m² 0 m²Ground truth Prediction
(a) HR, test set 1
What is the area covered by rectangular buildings?
Between 100 and
1000 m²Between 100 and
1000 m²Ground truth Prediction
(b) HR, test set 1
Is there a residential building at the bottom
of the place of worship?
yes yesGround truth Prediction
(c) HR, test set 1
How many residential buildings at the bottom
of a road are there?
4 3Ground truth Prediction
(d) HR, test set 1
What is the amount of large buildings?
3 1Ground truth Prediction
(f) HR, test set 1
How many buildings on the left of a road
are there in the image?
38 0Ground truth Prediction
(e) HR, test set 1
What is the area covered by rectangular parkings?
between 100m2 and
1000m2between 100m2 and
1000m2Ground truth Prediction
(g) HR, test set 2
What is the amount of small buildings?
9 0Ground truth Prediction
(h) HR, test set 2
What is the amount of large residential buildings?
2 1Ground truth Prediction
(i) HR, test set 2
Fig. 8. Samples from the high resolution test sets: (a)-(f) are from the ﬁrst set of the HR dataset, (g)-(i) are from the second set of the HR dataset.
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 9
TABLE II
RESULTS ON THE TEST SET OF THE LOW RESOLUTION DATASET . THE
STANDARD DEVIATION IS REPORTED IN BRACKETS .
Type Accuracy
Count 67.01% (0.59%)
Presence 87.46% (0.06%)
Comparison 81.50% (0.03%)
Rural/Urban 90.00% (1.41%)
AA 81.49% (0.49%)
OA 79.08% (0.20%)
TABLE III
RESULTS ON BOTH TEST SETS OF THE HIGH RESOLUTION DATASET . THE
STANDARD DEVIATION IS REPORTED IN BRACKETS .
Type Accuracy Accuracy
Test set 1 Test set 2
Count 68.63% (0.11%) 61.47% (0.08%)
Presence 90.43% (0.04%) 86.26% (0.47%)
Comparison 88.19% (0.08%) 85.94% (0.12%)
Area 85.24% (0.05%) 76.33% (0.50%)
AA 83.12% (0.03%) 77.50% (0.29%)
OA 83.23% (0.02%) 78.23% (0.25%)
on the low resolution dataset (see Table II) and of 83% on
the ﬁrst test set of the high resolution dataset (Table III),
indicating that the task of automatically answering question
based on remote sensing images is possible. When looking at
the accuracies per question type (in Tables II and III), it can
be noted that the model performs inconsistently with respect
to the task the question is tackling: while a question about the
presence of an object is generally well answered (87.46% in
the LR dataset, 90.43% in the ﬁrst test set of the HR dataset),
counting questions gives poorer performances (67.01% and
68.63% respectively). This can be explained by the fact that
presence questions can be seen as simpliﬁed counting ques-
tions to which the answers are restricted to two options: ”0”
or ”1 or more”. Classical VQA models are known to struggle
with the counting task [38]. An issue which partly explains
these performances in the counting task is the separation of
connected instances. This problem has been raised for the case
of buildings in [33] and is illustrated in Figure 8(f), where the
ground truth is indicating three buildings, which could also be
only one. We found another illustration of this phenomenon
in the second test set in Figure 8(i). This issue mostly arises
when counting roads or buildings.
Thanks to the answers’ quantization, questions regarding the
areas of objects are generally well answered with an accuracy
of 85.24% in the ﬁrst test set of the HR dataset. This is
illustrated in Figures 8(a,b), where presence of buildings (by
the mean of the covered area) is well detected.
However, we found that our model performs poorly with
questions regarding the relative positions of objects, such
as those illustrated in Figures 8(c-e). While Figure 8(c)
is correct, despite the question being difﬁcult, Figure 8(d)
shows a small mistake from the model and Figure 8(e) is
completely incorrect. These problems can be explained by
the fact that the questions are on high semantic level and
therefore difﬁcult for a model considering a simple fusion
scheme, as the one presented in section III.
Is it a rural or an urban area?
Rural RuralGround truth Prediction
Is it a rural or an urban area?
Urban UrbanGround truth Prediction
(a) LR, test set (b) LR, test set
Are there more water areas than
commercial buildings?
Yes NoGround truth Prediction
Are there less buildings than water areas?
No NoGround truth Prediction
(c) LR, test set (d) LR, test setFig. 9. Samples from the low resolution test set.
Regarding the low resolution dataset, rural/urban questions
are generally well answered (90% of accuracy), as shown
in Figure 9(a,b). Note that the ground truth for this type
of questions is deﬁned as a hard threshold on the number
of buildings, which causes an area as the one shown in
Figure 9(b) to be labeled as urban.
However, the low resolution of Sentinel-2 images can be
problematic when answering questions about relatively small
objects. For instance, in Figures 9(c,d), we can not see any
water area nor determine the type of buildings, which causes
the model’s answer to be unreliable.
Generalization to unseen areas:
The performances on the second test set of the HR dataset
show that the generalization to new geographic areas
is problematic for the model, with an accuracy drop of
approximately 5%. This new domain has a stronger impact
on the most difﬁcult tasks (counting and area computation).
This can be explained when looking at Figures 8(g-i). We can
see that the domain shift is important on the image space, as
a different sensor was used for the acquisition. Furthermore,
the urban organization of Philadelphia is different from that
of the city of New York. This causes the buildings to go
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 10
Yes No Rural Urban 0 1-10 101-1000 1000+
Yes
No
Rural
Urban
0
1-10
11-100
101-1000
1000+11-100True
Predicted06544022980
Fig. 10. Confusion matrix for the low resolution dataset (logarithm scale)
on the test set. Red lines group answers by type (”Yes/No”, ”Rural/Urban”,
numbers).
undetected by the model in Figure 8(h), while the parkings
can still be detected in Figure 8(g) possibly thanks to the
cars. This decrease in performance could be reduced by
using domain adaptation techniques. Such a method could
be developed for the image space only (a review of domain
adaptation for remote sensing is done in [39]) or at the
question/image level (see [40], which presents a method for
domain adaptation in the context of VQA).
Answer’s categories:
The confusion matrices indicate that the models generally
provide logical answers, even when making mistakes
(e.g. it might answer ”yes” instead of ”no” to a question
about the presence of an object, but not a number). Rare
exceptions to this are observed for the ﬁrst test set of the
HR dataset (see Figure 11(a)), on which the model gives 23
illogical answers (out of the 316941 questions of this test set).
Language biases:
A common issue in VQA models, raised in [41], is the fact
that strong language biases are captured by the model. When
this is the case, the answer provided by the model mostly
depends on the question, rather than on the image. To assess
this, we evaluated the proposed models by randomly selecting
an image from the test set for each question. We obtained an
overall accuracy of 73.78% on the LR test set, 73.78% on
the ﬁrst test set of the HR dataset and 72.51% on the second
test set. This small drop of accuracy indicates that indeed,
the models rely more on the questions than on the image to
provide an answer. Furthermore, the strongest drop of accuracy
is seen on the HR dataset, indicating that the proposed model
extracts more information from the high resolution data.
Importance of the number of training samples:
We show in Figure 12 the evolution of the accuracies when the
model is trained with a fraction of the HR training samples.
When using only 1% of the available training samples, the
model already gets 65% in average accuracy (vs 83% for themodel trained on the whole training set). However, it can be
seen that, for numerical tasks (counts and area estimation),
larger amounts of samples are needed to achieve the perfor-
mances reported in Table III. This experiment also shows that
the performances start to plateau after 10% of the training data
is used: this indicates that the proposed model would not proﬁt
substantially from a larger dataset.
Restricted set of questions:
While not appearing in the numerical evaluation, an important
issue with our results is the relative lack of diversity in the
dataset. Indeed, due to the source of our data (OSM), the
questions are only on a speciﬁc set of static objects (e.g.
buildings, roads, . . . ). Other objects of interest for applications
of a VQA system to remote sensing would also include differ-
ent static objects (e.g. thatched roofs mentioned in section I),
moving objects (e.g. cars), or seasonal aspects (e.g. for crop
monitoring). Including these objects would require another
source of data, or manual construction of question/answer
pairs.
Another limitation comes from the dataset construction
method described in subsection II-A. We deﬁned ﬁve types
of questions (count, comparison, presence, area, rural/urban
classiﬁcation). However, they only start to cover the range of
questions which would be of interest. For instance, questions
about the distance between two points (deﬁned by textual
descriptions), segmentation questions (e.g. ”where are the
buildings in this image?”) or higher semantic level question
(e.g. ”does this area feel safe?”) could be added.
While the ﬁrst limitation (due to the data source) could be
tackled using other databases (e.g. from national institutes) and
the second limitation (due to the proposed method) could be
solved by adding other question construction functions to the
model, it would be beneﬁcial to use human annotators using
a procedure similar to [5] to diversify the samples.
V. C ONCLUSION
We introduce the task of Visual Question Answering from
remote sensing images as a generic and accessible way of ex-
tracting information from remotely sensed data. We present a
method for building datasets for VQA, which can be extended
and adapted to different data sources, and we proposed two
datasets targeting different applications. The ﬁrst dataset uses
Sentinel-2 images, while the second dataset uses very high
resolution (30cm) aerial orthophotos from USGS.
We analyze these datasets using a model based on deep
learning, using both convolutional and recurrent neural
networks to analyze the images and associated questions. The
most probable answer from a predeﬁned set is then selected.
This ﬁrst analysis shows promising results, suggesting the
potential for future applications of such systems. These re-
sults outline future research directions which are needed to
overcome language biases and difﬁcult tasks such as counting.
The former can be tackled using an attention mechanism [24],
while the latter could be tackled by using dedicated compo-
nents for counting questions [33] in a modular approach.
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 11
0654402298022025162754True
PredictedYesNo0m²1m² - 10m²10m² - 100m²100m² - 1000m²1000m²+012345678910
Yes
No
0m²
1m² - 10m²
10m² - 100m²
1000m²+
0
1
2
3
4
5
6
7
8
9
10100m² - 1000m²
(a) Test set 1
0654402298022025True
PredictedYesNo0m²1m² - 10m²10m² - 100m²100m² - 1000m²1000m²+012345678910
Yes
No
0m²
1m² - 10m²
10m² - 100m²
1000m²+
0
1
2
3
4
5
6
7
8
9
10100m² - 1000m²
 (b) Test set 2
Fig. 11. Subsets of the confusion matrices for the high resolution dataset (counts are at logarithm scale) on both test sets. Red lines group answers by type
(”Yes/No”, areas, numbers).
Fig. 12. Evolution of the accuracies (evaluated on the ﬁrst HR test set) after
training with subsets of different size of the HR training set.
Issues regarding the current database raised in section IV
also need to be addressed to obtain a system capable of
answering a more realistic range of questions. This can be
done by making the proposed dataset construction method
more complex or by using human annotators.
ACKNOWLEDGMENT
The authors would like to thank CNES for the funding of
this study (R&T project ”Application des techniques de Visual
Question Answering `a des donnes d’imagerie satellitaire”).
REFERENCES
[1] K. Anderson, B. Ryan, W. Sonntag, A. Kavvada, and L. Friedl, “Earth
observation in service of the 2030 agenda for sustainable development,”
Geo-spatial Information Science , vol. 20, no. 2, pp. 77–96, 2017.
[2] Y . Gu, J. Chanussot, X. Jia, and J. A. Benediktsson, “Multiple kernel
learning for hyperspectral image classiﬁcation: A review,” IEEE Trans-
actions on Geoscience and Remote Sensing , vol. 55, no. 11, pp. 6547–
6565, Nov 2017.
[3] S. Li, W. Song, L. Fang, Y . Chen, P. Ghamisi, and J. A. Benediktsson,
“Deep learning for hyperspectral image classiﬁcation: An overview,”
IEEE Transactions on Geoscience and Remote Sensing , pp. 1–20, 2019.[4] J. E. Vargas-Mu ˜noz, S. Lobry, A. X. Falc ˜ao, and D. Tuia, “Correcting
rural building annotations in openstreetmap using convolutional neural
networks,” ISPRS Journal of Photogrammetry and Remote Sensing , vol.
147, pp. 283–293, 2019.
[5] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick,
and D. Parikh, “VQA: Visual Question Answering,” in International
Conference on Computer Vision , 2015.
[6] S. Lobry, J. Murray, D. Marcos, and D. Tuia, “Visual question answering
from remote sensing images,” in IEEE International Geoscience and
Remote Sensing Symposium , July 2019.
[7] Z. Shi and Z. Zou, “Can a machine generate humanlike language descrip-
tions for a remote sensing image?” IEEE Transactions on Geoscience
and Remote Sensing , vol. 55, no. 6, pp. 3623–3634, June 2017.
[8] X. X. Zhu, D. Tuia, L. Mou, G. Xia, L. Zhang, F. Xu, and F. Fraundorfer,
“Deep learning in remote sensing: A comprehensive review and list of
resources,” IEEE Geoscience and Remote Sensing Magazine , vol. 5,
no. 4, pp. 8–36, Dec 2017.
[9] F. Hu, G.-S. Xia, J. Hu, and L. Zhang, “Transferring deep convolutional
neural networks for the scene classiﬁcation of high-resolution remote
sensing imagery,” Remote Sensing , vol. 7, no. 11, pp. 14 680–14 707,
2015.
[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:
A Large-Scale Hierarchical Image Database,” in IEEE Conference on
Computer Vision and Pattern Recognition , 2009.
[11] Q. Wang, S. Liu, J. Chanussot, and X. Li, “Scene classiﬁcation with
recurrent attention of VHR remote sensing images,” IEEE Transactions
on Geoscience and Remote Sensing , vol. 57, no. 2, pp. 1155–1167, Feb
2019.
[12] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu,
M. Pelillo, and L. Zhang, “DOTA: A large-scale dataset for object
detection in aerial images,” in IEEE Conference on Computer Vision
and Pattern Recognition , 2018, pp. 3974–3983.
[13] Q. Li, L. Mou, Q. Xu, Y . Zhang, and X. X. Zhu, “R3-Net: A deep
network for multioriented vehicle detection in aerial images and videos,”
IEEE Transactions on Geoscience and Remote Sensing , pp. 1–15, 2019.
[14] M. V olpi and D. Tuia, “Dense semantic labeling of subdecimeter reso-
lution images with convolutional neural networks,” IEEE Transactions
on Geoscience and Remote Sensing , vol. 55, no. 2, pp. 881–893, 2016.
[15] E. Maggiori, Y . Tarabalka, G. Charpiat, and P. Alliez, “Can semantic
labeling methods generalize to any city? the INRIA aerial image labeling
benchmark,” in IEEE International Geoscience and Remote Sensing
Symposium . IEEE, 2017, pp. 3226–3229.
[16] B. Huang, K. Lu, N. Audeberr, A. Khalel, Y . Tarabalka, J. Malof,
A. Boulch, B. Le Saux, L. Collins, K. Bradbury et al. , “Large-scale
semantic classiﬁcation: outcome of the ﬁrst year of INRIA aerial image
labeling benchmark,” in IEEE International Geoscience and Remote
Sensing Symposium . IEEE, 2018, pp. 6947–6950.
[17] I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu,
F. Hughes, D. Tuia, and R. Raskar, “Deepglobe 2018: A challenge to
PRE-PRINT. FINAL VERSION IN IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 12
parse the earth through satellite images,” in IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops . IEEE, 2018.
[18] L. Zhou, C. Zhang, and M. Wu, “D-linknet: Linknet with pretrained
encoder and dilated convolution for high resolution satellite imagery
road extraction.” in IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops , 2018, pp. 182–186.
[19] R. Hamaguchi and S. Hikosaka, “Building detection from satellite
imagery using ensemble of size-speciﬁc detectors,” in 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW) . IEEE, 2018, pp. 223–2234.
[20] C. Tian, C. Li, and J. Shi, “Dense fusion classmate network for land
cover classiﬁcation.” in IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops , 2018, pp. 192–196.
[21] X. Zhang, X. Li, J. An, L. Gao, B. Hou, and C. Li, “Natural language
description of remote sensing images based on deep learning,” in IEEE
International Geoscience and Remote Sensing Symposium , July 2017,
pp. 4798–4801.
[22] X. Zhang, X. Wang, X. Tang, H. Zhou, and C. Li, “Description gener-
ation for remote sensing images using attribute attention mechanism,”
Remote Sensing , vol. 11, no. 6, p. 612, 2019.
[23] B. Wang, X. Lu, X. Zheng, and X. Li, “Semantic descriptions of
high-resolution remote sensing images,” IEEE Geoscience and Remote
Sensing Letters , 2019.
[24] Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, and A. v.d. Hengel, “Visual
question answering: A survey of methods and datasets,” Computer Vision
and Image Understanding , 2017.
[25] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and
M. Rohrbach, “Multimodal compact bilinear pooling for visual question
answering and visual grounding,” arXiv preprint arXiv:1606.01847 ,
2016.
[26] H. Ben-Younes, R. Cadene, M. Cord, and N. Thome, “MUTAN: Multi-
modal tucker fusion for Visual Question Answering,” in Proceedings of
the IEEE international conference on computer vision , 2017, pp. 2612–
2620.
[27] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and
L. Zhang, “Bottom-up and top-down attention for image captioning and
visual question answering,” in Proceedings of the IEEE conference on
computer vision and pattern recognition , 2018, pp. 6077–6086.
[28] R. Shrestha, K. Kaﬂe, and C. Kanan, “Answer them all! toward universal
visual question answering models,” in IEEE Conference on Computer
Vision and Pattern Recognition , 2019, pp. 10 472–10 481.
[29] J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller,
A. Tatarowicz, B. White, S. White et al. , “VizWiz: nearly real-time
answers to visual questions,” in ACM symposium on User interface
software and technology . ACM, 2010, pp. 333–342.
[30] D. Gurari, Q. Li, A. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo,
and J. Bigham, “VizWiz Grand Challenge: Answering visual questions
from blind people,” IEEE Conference on Computer Vision and Pattern
Recognition , 2018.
[31] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. Zitnick, and
R. Girshick, “CLEVR: A diagnostic dataset for compositional language
and elementary visual reasoning,” IEEE Conference on Computer Vision
and Pattern Recognition , 2017.
[32] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” IEEE Conference on Computer Vision and Pattern
Recognition , 2016.
[33] S. Lobry and D. Tuia, “Deep Learning Models to Count Buildings
in High-Resolution Overhead Images,” in Joint Urban Remote Sensing
Event , 2019.
[34] R. Kiros, Y . Zhu, R. Salakhutdinov, R. Zemel, A. Torralba, R. Urtasun,
and S. Fidler, “Skip-thought vectors,” Neural Information Processing
Systems , 2015.
[35] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,
and S. Fidler, “Aligning books and movies: Towards story-like visual
explanations by watching movies and reading books,” in IEEE interna-
tional conference on computer vision , 2015, pp. 19–27.
[36] A. Li, Z. Lu, L. Wang, T. Xiang, and J.-R. Wen, “Zero-shot scene
classiﬁcation for high spatial resolution remote sensing images,” IEEE
Transactions on Geoscience and Remote Sensing , vol. 55, no. 7, pp.
4157–4167, 2017.
[37] D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
International Conference on Learning Representations , 2015.
[38] Y . Zhang, J. Hare, and A. Prgel-Bennett, “Learning to count objects
in natural images for visual question answering,” in International
Conference on Learning Representations , 2018.
[39] D. Tuia, C. Persello, and L. Bruzzone, “Domain adaptation for the
classiﬁcation of remote sensing data: An overview of recent advances,”IEEE Geoscience and Remote Sensing Magazine , vol. 4, no. 2, pp. 41–
57, June 2016.
[40] W.-L. Chao, H. Hu, and F. Sha, “Cross-dataset adaptation for visual
question answering,” in IEEE Conference on Computer Vision and
Pattern Recognition , 2018, pp. 5716–5725.
[41] Y . Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making
the V in VQA matter: Elevating the role of image understanding in
Visual Question Answering,” in IEEE Conference on Computer Vision
and Pattern Recognition , 2017.
Sylvain Lobry (S16 - M17) received the Engi-
neering degree from Ecole pour l’Informatique et
les Techniques Avanc ´ees (EPITA), Kremlin Bic ˆetre,
France in 2013, the Masters degree in science and
technology from the University Pierre et Marie Cur-
rie (Paris 6), Paris in 2014, and the Ph.D. degree
from Telecom Paris, Paris, France, in 2017. He is
currently a Post-Doctoral Researcher with the Geo-
Information Science and Remote Sensing Labora-
tory, Wageningen University, The Netherlands. His
research interests include radar image processing
and multimodal processing of heterogeneous satellite data.
Diego Marcos Diego Marcos obtained an MSc
degree on Computational Sciences and Engineering
from the Ecole Polytechnique F ´ed´erale de Lausanne,
Switzerland, in 2014 and a Ph.D. degree in environ-
mental sciences from Wageningen University, The
Netherlands, in 2019. He is currently a Post-Doctoral
Researcher with the Geo-Information Science and
Remote Sensing Laboratory, at Wageningen Univer-
sity. His research interests include computer vision
and deep learning interpretability applied to geospa-
tial data.
Jesse Murray received an MSc degree in Geo-
Information Science from Wageningen University,
Wageningen, The Netherlands, in 2019. He is cur-
rently a Ph.D. candidate with the Geodetic Engineer-
ing Laboratory, at the Ecole Polytechnique Fdrale
de Lausanne, Switzerland. His research interests
include image processing and 3D geometry recon-
struction using computer vision and spatio-temporal
data.
Devis Tuia Devis Tuia (S07 - M09 - SM15) re-
ceived the Ph.D. degree from the University of
Lausanne, Lausanne, Switzerland, in 2009. He was
a Post-Doctoral Researcher in Valncia, Boulder, CO,
USA, and ´Ecole polytechnique f ´ed´erale de Lausanne
(EPFL), Lausanne. From 2014 to 2017, he was an
Assistant Professor with the University of Zurich,
Zrich, Switzerland. He is currently a Full Professor
with the Geo-Information Science and Remote Sens-
ing Laboratory, Wageningen University, Wagenin-
gen, The Netherlands. His research interests include
algorithms for data fusion of geospatial data (including remote sensing) using
machine learning and computer vision.
A Visual Active Search Framework for Geospatial Exploration
Anindya Sarkar1Michael Lanier1Scott Alfeld2Jiarui Feng1Roman Garnett1
Nathan Jacobs1Yevgeniy V orobeychik1
1{anindya,lanier.m,feng.jiarui,garnett,jacobsn,yvorobeychik }@wustl.edu,
2salfeld@amherst.edu
1Department of Computer Science & Engineering, Washington University in St. Louis
2Department of Computer Science, Amherst College
Abstract
Many problems can be viewed as forms of geospatial
search aided by aerial imagery, with examples ranging from
detecting poaching activity to human trafficking. We model
this class of problems in a visual active search (VAS) frame-
work, which has three key inputs: (1) an image of the entire
search area, which is subdivided into regions, (2) a local
search function, which determines whether a previously un-
seen object class is present in a given region, and (3) a
fixed search budget, which limits the number of times the
local search function can be evaluated. The goal is to max-
imize the number of objects found within the search budget.
We propose a reinforcement learning approach for VAS that
learns a meta-search policy from a collection of fully anno-
tated search tasks. This meta-search policy is then used to
dynamically search for a novel target-object class, lever-
aging the outcome of any previous queries to determine
where to query next. Through extensive experiments on sev-
eral large-scale satellite imagery datasets, we show that the
proposed approach significantly outperforms several strong
baselines. We also propose novel domain adaptation tech-
niques that improve the policy at decision time when there
is a significant domain gap with the training data. Code is
publicly available at this link.
1. Introduction
Consider a large national park that hosts endangered an-
imals, which are also in high demand on a black market,
creating a major poaching problem. An important strategy
in an anti-poaching portfolio is to obtain aerial imagery us-
ing drones that helps detect poaching activity, either ongo-
ing, or in the form of traps laid on the ground [1, 2, 3, 9, 8].
The quality of the resulting photographs, however, is gen-
erally somewhat poor, making the detection problem ex-
tremely difficult. Moreover, park rangers can only inspect
relatively few small regions to confirm poaching activity,
doing so sequentially. Crucially, inspecting such regionsyields new ground truth information about poaching activ-
ity that we can use to decide which regions to inspect in
the future. We can distill some key generalizable structure
Figure 1: A comparison of a greedy search policy (dashed line)
with an active search strategy (solid line) for the small car tar-
get class. The greedy policy is not able to adapt when a car is
not found in the starting cell and needlessly searches many similar
cells. The active strategy adapts and explores regions with differ-
ent visual characteristics, eventually finding the objects of interest.
from this scenario: given a broad area image (often with
a relatively low resolution), sequentially query small areas
within it (e.g., by sending park rangers to the associated re-
gions, on the ground), with each query returning the ground
truth, to identify as many target objects (e.g., poachers or
traps) as possible. The number of queries we can make is
typically limited, for example, by budget or resource con-
straints. Moreover, query results (e.g., detected poaching
activity in a particular region) are highly informative about
the locations of target objects in other regions , for example,
due to spatial correlation. We refer to this general modeling
framework as visual active search (VAS) . Numerous other
scenarios share this broad structure, such as identification
of drug or human trafficking sites, broad area search-and-
rescue, identifying landmarks, and many others.
A simple solution to the broad-area search problem is to
divide the broad area into many grid cells, train a classifier
1arXiv:2211.15788v3  [cs.CV]  29 Oct 2023
to predict existence of a target object in each grid cell, and
simply explore the top Kcells in terms of predicted likeli-
hood of the object being in the cell. We call this the greedy
policy, which essentially reduces geospatial active search to
the familiar object identification (or detection) problem. In
Figure 1, we offer some intuition about why this idea fails
to capture important problem structure. Suppose we look
for small cars in an image, starting in the grid marked start,
which we initially think is the best place to look. The greedy
policy being a one-shot predictor of likely grid cells con-
taining target, continues to explore similar regions (marked
as). What this approach ignores, as does framing the
problem as traditional one-shot object identification, is the
fact that both success and failure of past queries are infor-
mative due to complex spatial correlation among objects
and other patterns in the scene; here, because the car was
not found, we proceed to instead explore regions that have
somewhat different characteristics. The key to visual active
search, therefore, is to learn how to make use of the ground
truth information obtained over a sequence of queries to de-
cide where to query next. Additionally, Section 5 of [11]
provides a rigorous analysis of the general sub-optimality
of greedy policies in active search settings.
Relationship to Active Search and Active Learning VAS
is closely related to active search [11, 10, 15, 13]. Ac-
tive search is typically concerned with binary classification,
and aims to maximize the number of discovered positively-
labeled inputs. It uses a function f, which predicts labels of
inputs x, as a means to this end, with each query serving the
dual-purpose of improving fas well as identifying a posi-
tive instance. A central concern in active search, therefore,
is achieving a balance of exploration (learning f) and ex-
ploitation (identifying target inputs). This consideration is
also the key distinction between active search and active
learning [25], which is concerned solely with improving
the predictive quality of f. Thus, if we had only a single
query, active learning would typically choose xfor which
prediction is highly uncertain, whereas active search would
choose xfor which f(x)is most confidently positive. We
empirically show that active learning is inappropriate for
solving the active search problem.
However, current active search approaches typically lack
a pre-search training phase, and are therefore effective in
relatively low dimensions and for relatively simple model
classes such as k-nearest-neighbors. In VAS, in contrast,
our goal is to learn how to search , that is, to learn how to
best use information obtained from previous search queries
in choosing the next query. We experimentally demonstrate
the advantage of VASover conventional active search below.
Contributions We propose a deep reinforcement learning
approach to solve the VAS problem. Our key contribution
is a novel policy architecture which makes use of a natural
representation of search state, in addition to the task im-age input, which the policy uses to dynamically adapt to the
task at hand at decision time, without additional training.
Additionally, we consider a variant of VAS in which the na-
ture of input tasks at decision time is sufficiently different
to warrant test-time adaptation, and propose several adap-
tation approaches that take advantage of the VAS problem
structure. We extensively evaluate our proposed approaches
toVAS on two satellite imagery datasets in comparison with
several baseline, including a state-of-the-art approach for a
related problem of identifying regions of an image to zoom
into [30]. Our results show that our approach significantly
outperforms all baselines.
In summary, we make the following contributions:
• We propose visual active search (VAS) , a novel visual
reasoning model that represents an important class of
geospatial exploration problems, such as identifying
poaching activities, illegal trafficking, etc.
• We propose a deep reinforcement learning approach
forVAS that learns how to search for target objects in
a broad geospatial area based on aerial imagery.
• We propose two new variants of test-time adaptation
(TTA) variants of VAS: (a)Online TTA and (b) Stepwise
TTA, as well as an improvement of the FixMatch state-
of-the-art TTA method [26].
• We perform extensive experiments on two publicly
available satellite imagery datasets, xView and DOTA,
in a variety of settings, and demonstrate that proposed
approaches significantly outperform all baselines.
2. Related Work
Foveated Processing of Large Images Numerous pa-
pers [33, 31, 30, 36, 32, 22, 29, 20, 19, 35] have explored the
use of low-resolution imagery to guide the selection of im-
age regions to process at high resolution, including a num-
ber using reinforcement learning to this end. Our setting is
quite different, as we aim to choose a sequence of regions
to query, where each query yields the true label , rather than
a higher resolution image region, and these labels are im-
portant for both guiding further search, and as an end goal.
Reinforcement Learning for Visual Navigation Rein-
forcement learning has also been extensively used for vi-
sual navigation tasks, such as point and object localiza-
tion [5, 18, 21, 7]. While similar at the high level, these
tasks involve learning to decide on a sequence of visual
navigation steps based on a local view of the environment
and a kinematic model of motion, and commonly do not in-
volve search budget constraints. In our case, in contrast, the
full environment is observed initially (perhaps at low reso-
lution), and we sequentially decide which regions to query,
and are not limited to a particular kinematic model.
Active Search and Related Problem Settings Garnett et
al. [11] first introduced Active Search (AS) . Unlike Active
Learning [24], ASaims to discover members of valuable
2
and rare classes rather than on learning an accurate model.
Garnett et al. [11] demonstrated that for any l>m, al-step
lookahead policy can be arbitrarily superior than an m-step
one, showing that a nonmyopic active search approach can
be significantly better than a myopic one-step lookahead.
Jiang et al. [15, 14] proposed approaches for efficient non-
myopic active search, while Jiang et al. [13] introduced con-
sideration of search cost into the problem.
We note two crucial differences between our setting and
the previous works on active search. First, we are the first
to consider the problem in the context of vision, where the
problem is high-dimensional, while prior techniques rely on
a relatively low dimensional feature space. Second, we use
reinforcement learning as a means to learn a search policy,
in contrast to prior work on active search which aims to
design efficient search algorithms.
3. Model
At the center of our task is an aerial image xwhich is
partitioned into Ngrid cells, x=(x(1), x(2), ..., x(N)). We
can also view xas the disjoint union of these Ngrid cells,
each of which is a sub-image. A subset (possibly empty) of
these grid cells contain an instance of the target object. We
formalize this by associating each grid cell jwith a binary
label y(j)∈{0,1}, where y(j)=1iff grid cell jcontains
the target object. Let y=(y(1), y(2), ..., y(N)).
We do not know ya priori, but can sequentially query to
identify grid cells that contain the target object. Whenever
we query a grid cell j, we obtain both the associated label
y(j), (i.e., whether it contains the target object) andaccrue
utility if the queried cell actually contains a target object.
Our ultimate goal is to find as many target objects as possi-
ble through a sequence of such queries given a total query
budget constraint C,
Formally, let c(j, k)be the cost of querying grid cell kif
we start in grid cell j. For the very first query, we can de-
fine a dummy initial grid cell d, so that cost function c(d, k)
captures the initial query cost. Let qtdenote a query per-
formed in step t. Our ultimate goal is to solve the following
optimization problem:
max
{qt}U(x;{qt})≡∑
ty(qt)
s.t.∶∑
t≥0c(qt−1, qt)≤C,(1)
where c(q−1, q0)=c(d, q0).
In order to succeed, we need to use the labels from previ-
ously queried cells to decide which cell to query next. This
is a conventional setup in active search, where an impor-
tant means to accomplish such learning is by introducing
a model fto predict a mapping between (in our instance)
a grid cell and the associated label (whether it contains a
target object) [10, 15, 13]. However, in many domains of
policy networkfeature extractiongrid prediction
query/search top cellgrid state (o)updatereducesamplesearch budget (B)Figure 2: An overview of the VAS framework.
interest, such as most visual domains of the kind we con-
sider, the query budget Cand the number of grid cells N
are very small compared to the dimension of the input x,
far too small to learn a meaningful prediction f. Instead,
we suppose that we have a dataset of tasks (aerial images)
for which we have labeled whether each of the grid cells
contains the target object. Let this dataset be denoted by
D={(xi, yi)}, with each xi=(x(1)
i, x(2)
i, . . . , x(N)
i)the
task image and yi=(y(1)
i, y(2)
i, . . . , y(N)
i)its correspond-
ing grid cell labels. Then, at decision (or inference) time,
we observe the task aerial image x, including its partition
into the grid cells, and choose queries {qt}sequentially to
maximize U(x;{qt}).
We consider two variations of the model above. In the
first, each instance (xi, yi)in the training data D, as well
as(x, y)at decision time (when yis unobserved before
queries) are generated i.i.d. from the same distribution. In
the second variation, while instances (x, y)are still i.i.d. at
decision time, their distribution can be different from that
of the training data D. The latter variation falls within the
broader category of test-time adaptation (TTA) settings, but
with the special structure pertinent to our model above.
4. Solution Approach
Visual active search over the area defined by xand its
constituent grid cells is a dynamic decision problem. As
such, we model it as a budget-constrained episodic Markov
decision process (MDP), where the search budget Cis de-
fined for each instance xat decision time . In this MDP,
the actions are simply choices over which grid cell to query
next; we denote the set of grids by A={1, . . . , N}. Since
in our model there is never any value to query a grid cell
more than once, we restrict actions available at each step
to be only grids that have not yet been queried (in princi-
ple, this restriction can also be learned). Policy network
inputs include: 1) the overall input x, which is crucial in
providing the broad perspective on each search problem, 2)
outcomes of past search queries o(we detail our representa-
tion of this presently), and 3) remaining budget B≤C. State
transition simply updates the remaining budget and adds the
outcome of the latest search query to state. Finally, an im-
mediate reward for query a grid cell jisR(x, o, j)=y(j).
We represent outcomes of search query history oas fol-
3
feature extraction networkgrid prediction networkinput image (x)
tilegrid state[o(1), o(2),…, o(N)]ResNet 34 (frozen)tileNx14x14Dx14x141x1convflatten1x1conv1x14x14
updateremaining query budget (B)
…step 1grid probabilitiesMLPstep 2step KFigure 3: Our V AS policy network architecture, showing the grid probabilities at three different steps.
lows. Each element of ocorresponds to a grid cell j, so that
o=(o(1), . . . , o(N)).o(j)=0ifjhas not been previously
queried. If grid cell jhas been previously queried,
o(j)← /leftr⫯g⊸tl⫯ne{1,ify(j)=1
−1,ify(j)=0.(2)
Armed with this MDP problem representation, we next
describe our proposed deep reinforcement learning ap-
proach for learning a search policy that makes use of a
dataset Dof past search tasks. Specifically, we use the RE-
INFORCE policy gradient algorithm [28] to directly learn
a search policy ψ(x, o, B ;θ)where θare the parameters of
the policy that we learn. Specifically, we maximize the fol-
lowing objective function:
∇J(θ)=M
∑
i=1Ti
∑
t=11∑t≥0c(qt−1,qt)≤C∇logψθ(ai
t∣xi, oi
t, Bi
t)Ri
t
(3)
Where Mis the number of example search task seen during
training and Rtis the discounted cumulative reward defined
asRt=∑T
k=tγk−tRkwith a discount factor γ∈[0,1].
The output of the search policy ψis a probability distri-
bution over A, with ψj(x, o, B ;θ)the probability that grid
cellj∈Ais selected by the policy ψ.
In general, ψwill output a positive probability over all
possible grid cells j∈A. However, in our setting there is
no benefit to querying any grid cell j∈Athat has previ-
ously been queried, i.e., for which o(j)≠0. Consequently,
both at training (when the next decision is generated) and
decision time, we restrict consideration only to j∈Awith
o(j)=0, that is, which have yet to be queried, and sim-
ply renormalize the output probabilities of ψ. Formally, we
define ψ′
j(x, o, B ;θ)=0forjwitho(j)≠0, and define
ψ′
j(x, o, B ;θ)=ψj(x, o, B ;θ)
∑k∈A∶o(k)=0ψk(x, o, B ;θ).
Grid cells jare then samples from ψ′
jat each search step
during training. At decision time, on the other hand, we
choose the grid cell jwith the maximum value of ψ′
j.
This approach allows us to simply train the policy networkψwithout concern about feasibility of particular grid cell
choices at decision time. In addition, to ensure that the pol-
icy is robust to search budget uncertainty, we use randomly
generated budgets Cat training time for different task in-
stances. In the case of query costs c(j, k)=1for all grid
cells j, k, each episode has a fixed length C. In general,
episodes have no fixed length, and end whenever we ex-
haust the total cost budget C. The overview of our proposed
VAS framework is depicted in Figure 2.
Next, we detail the proposed policy network architec-
ture, and subsequently describe an adaptation of our ap-
proach when instances at decision time follow a different
distribution from those in the training data D, that is, the
test-time adaptation (TTA) setting.
4.1. Policy Network Architecture
As shown in Figure 3, the policy network ψ(x, o, B ;θ)is
composed of two components: 1) the image feature extrac-
tion component f(x;ϕ)which maps the aerial image xto a
low-dimensional latent feature representation z, and 2) the
grid selection component g(z, o, B ;ζ), which combines the
latent image representation zwith outcome of past search
queries oand remaining budget Bto produce a probability
distribution over grid cells to search in the next time step.
Thus, the joint parameters of ψareθ=(ϕ, ζ).
We use a frozen ResNet-34 [12], pretrained on Ima-
geNet [16], as the feature extraction component f, followed
by a1×1convolution layer. We combine this with the bud-
getBand past query information oas follows. We apply
the tiling operation in order to convert ointo a represen-
tation with the same dimensions as the extracted features
z=f(x), aiding us to effectively combine latent image
feature and auxiliary state feature while preserving the grid
specific spatial and query related information. Similarly, we
apply tiling to the scalar budget Bto transform it to match
the size of zand the tiled version of o. Finally, we concate-
nate the features (z, o, B)along the channels dimension and
pass them through the grid prediction network g. This con-
sists of 1×1convolution to reduce dimensionality, flatten-
ing, a small MLP with ReLU activations, and a final output
(softmax) that represents the current grid probability. This
4
yields the full policy network to be trained end to end via
REINFORCE: ψ(x, o, B ;θ)=g(f(x;ϕ), o, B ;ζ).
4.2. Test-Time Adaptation
A central issue in our model, as in traditional active
search, is that tasks faced at decision time may in some re-
spects be novel, unlike tasks faced previously (e.g., repre-
sented in the dataset D). We view this issue through the
lens of test-time adaptation (TTA) , in which predictions are
made on data that comes from a different distribution from
training data. While a myriad of TTA techniques have been
developed, they have focused almost exclusively on super-
vised learning problems, rather than active search settings
of the kind we study. Nevertheless, two common techniques
can be either directly applied, or adapted, to our setting: 1)
Test-Time Training (TTT) [27] and 2) FixMatch [26].
TTT makes use of a self-supervised objective at both
training and prediction time by adding a self-supervised
head ras a component of the policy model. The asso-
ciated self-supervised loss (which is added during train-
ing) is a quadratic reconstruction loss ∣∣x−r(z;η)∣∣, where
z=f(x;ϕ)is the latent embedding of the input aerial image
xandηthe parameters of r. At decision time, a new task
image xis used to update policy parameters using just the
reconstruction loss before we begin the search. Adaptation
ofTTT to our V AS domain is therefore direct.
The original variant of FixMatch uses pseudo-labels at
decision time, which are predictions on weakly augmented
variants of the input image x(keeping only those which are
highly confident), to update model parameters. In our do-
main, however, we can leverage the fact that we obtain ac-
tual labels whenever we query regions of the image. We
make use of this additional information as follows. When-
ever a query jis successful (i.e., y(j)=1), we construct a
label vector as the one-hot vector with a 1 in the location of
the successful grid cell j. However if y(j)=0, we associate
each queried grid cell with a 0, and assign uniform proba-
bility distribution over all unqueried grids. We then update
model parameters using a cross-entropy loss.
Even as we adapted them, TTT andFixMatch do not
fully take advantage of the rich information obtained at de-
cision time in the VAS context as we proceed through each
input task: we not only observe the input image x, but
also observe query results over time during the search. We
therefore propose two new variants of TTA which are spe-
cific to the VAS setting: (a) Online TTA and (b) Stepwise
TTA. InOnline TTA , we update parameters of the policy
network after each task is completed during decision time,
which yields for us both the input xand the observations
oof the search results, which only partially correspond to
y, since we have only observed the contents of the previ-
ously queried grid cells. Nevertheless, we can simply use
this partial information oas a part of the REINFORCE pol-icy gradient update step to update the policy parameters θ.
InStepwise TTA , we update the policy network parameters,
even during the execution of a particular task, at decision
time, once every m<Csteps. The main difference between
Online andStepwise variations of our TTA approaches is
consequently the frequency of updates. Note that we can
readily compose both of these TTA approaches with con-
ventional TTA methods, such as TTT andFixMatch .
5. Experiments
Evaluation Metric We evaluate the proposed approaches
in terms of the average number of target objects discovered
(we shorten it to ANT ).
Baselines We compare the proposed V AS policy learning
framework with the following baselines:
1.random search , where each grid is chosen uniformly
at random among those which haven’t been explored,
2.greedy classification , in which we train a classifier ψgc
to predict whether a particular grid has a target object
and search the grids most likely to contain the target
until the search budget is exhausted, and
3.greedy selection , based on the approach by Uzkent and
Ermon [30] which trains a policy ψgswhich yields a
probability of zooming into each grid cell j. We select
grids according to ψgsuntil the budget Cis saturated.
4.active learning , in which we randomly select the first
grid to query and then choose C−1grids using a state-
of-the-art active learning approach by Yoo et al. [37].
5.conventional active search , an active search method by
Jiang et al. [15], using a low-dimensional feature rep-
resentation for each image grid from the same feature
extraction network as in our approach.
Query Costs We consider two ways of generating query
costs: (i) c(i, j)=1for all i, j, where Cis just the number
of queries, and (ii) c(i, j)is based on Manhattan distance
between iandj. Most of the results we present reflect the
second setting; the results for uniform query costs are qual-
itatively similar and provided in the Supplement.
Datasets We evaluate the proposed approach using two
datasets: xView [17] and DOTA [34]. xView is a satellite
imagery dataset which consists of large satellite images rep-
resenting 60 categories, with approximately 3000 pixels in
each dimensions. We use 67% and33% of the large satel-
lite images to train and test the policy network respectively.
DOTA is also a satellite imagery dataset. We re-scale the
original∼3000×3000px images to 1200 ×1200px. Un-
less otherwise specified, we use N=36non-overlapping
pixel grids each of size 200 ×200.
5.1. Results on the xView Dataset
We begin by evaluating the proposed approaches on the
xView dataset, varying search budgets C∈{25,50,75}and
5
number of grid cells N∈{30,48,99}. We consider two tar-
get classes: small car andbuilding . As the dataset contains
variable size images, take random crops of 2500×3000 for
N=30,2400×3200 pixels for N=48, and 2700×3300
forN=99, thereby ensuring equal grid cell sizes.
Table 1: ANT comparisons for the small car target class on xView.
Method C=25 C=50 C=75
random search (N=30) 3.41 3.95 4.52
greedy classification (N=30) 3.91 4.60 4.76
greedy selection [30] (N=30) 3.90 4.63 4.78
active learning [37] (N=30) 3.92 4.58 4.73
conventional active search [15] (N=30) 3.61 4.17 4.70
VAS (N=30) 4.61 7.49 9.88
random search (N=48) 3.20 3.66 4.11
greedy classification (N=48) 3.87 4.29 4.52
greedy selection [30] (N=48) 3.89 4.42 4.53
active learning [37] (N=48) 3.87 4.28 4.51
conventional active search [15] (N=48) 3.26 3.74 4.32
VAS (N=48) 4.56 7.45 9.63
random search (N=99) 1.10 2.15 2.96
greedy classification (N=99) 1.72 2.79 3.36
greedy selection [30] (N=99) 1.78 2.83 3.41
active learning [37] (N=99) 1.69 2.78 3.33
conventional active search [15] (N=99) 1.42 2.31 3.10
VAS (N=99) 2.72 4.42 5.78
Table 2: ANT comparisons for the building target class on xView.
Method C=25 C=50 C=75
random search (N=30) 3.97 4.94 5.39
greedy classification (N=30) 4.69 5.27 5.80
greedy selection [30] (N=30) 4.84 5.33 5.82
active learning [37] (N=30) 4.67 5.24 5.80
conventional active search [15] (N=30) 4.15 5.20 5.51
VAS (N=30) 5.65 9.31 12.20
random search (N=48) 3.47 3.96 4.26
greedy classification (N=48) 3.90 4.43 4.61
greedy selection [30] (N=48) 3.95 4.51 4.67
active learning [37] (N=48) 3.88 4.43 4.60
conventional active search [15] (N=48) 3.70 4.11 4.38
VAS (N=48) 5.61 9.26 12.15
random search (N=99) 1.55 2.99 4.18
greedy classification (N=99) 2.17 3.96 4.84
greedy selection [30] (N=99) 2.29 4.21 5.22
active learning [37] (N=99) 2.17 3.95 4.82
conventional active search [15] (N=99) 1.68 3.10 4.33
VAS (N=99) 4.29 6.91 8.98
The results are presented in Table 1 for the small car
class and in Table 2 for the building class. We see sub-
stantial improvements in performance of the proposed VAS
approach compared to all baselines, ranging from 15–260%
improvement relative to the most competitive state-of-the-
art approach, greedy selection . There are two general con-
sistent trends. First, as the number of grids Nincreases
compared to C(corresponding to sets of rows in either ta-
ble), performance of all methods declines, as the task be-
comes more challenging. However, the decline in perfor-mance is typically much greater for our baselines than for
VAS. Second, overall performance improves as Cincreases
(columns in both tables), and the relative advantage of VAS
increases, as it is better able to take advantage of the greater
budget than the baselines.
Figure 4: Comparison of policies learned using VAS(left) and the greedy
selection baseline method (right).
In Figure 5 we visually illustrate VAS search strategy in
comparison with the greedy selection baseline (the best per-
forming baseline). The plus signs correspond to success-
ful queries, to unsuccessful queries, and arrows repre-
sent query order. This shows that VAS quickly learns to
take advantage of the visual similarities between grids (af-
ter the first several failed queries, the rest are successful),
whereas our most competitive baseline— greedy selection —
fails to take advantage of such information. During the ini-
tial search phase, the V AS policy explores different types of
grids before exploiting grids it believes to have target ob-
jects.
Finally, we perform an ablation study to understand the
added value of including remaining budget Bas an input
in the VAS policy network. To this end, we modify the
combined feature representation of size (2N+1)×14×14,
consisting of input and auxiliary state features, each of size
N×14×14, and a single channel of size 14×14containing
the information of remaining search budget, as depicted in
Figure 3. We only eliminate the channel from the combined
feature representation that contains the information about
the number of queries left, resulting in 2N×14×14size
feature map. The resulting policy network is then trained
just as the original VAS architecture.
Table 3: Comparative ANT performance of VAS without remaining
search budget andVAS using small car as the target class.
Method C=25 C=50 C=75
VAS w/o remaining search budget (N=30) 4.47 7.38 9.62
VAS (N=30) 4.61 7.49 9.88
VAS w/o remaining search budget (N=48) 4.34 7.31 9.49
VAS (N=48) 4.56 7.45 9.63
VAS w/o remaining search budget (N=99) 2.63 4.29 5.69
VAS (N=99) 2.72 4.42 5.78
We compare the performance of the policy without re-
maining search budget (referred to as VAS without remain-
ing search budget ) with VAS in Table 3. Across all problem
sizes and search budgets, we observe a relatively small but
6
consistent improvement ( ∼1–3%) from using the remaining
search budget Bas an explicit input to the policy network.
5.2. Results on the DOTA Dataset
Next, we repeat our experiments on the DOTA dataset.
We use large vehicle andship as our target classes. In both
cases, we also report results with non-overlapping pixel
grids of size 200×200and150×150(N=36andN=64,
respectively). We again use C∈{25,50,75}.
Table 4: ANT comparisons for the large vehicle target class on DOTA.
Method C=25 C=50 C=75
random search (N=36) 1.79 3.50 5.10
greedy classification (N=36) 2.64 4.07 5.88
greedy selection [30] (N=36) 2.82 4.21 5.97
active learning [37] (N=36) 2.63 4.06 5.84
conventional active search [15] (N=36) 1.92 3.63 5.34
VAS (N=36) 4.63 6.79 8.07
random search (N=64) 1.48 2.96 3.91
greedy classification (N=64) 2.59 3.77 5.48
greedy selection [30] (N=64) 2.72 4.10 5.77
active learning [37] (N=64) 2.57 3.74 5.47
conventional active search [15] (N=64) 1.64 3.15 4.23
VAS (N=64) 5.33 8.47 10.51
Table 5: ANT comparisons for the shiptarget class on the DOTA dataset.
Method C=25 C=50 C=75
random search (N=36) 1.73 3.07 4.26
greedy classification (N=36) 2.04 3.65 4.92
greedy selection [30] (N=36) 2.33 3.84 5.01
active learning [37] (N=36) 2.01 3.64 4.91
conventional active search [15] (N=36) 1.86 3.25 4.40
VAS (N=36) 3.31 5.34 6.74
random search (N=64) 1.26 2.33 3.14
greedy classification (N=64) 1.89 3.06 3.75
greedy selection [30] (N=64) 2.07 3.32 4.02
active learning [37] (N=64) 1.87 3.05 3.72
conventional active search [15] (N=64) 1.41 2.48 3.38
VAS (N=64) 3.58 6.38 7.83
The results are presented in Tables 4 and 5, and are
broadly consistent with our observations on the xView
dataset, with VAS outperforming all baselines by ∼40−
80%, with the greatest improvement typically coming with
a higher search budget C.
5.3. Visualization of V AS Strategy
In figure 5, we demonstrate the sequential behavior of a
pretrained V AS policy during inference. We have shaded
the grid such that darker indicates higher probability. The
darkest grid at each step is the target to be revealed. In the
first row we have V AS searching for large vehicles . In step
1, V AS looks at the roof of a building, which looks very
similar to a large vehicle in the overhead view. Next in step
3, it searches a grid with a large air conditioner which also
looks similar to a large vehicle . Having viewed these twoconfusers, V AS now learns that the rest of the grids with
building roofs likely contain no large vehicles . It is impor-
tant to note that this eliminates a large portion of the middle
of the image from consideration as it is entirely roof tops.
In step 5, it moves to an area which is completely different,
a road where it finds a large vehicle . V AS now aggressively
begins examining grids with roads. In steps 7 through 13
it searches roads discovering large vehicle . Finally in step
15 it explores to a parking lot containing a large vehicle . In
our middle example we have V AS targeting small cars . In
step 1, V AS targets a road and fails to find a car. In step
3, it searches another road in a different region and finds a
car. Having explored regions with prominent major roads
it moves to a parking lot in step 5 and finds a car. It now
searches a similar parking lot in step 7. Having explored
grids with parking lots it goes back to searching minor roads
for the duration of its search. V AS does not visit a parking
lot in the north east corner, but this parking lot is visually
much different from the other two (i.e. it’s not rectangular).
In our bottom example we have V AS searching for ships .
In step 1, V AS searches near a harbor. Having found a ship
it begins exploring similar harbor regions. In step 3 and
5 it searches other parts of the same harbor finding ships .
In steps 7-9, it searches areas similar to the harbor with-
outships . V AS now learns that ships are not likely present
in the rest of the dock and explores different regions leav-
ing the rest of the dock unexplored. These three examples
demonstrate V AS’s tendency for the explore-exploit behav-
ior typical of reinforcement learning algorithms. Addition-
ally, we note that V AS has an ability to eliminate large areas
that would otherwise confuse standard greedy approaches.
5.4. Efficacy of Test-Time Adaptation
One of the important features of the visual active search
problem is that queries actually allow us to observe partial
information about target labels at inference time . Here, we
evaluate how our approaches to TTA that take advantage
of this information perform compared to the baseline VAS
without TTA , as well as state-of-the-art TTA baselines dis-
cussed in Section 4.2 (where FixMatch is adapted to also
take advantage of observed labels).
Consider first the case where there is no difference be-
tween training and test distribution over classes. As before
we consider xView and DOTA for analysis. The results are
presented in Figure 6, and show a consistent pattern. The
TTT approach performs the worst, followed by (out adap-
tation of) FixMatch , which is only slightly better than TTT.
Stepwise TTA outperforms both TTT andFixMatch , albeit
slightly, and Online TTA is, somewhat surprisingly much
better than all others (this is surprising since it has a lower
frequency of model update compared to Stepwise TTA ).
Finally, we consider a TTA setting in which the domain
exhibits a non-trivial distributional shift at inference time.
7
step 1
 step 3
 step 5
 step 7
 step 9
 step 11
 step 13
 step 15
Figure 5: Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS for different target types.
Figure 6: Comparative results of TTA methods on V AS framework.
xView (top; small car target): (left) N=48, (right) N=99. DOTA
(bottom; large vehicle target): (left) N=36, (right) N=64.
In this case, we would expect the conventional TTT andFix-
Match methods to be more competitive, as they have been
specifically designed to account for distribution shift. We
model distribution shift by training the search policy using
one target object, and then applying it in the decision con-
text for another target object. Specifically, for xView, we
usesmall car as the target class during training, and build-
ingas the target class at test time. Similarly, on the DOTA
dataset we use large vehicle as the target class at training
time, and use ship as the target at test time.
The results for the TTA setting with distribution shift are
presented in Table 6 and 7 for the xView and the DOTA
dataset respectively, where we also add a comparison to the
VAS without TTA of any kind. We observe that the results
here remain consistent, with the proposed Online TTA out-
performing the other approaches, with Stepwise TTA yield-
ing the second-best performance.Table 6: Comparative results on xView dataset with small car andBuild-
ingas the target class during training and inference respectively.
Method C=25 C=50 C=75
without TTA (N=30) 5.28 8.58 11.42
TTT [27] (N=30) 5.30 8.61 11.45
FixMatch [26] (N=30) 5.31 8.62 11.47
Stepwise TTA (N=30) 5.33 8.64 11.50
Online TTA (N=30) 5.42 8.69 11.58
Table 7: Comparative results on DOTA dataset with large vehicle and
ship as the target class during training and inference respectively.
Method C=25 C=50 C=75
without TTA (N=36) 2.69 4.38 5.84
TTT [27] (N=36) 2.70 4.39 5.84
FixMatch [26] (N=36) 2.70 4.39 5.84
Stepwise TTA (N=36) 2.71 4.40 5.85
Online TTA (N=36) 2.73 4.42 5.98
6. Conclusion
Our results show that VAS is an effective framework for
geospatial broad area search. Notably, by applying simple
TTA techniques, the performance of VAS can be further im-
proved at test time in a way that is robust to target class shift.
The proposed VAS framework also suggests a myriad of fu-
ture directions. For example, it may be useful to develop
more effective approaches for learning to search within a
task, as is common in past active search work. Addition-
ally, the search process may often involve additional con-
straints, such as constraints on the sequence of regions to
query. Moreover, it’s natural to generalize query outcomes
to be non-binary (e.g., returning the number of target object
instances in a region).
Acknowledgments This research was partially supported
by the NSF (IIS-1905558, IIS-1903207, and IIS-2214141),
ARO (W911NF-18-1-0208), Amazon, NVIDIA, and the
Taylor Geospatial Institute.
8
References
[1] Elizabeth Bondi, Debadeepta Dey, Ashish Kapoor, Jim Pi-
avis, Shital Shah, Fei Fang, Bistra Dilkina, Robert Han-
naford, Arvind Iyer, Lucas Joppa, et al. Airsim-w: A simu-
lation environment for wildlife conservation with uavs. In
ACM SIGCAS Conference on Computing and Sustainable
Societies , pages 1–12, 2018.
[2] Elizabeth Bondi, Fei Fang, Mark Hamilton, Debarun Kar,
Donnabell Dmello, Jongmoo Choi, Robert Hannaford,
Arvind Iyer, Lucas Joppa, Milind Tambe, et al. Spot poach-
ers in action: Augmenting conservation drones with auto-
matic detection in near real time. In AAAI Conference on
Artificial Intelligence , 2018.
[3] Elizabeth Bondi, Raghav Jain, Palash Aggrawal, Saket
Anand, Robert Hannaford, Ashish Kapoor, Jim Piavis, Shital
Shah, Lucas Joppa, Bistra Dilkina, et al. Birdsai: A dataset
for detection and tracking in aerial thermal infrared videos.
InIEEE/CVF Winter Conference on Applications of Com-
puter Vision , pages 1747–1756, 2020.
[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In Pro-
ceedings of the IEEE/CVF international conference on com-
puter vision , pages 9650–9660, 2021.
[5] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Ab-
hinav Gupta, and Russ R Salakhutdinov. Object goal navi-
gation using goal-oriented semantic exploration. Advances
in Neural Information Processing Systems , 33:4247–4258,
2020.
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
[7] Kshitij Dwivedi, Gemma Roig, Aniruddha Kembhavi, and
Roozbeh Mottaghi. What do navigation agents learn about
their environment? In IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 10276–10285, 2022.
[8] Fei Fang, Thanh Hong Nguyen, Rob Pickles, Wai Y Lam,
Gopalasamy R Clements, Bo An, Amandeep Singh, Milind
Tambe, and Andrew Lemieux. Deploying paws: Field opti-
mization of the protection assistant for wildlife security. In
AAAI Conference on Artificial Intelligence , volume 16, pages
3966–3973, 2016.
[9] Fei Fang, Peter Stone, and Milind Tambe. When security
games go green: Designing defender strategies to prevent
poaching and illegal fishing. In International Joint Confer-
ence on Artificial Intelligence , 2015.
[10] Roman Garnett, Thomas G ¨artner, Martin V ogt, and J ¨urgen
Bajorath. Introducing the ‘active search’method for iterative
virtual screening. Journal of Computer-Aided Molecular De-
sign, 29(4):305–314, 2015.
[11] Roman Garnett, Yamuna Krishnamurthy, Xuehan Xiong,
Jeff Schneider, and Richard Mann. Bayesian optimal active
search and surveying. arXiv preprint arXiv:1206.6406 , 2012.[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016.
[13] Shali Jiang, Roman Garnett, and Benjamin Moseley. Cost
effective active search. Advances in Neural Information Pro-
cessing Systems , 32, 2019.
[14] Shali Jiang, Gustavo Malkomes, Matthew Abbott, Benjamin
Moseley, and Roman Garnett. Efficient nonmyopic batch
active search. Advances in Neural Information Processing
Systems , 31, 2018.
[15] Shali Jiang, Gustavo Malkomes, Geoff Converse, Alyssa
Shofner, Benjamin Moseley, and Roman Garnett. Efficient
nonmyopic active search. In International Conference on
Machine Learning , pages 1714–1723. PMLR, 2017.
[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. Communications of the ACM , 60(6):84–90, 2017.
[17] Darius Lam, Richard Kuzma, Kevin McGee, Samuel Doo-
ley, Michael Laielli, Matthew Klaric, Yaroslav Bulatov, and
Brendan McCord. xview: Objects in context in overhead
imagery. arXiv preprint arXiv:1802.07856 , 2018.
[18] Bar Mayo, Tamir Hazan, and Ayellet Tal. Visual navigation
with spatial attention. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 16898–16907,
June 2021.
[19] Chenlin Meng, Enci Liu, Willie Neiswanger, Jiaming Song,
Marshall Burke, David Lobell, and Stefano Ermon. Is-
count: Large-scale object counting from satellite images
with covariate-based importance sampling. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 36,
pages 12034–12042, 2022.
[20] Lingchen Meng, Hengduo Li, Bor-Chun Chen, Shiyi Lan,
Zuxuan Wu, Yu-Gang Jiang, and Ser-Nam Lim. Adavit:
Adaptive vision transformers for efficient image recognition.
InIEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 12309–12318, 2022.
[21] Mahdi Kazemi Moghaddam, Ehsan Abbasnejad, Qi Wu,
Javen Qinfeng Shi, and Anton Van Den Hengel. Foresi:
Success-aware visual navigation agent. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV) , pages 691–700, January 2022.
[22] Athanasios Papadopoulos, Pawel Korus, and Nasir Memon.
Hard-attention for scalable image classification. In Advances
in Neural Information Processing Systems , volume 34, pages
14694–14707, 2021.
[23] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347 , 2017.
[24] Burr Settles. Active learning literature survey. 2009.
[25] Burr Settles. Active learning. Synthesis lectures on artificial
intelligence and machine learning , 6(1):1–114, 2012.
[26] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,
Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying
semi-supervised learning with consistency and confidence.
Advances in neural information processing systems , 33:596–
608, 2020.
9
[27] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei
Efros, and Moritz Hardt. Test-time training with self-
supervision for generalization under distribution shifts. In
International conference on machine learning , pages 9229–
9248. PMLR, 2020.
[28] Richard S Sutton, David McAllester, Satinder Singh, and
Yishay Mansour. Policy gradient methods for reinforcement
learning with function approximation. Advances in neural
information processing systems , 12, 1999.
[29] Chittesh Thavamani, Mengtian Li, Nicolas Cebron, and
Deva Ramanan. Fovea: Foveated image magnification for
autonomous navigation. In IEEE/CVF International Confer-
ence on Computer Vision , pages 15539–15548, 2021.
[30] Burak Uzkent and Stefano Ermon. Learning when and where
to zoom with deep reinforcement learning. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12345–12354, 2020.
[31] Burak Uzkent, Christopher Yeh, and Stefano Ermon. Effi-
cient object detection in large images using deep reinforce-
ment learning. In IEEE/CVF Winter Conference on Applica-
tions of Computer Vision (WACV) , March 2020.
[32] Yi Wang, Youlong Yang, and Xi Zhao. Object detection us-
ing clustering algorithm adaptive searching regions in aerial
images. In European Conference on Computer Vision , pages
651–664, 2020.
[33] Zuxuan Wu, Caiming Xiong, Yu-Gang Jiang, and Larry S
Davis. Liteeval: A coarse-to-fine framework for resource ef-
ficient video recognition. In Advances in Neural Information
Processing Systems , volume 32, 2019.
[34] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-
longie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-
pei Zhang. Dota: A large-scale dataset for object detection
in aerial images. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3974–3983,
2018.
[35] Chenhongyi Yang, Zehao Huang, and Naiyan Wang. Query-
det: Cascaded sparse query for accelerating high-resolution
small object detection. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 13668–13677,
June 2022.
[36] Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and
Gao Huang. Resolution adaptive networks for efficient in-
ference. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2020.
[37] Donggeun Yoo and In So Kweon. Learning loss for ac-
tive learning. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 93–102,
2019.
10
APPENDIX: A Visual Active Search Frame-
work for Geospatial Exploration
In this appendix, we provide details that could not be in-
cluded in the main paper owing to space constraints, includ-
ing: (A) Performance of V AS under uniform query cost;
(B) V AS Pseudocode; (C) Policy architecture and train-
ing hyperparameter; (D) Search Performance Comparison
with Different Feature Extractor Module; (E) More Visual
Illustration of V AS and the Most Competitive Greedy Se-
lection baseline Method; (F) Assessment of V AS and other
Baseline Methods with a Different Evaluation Metric; (G)
Search Performance Comparison with Other Policy Learn-
ing Algorithm (PPO); (H) Sensitivity Analysis of VAS; (I)
Efficacy of TTA on Search Tasks involving Large Number
of Grids; (J) Saliency map visualization of VAS;
A. Performance of V AS under Uniform Query
Cost
In this section, we report the performance of V AS under
uniform query cost. The results are presented in the follow-
ing Table 8 for the small car target class and in Table 9 for
thebuilding class from the xView dataset. We observe sig-
nificant improvements in performance of the proposed V AS
approach compared to all baselines, ranging from 11−25%
improvement relative to the most competitive greedy selec-
tionapproach.
Table 8: ANT comparisons for the small car target class.
Method C=12 C=15 C=18
random search (N=30) 4.57 5.66 6.85
greedy classification (N=30) 5.31 6.24 7.25
greedy selection [30] (N=30) 5.47 6.45 7.46
active learning [37] (N=30) 5.28 6.21 7.22
conventional AS [15] (N=30) 4.86 5.97 6.92
VAS (N=30) 6.03 7.24 8.24
random search (N=48) 3.80 4.97 5.98
greedy classification (N=48) 4.69 5.48 6.79
greedy selection [30] (N=48) 4.92 5.81 6.98
active learning [37] (N=48) 4.68 5.46 6.78
conventional AS [15] (N=48) 3.96 5.45 6.14
VAS (N=48) 5.62 6.81 7.86
random search (N=99) 3.12 3.61 4.45
greedy classification (N=99) 3.68 4.22 4.97
greedy selection [30] (N=99) 3.81 4.52 5.28
active learning [37] (N=99) 3.65 4.19 4.93
conventional AS [15] (N=99) 3.24 3.87 4.61
VAS (N=99) 4.61 5.64 6.55
We also present the results for large vehicle andship tar-
get class from DOTA dataset in the following Table 10 and
11 respectively. We see the proposed V AS performs notice-
ably better than all baselines, ranging from 16–56% relativeTable 9: ANT comparisons for the building target class.
Method C=12 C=15 C=18
random search (N=30) 5.54 7.18 8.58
greedy classification (N=30) 5.88 7.72 9.21
greedy selection [30] (N=30) 6.39 7.95 9.52
active learning [37] (N=30) 5.86 7.68 9.16
conventional AS [15] (N=30) 5.76 7.37 8.87
VAS (N=30) 7.56 9.02 10.41
random search (N=48) 4.97 6.41 7.66
greedy classification (N=48) 5.68 6.95 8.40
greedy selection [30] (N=48) 5.93 7.26 8.71
active learning [37] (N=48) 5.68 6.93 8.37
conventional AS [15] (N=48) 5.22 6.67 7.84
VAS (N=48) 6.85 8.29 9.65
random search (N=99) 4.35 5.37 6.44
greedy classification (N=99) 4.92 6.02 7.41
greedy selection [30] (N=99) 5.38 6.53 7.79
active learning [37] (N=99) 4.91 6.00 7.40
conventional AS [15] (N=99) 4.55 5.64 6.75
VAS (N=99) 6.75 8.27 9.46
to the state-of-the-art greedy selection approach. The exper-
imental outcomes in different settings are qualitatively sim-
ilar to the settings under Manhattan distance-based query
cost.
Table 10: ANT comparisons for the large vehicle target
class.
Method C=12 C=15 C=18
random search (N=36) 3.44 4.08 5.19
greedy classification (N=36) 3.95 4.62 5.56
greedy selection [30] (N=36) 4.18 4.86 5.89
active learning [37] (N=36) 3.92 4.60 5.54
conventional AS [15] (N=36) 3.71 4.22 5.28
VAS (N=36) 5.14 6.05 7.00
random search (N=64) 3.40 4.03 5.14
greedy classification (N=64) 3.87 4.59 5.55
greedy selection [30] (N=64) 3.99 4.77 5.67
active learning [37] (N=64) 3.85 4.54 5.51
conventional AS [15] (N=64) 3.61 4.12 5.26
VAS (N=64) 6.30 7.65 8.90
B. V AS Pseudocode
We have included the pseudocode of our proposed Visual
Active Search algorithm in table 1.
C. Policy architecture, training hyperparame-
ter, and the details of TTA
In table 12, we detail the VAS policy architecture with
number of target grids as N. We use a learning rate of 10−4,
11
Table 11: ANT comparisons for the ship target class.
Method C=12 C=15 C=18
random search (N=36) 2.69 3.38 4.46
greedy classification (N=36) 3.21 3.99 5.11
greedy selection [30] (N=36) 3.44 4.23 5.32
active learning [37] (N=36) 3.18 3.95 5.07
conventional AS [15] (N=36) 2.97 3.56 4.77
VAS (N=36) 4.58 5.34 6.23
random search (N=64) 2.54 3.01 4.21
greedy classification (N=64) 3.34 3.74 4.94
greedy selection [30] (N=64) 3.62 3.95 5.10
active learning [37] (N=64) 3.32 3.71 4.93
conventional AS [15] (N=64) 2.87 3.38 4.53
VAS (N=64) 5.04 6.50 7.38
Algorithm 1 The VAS algorithm.
Require: A search task instance (xi, yi); budget constraint C;
search policy ψ(xi, o, B)with parameters θ;
1:Initialize o0=[0...0];B0=C; step t=0
2:while Bt>0do
3: ˜y=ψ(xi, ot, Bt)
4: j← /leftr⫯g⊸tl⫯neSamplej∈{Unexplored Grids }[˜y]
5: Query grid cell with index jand observe true label y(j).
6: Obtain reward Rt=y(j).
7: Update ottoot+1witho(j)=2y(j)−1.
8: Update BttoBt+1withBt+1=Bt−c(k, j)(assuming
we query k’th grid at (t−1)).
9: Collect transition tuple ( τ) at step t, i.e., τt=(state
=(xi, ot, Bt), action = j, reward = Rt, next state =
(xi, ot+1, Bt+1) ).
10: t← /leftr⫯g⊸tl⫯net+1
11:end while
12:Update the search policy parameters, i.e., θusing REIN-
FORCE objective as in 3 based on the collected transition
tuples ( τt) throughout the episode.
13:Return updated search policy parameters, i.e., θ.
batch size of 16, number of training epochs 200, and the
Adam optimizer to train the policy network in all results.
We add a self-supervised head rto the V AS policy archi-
tecture for TTT. The architecture of self-supervised head is
detailed in table 13. We applied a series of 4 up-convolution
layers with intermediate ReLU activations followed by a
tanh activation layer on the semantic features extracted us-
ing ResNet34. For FixMatch, our V AS architecture remains
unchanged, and we apply only spatially invariant augmen-
tations (e.g auto contrast, brightness, color, and contrast)
and ignore all translation augmentations (translate X, trans-
late Y , ShearX etc.) to obtain the augmented version of the
input image. We update the model parameters after every
query step using a cross-entropy loss between a pseudo-
target and a predicted vector as described below. We define
the pseudo-target vector as follows. Whenever a query jissuccessful ( yj=1), we construct a label vector as the one-
hot vector with a 1 in the jth grid cell. However if yj=0, we
associate each queried grid cell with a 0, and assign a uni-
form probability distribution over all unqueried grids. Pre-
diction vector is the “logit” representation obtained from the
V AS policy. We used the Adam optimizer with a learning
rate of 10−4for both TTT and FixMatch.
Table 12: V AS Policy Architecture
Layers Configuration o/p Feature Map size
Input RGB Image 3 ×2500×3000
Feat. Extraction ResNet-34 512 ×14×14
Conv1 c:N k: 1×1 N×14×14
Tile1 Grid State ( o) N×14×14
Tile2 Query Left ( B) 1×14×14
Channel Concat Conv1,Tile1,Tile2 (2N+1)×14×14
Conv2 c:3 k: 1×1 3×14×14
Flattened Conv2 588
FC1+ReLU ( 588−>2N) 2N
FC2 ( 2N−>N) N
Table 13: Self-supervised head Architecture
Layers Configuration
Input: Latent Feature 36×14×14
1st Up-conv layer in-channel:36;out-channel:36;k: 3×3;stride:2;padd:0
Activation Layer ReLU
2nd Up-conv layer in-channel:36;out-channel:24;k: 3×3;stride:2;padd:1
Activation Layer ReLU
3rd Up-conv layer in-channel:24;out-channel:12;k: 2×2;stride:4;padd:1
Activation Layer ReLU
4th Up-conv layer in-channel:12; out-channel:3; k: 2×2; stride:2; padd:0
Normalization layer tanh
D. Search Performance Comparison with Dif-
ferent Feature Extractor Module
In this section, we compare the performance of V AS with
different feature extraction module. We use state-of-the-art
feature extraction modules, such as ViT [6] and DINO [4]
for comparison. The Vision Transformer (ViT) [6] is a
transformer encoder model (BERT-like) pretrained on a
large collection of images in a self-supervised fashion,
namely ImageNet-21k (a collection of 14 million images),
at a resolution of 224×224pixels, with patch resolution
of16×16. Note that, we use off the shelf pretrained ViT
model provided by huggingface (google/vit-base-patch16-
224-in21k). We call the resulting policy VAS-ViT . Similar to
12
ViT, DINO [4] is also based on transformer encoder model.
Images are presented to the DINO model as a sequence of
fixed-size patches (resolution 8x8), which are linearly em-
bedded. For our experiment, we use DINO pretrained on
ImageNet-1k, at a resolution of 224x224 pixels. For our ex-
periments, we use pretrained DINO model provided by hug-
gingface (facebook/dino-vits8). We call the resulting policy
as V AS-DINO. In table 14, 15 we report the performance
of V AS-ViT and V AS-DINO and compare them with V AS.
Table 14: ANT comparisons with different feature extraction
module for the small car target class on xView.
Method C=25 C=50 C=75
VAS-DINO (N=30) 4.56 7.41 9.83
VAS-ViT (N=30) 4.64 7.47 9.86
VAS (N=30) 4.61 7.49 9.88
VAS-DINO (N=48) 4.52 7.41 9.59
VAS-ViT (N=48) 4.56 7.44 9.68
VAS (N=48) 4.56 7.45 9.63
Table 15: ANT comparisons with different feature extraction
module for the large vehicle target class on DOTA.
Method C=25 C=50 C=75
VAS-DINO (N=36) 4.56 6.75 8.03
VAS-ViT (N=36) 4.60 6.82 8.09
VAS (N=36) 4.63 6.79 8.07
VAS-DINO (N=64) 5.27 8.44 10.45
VAS-ViT (N=64) 5.31 8.51 10.48
VAS (N=64) 5.33 8.47 10.51
E. More Visual Illustration of V AS and the
Most Competitive Greedy Selection base-
line Method
In this section, we provide additional visualization of
comparative exploration behaviour of V AS and and the most
competitive greedy selection baseline approach. In figure 7,
we compare the search strategy with large vehicle as a tar-
get class. In figure 8, we compare the behaviour with small
caras a target class. In figure 9, we analyze the exploration
behaviour with ship as a target class.
These additional visualizations again justify the efficacy
of V AS over the strongest baseline method.
Figure 7: Comparison of policies learned using VAS(left) and the greedy
selection baseline method (right).
Figure 8: Comparison of policies learned using VAS(left) and the greedy
selection baseline method (right).
Figure 9: Comparison of policies learned using VAS(left) and the greedy
selection baseline method (right).
F. Assessment of V AS and other Competitive
Baseline Methods with a Different Evalua-
tion Metric
We additionally compare the search performance of V AS
with all the baseline methods using a metric, which we call
Effective Success Rate (ESR) . A na ¨ıve way to evaluate the
proposed approaches is to simply use success rate , which is
is the fraction of total search steps Kthat identify a target
object. However, if Kexceeds the total number of target
objects in x, normalizing by Kis unreasonable, as even a
perfect search strategy would appear to work poorly. Con-
sequently, we propose effective success rate (ESR) as the
efficacy metric, defined as follows:
ESR=#Targets Discovered
min{#Targets , K }(4)
Thus, we divide by the number of targets one can possibly
discover given a search budget K, rather than simply the
search budget.
F.1. Results on the xView Dataset with ESR as Eval-
uation Metric
We initiate our analysis by assessing the proposed
methodologies using the xView dataset, for varying search
13
budgets K∈{12,15,18}and number of grid cells N∈
{30,48,99}. We also consider two target classes for our
search: small car andbuilding . As the dataset contains
variable size images, take random crops of 2500×3000 for
N=30,2400×3200 pixels for N=48, and 2700×3300 for
N=99, thereby guarantees uniform grid cell dimensions
across the board.
Table 16: ESR comparisons for the small car target class
on the xView dataset.
Method K=12 K=15 K=18
random search (N=30) 0.598 0.632 0.704
greedy classification (N=30) 0.619 0.675 0.718
greedy selection [30] (N=30) 0.627 0.684 0.729
VAS (N=30) 0.766 0.826 0.861
random search (N=48) 0.489 0.517 0.558
greedy classification (N=48) 0.512 0.551 0.589
greedy selection [30] (N=48) 0.524 0.568 0.596
VAS (N=48) 0.694 0.722 0.741
random search (N=99) 0.336 0.369 0.378
greedy classification (N=99) 0.365 0.384 0.405
greedy selection [30] (N=99) 0.376 0.395 0.418
VAS (N=99) 0.564 0.587 0.602
Table 17: ESR comparisons for the building target class on
the xView dataset.
Method K=12 K=15 K=18
random search (N=30) 0.663 0.681 0.697
greedy classification (N=30) 0.701 0.734 0.767
greedy selection [30] (N=30) 0.708 0.740 0.786
VAS (N=30) 0.854 0.886 0.912
random search (N=48) 0.526 0.547 0.556
greedy classification (N=48) 0.548 0.569 0.585
greedy selection [30] (N=48) 0.552 0.574 0.604
VAS (N=48) 0.677 0.716 0.738
random search (N=99) 0.443 0.462 0.483
greedy classification (N=99) 0.460 0.482 0.504
greedy selection [30] 0.469 0.488 0.514
VAS (N=99) 0.654 0.676 0.690
The results are presented in Table 16 for the small car
class and in Table 17 for the building class. We see sig-
nificant improvements in performance of the proposed VAS
approach compared to all baselines, ranging from ∼15–50%
improvement relative to the most competitive state-of-the-
art method, greedy selection .F.2. Results on the DOTA Dataset with ESR as Eval-
uation Metric
We also conduct our experiments on the DOTA dataset.
We use large vehicle andship as our target classes. In both
cases, we also report results with non-overlapping pixel
grids of size 200×200and150×150(N=36andN=64,
respectively). We again use K∈{12,15,18}.
Table 18: ESR comparisons for the large vehicle target
class on the DOTA dataset.
Method K=12 K=15 K=18
random search (N=36) 0.460 0.498 0.533
greedy classification (N=36) 0.602 0.624 0.641
greedy selection [30] (N=36) 0.618 0.637 0.647
VAS (N=36) 0.736 0.744 0.767
random search (N=64) 0.389 0.405 0.442
greedy classification (N=64) 0.606 0.612 0.618
greedy selection [30] (N=64) 0.612 0.618 0.626
VAS (N=64) 0.724 0.738 0.749
Table 19: ESR comparisons for the ship target class on the
DOTA dataset.
Method K=12 K=15 K=18
random search (N=36) 0.491 0.564 0.590
greedy classification (N=36) 0.602 0.629 0.657
greedy selection [30] (N=36) 0.609 0.638 0.665
VAS (N=36) 0.757 0.764 0.776
random search (N=64) 0.334 0.379 0.417
greedy classification (N=64) 0.524 0.541 0.559
greedy selection [30] (N=64) 0.531 0.552 0.576
VAS (N=64) 0.700 0.712 0.733
The results are presented in Tables 18 and 19, and
are broadly consistent with our observations on the xView
dataset, with VAS outperforming all baselines by ∼16–25%,
with the greatest improvement typically coming on more
difficult tasks (small Kcompared to N).
G. Search Performance Comparison with
Other Policy Learning Algorithm (PPO)
We conduct experiments with other policy learning algo-
rithm, such as PPO. With PPO [23], the idea is to constrain
our policy update with a new objective function called the
clipped surrogate objective function that will constrain the
policy change in a small range [1−ϵ,1+ϵ]. Here, ϵis a
hyperparameter that helps us to define this clip range. In
all our experiment with PPO, we use clip range ϵ=0.2
as provided in the main paper [23]. We keep all other hy-
perparameters including policy architecture fixed. We call
14
the resulting policy VAS-PPO . In table 20, 21 we present
the result of V AS-PPO and compare the performance with
V AS. our experimental finding suggests that PPO doesn’t
yield any extra benefits in spite of having added complexity
overhead due to the clipped surrogate objective.
Table 20: ANT comparisons with different policy learning algo-
rithm for the small car target class on xView.
Method C=25 C=50 C=75
VAS-PPO (N=30) 4.15 6.82 9.16
VAS (N=30) 4.61 7.49 9.88
VAS-PPO (N=48) 4.03 6.87 9.02
VAS (N=48) 4.56 7.45 9.63
Table 21: ANT comparisons with different policy learning algo-
rithm for the large vehicle target class on DOTA.
Method C=25 C=50 C=75
VAS-PPO (N=36) 4.01 6.24 7.56
VAS (N=36) 4.63 6.79 8.07
VAS-PPO (N=64) 4.89 7.93 10.12
VAS (N=64) 5.33 8.47 10.51
H. Sensitivity Analysis of VAS
We further analyze the behavior of VAS when we inter-
vene the outcomes of past search queries oin the following
ways: (i) Regardless of the “true” outcome, we set the query
outcome to be “unsuccessful” at every stage of the search
process and observe the change in exploration behavior of
VAS, as depicted in fig 10, 11, 12. (ii) Following a similar
line, we also enforce the query outcome to be “successful”
at each stage and observe how it impacts in exploration be-
havior of VAS, as depicted in fig 10, 11, 12.
Early V AS steps are similar between strictly positive and
strictly negative feedback scenarios. This is due to the grid
prediction network’s input similarity in early stages of V AS.
The imagery and search budget are constant between the
two, and the grid state vector between the two are mostly
the same (as they are both initialized to all zeros). Follow-
ing from step 7 we see V AS diverge. A pattern that emerges
is that when V AS receives strictly negative feedback, it be-
gins to randomly explore. After every unsuccessful query,
V AS learns that similar areas are unlikely to contain objects
of interest and so it rarely visits similar areas. This is most
clear in figure 12 where we see at step 11 it explores an
area that’s completely water. It then visits a distinctive areathat’s mostly water but with land (and no harbor infrastruc-
ture). In strictly positive feedback scenarios we see V AS
aggressively exploit areas that are similar to ones its already
seen, as those areas have been flagged as having objects of
interest. Consider the bottom row for each of figures 10,
11, and 12. In figure 10, after a burn in phase we see V AS
looking at roadsides starting in step 9. In figure 11, V AS
seeks to capture roads. By step 15, V AS has an elevated
probability for nearly the entire circular road in the upper
left of the image. In figure 12, V AS seeks out areas that
look like harbors. Together these examples demonstrate a
key feature of reinforcement learning: the ability to explore
and exploit. Additionally, they show that V AS is sensitive
to query results and uses the grid state to guide its search.
In fig 13, 14, 15, we provide a similar visualization of V AS
under Manhattan distance based query cost.
I. Efficacy of TTA on Search Tasks involving
Large Number of Grids
We conduct experiments with number of grids Nas
900. We train V AS using small car as target while eval-
uate with building as target class. We report the result
in table 22. We observe a significant improvement (up to
4%) in search performance by leveraging TTA in our pro-
posed V AS framework. Specifically, the performance gap
becomes more noticeable as the search budget increases.
We observe a similar trend when we train V AS with building
as target and evaluate using small car as target as presented
in table 23. Such results reinforce the importance of TTA in
scenarios (especially when the search budget is large) when
the search target differs between training and execution en-
vironments.
Table 22: Comparative results on xView dataset with small car
andBuilding as the target class during training and inference re-
spectively under uniform query cost setting.
Method C=18 C=24 C=30 C=60
without TTA (N=900) 3.32 4.30 5.41 10.39
Stepwise TTA (N=900) 3.38 4.37 5.54 10.68
Online TTA (N=900) 3.41 4.42 5.60 10.81
Table 23: Comparative results on xView dataset with building
andsmall car as the target class during training and inference re-
spectively under uniform query cost setting.
Method C=18 C=24 C=30 C=60
without TTA (N=900) 1.61 2.07 2.60 4.93
Stepwise TTA (N=900) 1.63 2.10 2.66 5.04
Online TTA (N=900) 1.66 2.15 2.71 5.12
15
(a) The original image
step 1
 step 3
 step 5
 step 7
 step 9
 step 11
 step 13
 step 15
(b)(Top row )Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS while enforcing
the query outcomes at every stage being “ unsuccessful ”.(Bottom row )Query sequences, and corresponding heat maps (darker indicates
higher probability), obtained using V AS while enforcing the query outcomes at every stage being “ successful ”.
Figure 10: Sensitivity Analysis of VAS with a sample test image and large vehicle as target class under uniform query cost.
J. Saliency map visualization of VAS
In Figure (16,17,18), we show the saliency maps ob-
tained using a pre-trained V AS policy at different stages of
the search process. Note that, at every step, we obtain the
saliency map by computing the gradient of the output that
corresponds to the query index with respect to the input.
Figure 16 corresponds to the large vehicle target class while
the Figure 17 and Figure 18 correspond to the small vehi-
cle. All saliency maps were obtained using the same searchbudget (K = 15). These visualizations capture different as-
pects of the V AS policy. Figure 16 shows its adaptability,
as we see how heat transfers from non-target grids to the
grids containing targets as search progresses. By compar-
ing saliency maps at different stages of the search process,
we see that, V AS explores different regions of the image at
different stages of search, illustrating that our approach im-
plicitly trades off exploration and exploitation in different
ways as search progresses. Figure 17 shows the effect of su-
pervised training on V AS policy. If we observe the saliency
16
(a) The original image
step 1
 step 3
 step 5
 step 7
 step 9
 step 11
 step 13
 step 15
(b)(Top row )Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS while enforcing
the query outcomes at every stage being “ unsuccessful ”.(Bottom row )Query sequences, and corresponding heat maps (darker indicates
higher probability), obtained using V AS while enforcing the query outcomes at every stage being “ successful ”.
Figure 11: Sensitivity Analysis of VAS with a sample test image and caras target class under uniform query cost.
maps across time, we see that V AS never searches for small
vehicles in the sea, having learned not to do this from train-
ing with similar images. Additionally, we notice that the
saliency map’s heat expands from left to right as the time
step increases, encompassing more target grids, leading to
the discovery of more target objects. We observe similar
phenomena in figure 18. We can see that while earlier in
the search process queries tend to be less successful, as the
search evolves, our approach successfully identifies a clus-ter of grids that contain the desired object, exploiting spatial
correlation among them. Additionally, at different stages of
the search process, V AS identifies different clusters of grids
that include the target object.
17
(a) The original image
step 1
 step 3
 step 5
 step 7
 step 9
 step 11
 step 13
 step 15
(b)(Top row )Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS while enforcing
the query outcomes at every stage being “ unsuccessful ”.(Bottom row )Query sequences, and corresponding heat maps (darker indicates
higher probability), obtained using V AS while enforcing the query outcomes at every stage being “ successful ”.
Figure 12: Sensitivity Analysis of VAS with a sample test image and ship as target class under uniform query cost.
18
(a) The original image
step 1
 step 3
 step 5
 step 7
 step 9
 step 11
 step 13
 step 15
(b)(Top row )Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS. (Middle row )
Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS while enforcing the query
outcomes at every stage being “ unsuccessful ”.(Bottom row )Query sequences, and corresponding heat maps (darker indicates higher
probability), obtained using V AS while enforcing the query outcomes at every stage being “ successful ”.
Figure 13: Sensitivity Analysis of VAS with a sample test image and large vehicle as target class under distance based query
cost.
19
(a) The original image
step 1
 step 3
 step 5
 step 7
 step 9
 step 11
 step 13
 step 15
(b)(Top row )Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS. (Middle row )
Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS while enforcing the query
outcomes at every stage being “ unsuccessful ”.(Bottom row )Query sequences, and corresponding heat maps (darker indicates higher
probability), obtained using V AS while enforcing the query outcomes at every stage being “ successful ”.
Figure 14: Sensitivity Analysis of VAS with a sample test image and caras target class under distance based query cost.
20
(a) The original image
step 1
 step 3
 step 5
 step 7
 step 9
 step 11
 step 13
 step 15
(b)(Top row )Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS. (Middle row )
Query sequences, and corresponding heat maps (darker indicates higher probability), obtained using V AS while enforcing the query
outcomes at every stage being “ unsuccessful ”.(Bottom row )Query sequences, and corresponding heat maps (darker indicates higher
probability), obtained using V AS while enforcing the query outcomes at every stage being “ successful ”.
Figure 15: Sensitivity Analysis of VAS with a sample test image and ship as target class under distance based query cost.
21
(a) The original image with query sequence.
step 1 step 5 step 10 step 15
(b) Saliency maps (red indicates high saliency), obtained using V AS at different stages of search process with large vehicle as target.
Figure 16: Saliency map visualization of VAS under uniform cost budget.
22
(a) The original image with query sequence.
step 1 step 5 step 10 step 15
(b) Saliency maps (red indicates high saliency), obtained using V AS at different stages of search process with small car as target.
Figure 17: Saliency map visualization of VAS under uniform cost budget.
23
(a) The original image with query sequence.
step 1 step 5 step 10 step 15
(b) Saliency maps (red indicates high saliency), obtained using V AS at different stages of search process with small car as target.
Figure 18: Saliency map visualization of VAS under uniform cost budget.
24
Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images
Aayush Dhakal1Adeel Ahmad1,2Subash Khanal1Srikumar Sastry1Nathan Jacobs1
1Washington University in St. Louis2Taylor Geospatial Institute
Abstract
We propose a novel weakly supervised approach for cre-
ating maps using free-form textual descriptions (or cap-
tions). We refer to this new line of work of creating tex-
tual maps as zero-shot mapping. Prior works have ap-
proached mapping tasks by developing models that predict
over a fixed set of attributes using overhead imagery. How-
ever, these models are very restrictive as they can only solve
highly specific tasks for which they were trained. Map-
ping text, on the other hand, allows us to solve a large va-
riety of mapping problems with minimal restrictions. To
achieve this, we train a contrastive learning framework
called Sat2Cap on a new large-scale dataset of paired over-
head and ground-level images. For a given location, our
model predicts the expected CLIP embedding of the ground-
level scenery. Sat2Cap is also conditioned on temporal in-
formation, enabling it to learn dynamic concepts that vary
over time. Our experimental results demonstrate that our
models successfully capture fine-grained concepts and ef-
fectively adapt to temporal variations. Our approach does
not require any text-labeled data making the training eas-
ily scalable. The code, dataset, and models will be made
publicly available.
1. Introduction
Creating maps of different attributes is an important task
in many domains. Traditionally, methods of mapping in-
volve exhaustive data collection across vast regions, which
is both time-consuming and labor-intensive. To address this
issue, recent studies have explored the use of Deep Learn-
ing models, with their strong visual learning capabilities, to
directly predict attributes of interest through overhead im-
agery. Salem et al. [2] used overhead images to map tran-
sient attributes [3] and scene categories [4] across large re-
gions, while Streltsov et al. [5] predicted residential build-
ing energy consumption using overhead imagery. Similarly,
Bency et al. [6] also used satellite images to map housingprices. However, all these prior methods focused on learn-
ing some specific pre-defined attributes. These attribute-
specific models are quite restrictive as they cannot map any-
thing beyond their preset list of variables. To overcome this
limitation, we created a novel framework that enables us to
map fine-grained textual descriptions (captions). Our ap-
proach allows us to theoretically map anything that can be
expressed in natural language, and thus, serves as a general
framework for zero-shot mapping.
Recently, several works [7, 8, 9] have delved into model-
ing the relationship between images and text. Models such
as CLIP [7] and ALBEF [8] are trained on large database of
captioned-images to learn a multimodal embedding space
that unifies vision and text space. These embedding spaces
can be utilized to learn the textual descriptors of a given
image. However, a limitation of using overhead imagery
is its tendency to provide coarse and generic features of a
given area. We observe that this property of overhead im-
ages holds true in the CLIP embedding space as well, where
these images are related to coarse textual concepts like city,
beach, or property. These images capture a broad perspec-
tive from above, offering limited insight into the intricate
concepts and dynamics within the location. Ground-level
images, on the other hand, provide more detailed infor-
mation about a place. The CLIP embedding space has a
better understanding of fine-grained concepts for ground-
level images since it was primarily trained on them and
their descriptive captions. Yet, several challenges hinder
the direct utilization of ground-level imagery for mapping
tasks. Firstly, ground-level images are sparsely available
i.e., obtaining a ground-level image for every location on
Earth is not feasible. Secondly, the coverage and quality of
a ground-level image for the same location can have large
variations which could introduce unwanted variations dur-
ing inference.
To address these issues, we present a novel weakly-
supervised cross-view approach for learning fine-grained,
and dynamic textual concepts for geographic locations.
First, we create a large-scale dataset with paired overhead
and ground-level images. Our dataset uses a subset of the
1arXiv:2307.15904v1  [cs.CV]  29 Jul 2023
Figure 1: Captions generated by the CLIPCAP model [1] using CLIP embeddings vs. Dynamic Sat2Cap embeddings. (Row-
1) shows the results from CLIP embeddings which produce many generic descriptions. (Row-2 and 3) shows the results from
our Sat2Cap embeddings for the month of May and January respectively. The captions generated using Sat2Cap embeddings
are more fine-grained and dynamic. While (d) does not add any winter properties for the January query, this behavior is
expected as the image is over Australia where January falls in the middle of summer.
YFCC100M [10]. More details about the dataset are pre-
sented in Section 3.1. Using this paired dataset, we learn the
CLIP distribution of the ground-level scene for a given loca-
tion. CLIP embeddings of ground-level images can describe
detailed textual concepts of that location. Our Sat2Cap
model learns to predict the expected CLIP embedding of the
ground-level scene using the overhead image. Compared to
the CLIP embeddings, Sat2Cap embeddings tend to capture
more fine-grained textual concepts for a given geolocation
as seen in Figure 1.
To account for the temporal associations between vari-
ous concepts and a location, our model is conditioned on
temporal data, specifically, the date and time stamps from
the Flickr imagery. This allows our model to learn fine-
grained concepts that can be dynamically adapted to dif-
ferent date and time settings. Figure 1 shows an example
of CLIP-generated coarse captions vs. Sat2Cap-generated
fine-grained dynamic captions.
Our method is also weakly-supervised and thus does not
require any text-labeled data. Creating a large-scale dataset
of fine-grained captions and geolocation can be challeng-
ing. However, our approach only requires geotagged and
timestamped ground-level images which are easily accessi-
ble and scalable. Additionally, our framework is designed
to learn high-resolution information of a location. This rich
information can be used as an additional signal to solve a
number of other downstream tasks. The following pointssummarize the primary contributions of our work:
• A novel weakly-supervised approach for learning fine-
grained dynamic textual concepts of geographic loca-
tions
• A model for effective cross-view image retrieval be-
tween overhead images and ground-level images taken
in the wild
• A zero-shot approach for creating large-scale textual
maps
• A new large-scale cross-view dataset
2. Related Works
2.1. Deep Learning Based Mapping
Creating maps of attributes of interest is an important
task in many domains. Deep Learning methods have been
used extensively [11, 12, 13, 14, 15] in recent years to make
mapping efficient and scalable. Alhassan et al. [16] fine-
tuned imagenet pretrained models to make landcover pre-
dictions. Similarly [17, 18] leveraged large-scale annotated
data from different sensors to improve landuse and land-
cover classification using deep learning methods. Apart
from remote sensing, other areas have also leveraged deep
learning for their own mapping tasks. Using high-resolution
2
images, [19] trained several deep learning architectures for
automated CNN-based mapping of Martian rockfalls. On
the other hand, [20] used an unsupervised approach to map
regions with high “Au” deposits.
There have been other works that specifically focus on
mapping visual attributes. For instance, [21] used features
from both overhead and ground-level imagery, and intro-
duced a cross-view approach to map scenicness. Later
works focused on creating dynamic maps. Both [22, 2]
conditioned their model on temporal information along with
overhead images to learn dynamic concepts for a given loca-
tion. While the prior works mostly focus on mapping some
specific attribute, we attempt to generalize the mapping pro-
cess. Hence, in our work, we introduce a framework to cre-
ate maps of free-form textual prompts, which we call Tex-
tual Maps.
2.2. Vision-Language Pretraining
Recently, Vision-Language (VL) models have shown
great promise in their ability to model complex relation-
ships between the vision and text space. ConVIRT [23]
and VirTex [24] both introduced methods that used image-
text pairs to learn rich visual representations. CLIP [7]
demonstrated the results of VL pretraining on a large-scale
dataset (400M pairs) and validated the efficacy of large-
scale VL pretraining for several downstream tasks. Flo-
rence [25] and ALIGN [26] further increased the scale of
data by training on 900M and 1.8B pairs respectively. Other
works [8, 9, 27, 28] have since focused on learning better
VL embedding space. With the existence of these pow-
erful pretrained VL models, many researchers have uti-
lized their embedding spaces to solve specific downstream
tasks. CLIPCap [1], and [29] used CLIP space to gener-
ate image captions. Other models like [30, 31, 32] uti-
lized the CLIP space for text-to-image generation. Several
works [33, 34, 35] have also used the CLIP space for im-
age retrieval tasks. In our work, we utilize the rich CLIP
space to bridge the gap between geolocations and their fine-
grained textual descriptions.
3. Method
Our objective is to learn an embedding space that
describes the expected ground-level scene given a geo-
graphic location and an overhead image. Secondly, our
embedding space needs to dynamically adapt to temporal
manipulations for the same location. We have ground-
level images {g1, g2, ...gn}, corresponding overhead im-
ages{o1, o2, ...on}, and respective metadata for the ground-
level images {e1, e2, ...en}. Each eicontains the latitude
and longitude information of the sample, as well as the date
and time when the ground-level image was captured. We
also have a CLIP image encoder fθthat generates CLIP em-
beddings for a given ground-level image.3.1. Dataset
We created a large-scale cross-view dataset to train our
model. The ground-level images in the dataset are taken
from the YFCC100M [10] dataset. The YFCC100M dataset
contains 99.3 million images, collected from Flickr. Our
cross-view dataset uses a smaller sample from this collec-
tion which excludes all US imagery. Our dataset contains
close to 6M images. Each of these images has a geoloca-
tion, timestamp, and other meta information such as tags,
description, camera type, etc. For each Flickr image, we
download an overhead image centered at its location. We
use the Bing Maps API to download 800x800patch satel-
lite images at 0.6m/px resolution.
3.2. Approach
We initialize our Sat2Cap image encoder gθwith
the weights of fθ. A batch of ground-level images
{g1, g2, ..., g k}is passed through the CLIP encoder to get
the ground-level CLIP embeddings. These embeddings
serve as the target for alignment. A batch of correspond-
ing overhead images {o1, o2, ..., o k}is passed through the
Sat2Cap image encoder to obtain the embeddings, as fol-
lows:
Gi=fθ(gi) (1)
Oi=gθ(oi) (2)
To align the overhead image embeddings with the
ground-level CLIP embeddings, we contrastively train our
model using the InfoNCE [36] loss as follows:
L=1
kk∑
i=0−logexp(Oi·Gi/τ)
∑k
j=0exp(Oi·Gj/τ)(3)
We optimize this loss to minimize the distance between
co-located overhead and ground-level images in the CLIP
space. It is worth noting that throughout the training pro-
cess, the CLIP image encoder remains frozen. Hence,
with our training procedure, we essentially allow the over-
head images to move close to images from their respective
ground-level scene in the CLIP space. Our results from
Section 4.1 show that Sat2Cap learns a strong correlation
between co-located overhead and ground-level images.
3.3. Learning Dynamic Concepts of Places
Many ground-level concepts are temporally dependent.
Concepts like ‘crowded street’, ‘snowy place’ etc can dra-
matically vary based on the exact time we query about them.
In order to model such dynamic concepts, we condition
Sat2Cap on the timestamps of the ground-level images.
For each sample, we extract the year, month, day, and
hour in which the ground-level image was taken. We also
add the geolocation information to provide a stronger signal
3
Figure 2: A weakly-supervised learning framework to learn fine-grained and dynamic concepts of geolocations without
explicit text labels.
Method Overhead2Ground (10K) Ground2Overhead (10K)
Model Dynamic Encoder Dropout Meta Information R@5 ↑R@10 ↑Median-R ↓ R@5 ↑R@10 ↑Median-R ↓
CLIP - - - 0.007 0.013 1700 0.108 0.019 2857
ours✗ ✗ ✗ 0.398 0.493 15 0.356 0.450 11
✓ ✗ ✗ 0.322 0.413 34 0.254 0.343 20
✓ ✗ ✓ 0.368 0.467 23 0.298 0.398 13
✓ ✓ ✗ 0.467 0.564 13.5 0.366 0.462 7
✓ ✓ ✓ 0.493 0.591 12 0.390 0.482 6
Table 1: Cross-view retrieval performance of Sat2Cap model: We experiment with three different settings in our model.
First, we experiment with the effect of using the Dynamic Encoder. Secondly, we look at the performance degradation in
scenarios where meta-information is not available in inference. Finally, we experiment with randomly dropping out the
Dynamic Encoder during training.
to the model. We encode this meta-information using sin-
cos encoding. We use a very shallow fully connected layer
which we call the Dynamic Encoder represented by hθ. The
encoded meta information is passed through the Dynamic
Encoder whose output is added element-wise to the unnor-
malized output from the Sat2Cap model. We then normal-
ize the final sum to compute our objective. Our framework,
shown in Figure 2, is defined as:
Oi=gθ(oi) (4)Ei=hθ(ei) (5)
where eiis the output of sin-cos encoding of the date, time,
and location information for sample i.
Si=Oi+Ei (6)
Now the objective function is updated to:
Ldynamic =1
kk∑
i=0−logexp(Si·Gi/τ)
∑k
j=0exp(Si·Gj/τ)(7)
4
Figure 3: Top-9 overhead-to-ground image retrieval: We use the Sat2Cap embeddings of the overhead images and CLIP
embeddings of the ground-level images and show the 9 closest ground-level images retrieved for a query overhead image.
The retrieval was performed using 10,000 samples.
Our training dataset captures the ground-level scenes at
various times. If the model is only allowed to learn using the
overhead image of a location, it will be forced to learn an av-
erage concept for all temporal settings. By conditioning the
problem on additional temporal data, our model learns dif-
ferent ground-level concepts for different times of the day
and year. This ultimately allows Sat2Cap to dynamically
adapt to temporal variations for the same geolocation.
To prevent overfitting to the meta-information, we imple-
ment random dropout of the Dynamic Encoder during train-
ing. This improves retrieval performance but can decrease
the model’s sensitivity to temporal variations. The dropout
causes the model to learn to disregard the meta-information
as it is frequently dropped out during training. Therefore,
we view dropout as a hyperparameter that can be adjusted
to control the dynamic sensitivity of our model.
3.4. Implementations Details
We use a ViT-32B as the CLIP image encoder. This im-
age encoder is kept frozen throughout our training proce-
dure. We use a ViT-32B architecture as the backbone for
our Sat2Cap model. The Sat2Cap backbone is initialized
using CLIP weights. Following [7] we use an AdamW opti-
mizer [37] with a learning rate of 1e−05withβ1= 0.9and
β2= 0.98. We also use a learnable temperature parameterwhich is initialized at τ= 0.07. We use Cosine Annealing
with Warm Restarts [38] as the learning rate scheduler.
We augment the overhead images using RandomRe-
sizedCrop and RandAugment [39]. The overhead images
are normalized using the mean and standard deviation of
the dataset. The training was carried out using Nvidia A100
40GB GPU. Since a larger number of negative samples is
beneficial for contrastive learning, we simulate a large batch
size using a memory bank approach. We initialize a queue
of size 9600 and fill it with precomputed ground-level im-
age CLIP embeddings which are used as negative samples
for computing the loss.
4. Experiments and Results
Our model learns a powerful geo-text embedding space
that can be used for a variety of applications. We experi-
mented with four tasks and show the quantitative and qual-
itative results on them.
4.1. Application 1: Cross-View Image Retrieval
In this experiment, we show that our model learns a
strong relationship between co-located overhead images
and ground-level images in the CLIP space. We randomly
sample 10,000 image pairs from the test set for this exper-
iment. First, we compute the Sat2Cap embeddings for all
5
Figure 4: Top-9 overhead-to-ground image retrieval with temporal manipulation: We show the 9 closest ground-level
images for a query overhead image at two different time settings (11:00 p.m. and 08:00 a.m.).
overhead images in the 10k testset. Then we compute the
CLIP embeddings for all ground-level images in the set. We
then compute top-k and median rank metrics between the
Sat2Cap overhead embeddings and the CLIP ground em-
beddings. Table 1 shows all retrieval results.
As a baseline, we use the distance between CLIP over-
head embeddings and CLIP ground embeddings. We get
an extremely low R@10 score of 0.013 and a median rank
of 1700. These low scores essentially tell us that the over-
head images and corresponding ground-level images lie far
apart in the CLIP space. For Sat2Cap, we first experiment
without using the Dynamic Encoder. Just by contrastively
training the Sat2Cap image encoder with ground-level CLIP
embeddings, we achieve a high R@10 score of 0.493 and a
median rank of 15.
All remaining experiments are conducted on models that
were trained using the Dynamic Encoder. Table 1 shows
that initially, the retrieval scores drop when using the Dy-
namic Encoder. We suspect this happens because the model
starts to overfit on the meta-information, ignoring important
cues from the overhead images. We also see a 5.4% drop in
R@10 metrics when we remove the meta-information dur-ing inference. To reduce the possibility of overfitting, we
randomly drop the Dynamic Encoder during training. We
see that simply adding dropout during training increases
the R@10 score by 12.4%. Another important observa-
tion is that the effect of removing meta-information dur-
ing inference is less severe when using dropout. Hence,
our model achieves good cross-view retrieval scores even if
meta-information is not available during inference.
Figure 3 shows the top 9 closest images retrieved from a
given overhead image. We see that our model is able to re-
trieve ground-level images by relating concepts rather than
direct visual matching. For example, in (a), our model re-
trieves images of people playing golf for an overhead image
of a golf course. Similarly, in (d), our query image seems
to be located over a farm. Here, Sat2Cap has learned to
associate the concept of farmland with cattle and livestock.
It retrieves images of horses and goats which are concepts
that likely reside in the location but are not visible in the
overhead image. This suggests that our model can map fine-
grained concepts of the ground-level scene to a given geolo-
cation. Sat2Cap is also capable of dynamic image retrieval.
Figure 4 shows the top 9 images retrieved at two different
6
Figure 5: Country-level maps of textual descriptions : (Col 1-2) shows the country-level maps created using Sat2Cap for
two prompts: “Kids playing in the sand” and “A busy street in downtown”. (Col 3) shows a landcover map of the respective
countries for comparison.
time settings (11:00 p.m. vs 08:00 a.m.).
4.2. Application 2: Fine-grained and Dynamic Cap-
tion Generation
Our embedding space captures detailed and fine-grained
textual concepts for geographic locations. While the CLIP
space can only provide coarse-level generic descriptions
from an overhead image, our model learns more fine-
grained visual concepts that someone on the ground might
observe. To generate captions from our embeddings, we
use the CLIPCAP [1] model, which maps CLIP space to
text space.
Figure 1 shows that using CLIP embeddings of the over-
head images, the model can only describe generic concepts
of a location like a beach, island, property, etc. Our Sat2Cap
embeddings on the other hand produce much more fine-
grained, as well as, aesthetically pleasing captions. For
example: in figure (a) CLIP generates the caption “aerial
view of a beach” missing out on other important details of
the area. Our model on the other hand generates the cap-
tion “sea facing apartment with swimming pool, terrace in
a quiet residential area”, capturing many intricate concepts
that reside within that location.
Sat2Cap also models temporal variations allowing us to
generate different captions for different times. Figure 1
shows the captions generated for two different months, Mayvs. January. We see that the model reasonably accounts
for the seasonal variations for different months of the year.
However, in figure (d), we see that the model does not add
any cold/winter-specific information for the January input.
This is expected behavior since the image is from Australia,
where the month of January falls right in the middle of sum-
mer.
4.3. Application 3: Zero-Shot Map of Fine-grained
Concepts
We use the rich Sat2Cap embedding space to create
country-level maps of fine-grained textual prompts in a
zero-shot manner. Firstly, we choose two countries to cre-
ate maps: England and Netherlands. Then, we download
satellite imagery that covers these regions. Specifically, we
download 800x800 patches of Bing Map Images at 0.6m/px
resolution. We precompute the Sat2Cap embeddings for all
the images and save them on a disc. Now for any given
text query, we compute the similarity of the CLIP text em-
bedding with all overhead images of the region. Then we
normalize these similarities between 0and1and use the
normalized similarities to create textual maps. The process
of computing similarities for an entire country took only
about 4-5 seconds. Hence, our framework is quite efficient
for mapping large regions and thus could be easily extended
to map the entire world.
7
Figure 6: Localizing textual queries at finer resolution : For each prompt, the image on left shows the big region which is
used for inference. The image on the right shows an image of the ground-level scene at the point with the highest activation,
which was taken by entering the location in Google Maps
Figure 5 shows the maps for two prompts: “Kids playing
in the sand” and “A busy street in downtown”. We added the
phrase “a photo of” at the beginning of each prompt. For
the first prompt, we see that our model activates locations
around the ocean and beaches. Activations in both coun-
tries are high in areas where you might observe a kid play-
ing in the sand. The second prompt activates locations with
major cities in both countries. For England, we see high
activations around London, Oxford, Birmingham, Manch-
ester, Liverpool, etc. For the Netherlands, we see high acti-
vations in Amsterdam, Rotterdam, The Hague, Maastricht,
Groningen, etc as well as other smaller cities. We compare
this map with ESRI’s Sentinel-2 landcover map. From the
landcover maps, we see that our model correctly activates
the fine-grained prompt “A busy street in downtown” in the
urban areas. Thus, we introduce to a novel way to create
large-scale maps in a zero-shot setting.
4.4. Application 4: Geolocalizing Textual Queries
Our model can be used to localize textual queries at a
finer resolution. For this experiment, we draw a 24km2
bounding box over a region. We compute the Sat2Cap sim-
ilarity for all the overhead images in that box with a given
text query. We, then, normalize the similarities between 0
and1and clip the values below 0.5. Figure 6 shows the
results of this experiment. The red spot indicates the loca-
tion with the highest activations for the given query. For
each query, the left figure shows the total area of inference,
and the right figure shows a fine-grained image at the loca-
tion with the highest activation, obtained from Google. Wesee that our model is capable of making reasonable local-
ization for the given queries. For example: in (a) our model
activates over a soccer stadium. Similarly, for (c) and (d),
our model has high activations over an amusement park and
the “National Railway Museum” respectively. Figure (b)
shows that when we compose the concept of people with
animals, our model shows very high activation in farm-like
areas which is where these two concepts would most likely
co-occur. These results show that our model can reasonably
localize the most plausible point within a given area, where
one might observe a given query. This property can be ben-
eficial in solving visual search problems in the geospatial
domain.
5. Conclusion
We introduced a novel weakly supervised framework to
learn a rich embedding space between geolocation and fine-
grained captions. Our method does not require any text-
labeled data making it easy to train and scale. We demon-
strated 4 interesting applications of our model. First, we
showed that our model can be used for cross-view image
retrieval even when using uncurated ground-level images.
Secondly, we showed that our model can be used for gen-
erating fine-grained and dynamic captions for geolocations.
Third, we showed that our model can effectively localize
textual concepts within a given geospatial region. Finally,
we demonstrated how Sat2Cap embeddings can be used for
the newly defined task of large-scale zero-shot mapping.
8
References
[1] R. Mokady, A. Hertz, and A. H. Bermano, “Clip-
cap: Clip prefix for image captioning,” arXiv preprint
arXiv:2111.09734 , 2021.
[2] T. Salem, S. Workman, and N. Jacobs, “Learning a dynamic
map of visual appearance,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pp. 12435–12444, 2020.
[3] P.-Y . Laffont, Z. Ren, X. Tao, C. Qian, and J. Hays, “Tran-
sient attributes for high-level understanding and editing of
outdoor scenes,” ACM Transactions on graphics (TOG) ,
vol. 33, no. 4, pp. 1–11, 2014.
[4] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba,
“Places: A 10 million image database for scene recognition,”
IEEE transactions on pattern analysis and machine intelli-
gence , vol. 40, no. 6, pp. 1452–1464, 2017.
[5] A. Streltsov, J. M. Malof, B. Huang, and K. Bradbury, “Esti-
mating residential building energy consumption using over-
head imagery,” Applied Energy , vol. 280, p. 116018, 2020.
[6] A. J. Bency, S. Rallapalli, R. K. Ganti, M. Srivatsa, and
B. Manjunath, “Beyond spatial auto-regressive models: Pre-
dicting housing prices with satellite imagery,” in 2017
IEEE winter conference on applications of computer vision
(WACV) , pp. 320–329, IEEE, 2017.
[7] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,
S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. ,
“Learning transferable visual models from natural language
supervision,” in International conference on machine learn-
ing, pp. 8748–8763, PMLR, 2021.
[8] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and
S. C. H. Hoi, “Align before fuse: Vision and language rep-
resentation learning with momentum distillation,” Advances
in neural information processing systems , vol. 34, pp. 9694–
9705, 2021.
[9] L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu,
X. Liang, Z. Li, X. Jiang, and C. Xu, “Filip: fine-grained
interactive language-image pre-training,” arXiv preprint
arXiv:2111.07783 , 2021.
[10] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,
D. Poland, D. Borth, and L.-J. Li, “Yfcc100m: The new
data in multimedia research,” Communications of the ACM ,
vol. 59, no. 2, pp. 64–73, 2016.
[11] L. Ilic, M. Sawada, and A. Zarzelli, “Deep mapping gentrifi-
cation in a large canadian city using deep learning and google
street view,” PloS one , vol. 14, no. 3, p. e0212814, 2019.
[12] T. Behrens, K. Schmidt, R. A. MacMillan, and R. A. Vis-
carra Rossel, “Multi-scale digital soil mapping with deep
learning,” Scientific reports , vol. 8, no. 1, p. 15244, 2018.
[13] Z. Zong, J. Feng, K. Liu, H. Shi, and Y . Li, “Deepdpm: Dy-
namic population mapping via deep neural network,” in Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
vol. 33, pp. 1294–1301, 2019.[14] M. Onishi and T. Ise, “Explainable identification and map-
ping of trees using uav rgb image and deep learning,” Scien-
tific reports , vol. 11, no. 1, p. 903, 2021.
[15] C. Greenwell, S. Workman, and N. Jacobs, “What goes
where: Predicting object distributions from above,” in
IGARSS 2018-2018 IEEE International Geoscience and Re-
mote Sensing Symposium , pp. 4375–4378, IEEE, 2018.
[16] V . Alhassan, C. Henry, S. Ramanna, and C. Storie, “A deep
learning framework for land-use/land-cover mapping and
analysis using multispectral satellite imagery,” Neural Com-
puting and Applications , vol. 32, pp. 8529–8544, 2020.
[17] K. Karra, C. Kontgis, Z. Statman-Weil, J. C. Mazzariello,
M. Mathis, and S. P. Brumby, “Global land use / land cover
with sentinel 2 and deep learning,” in 2021 IEEE Interna-
tional Geoscience and Remote Sensing Symposium IGARSS ,
pp. 4704–4707, 2021.
[18] B. Feizizadeh, D. Omarzadeh, M. Kazemi Garajeh, T. Lakes,
and T. Blaschke, “Machine learning data-driven approaches
for land use/cover mapping and trend analysis using google
earth engine,” Journal of Environmental Planning and Man-
agement , vol. 66, no. 3, pp. 665–697, 2023.
[19] V . T. Bickel, S. J. Conway, P.-A. Tesson, A. Manconi,
S. Loew, and U. Mall, “Deep learning-driven detection and
mapping of rockfalls on mars,” IEEE Journal of Selected
Topics in Applied Earth Observations and Remote Sensing ,
vol. 13, pp. 2831–2841, 2020.
[20] S. Zhang, E. J. M. Carranza, H. Wei, K. Xiao, F. Yang, J. Xi-
ang, S. Zhang, and Y . Xu, “Data-driven mineral prospectivity
mapping by joint application of unsupervised convolutional
auto-encoder network and supervised convolutional neural
network,” Natural Resources Research , vol. 30, pp. 1011–
1031, 2021.
[21] S. Workman, R. Souvenir, and N. Jacobs, “Understanding
and mapping natural beauty,” in Proceedings of the IEEE In-
ternational Conference on Computer Vision , pp. 5589–5598,
2017.
[22] S. Workman and N. Jacobs, “Dynamic traffic modeling
from overhead imagery,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pp. 12315–12324, 2020.
[23] Y . Zhang, H. Jiang, Y . Miura, C. D. Manning, and C. P.
Langlotz, “Contrastive learning of medical visual represen-
tations from paired images and text,” in Machine Learning
for Healthcare Conference , pp. 2–25, PMLR, 2022.
[24] K. Desai and J. Johnson, “Virtex: Learning visual rep-
resentations from textual annotations,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pp. 11162–11173, 2021.
[25] L. Yuan, D. Chen, Y .-L. Chen, N. Codella, X. Dai, J. Gao,
H. Hu, X. Huang, B. Li, C. Li, et al. , “Florence: A
new foundation model for computer vision,” arXiv preprint
arXiv:2111.11432 , 2021.
[26] C. Jia, Y . Yang, Y . Xia, Y .-T. Chen, Z. Parekh, H. Pham,
Q. Le, Y .-H. Sung, Z. Li, and T. Duerig, “Scaling up vi-
sual and vision-language representation learning with noisy
9
text supervision,” in International Conference on Machine
Learning , pp. 4904–4916, PMLR, 2021.
[27] J. Yang, C. Li, P. Zhang, B. Xiao, C. Liu, L. Yuan, and J. Gao,
“Unified contrastive learning in image-text-label space,” in
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pp. 19163–19173, 2022.
[28] J. Yang, J. Duan, S. Tran, Y . Xu, S. Chanda, L. Chen,
B. Zeng, T. Chilimbi, and J. Huang, “Vision-language pre-
training with triple contrastive learning,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 15671–15680, 2022.
[29] J. Cho, S. Yoon, A. Kale, F. Dernoncourt, T. Bui, and
M. Bansal, “Fine-grained image captioning with clip re-
ward,” arXiv preprint arXiv:2205.13115 , 2022.
[30] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen,
“Hierarchical text-conditional image generation with clip la-
tents,” arXiv preprint arXiv:2204.06125 , 2022.
[31] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin,
B. McGrew, I. Sutskever, and M. Chen, “Glide: Towards
photorealistic image generation and editing with text-guided
diffusion models,” arXiv preprint arXiv:2112.10741 , 2021.
[32] Z. Wang, W. Liu, Q. He, X. Wu, and Z. Yi, “Clip-gen:
Language-free training of a text-to-image generator with
clip,” arXiv preprint arXiv:2203.00386 , 2022.
[33] M. Hendriksen, M. Bleeker, S. Vakulenko, N. van Noord,
E. Kuiper, and M. de Rijke, “Extending clip for category-to-
image retrieval in e-commerce,” in Advances in Information
Retrieval: 44th European Conference on IR Research, ECIR
2022, Stavanger, Norway, April 10–14, 2022, Proceedings,
Part I , pp. 289–303, Springer, 2022.
[34] A. Sain, A. K. Bhunia, P. N. Chowdhury, S. Koley, T. Xi-
ang, and Y .-Z. Song, “Clip for all things zero-shot sketch-
based image retrieval, fine-grained or not,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 2765–2775, 2023.
[35] A. Baldrati, M. Bertini, T. Uricchio, and A. Del Bimbo, “Ef-
fective conditioned and composed image retrieval combin-
ing clip-based features,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pp. 21466–21474, 2022.
[36] A. v. d. Oord, Y . Li, and O. Vinyals, “Representation
learning with contrastive predictive coding,” arXiv preprint
arXiv:1807.03748 , 2018.
[37] I. Loshchilov and F. Hutter, “Decoupled weight decay regu-
larization,” arXiv preprint arXiv:1711.05101 , 2017.
[38] I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient de-
scent with warm restarts,” arXiv preprint arXiv:1608.03983 ,
2016.
[39] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le, “Ran-
daugment: Practical automated data augmentation with a re-
duced search space,” in Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition work-
shops , pp. 702–703, 2020.
10
Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images
(Supplementary Material)
A. Text to Overhead Image Retrieval
Our framework uses ground-level images as pseudo-
labels to learn the textual concepts of geolocation. Although
Sat2Cap does not require any text labels during training,
it effectively learns an embedding space where geoloca-
tion and their fine-grained descriptions are well aligned.
To show this, we randomly selected 1000 overhead images
from our training set, and compute their Sat2Cap embed-
dings. For a given text query, we generate the CLIP[1] text
embedding and compute its similarity with all images in the
test set. Figure 1 shows examples of 4 closest overhead im-
ages retrieved for a given query.
We experiment with small perturbations of prompts to
analyze how our retrieval results change with minute vari-
ations of query. We see in Figure 1, the prompt “people
driving cars” retrieves city or residential areas. However,
replacing the phrase “driving cars” with “riding horses” re-
trieves locations with farmland. Similarly, the prompt “per-
son on a long hike” exclusively retrieves mountainous re-
gions, while the prompt “person on a long run” retrieves
images that looks like trails nearby residential areas. Hence,
Sat2Cap embeddings demonstrate a good understanding of
fine-grained variations of textual concepts.
B. More Large-scale Textual Maps
We create country-level maps of England and Nether-
lands for four different prompts: a) Farmers harvesting
crops, b) Cars stuck in traffic, c) Animals grazing in the
fields, and d) People fishing on a boat. To generate the tex-
tual maps for each prompt, we compute the Sat2Cap em-
beddings for all images of the country and compute its sim-
ilarity with the CLIP text embeddings of the given prompt.
We normalize the similarities and plot those to create the
textual maps. Figure 2 shows textual maps of Netherlands
and England for each prompt. We also placed a landcover
map on the bottom of each text map as a reference of places
with likely activations for the given prompt.
By comparing with the respective landcover maps, we
see that the Sat2Cap embeddings activates reasonable loca-
tions on a map for a given prompt. For example, the prompt
“Farmers harvesting crops” gets activated mostly in crop-
land, while the prompt “Cars stuck in traffic” is activated
in urban areas. Similarly, the textual maps of the prompts
“Animals grazing in the fields”, and “People fishing on a
boat” look similar to the rangeland and water landcover re-
spectively. In (d), we see high activations in the top-left
corner for England, which does not match the water land-
cover. This region is the Lake District, which has numerousbeautiful lakes.
C. Dynamic Caption Generation
We take a single overhead image and show the dynamic
captions Sat2Cap embedding can generate. Figure 3 shows
our results on a test image at four different temporal set-
tings. The generated captions capture both the semantic
concepts from the given image as well as the temporal con-
cepts that are added to it. As you move from May to Decem-
ber, the concepts of winter become more prominent in the
captions. Similarly, as you move from 10:00 am to 11:00
pm, we see the concepts associated with night are better
highlighted. One interesting observation is that we are not
getting trivial changes, such as simply adding in winter or
at night to the captions. Rather, the entire concept that the
captions describe also changes.
D. Dataset
We introduced a cross-view dataset with overhead im-
ages and co-located ground-level images taken from the
YFCC100M [2] dataset. Figure 4 shows a few samples
from our dataset. The ground-level images provide us with
detailed fine-grained concepts of a location that cannot be
directly inferred when looking at the overhead imagery.
E. Overhead to Ground Image Retrieval
In figure 5, we show additional results of overhead-
to-ground image retrieval. We see that Sat2Cap embed-
dings accurately relate overhead imagery with fine-grained
ground-level concepts. An interesting thing to note is that
the relationship is not based primarily on visual feature
matching but rather based on agreement of concepts. For
example, the overhead image of a running track retrieves
images of people playing different sports, while an image
over the ocean retrieves images of people enjoying various
water sports.
References
[1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 1
[2] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin
Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia
Li. Yfcc100m: The new data in multimedia research. Com-
munications of the ACM , 59(2):64–73, 2016. 1
1arXiv:2307.15904v1  [cs.CV]  29 Jul 2023
Figure 1: Top-4 text-to-overhead retrieval: We retrieve the top-4 closest overhead image from a given text prompt. Our
results show that Sat2Cap embeddings can accurately relate geolocations with fine-grained textual prompts.
2
Figure 2: Zero-shot map of countries: We show the textual maps of England and Netherlands for different queries. We
also show a landcover map as a guide for plausible locations where the query is likely to be activated. In (d), we see high
activations in the top-left corner of england which lies in the “Lake District National Park”.
3
Figure 3: Dynamic Caption Generation: Our Sat2Cap embeddings dynamically adapt to temporal manipulations, facilitat-
ing dynamic caption generation.
Figure 4: Examples of co-located overhead and ground images in our dataset. The ground-level images describe more
detailed concepts of the given locations than their overhead counterparts.
4
Figure 5: Top-9 overhead-to-ground retrieval : Our model is capable of inferring fine-grained concepts of ground-level
scenes through overhead imagery. Sat2Cap accurately retrieves probable concepts for a given geolocation using an overhead
image.
5
ViTs for SITS: Vision Transformers for Satellite Image Time Series
Michail Tarasiou Erik Chavez Stefanos Zafeiriou
Imperial College London
{michail.tarasiou10, erik.chavez, s.zafeiriou }@imperial.ac.uk
Abstract
In this paper we introduce the Temporo-Spatial Vision
Transformer (TSViT), a fully-attentional model for general
Satellite Image Time Series (SITS) processing based on
the Vision Transformer (ViT). TSViT splits a SITS record
into non-overlapping patches in space and time which
are tokenized and subsequently processed by a factorized
temporo-spatial encoder. We argue, that in contrast to nat-
ural images, a temporal-then-spatial factorization is more
intuitive for SITS processing and present experimental evi-
dence for this claim. Additionally, we enhance the model’s
discriminative power by introducing two novel mechanisms
for acquisition-time-specific temporal positional encodings
and multiple learnable class tokens. The effect of all
novel design choices is evaluated through an extensive
ablation study. Our proposed architecture achieves state-
of-the-art performance, surpassing previous approaches
by a significant margin in three publicly available SITS
semantic segmentation and classification datasets. All
model, training and evaluation codes can be found at
https://github.com/michaeltrs/DeepSatModels .
1. Introduction
The monitoring of the Earth surface man-made impacts
or activities is essential to enable the design of effective in-
terventions to increase welfare and resilience of societies.
One example is the sector of agriculture in which monitor-
ing of crop development can help design optimum strategies
aimed at improving the welfare of farmers and resilience of
the food production system. The second of United Nations
Sustainable Development Goals (SDG) of Ending Hunger
relies on increasing the crop productivity and revenues of
farmers in poor and developing countries [35] - approxi-
mately 2.5 billion people’s livelihoods depend mainly on
producing crops [10]. Achieving SDG 2 goals requires to be
able to accurately monitor yields and the evolution of culti-
vated areas in order to measure the progress towards achiev-
ing several goals, as well as to evaluate the effectiveness of
different policies or interventions. In the European Union
Figure 1. Model and performance overview. (top) TSViT archi-
tecture. A more detailed schematic is presented in Fig.4. (bottom)
TSViT performance compared with previous arts (Table 2).
(EU) the Sentinel for Common Agricultural Policy program
(Sen4CAP) [2] focuses on developing tools and analytics to
support the verification of direct payments to farmers with
underlying environmental conditionalities such as the adop-
tion of environmentally-friendly [50] and crop diversifica-
tion [51] practices based on real-time monitoring by the
European Space Agency’s (ESA) Sentinel high-resolution
satellite constellation [1] to complement on site verifica-
tion. Recently, the volume and diversity of space-borne
Earth Observation (EO) data [63] and post-processing tools
[18, 61, 70] has increased exponentially. This wealth of
resources, in combination with important developments in
machine learning for computer vision [20, 28, 53], provides
an important opportunity for the development of tools for
the automated monitoring of crop development.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10418

Towards more accurate automatic crop type recognition,
we introduce TSViT, the first fully-attentional1architecture
for general SITS processing. An overview of the proposed
architecture can be seen in Fig.1 (top). Our novel design
introduces some inductive biases that make TSViT particu-
larly suitable for the target domain:
• Satellite imagery for monitoring land surface variabil-
ity boast a high revisit time leading to long temporal
sequences. To reduce the amount of computation we
factorize input dimensions into their temporal and spa-
tial components, providing intuition (section 3.4) and
experimental evidence (section 4.2) about why the or-
der of factorization matters.
• TSViT uses a Transformer backbone [64] following
the recently proposed ViT framework [13]. As a result,
every TSViT layer has a global receptive field in time
or space, in contrast to previously proposed convolu-
tional and recurrent architectures [14, 24, 40, 45, 49].
• To make our approach more suitable for SITS mod-
elling we propose a tokenization scheme for the in-
put image timeseries and propose acquisition-time-
specific temporal position encodings in order to extract
date-aware features and to account for irregularities in
SITS acquisition times (section 3.6).
• We make modifications to the ViT framework (sec-
tion 3.2) to enhance its capacity to gather class-specific
evidence which we argue suits the problem at hand
and design two custom decoder heads to accommodate
both global and dense predictions (section 3.5).
Our provided intuitions are tested through extensive abla-
tion studies on design parameters presented in section 4.2.
Overall, our architecture achieves state-of-the-art perfor-
mance in three publicly available datasets for classification
and semantic segmentation presented in Table 2 and Fig.1.
2. Related work
2.1. Crop type recognition
Crop type recognition is a subcategory of land use recog-
nition which involves assigning one of Kcrop categories
(classes) at a set of desired locations on a geospatial grid.
For successfully doing so modelling the temporal patterns
of growth during a time period of interest has been shown
to be critical [15, 44]. As a result, model inputs are time-
series of Tsatellite images of spatial dimensions H×W
withCchannels, X∈RT×H×W×Crather than single ac-
quisitions. There has been a significant body of work on
crop type identification found in the remote sensing liter-
ature [8, 9, 19, 39, 41, 55]. These works typically involve
1without any convolution operationsmultiple processing steps and domain expertise to guide
the extraction of features, e.g. NDVI [25], that can be
separated into crop types by learnt classifiers. More re-
cently, Deep Neural Networks (DNN) trained on raw op-
tical data [22, 26, 29, 46, 47, 62] have been shown to outper-
form these approaches. At the object level, (SITS classi-
fication) [24, 40, 48] use 1D data of single-pixel or parcel-
level aggregated feature timeseries, rather than the full SITS
record, learning a mapping f:RT×C→RK. Among
these works, TempCNN [40] employs a simple 1D convo-
lutional architecture, while [48] use the Transformer archi-
tecture [64]. DuPLo [24] consists of an ensemble of CNN
and RNN streams in an effort to exploit the complementar-
ity of extracted features. Finally, [16] view satellite images
as un-ordered sets of pixels and calculate feature statistics
at the parcel level, but, in contrast to previously mentioned
approaches, their implementation requires knowledge of the
object geometry. At the pixel level (SITS semantic seg-
mentation), models learn a mapping f(X)∈RH×W×K.
For this task, [47] show that convolutional RNN variants
(CLSTM, CGRU) [54] can automatically extract useful fea-
tures from raw optical data, including cloudy images, that
can be linearly separated into classes. The use of CNN
architectures is explored in [45] who employ two models:
a UNET2D feature extractor, followed by a CLSTM tem-
poral model (UNET2D-CLSTM); and a UNET3D fully-
convolutional model. Both are found to achieve equivalent
performances. In a similar spirit, [7] use a FPN [31] feature
extractor, coupled with a CLSTM temporal model (FPN-
CLSTM). The UNET3Df architecture [60] follows from
UNET3D but uses a different decoder head more suited to
contrastive learning. The U-TAE architecture [14] follows
a different approach, in that it employs the encoder part of
a UNET2D, applied on parallel on all images, and a sub-
sequent temporal attention mechanism which collapses the
temporal feature dimension. These spatial-only features are
further processed by the decoder part of a UNET2D to ob-
tain dense predictions.
2.2. Self-attention in vision
Convolutional [20, 28, 57] and fully-convolutional net-
works (FCN) [52, 53] have been the de-facto model of
choice for vision tasks over the past decade. The convo-
lution operation extracts translation-equivariant features via
application of a small square kernel over the spatial extent
of the learnt representation and grows the feature recep-
tive field linearly over the depth of the network. In con-
trast, the self-attention operation, introduced as the main
building block of the Transformer architecture [64], uses
self-similarity as a means for feature aggregation and can
have a global receptive field at every layer. Following the
adoption of Transformers as the dominant architecture in
natural language processing tasks [6, 12, 64], several works
10419

have attempted to exploit self-attention in vision architec-
tures. Because the time complexity of self-attention scales
quadratically with the size of the input, its naive implemen-
tation on image data, which typically contain more pixels
than text segments contain words, would be prohibitive. To
bypass this issue, early works focused on improving effi-
ciency by injecting self-attention layers only at specific lo-
cations within a CNN [5, 67] or by constraining their re-
ceptive field to a local region [38, 42, 65], however, in prac-
tice, these designs do not scale well with available hard-
ware leading to slow throughput rates, large memory re-
quirements and long training times. Following a different
approach, the Vision Transformer (ViT) [13], presented in
further detail in section 3.1, constitutes an effort to apply
a pure Transformer architecture on image data, by propos-
ing a simple, yet efficient image tokenization strategy. Sev-
eral works have drawn inspiration from ViT to develop
novel attention-based architectures for vision. For image
recognition, [32, 69] re-introduce some of the inductive bi-
ases that made CNNs successful in vision, leading to im-
proved performances without the need for long pre-training
schedules, [43, 59] employ Transformers for dense predic-
tion, [11,58,71] for object detection and [3,36,68] for video
processing. Among these works, our framework is more
closely related to [3] who use ViT for video processing.
However, we deviate significantly from their design by in-
troducing a dictionary of acquisition-time-specific position
encodings to accommodate an uneven distribution of im-
ages in time, by employing a different input factorization
strategy more suitable to SITS, and by being interested in
both global and dense predictions (section 3.4). Finally,
we are using a multi-token strategy that allows better han-
dling of spatial interactions and leads to improved class
separation. Multiple clstokens have also been employed
in [11, 59]. However, while both studies use them as class
queries inputs to a decoder module, we introduce the clsto-
kens as an input to the encoder in order to collapse the time
dimension and obtain class-specific features.
3. Method
In this section we present the TSViT architecture in de-
tail. First, we give a brief overview of the ViT (section 3.1)
which provided inspiration for this work. In section 3.2 we
present our modified TSViT backbone, followed by our to-
kenization scheme (section 3.3), encoder (section 3.4) and
decoder (section 3.5) modules. Finally, in section 3.6, we
discuss several considerations behind the design of our po-
sition encoding scheme.
3.1. Primer on ViT
Inspired by the success of Transformers in natural lan-
guage processing tasks [64] the ViT [13] is an application
of the Transformer architecture to images with the fewest
Figure 2. Backbone architectures. (a) Transformer backbone,
(b)ViT architecture, (c)TSViT backbone employs additional cls
tokens (red), each responsible for predicting a single class.
possible modifications. Their framework involves the to-
kenization of a 2D image X∈RH×W×Cto a set of
patch tokens Z∈RN×dby splitting it into a sequence of
N=⌊H
h⌋⌊W
w⌋same-size and non-overlapping patches of
spatial extent (h×w)which are flattened into 1D tokens
xi∈RhwCand linearly projected into ddimensions. Over-
all, the process of token extraction is equivalent to the appli-
cation of 2D convolution with kernel size (h×w)at strides
(h, w)across respective dimensions. The extracted patches
are used to construct model inputs as follows:
Z0=concat (zcls,Z+P)∈RN+1×d(1)
A set of learned positional encoding vectors P∈RN×d,
added to Z, are employed to encode the absolute posi-
tion information of each token and break the permutation
invariance property of the subsequent Transformer layers.
A separate learned class ( cls) token zcls∈Rd[12] is
prepended to the linearly transformed and positionally aug-
mented patch tokens leading to a length N+ 1 sequence
of tokens Z0which are used as model inputs. The Trans-
former backbone consists of Lblocks of alternating lay-
ers of Multiheaded Self-Attention (MSA) [64] and residual
Multi-Layer Perceptron (MLP) (Fig.2(a)).
Yl=MSA (LN(Zl)) +Zl(2)
Zl+1=MLP (LN(Yl)) +Yl(3)
Prior to each layer, inputs are normalized following Layer-
norm (LN) [4]. MLP blocks consist of two layers of linear
projection followed by GELU non-linear activations [21].
In contrast to CNN architectures, in which spatial dimen-
sions are reduced while feature dimensions increase with
10420

Figure 3. SITS Tokenization . We embed each satellite image
independently following ViT [13]
layer depth, Transformers are isotropic in that all feature
maps Zl∈R1+N×dhave the same shape throughout the
network. After processing by the final layer L, all tokens
apart from the first one (the state of the clstoken) are dis-
carded and unormalized class probabilities are calculated by
processing this token via a MLP. A schematic representation
of the ViT architecture can be seen in Fig.2(b).
3.2. Backbone architecture
In the ViT architecture, the clstoken progressively re-
fines information gathered from all patch tokens to reach a
final global representation used to derive class probabilities.
Our TSViT backbone, shown in Fig.2(c), essentially fol-
lows from ViT, with modifications in the tokenization and
decoder layers. More specifically, we introduce K(equal to
the number of object classes) additional learnable clstokens
Zcls∈RK×d, compared to ViT which uses a single token.
Z0=concat (Zcls,Z+P)∈RN+K×d(4)
Without deviating from ViT, all clsand positionally aug-
mented patch tokens are concatenated and processed by the
Llayers of a Transformer encoder. After the final layer,
we discard all patch tokens and project each clstoken into
a scalar value. By concatenating these values we obtain a
length Kvector of unormalised class probabilities. This
design choice brings the following two benefits: 1) it in-
creases the capacity of the clstoken relative to the patch
tokens, allowing them to store more patterns to be used by
the MSA operation; introducing multiple clstokens can be
seen as equivalent to increasing the dimension of a single
clstoken to an integer multiple of the patch token dimen-
siondcls=kdpatch and split the clstoken into kseparate
subspaces prior to the MSA operation. In this way we can
increase the capacity of the clstokens while avoiding is-
sues such as the need for asymmetric MSA weight matri-ces for clsandpatch tokens, which would effectively more
than double our model’s parameter count. 2) it allows for
more controlled handling of the spatial interactions between
classes. By choosing k=Kand enforcing a bijective map-
ping from clstokens to class predictions, the state of each
clstoken becomes more focused to a specific class with net-
work depth. In TSViT we go a step further and explicitly
separate clstokens by class after processing with the tempo-
ral encoder to allow only same- cls-token interactions in the
spatial encoder. In section 3.4 we argue why this is a very
useful inductive bias for modelling spatial relationships in
crop type recognition.
3.3. Tokenization of SITS inputs
A SITS record X∈RT×H×W×Cconsists of a series
ofTsatellite images of spatial dimensions H×Wwith
Cchannels. For the tokenization of our 3D inputs, we can
extend the tokenization-as-convolution approach to 3D data
and apply a 3D kernel with size (t×h×w)at stride (t, h, w )
across temporal and spatial dimensions. In this manner
N=⌊T
t⌋⌊H
h⌋⌊W
w⌋non-overlapping tokens xi∈RthwC
are extracted, and subsequently projected to ddimensions.
Using t >1, all extracted tokens contain spatio-temporal
information. For the special case of t= 1 each token
contains spatial-only information for each acquisition time
and temporal information is accounted for only through the
encoder layers. Since the computation cost of global self-
attention layers is quadratic w.r.t. the length of the token se-
quence O(N2), choosing larger values for t, h, w can lead
to significantly reduced number of FLOPS. In our experi-
ments, however, we have found small values for t, h, w to
work much better in practice. For all presented experiments
we use a value of t= 1motivated in part because this choice
simplifies the implementation of acquisition-time-specific
temporal position encodings, described in section 3.6. With
regards to the spatial dimensions of extracted patches we
have found small values to work best for semantic segmen-
tation, which is reasonable given that small patches retain
additional spatial granularity. In the end, our tokenization
scheme is similar to ViT’s applied in parallel for each acqui-
sition as shown in Fig.3, however, at this stage, instead of
unrolling feature dimensions, we retain the spatial structure
of the original input as reshape operations will be handled
by the TSViT encoder submodules.
3.4. Encoder architecture
In the previous section we presented a motivation for us-
ing small values t, h, w for the extracted patches. Unless
other measures are taken to reduce the model’s computa-
tional cost this choice would be prohibitive for process-
ing SITS with multiple acquisition times. To avoid such
problems, we choose to factorize our inputs across their
temporal and spatial dimensions, a practice commonly em-
10421

Figure 4. TSViT submodules. (a) Temporal encoder. We reshape tokenized inputs, retaining the spatio-temporal structure of SITS, into
a set of timeseries for each spatial location, add temporal position encodings PT[t,:]for acquisition times t, concatenate local clstokens
ZTcls (eq.5) and process in parallel with a Transformer. Only the first Koutput tokens are retained. (b)Spatial encoder. We reshape
the outputs of the temporal encoder into a set of spatial feature maps for each clstoken, add spatial position encodings PS, concatenate
global clstokens ZScls(eq.6) and process in parallel with a Transformer. (c)Segmentation head. Each local clstoken is projected into hw
values denoting class-specific evidence for every pixel in a patch. All patches are then reassembled into the original image dimensions. (d)
Classification head. Global clstokens are projected into scalar values, each denoting evidence for the presence of a specific class.
ployed for video processing [17,27,37,56,66,72]. We note
that all these works use a spatial-temporal factorization or-
der, which is reasonable when dealing with natural images,
given that it allows the extraction of higher level, semanti-
cally aware spatial features, whose relationship in time is
useful for scene understanding. However, we argue that in
the context of SITS, reversing the order of factorization is
a meaningful design choice for the following reasons: 1) in
contrast to natural images in which context can be useful for
recognising an object, in crop type recognition context can
provide little information, or can be misleading. This arises
from the fact that the shape of agricultural parcels, does not
need to follow its intended use, i.e. most crops can gener-
ally be cultivated independent of a field’s size or shape. Of
course there exist variations in the shapes and sizes of agri-
cultural fields [34], but these depend mostly on local agri-
cultural practices and are not expected to generalize over
unseen regions. Furthermore, agricultural parcels do not in-
herently contain sub-components or structure. Thus, know-
ing what is cultivated in a piece of land is not expected to
provide information about what grows nearby. This is in
contrast to other objects which clearly contain structure, e.g.
in human face parsing there are clear expectations about
the relative positions of various face parts. We test this hy-
pothesis in the supplementary material by enumerating over
all agricultural parcels belonging to the most popular crop
types in the T31TFM S2tile in France and taking crop-type-
conditional pixel counts over a 1km square region from their
centers. Then, we calculate the cosine similarity of thesevalues with unconditional pixel counts over the extent of
the T31TFM tile and find a high degree of similarity, sug-
gesting that there are no significant variations between these
distributions; 2) a small region in SITS is far more informa-
tive than its equivalent in natural images, as it contains more
channels than regular RGB images ( S2imagery contains 13
bands in total) whose intensities are averaged over a rela-
tively large area (highest resolution of S2images is 10×10
m2); 3) SITS for land cover recognition do not typically
contain moving objects. As a result, a timeseries of single
pixel values can be used for extracting features that are in-
formative of a specific object part found at that particular
location. Therefore, several objects can be recognised us-
ing only information found in a single location; plants, for
example, can be recognised by variations of their spectral
signatures during their growth cycle. Many works perform-
ing crop classification do so using only temporal informa-
tion in the form of timeseries of small patches [47], pixel
statistics over the extent of parcels [46] or even values from
single pixels [40, 48]. On the other hand, the spatial pat-
terns in a single image are uninformative of the crop type,
as evidenced by the low performance of systems relying on
single images [14]. Our encoder architecture can be seen
in Fig.4(a,b). We now describe the temporal and spatial en-
coder submodules.
Temporal encoder Thus, we tokenize a SITS record
X∈RT×H×W×Cinto a set of tokens of size (NT×NH×
NW×d), as described in section 3.3 and subsequently re-
shape to ZT∈RNHNW×NT×d, to get a list of token time-
10422

series for all patch locations. The input to the temporal en-
coder is:
Z0
T=concat (ZTcls,ZT+PT[t,:])∈RNHNW×K+NT×d
(5)
where PT[t,:]∈RNT×dandZTcls∈RK×dare respec-
tively added and prepended to all NHNWtimeseries and
t∈RTis a vector containing all Tacquisition times. All
samples are then processed in parallel by a Transformer
module. Consequently, the final feature map of the tem-
poral encoder becomes ZL
T∈RNHNW×K+NT×din which
the first Ktokens in the temporal dimension correspond to
the prepended clstokens. We only keep these tokens, dis-
carding the remaining NTvectors.
Spatial encoder We now transpose the first and second
dimensions in the temporal encoder output, to obtain a list
of patch features ZS∈RK×NHNW×dfor all output classes.
In a similar spirit, the input to the spatial encoder becomes:
Z0
S=concat (ZScls,ZS+PS)∈RK×1+NHNW×d(6)
where PS∈RNHNW×dare respectively added to all
Kspatial representations and each element of ZScls∈
RK×1×dis prepended to each class-specific feature map.
We note, that while in the temporal encoder clstokens were
prepended to all patch locations, now there is a single cls
token per spatial feature map such that ZScls are used to
gather global SITS-level information. Processing with the
spatial encoder leads to a similar size output feature map
ZL
S∈RK×1+NHNW×d.
3.5. Decoder architecture
The TSViT encoder architecture described in the pre-
vious section is designed as a general backbone for SITS
processing. To accommodate both global and dense pre-
diction tasks we design two decoder heads which feed on
different components of the encoder output. We view the
output of the encoder as ZL
S= [ZL
Sglobal |ZL
Slocal ]respec-
tively corresponding to the states of the global and local
clstokens. For image classification , we only make use of
ZL
Sglobal ∈RK×d. We proceed, as described in sec.3.2, by
projecting each feature into a scalar value and concatenate
these values to obtain global unormalised class probabilities
as shown in Fig.4(d). Complementarily, for semantic seg-
mentation we only use ZL
Slocal ∈RK×NHNW×d. These
features encode information for the presence of each class
over the spatial extent of each image patch. By project-
ing each feature into hwdimensions and further reshaping
the feature dimension to (h×w)we obtain a set of class-
specific probabilities for each pixel in a patch. It is pos-
sible now to merge these patches together into an output
map(H×W×K)which represents class probabilities for
each pixel in the original image. This process is presented
schematically in Fig.4(c).3.6. Position encodings
As described in section 3.4, positional encodings are in-
jected in two different locations in our proposed network.
First, temporal position encodings are added to all patch to-
kens before processing by the temporal encoder (eq.5). This
operation aims at breaking the permutation invariance prop-
erty of MSA by introducing time-specific position biases to
all extracted patch tokens. For crop recognition encoding
the absolute temporal position of features is important as
it helps identifying a plant’s growth stage within the crop
cycle. Furthermore, the time interval between successive
images in SITS varies depending on acquisition times and
other factors, such as the degree of cloudiness or corrupted
data. To introduce acquisition-time-specific biases into the
model, our temporal position encodings PT[t,:]depend di-
rectly on acquisition times t. More specifically, we make
note of all acquisition times t′= [t1, t2, ..., t T′]found in
the training data and construct a lookup table PT∈RT′×d
containing all learnt position encodings indexed by date.
Finding the date-specific encodings that need to be added
topatch tokens (eq.5) reduces to looking up appropriate in-
dices from PT. In this way temporal position encodings in-
troduce a dynamic prior of where to look at in the models’
global temporal receptive field, rather than simply encoding
the order of SITS acquisitions which would discard valu-
able information. Following token processing by the tem-
poral encoder, spatial position embeddings PSare added to
the extracted clstokens. These are not dynamic in nature
and are similar to the position encodings used in the orig-
inal ViT architecture, with the difference that these biases
are now added to Kfeature maps instead of a single one.
4. Experiments
We apply TSViT to two tasks using SITS records X∈
RT×H×W×Cas inputs: classification and semantic seg-
mentation. At the object level, classification models learn
a mapping f(X)∈RKfor the object occupying the center
of the H×Wregion. Semantic segmentation models learn
a mapping f(X)∈RH×W×K, predicting class probabili-
ties for each pixel over the spatial extent of the SITS record.
We use an ablation study on semantic segmentation to guide
model design and hyperparameter tuning and proceed with
presenting our main results on three publicly available SITS
semantic segmentation and classification datasets.
4.1. Training and evaluation
Datasets To evaluate the performance of our proposed
semantic segmentation model we are using three publicly
available S2land cover recognition datasets. The dataset
presented in [47] covers a densely cultivated area of interest
of102×42km2north of Munich, Germany and contains 17
distinct classes. Individual image samples cover a 240×240
m2area ( 24×24pixels) and contain 13 bands. The PASTIS
10423

dataset [14] contains images from four different regions in
France with diverse climate and crop distributions, span-
ning over 4000 km2and including 18 crop types. In total,
it includes 2.4k SITS samples of size 128×128, each con-
taining 33-61 acquisitions and 10 image bands. Because
the PASTIS sample size is too large for efficiently train-
ing TSViT with available hardware, we split each sample
into24×24patches and retain all acquisition times for a
total of 60k samples. To accommodate a large set of ex-
periments we only use fold-1 among the five folds provided
in PASTIS. Finally, we use the T31TFM-1618 dataset [60]
which covers a densely cultivated S2tile in France for years
2016-18 and includes 20 distinct classes. In total, it includes
140k samples of size 48×48, each containing 14-33 ac-
quisitions and 13 image bands. For the SITS classification
experiments, we construct the datasets from the respective
segmentation datasets. More specifically, for PASTIS we
use the provided object instance ids to extract 24×24pixel
regions whose center pixel falls inside each object and use
the class of this object as the sample class. The remain-
ing two datasets contain samples of smaller spatial extent,
making the above strategy not feasible in practice. Here, we
choose to retain the samples as they are and assign the class
of the center pixel as the global class. We note that this
strategy forces us to discard samples in which the center
pixels belongs to the background class. Additional details
are provided in the supplementary material.
Implementation details For all experiments presented
we train for the same number of epochs using the provided
data splits from the respective publications for a fair com-
parison. More specifically, we train on all datasets using
the provided training sets and report results on the valida-
tion sets for Germany and T31TFM-1618, and on the test
set for PASTIS. For training TSViT we use the AdamW op-
timizer [23] with a learning rate schedule which includes a
warmup period starting from zero to a maximum value 10−3
at epoch 10, followed by cosine learning rate decay [33]
down to 5∗10−6at the end of training. For Germany and
T31TFM-1618 we train with the above settings and report
the best performances between what we achieve and the
original studies. Since we split PASTIS, we are training
with both settings and report the best results. Overall, we
find that our settings improve model performance. We train
with a batch size of 16 or 32 and no regularization on ×2
Nvidia Titan Xp gpus in a data parallel fashion. All mod-
els are trained with a Masked Cross-Entropy loss, masking
the effect of the background class in both training loss and
evaluation metrics. We report overall accuracy (OA), aver-
aged over pixels, and mean intersection over union (mIoU)
averaged over classes. For SITS classification, in addition
to the 1D models presented in section 2 we modify the best
performing semantic segmentation models by aggregating
extracted features across space prior to the application of aAblation Settings mIoU
Factorization orderSpatial & Temporal 48.8
Temporal & Spatial 78.5
#clstokens1 78.5
K 83.6
Position encodingsStatic 80.8
Date lookup 83.6
Interactions between
clstokensTemporal Spatial
✓ ✓ 81.5
✓✓✓ 83.6
Patch size2×2 84.8
3×3 83.6
6×6 79.6
Table 1. Ablation on design choices for TSViT . All proposed
design choices are found to have a positive effect on performance.
classifier, thus, outputing a single prediction. Classification
models are trained with Focal Loss [30]. We report OA and
mean accuracy (mAcc) averaged over classes.
4.2. Ablation studies
We perform an ablation study on design parameters of
our framework using the Germany dataset [47]. Starting
with a baseline TSViT with L= 4 for both encoder net-
works, a single clstoken, h=w= 3, t= 1, d=
128we successively update our design after each ablation.
Here, we present the effect of the most important design
choices; additional ablations are presented in the supple-
mentary material. Overall, we find that the order of factor-
ization is the most important design choice in our proposed
framework. Using a spatio-temporal factorization from
the video recognition literature performs poorly at 48.8%
mIoU. Changing the factorization order to temporo-spatial
raises performance by an absolute +29.7%to78.5%mIoU.
Including additional clstokens increases performance to
83.6%mIoU ( +5.1%), so we proceed with using Kclsto-
kens in our design. We test the effect of our date-specific
position encodings compared to a fixed set of values and
find a significant −2.8%performance drop from using fixed
sizePTcompared to our proposed lookup encodings. Fur-
ther analysis is provided in the supplementary material. As
discussed in section 3.4 our spatial encoder blocks cross cls-
token interactions . Allowing interactions among all tokens
comes at a significant increase in compute cost, O(K2)to
O(K), and is found to decrease performance by −2.1%
mIoU. Finally, we find that smaller patch sizes generally
work better, which is reasonable given that tokens retain a
higher degree of spatial granularity and are used to predict
smaller regions. Using 2×2patches raises performance by
+1.2%mIoU to 84.8%compared to 3×3patches. Our fi-
nal design which is used in the main experiments presented
10424

Germany [47] PASTIS [14] T31TFM-1618 [60]
Model #params. (M) IT (ms) Semantic segmentation (OA / mIoU)
BiCGRU [47] 4.5 38.6 91.3 / 72.3 80.5 / 56.2 88.6 / 57.7
FPN-CLSTM [7] 1.2 19.5 91.8 / 73.7 81.9 / 59.5 88.4 / 57.8
UNET3D [45] 6.2 11.2 92.4 / 75.2 82.3 / 60.4 88.4 / 57.6
UNET3Df [60] 7.2 19.7 92.4 / 75.4 82.1 / 60.2 88.6 / 57.7
UNET2D-CLSTM [45] 2.3 35.5 92.9 / 76.2 82.7 / 60.7 89.0 / 58.8
U-TAE [14] 1.1 8.8 93.1 / 77.1 82.9 / 62.4 (83.2 / 63.1) 88.9 / 58.5
TSViT (ours) 1.7 11.8 95.0 / 84.8 83.4 / 65.1 (83.4 / 65.4) 90.3 / 63.1
Model #params. (M) IT (ms) Object classification (OA / mAcc)
TempCNN∗[40] 0.9 0.5 89.8 / 78.4 84.8 / 69.1 84.7 / 62.6
DuPLo∗[24] 5.2 2.9 93.1 / 82.2 84.8 / 69.4 83.9 / 69.5
Transformer∗[48] 18.9 4.3 92.4 / 84.3 84.4 / 68.1 84.3 / 71.4
UNET3D [45] 6.2 11.2 92.7 / 83.9 84.8 / 70.2 84.8 / 71.4
UNET2D-CLSTM [45] 2.3 35.5 93.0 / 84.0 84.7 / 70.3 84.7 / 71.6
U-TAE [14] 1.1 8.8 92.6 / 83.7 84.9 / 71.8 84.8 / 71.7
TSViT (ours) 1.7 11.8 94.7 / 88.1 87.1 / 75.5 87.8 / 74.2
Table 2. Comparison with state-of-the-art models from literature .(top) Semantic segmentation. (bottom) Object classification.∗1D
temporal only models. For each model we note its number of parameters ( #params. ×106) and inference time (IT) for a single sample
with T=52, H,W=24 and C=13 size input on a Nvidia Titan Xp gpu. We report overall accuracy (OA), mean intersection over union (mIoU)
and mean accuracy (mAcc). For PASTIS we report results for fold-1 only; average test set performance across all five folds is shown in
parenthesis for direct comparison with [14].
Figure 5. Visualization of predictions in Germany. The back-
ground class is shown in black, ”x” indicates a false prediction.
in Table 2 employs a temporo-spatial design with Kclsto-
kens, acquisition-time-specific position encodings, 2×2in-
put patches and four layers for both encoders.
4.3. Comparison with SOTA
In Table 2 and Fig.1, we compare the performance of
TSViT with state-of-the-art models presented in section 2.
For semantic segmentation, we find that all models from
literature perform similarly, with the BiCGRU being over-
all the worst performer, matching CNN-based architectures
only in T31TFM-1618. For all datasets, TSViT outperforms
previously suggested approaches by a very large margin. A
visualization of predictions in Germany for the top-3 per-formers is shown in Fig.5. In object classification, we ob-
serve that 1D temporal models are generally outperformed
by spatio-temporal models, with the exception of the Trans-
former [48]. Again, TSViT trained for classification consis-
tently outperforms all other approaches by a large margin
across all datasets. In both tasks, we find smaller improve-
ments for the pixel-averaged compared to class-averaged
metrics, which is reasonable given the large class imbalance
that characterizes the datasets.
5. Conclusion
In this paper we proposed TSViT, which is the first
fully-attentional architecture for general SITS processing.
Overall, TSViT has been shown to significantly outperform
state-of-the-art models in three publicly available land cover
recognition datasets, while being comparable to other mod-
els in terms of the number of parameters and inference time.
However, our method is limited by its quadratic complexity
with respect to the input size, which can lead to increased
hardware requirements when working with larger inputs.
While this may not pose a significant issue for semantic seg-
mentation or SITS classification, it can present challenges
for detection tasks that require isolating large objects, thus
limiting its application. Future research is needed to address
this limitation and enable TSViT to scale more effectively
to larger inputs.
Acknowledgements MT and EC acknowledge funding
from the European Union’s Climate-KIC ARISE grant. SZ
was partially funded by the EPSRC Fellowship DEFORM:
Large Scale Shape Analysis of Deformable Models of Hu-
mans (EP/S010203/1).
10425

References
[1] European Space Agency. The sentinel missions. https://
www.esa.int/Applications/Observing_the_
Earth/Copernicus/The_Sentinel_missions .
Accessed: 2022-11-11. 1
[2] European Space Agency. Sentinels for common agriculture
policy. http://esa-sen4cap.org/ . Accessed: 2022-
11-11. 1
[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen
Sun, Mario Lu ˇci´c, and Cordelia Schmid. Vivit: A video vi-
sion transformer. In 2021 IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , pages 6816–6826, 2021.
3
[4] Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer
normalization. ArXiv , abs/1607.06450, 2016. 3
[5] I. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Atten-
tion augmented convolutional networks. In 2019 IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
3285–3294, 2019. 3
[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-
hini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. Language models are
few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
M.F. Balcan, and H. Lin, editors, Advances in Neural Infor-
mation Processing Systems , volume 33, pages 1877–1901.
Curran Associates, Inc., 2020. 2
[7] Jorge Andres Chamorro Martinez, Laura Elena Cu ´e La
Rosa, Raul Queiroz Feitosa, Ieda Del’Arco Sanches, and
Patrick Nigri Happ. Fully convolutional recurrent networks
for multidate crop recognition from multitemporal image se-
quences. ISPRS Journal of Photogrammetry and Remote
Sensing , 171:188–201, 2021. 2, 8
[8] Christopher Conrad, Stefan Dech, Olena Dubovyk, Sebas-
tian Fritsch, Doris Klein, Fabian L ¨ow, Gunther Schorcht, and
Julian Zeidler. Derivation of temporal windows for accurate
crop discrimination in heterogeneous croplands of Uzbek-
istan using multitemporal rapideye images. Computers and
Electronics in Agriculture , 103:63–74, 2014. 2
[9] Christopher Conrad, Sebastian Fritsch, Julian Zeidler, Gerd
R¨ucker, and Stefan Dech. Per-field irrigated crop classifica-
tion in arid central asia using spot and aster data. Remote
Sensing , 2(4):1035–1056, 2010. 2
[10] Gordon Conway. One Billion Hungry: Can we Feed the
World? Cornell University Press, 2012. 1
[11] Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan
Zhang, Lu Yuan, and Lei Zhang. Dynamic detr: End-to-end
object detection with dynamic attention. In 2021 IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
2968–2977, 2021. 3
[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-formers for language understanding. In Proceedings of the
2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota, June 2019. Associa-
tion for Computational Linguistics. 2, 3
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions , 2021. 2, 3, 4
[14] Vivien Sainte Fare Garnot and Loic Landrieu. Panoptic
segmentation of satellite image time series with convolu-
tional temporal attention networks. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 4872–4881, October 2021. 2, 5, 7, 8
[15] V . S. F. Garnot, L. Landrieu, S. Giordano, and N. Chehata.
Time-space tradeoff in deep learning models for crop clas-
sification on satellite multi-spectral image time series. In
IGARSS 2019 - 2019 IEEE International Geoscience and Re-
mote Sensing Symposium , pages 6247–6250, 2019. 2
[16] Vivien Sainte Fare Garnot, Loic Landrieu, Sebastien Gior-
dano, and Nesrine Chehata. Satellite image time series clas-
sification with pixel-set encoders and temporal self-attention.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , June 2020. 2
[17] Rohit Girdhar and Deva Ramanan. Attentional pooling for
action recognition. In I. Guyon, U. V on Luxburg, S. Bengio,
H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, ed-
itors, Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc., 2017. 5
[18] Noel Gorelick, Matt Hancher, Mike Dixon, Simon
Ilyushchenko, David Thau, and Rebecca Moore. Google
earth engine: Planetary-scale geospatial analysis for every-
one. Remote Sensing of Environment , 202:18–27, 2017. Big
Remotely Sensed Data: tools, applications and experiences.
1
[19] Pengyu Hao, Yulin Zhan, Li Wang, Zheng Niu, and Muham-
mad Shakir. Feature selection of time series modis data for
early crop classification using random forest: A case study
in Kansas, USA. Remote Sensing , 7(5):5347–5369, 2015. 2
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 770–778, 2016. 1, 2
[21] Dan Hendrycks and Kevin Gimpel. Gaussian error linear
units (gelus). arXiv: Learning , 2016. 3
[22] Dino Ienco, Raffaele Gaetano, Claire Dupaquier, and Pierre
Maurel. Land cover classification via multitemporal spatial
data by deep recurrent neural networks. IEEE Geoscience
and Remote Sensing Letters , 14(10):1685–1689, 2017. 2
[23] Loshchilov Ilya, Hutter Frank, et al. Decoupled weight decay
regularization. Proceedings of ICLR , 2019. 7
[24] Roberto Interdonato, Dino Ienco, Raffaele Gaetano, and
Kenji Ose. DuPLO: A dual view point deep learning ar-
chitecture for time series classification. ISPRS Journal of
10426

Photogrammetry and Remote Sensing , 149:91 – 104, 2019.
2, 8
[25] Xue Jinru and Baofeng Su. Significant remote sensing veg-
etation indices: A review of developments and applications.
Journal of Sensors , 2017:1–17, 01 2017. 2
[26] Andreas Kamilaris and Francesc X. Prenafeta-Bold ´u. Deep
learning in agriculture: A survey. Computers and Electronics
in Agriculture , 147:70–90, 2018. 2
[27] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas
Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video
classification with convolutional neural networks. In 2014
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 1725–1732, 2014. 5
[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classification with deep convolutional neural net-
works. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-
berger, editors, Advances in Neural Information Processing
Systems , volume 25. Curran Associates, Inc., 2012. 1, 2
[29] Nataliia Kussul, Mykola Lavreniuk, Sergii Skakun, and An-
drii Shelestov. Deep learning classification of land cover and
crop types using remote sensing data. IEEE Geoscience and
Remote Sensing Letters , 14(5):778–782, 2017. 2
[30] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He,
and Piotr Doll ´ar. Focal loss for dense object detection.
CoRR , abs/1708.02002, 2017. 7
[31] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR) , July 2017. 2
[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,
Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans-
former: Hierarchical vision transformer using shifted win-
dows. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV) , pages 10012–10022,
October 2021. 3
[33] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradi-
ent descent with warm restarts. In International Conference
on Learning Representations , 2017. 7
[34] NASA. Agricultural patterns. https : / /
earthobservatory.nasa.gov/images/6605/
agricultural-patterns . Accessed: 2022-9-22. 5
[35] United Nations. Goal 2: Zero hunger. https://www.un.
org/sustainabledevelopment/hunger/ . Ac-
cessed: 2022-11-11. 1
[36] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Assel-
mann. Video transformer network. In 2021 IEEE/CVF In-
ternational Conference on Computer Vision Workshops (IC-
CVW) , pages 3156–3165, 2021. 3
[37] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan As-
selmann. Video transformer network. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) Workshops , pages 3163–3172, October 2021. 5
[38] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-
age transformer. In Jennifer Dy and Andreas Krause, ed-
itors, Proceedings of the 35th International Conference onMachine Learning , volume 80 of Proceedings of Machine
Learning Research , pages 4055–4064. PMLR, 10–15 Jul
2018. 3
[39] Charlotte Pelletier, Silvia Valero, Jordi Inglada, Nicolas
Champion, and G ´erard Dedieu. Assessing the robustness of
random forests to map land cover with high resolution satel-
lite image time series over large areas. Remote Sensing of
Environment , 187:156–168, 2016. 2
[40] Charlotte Pelletier, Geoffrey I. Webb, and Franc ¸ois Petitjean.
Temporal convolutional neural network for the classification
of satellite image time series. Remote Sensing , 11(5), 2019.
2, 5, 8
[41] Jos ´e M. Pe ˜na-Barrag ´an, Moffatt K. Ngugi, Richard E. Plant,
and Johan Six. Object-based crop identification using multi-
ple vegetation indices, textural features and crop phenology.
Remote Sensing of Environment , 115(6):1301–1316, 2011. 2
[42] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan
Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-
attention in vision models. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett, ed-
itors, Advances in Neural Information Processing Systems ,
volume 32, pages 68–80. Curran Associates, Inc., 2019. 3
[43] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. In 2021 IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
12159–12168, 2021. 3
[44] Bradley C. Reed, Jesslyn F. Brown, Darrel VanderZee,
Thomas R. Loveland, James W. Merchant, and Donald O.
Ohlen. Measuring phenological variability from satellite im-
agery. Journal of Vegetation Science , 5(5):703–714, 1994.
2
[45] Rose Rustowicz, Robin Cheong, Lijing Wang, Stefano Er-
mon, Marshall Burke, and David B. Lobell. Semantic seg-
mentation of crop type in Africa: A novel dataset and analy-
sis of deep learning methods. In CVPR Workshops , 2019. 2,
8
[46] M. Rußwurm and M. K ¨orner. Temporal vegetation mod-
elling using long short-term memory networks for crop iden-
tification from medium-resolution multi-spectral satellite im-
ages. In 2017 IEEE Conference on Computer Vision and Pat-
tern Recognition Workshops (CVPRW) , pages 1496–1504,
2017. 2, 5
[47] Marc Rußwurm and Marco K ¨orner. Multi-temporal land
cover classification with sequential recurrent encoders. IS-
PRS International Journal of Geo-Information , 7(4):129,
Mar 2018. 2, 5, 6, 7, 8
[48] Marc Rußwurm and Marco K ¨orner. Self-attention for raw
optical satellite time series classification. ISPRS Journal of
Photogrammetry and Remote Sensing , 169:421 – 435, 2020.
2, 5, 8
[49] Marc Rußwurm, Charlotte Pelletier, M. Zollner, S ´ebastien
Lef`evre, and Marco K ¨orner. Breizhcrops: A time se-
ries dataset for crop type mapping. ISPRS - International
Archives of the Photogrammetry, Remote Sensing and Spa-
tial Information Sciences , XLIII-B2-2020:1545–1551, 08
2020. 2
10427

[50] Sen4CAP. Agricultural practices. http : / / esa -
sen4cap . org / content / agricultural -
practices . Accessed: 2022-11-11. 1
[51] Sen4CAP. Crop diversification. http://esa-sen4cap.
org/content/crop-diversification . Accessed:
2022-11-11. 1
[52] Pierre Sermanet, David Eigen, Xiang Zhang, Michael Math-
ieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated
recognition, localization and detection using convolutional
networks. 2014. Publisher Copyright: © 2014 Interna-
tional Conference on Learning Representations, ICLR. All
rights reserved.; 2nd International Conference on Learning
Representations, ICLR 2014 ; Conference date: 14-04-2014
Through 16-04-2014. 2
[53] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully
convolutional networks for semantic segmentation. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
39(4):640–651, 2017. 1, 2
[54] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Ye-
ung, Wai-kin Wong, and Wang-chun WOO. Convolutional
LSTM network: A machine learning approach for precipi-
tation nowcasting. In C. Cortes, N. Lawrence, D. Lee, M.
Sugiyama, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems , volume 28, pages 802–810.
Curran Associates, Inc., 2015. 2
[55] Sofia Siachalou, Giorgos Mallinis, and Maria Tsakiri-Strati.
A hidden Markov models approach for crop classification:
Linking crop phenology to time series of multi-sensor re-
mote sensing data. Remote Sensing , 7(4):3633–3650, 2015.
2
[56] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos. In
NIPS , 2014. 5
[57] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations , 2015.
2
[58] Hwanjun Song, Deqing Sun, Sanghyuk Chun, Varun Jam-
pani, Dongyoon Han, Byeongho Heo, Wonjae Kim, and
Ming-Hsuan Yang. ViDT: An efficient and effective fully
transformer-based object detector. In International Confer-
ence on Learning Representations , 2022. 3
[59] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia
Schmid. Segmenter: Transformer for semantic segmenta-
tion. In 2021 IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 7242–7252, 2021. 3
[60] Michail Tarasiou, Riza Alp G ¨uler, and Stefanos Zafeiriou.
Context-self contrastive pre-training for crop type semantic
segmentation. IEEE Transactions on Geoscience and Re-
mote Sensing , pages 1–1, 2022. 2, 7, 8
[61] Michail Tarasiou and Stefanos Zafeiriou. Deepsatdata:
Building large scale datasets of satellite images for training
machine learning models. In IGARSS 2022 - 2022 IEEE
International Geoscience and Remote Sensing Symposium ,
pages 4070–4073, 2022. 1
[62] Michail Tarasiou and Stefanos Zafeiriou. Embedding earth:
Self-supervised contrastive pre-training for dense land cover
classification. 2022. 2[63] Andrew Tatem, Scott Goetz, and Simon Hay. Fifty years of
earth observation satellites. American Scientist , 96:390–398,
09 2008. 1
[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-
lia Polosukhin. Attention is all you need. In I. Guyon,
U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural In-
formation Processing Systems 30 , pages 5998–6008. Curran
Associates, Inc., 2017. 2, 3
[65] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,
Alan Yuille, and Liang-Chieh Chen. Axial-DeepLab: Stand-
alone axial-attention for panoptic segmentation. In ECCV ,
2020. 3
[66] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua
Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment net-
works: Towards good practices for deep action recognition.
In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling,
editors, Computer Vision – ECCV 2016 , pages 20–36, Cham,
2016. Springer International Publishing. 5
[67] X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neu-
ral networks. In 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 7794–7803, 2018. 3
[68] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,
Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end
video instance segmentation with transformers. In 2021
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 8737–8746, 2021. 3
[69] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
Zi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from
scratch on imagenet. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision (ICCV) , pages
558–567, October 2021. 3
[70] Peng Yue, Boyi Shangguan, Lei Hu, Liangcun Jiang, Chenx-
iao Zhang, Zhipeng Cao, and Yinyin Pan. Towards a train-
ing data model for artificial intelligence in earth observation.
International Journal of Geographical Information Science ,
36(11):2113–2137, 2022. 1
[71] Zixiao Zhang, Xiaoqiang Lu, Guojin Cao, Yuting Yang,
Licheng Jiao, and Fang Liu. Vit-yolo:transformer-based
yolo for object detection. In 2021 IEEE/CVF International
Conference on Computer Vision Workshops (ICCVW) , pages
2799–2808, 2021. 3
[72] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Tor-
ralba. Temporal relational reasoning in videos. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV) , September 2018. 5
10428

On the Deployment of Post-Disaster Building
Damage Assessment Tools using Satellite Imagery:
A Deep Learning Approach
Shahrzad Gholami1, Caleb Robinson1, Anthony Ortiz1, Siyu Yang1, Jacopo Margutti2,
Cameron Birge1, Rahul Dodhia1, Juan Lavista Ferres1
1AI for Good Research Lab, Microsoft, Redmond, USA,
2510 an initiative of the Netherlands Red Cross, The Hague, The Netherlands
Abstract —Natural disasters frequency is growing globally.
Every year 350 million people are affected and billions of
dollars of damage is incurred. Providing timely and appropriate
humanitarian interventions like shelters, medical aid, and food to
affected communities are challenging problems. AI frameworks
can help support existing efforts in solving these problems in
various ways. In this study, we propose using high-resolution
satellite imagery from before and after disasters to develop a
convolutional neural network model for localizing buildings and
scoring their damage level. We categorize damage to buildings
into four levels, spanning from not damaged to destroyed, based
on the xView2 dataset’s scale. Due to the emergency nature
of disaster response efforts, the value of automating damage
assessment lies primarily in the inference speed, rather than
accuracy. We show that our proposed solution works three
times faster than the fastest xView2 challenge winning solution
and over 50 times faster than the slowest first place solution,
which indicates a significant improvement from an operational
viewpoint. Our proposed model achieves a pixel-wise F1 score
of 0.74 for the building localization and a pixel-wise harmonic
F1 score of 0.6 for damage classification and uses a simpler
architecture compared to other studies. Additionally, we develop
a web-based visualizer that can display the before and after
imagery along with the model’s building damage predictions on
a custom map. This study has been collaboratively conducted to
empower a humanitarian organization as the stakeholder, that
plans to deploy and assess the model along with the visualizer
for their disaster response efforts in the field.
Index Terms —satellite imagery datasets, neural networks,
image segmentation, building damage classification, natural
disasters, humanitarian action
I. I NTRODUCTION
Natural disasters affect 350 million people each year causing
billions of dollars in damage and were the main driver of
hunger for 29 million people in 2021 [1]. Providing timely
humanitarian aid to affected communities is increasingly
challenging due to the growing frequency and severity of
such events [2]. Impact assessment of natural disasters in
a short time frame is a crucial step in emergency response
efforts as it helps first responders allocate resources effectively.
For example, dispatching aid, sending shelters, and allocating
building material for reconstruction can be more efficient with
estimates of where damaged buildings are, and how badly
damaged they are.Microsoft AI for Good/Humanitarian Action has collab-
orated with Netherlands Red Cross to use high-resolution
satellite imagery from before and after natural disasters,
delineated in the publicly available xBD dataset, to develop
an end-to-end Siamese convolutional neural network that can
localize buildings and score their damage level. Such a model
is trained on historical disaster data and then applied on
demand to identify damaged buildings during future disasters.
Such AI and data-driven decision-aid tools can empower
humanitarian organizations to take more informed actions
at the time of disaster and allocate their resources more
strategically during their field deployments. Throughout the
course of our collaboration, extensive deployment experience
shared by field experts and their valuable perspective as
a stakeholder were instrumental in informing our empirical
analysis of the model pipeline and will be vital in future
assessments of the model performance in the fields when
actual disasters happen.
In 2019, the xView2 challenge and the xBD dataset were
announced at the Computer Vision for Global Challenges
Workshop at the Conference on Computer Vision and
Pattern Recognition to benchmark automated computer vision
capabilities for localizing and scoring the degree of damage
to buildings after natural disasters [3]. In this challenge,
participants had to train their model offline and upload
their predictions for evaluation and display on the public
leaderboard based on a single unlabeled test dataset, which
they could download. While this challenge provided a great
opportunity for AI researchers to weigh in on damage
assessment tasks, it assumed no constraints on the level of
computational resources available to participants for model
training and did not strictly prevent the potential hand-labeling
and use of the test datasets in the training phase. The winning
solutions used large ensembles of models, and although they
perform well on the test set, they were not optimized for
inference runtime and require a prohibitively large amount
of compute resources to be run on large amounts of satellite
imagery on demand during disaster events. For example, the
first-place winner proposed an ensemble of four different
models, requiring 24 inference passes for each input.
In this study, we propose a single model which predicts
both building edges and damage levels and that can be run
efficiently on large amounts of input imagery. The proposed
multitask model includes a building segmentation module
and a damage classification module. We use a similar model
architecture proposed by previous studies on building damage
assessment [4], [5]; however, we use a simpler encoder and do
not include attention layers. We evaluate the performance of
our model extensively for several different splits of the dataset
to assess its robustness to unseen disaster scenarios. From an
operational perspective, the model’s runtime is of paramount
importance. Thus, we benchmark the inference speed of our
model against the winning solutions in the xView2 competition
and the existing models deployed by our stakeholder. We show
that our model works three times faster than the fastest xView2
challenge winning solution and over 50 times faster than the
slowest first place solution. The baseline solution available to
our stakeholder consists of two separate models for building
segmentation and damage classification [6]. We were able to
show that our proposed approach works 20% faster than the
baseline model available to the stakeholder and also conducts
the task in an end-to-end and more automated way, which can
improve their field operations and deployment.
Finally, we develop a web-based visualizer that can display
the before and after imagery along with the model’s building
damage predictions on a custom map. This is an important
step in deploying a model for real-world use cases. Even
a perfect building damage assessment model will not be
practically useful if there is not a mechanism for running
that model on new imagery and communicating the results
to decision-makers that are responding to live events. A web-
based visualizer allows anyone to see both the imagery and
predictions without GIS software for any type of disaster.
II. R ELATED WORK
Convolutional neural networks (CNN) have been used
for change detection tasks in satellite imagery for disaster
response and other domains including but not limited to
changes in infrastructures. [7] proposed using pre-trained
CNN features extracted through different convolutional layers
and concatenation of feature maps for pre- and post-event
images. The authors used pixel-wise Euclidean distance
to compute change maps and thresholding methods to
conduct classification. [8] leverages hurricane Harvey data, in
particular, to train CNNs to classify images as damaged and
undamaged. While they report very high accuracy numbers,
they did not focus on detecting building edges and used a
binary damage scale at the image-frame level. A Siamese
CNN approach was proposed in [9] to extract features directly
from the images, pixel by pixel. To reduce the influence of
imbalance between changed and unchanged pixels, the authors
used weighted contrastive loss. The unique property of the
extracted features was that the feature vectors associated with
changed pixel pairs were far away from each other in the
feature space, whereas the ones of unchanged pixel pairs
were close. Fully convolutional Siamese networks for changedetection were introduced in [4] and were proposed by other
studies as well [10], [11].
In [4], convolutional Siamese networks are trained end-to-
end from scratch using only the available change detection
datasets. The authors proposed fully convolutional encoder-
decoder networks that use the skip connection concept.
[12] presented an improved UNet++ model with dense skip
connections to learn multiscale and different semantic levels
of visual feature representations. Attention layers have been
proposed for general change detection networks [13] as well
as building damage assessment tasks as presented in [5]. Also,
[14] proposes an attention-based two-stream high-resolution
network to unify the building localization and classification
tasks into an end-to-end model via replacing the residual
blocks in HRNet [15] with attention-based residual blocks
to improve the model’s performance. RescueNet, an end-
to-end model that handles both segmentation and damage
classification tasks was proposed in [16]. It was trained using
a localization aware loss function, that consists of a binary
cross-entropy loss and dice loss for building segmentation and
a foreground-only selective categorical cross-entropy loss for
damage classification. [6] explored the applicability of CNN-
based models under scenarios similar to operational emergency
conditions with unseen data and the existence of time
constraints. [17] proposed a dual-task Siamese transformer
model to capture non-local features. Their model adopts
transformers as the backbone rather than a convolutional
neural network and relies on a lightweight decoder for the
downstream tasks.
Graph-based models have been explored in [18] for building
damage detection solutions to capture similarities between
neighboring buildings for predicting the damage. They used
the xBD dataset for cross-disaster generalization. While their
proposed approach showed some advantages in terms of
accuracy, it did not consistently outperform the Siamese
CNN model in terms of F1 score, which would be a more
appropriate metric for imbalanced datasets. Furthermore, [19]
proposed BLDNet based on a Siamese CNN combined with
a graph node classification approach to be trained in a semi-
supervised manner to reduce the number of labeled samples
needed to obtain new predictions. They benchmarked their
approach with a semi-supervised multiresolution autoencoder
and showed performance improvements. The extremely
imbalanced distributions of the building damages are
addressed in [20] by supplementing the architecture with a new
learning strategy comprising normality-imposed data-subset
generation and incremental training. However, they propose
a two-step solution approach for building localization and
damage classification. Self-supervised comparative learning
approach has been studied in [21] to address the task without
the requirement of labeled data. Their proposed approach is an
asymmetric twin network architecture evaluated on the xBD
dataset.
In this study, we propose a Siamese approach inspired by
[4], [5] where UNet architecture is used for the building
segmentation task and UNet’s encoders with shared parameters
for pre-disaster and post-disaster imagery, are used to
score building damage levels via an end-to-end approach.
Furthermore, we also evaluate the performance of our model
in various scenarios that resemble operational emergency
conditions. Web visualizer tools have been developed for
other specific domains like data-driven wildfire modeling
[22] and fire inspection prioritization [23] in the past. Our
developed web visualizer allows imagery and prediction layers
visualization for any disasters where before and after disaster
satellite images are available.
III. D ATA
In this study, we use the xBD dataset introduced in [24]
as a new large-scale dataset for the advancement of change
detection and building damage assessment for humanitarian
assistance and disaster recovery research. This dataset has been
sourced from the Maxar/DigitalGlobe Open Data Program. It
covers 19 different disasters from around the world for which
there exists high-resolution ( <0.8m/px resolution) imagery.
The disaster types include flood, wind, fire, earthquake,
tsunami, and volcano. The entire dataset contains 22,068
image tiles of 1024 ×1024 pixels that cover a total of 45,361.79
sq. km. There are 850,736 building polygons available along
with a damage level label that indicates: no-damage, minor-
damage, major-damage, and destroyed. The breakdown of the
number of polygons for pre-disaster images across different
disasters is shown in Table I. Figure 1 and Figure 2 show
some examples of pre- and post-disaster image frames from
the xBD dataset. See figure 5 for legend.
Name/Location Type # of polygons
Palu, Indonesia Earthquake/Tsunami 55,789
Mexico City, Mexico Earthquake/Tsunami 51,473
Nepal Flood 43,265
Hurricane Harvey, USA Flood 37,955
Hurricane Michael, USA Wind 35,501
Hurricane Matthew, USA Wind 23,964
Portugal Wildfire 23,413
Moore, OK Wind 22,958
Santa Rosa, CA Wildfire 21,955
SoCal, CA Wildfire 18,969
Sunda Strait, Indonesia Earthquake/Tsunami 16,847
Joplin, MO Wind 15,352
Tuscaloosa, AL Wind 15,006
Midwest USA Flood 13,896
Hurricane Florence, USA Flood 11,548
Woolsey, CA Wildfire 7,015
Pinery, Australia Wildfire 5,961
Lower Puna, HI V olcanic eruption 3,410
Guatemala V olcanic eruption 991
TABLE I
DISASTER EVENTS IN THE X BD DATASET .
IV. M ODEL ARCHITECTURE
We propose a deep learning model that conducts both
building segmentation and damage classification tasks via
a single pipeline. Our approach has some similarities to
the proposed method in [5]. However, our architecture
is less complex as we do not incorporate any attention
Fig. 1. Imagery samples from different disasters from DigitalGlobe.
(a) Pre-disaster
 (b) Post-disaster
 (c) Ground Truth
Fig. 2. Imagery samples with polygons showing building edges and colors
showing damage level.
layers in the model. One module of our model is based
on the UNet architecture proposed in [25], which obtains
the building segmentation mask. A single image frame is
fed to the fully convolutional UNet model where local
information is captured via encoder-decoder structures and
global information is captured via several skip connections.
In the damage assessment scenario, we have a pair of pre-
and post-disaster image frames, which are given as inputs
separately to the UNet module of our proposed method
using shared weights. We use the embedding layers from the
encoder part of the UNet architecture for pre- and post-disaster
images to learn about the changes. In other words, the second
module of our model is a separate decoder that conducts a
damage classification task on the subtracted embedding layers
using several convolutional layers. This idea is based on the
approach proposed in [4]. Figure 3 demonstrates the overall
schema of the architecture. Our UNet architecture has five
convolution blocks for the encoder part and four convolution
blocks for the decoder. Each downsampling block consists
of convolution, batch normalization, ReLU, and max-pooling
layers. Each upsampling block consists of upsampling with
bilinear interpolation, convolution, batch normalization, and
ReLU layers. For the damage decoder, the same upsampling
blocks apply to subtracted and concatenated representations
at each step. The details of the layers can be found in our
code repository made publicly available1. The output of the
damage classification mask has five channels for four damage
levels and one background label. We use weighted binary
cross-entropy loss for building segmentation and multi-label
cross-entropy loss for damage classification. In the building
1https://github.com/microsoft/building-damage-assessment-cnn-siamese
Fig. 3. We use a Siamese U-Net model architecture where the pre- and
post disaster imagery are fed into an encoder-decoder style segmentation
model (U-Net) with shared weights (blocks with the same color in the figure
share weights). The features generated by the segmentation encoder from
both inputs are subtracted and passed to an additional damage classification
decoder that generates per-pixel damage level predictions. The weights of the
damage classification decoder can be fine-tuned for specific disaster types,
while relying on building segmentation output from the building decoder.
segmentation loss function shown in equation 1, ωs,1andωs,0
denotes weights on building pixels and background pixels,
respectively. Subscript sdenotes the segmentation task. ysis
the ground truth label for each pixel and psis the predicted
probability. For both pre- and post-disaster image frames, loss
functions LspreandLspostare defined similarly and the UNet
model has shared weights across these two components.
Lspre=Lspost=−(ωs,1yslogps+ωs,0yslog (1−ps))
(1)
In equation 2, ωd,cdenotes weight on each damage class c.
We use subscript dto denote the damage classification task. yd
is the damage ground truth label for each pixel and pdis the
predicted probability. The damage loss, Ldmg, is calculated
only when a pixel is predicted as a building class or ˆys== 1 .
Ldmg =−P5
c=1ωd,c(yd(c) logpd(c)), ifˆys== 1 (2)
Equation 3 indicates the combined weighted loss function
for the tasks along with their corresponding weights.
Ltotal =ωspreLspre+ωspostLspost+ωdLdmg (3)
V. E XPERIMENTS AND RESULTS
For our first experiment, we divide each disaster’s tiles
available in the xBD dataset based on original tiles of
1024x1024 pixels into train/validation/test splits at the ratio
of 80:10:10 randomly, to train and evaluate the performance
of our model. Based on this way of splitting the dataset, it is
possible to have different tiles from the same disaster incident
across the training, validation, and test sets. To reduce the
size of the input images, we further crop each tile into 20
patches of 256x256 pixels. The number of final patches in the
train/validation sets is 176700:22220. We conduct tile-wise
normalization on the pre-disaster and post-disaster imageryseparately. We also apply random horizontal and vertical
flipping during the training to reduce overfitting.
We observed that it is quite challenging to train the entire
model from scratch for both tasks simultaneously as the
performance of the building segmentation step impacts the
performance of the damage classification task significantly. As
such, we train the model sequentially based on two different
sets of weights. First, we train the building segmentation
module by setting the weight for damage classification as
zero and setting the weights for the UNet in the loss function
equal to 0.5 for both pre-disaster and post-disaster building
segmentation tasks. We also set weights for building pixels
equal to 15 and background pixels equal to 1 as there
is a significant imbalance between the number of pixels
across these two classes. In other words, [ωspre, ωspost, ωd] =
[0.5,0.5,0]and[ωs,c=0, ωs,c=1] = [1 ,15]. Label c= 0denotes
background pixels and label c= 1 denotes building pixels.
Once we get reasonable performance on the validation
set for the first task, we freeze the parameters of the
UNet and we start training the model for the second task,
i.e., damage classification. Thus, we set the weights in the
loss function for pre-disaster and post-disaster segmentation
task as zero and we set the damage classification task
equal to 1. Due to high imbalance across different damage
classes, we assign higher weights to the major-damage class
(label=3) and destroyed class (label=4). In other words,
[ωd,c=0, ωd,c=1, ωd,c=2, ωd,c=3, ωd,c=4] = [1 ,35,70,150,120]
for the damage classification task and [ωspre, ωspost, ωd] =
[0,0,1]for building segmentation. Label d= 0 denotes
background pixels and labels d= 1 tod= 4 denotes damage
levels scaled from not-damaged to destroyed.
Since our model handles two tasks, we demonstrate
performance results separately on each task. The performance
results for the tile-wise random split are demonstrated in the
first row of Table III where the model is evaluated both
on the validation set and test set. Columns in the table
are named BLD-1, DMG-0, DMG-1, DMG-2, DMG-3, and
DMG-mean. The BLD-1 column denotes the F1 score for
class 0, which indicates building pixels. DMG-0, DMG-1,
DMG-2, and DMG-3 indicate pixel-wise F1 scores for no-
damage, minor-damage, major-damage, and destroyed classes,
respectively. DMG-mean denotes the harmonic F1 score across
all damage levels computed based on the following equation.
F1dmg=4
P4
c=11
F1c+ϵ(4)
As shown in the columns for extreme classes of not
damaged and destroyed, i.e., DMG-0 and DMG-3 in Table III,
we observe superior performance results compared to columns
DMG-1 and DMG-2 for the minor-damage and the major-
damage classes due to the strength in the signal for those
extreme classes. We used one Nvidia Tesla V100 GPU with
32G Memory to train the model and it took 6 days for the
training to complete. Adam optimization algorithm was used
and learning rate and batch size were set at 0.001 and 32.
Fig. 4. Example model predictions at three locations with varying amounts
of observed damage.
Fig. 5. Legend for damage level colors shown in Figure 2 and 4
Figure 4 demonstrates the predicted polygons for buildings
along with their predicted labels. In the second row, the model
was able to capture two missing buildings in the ground
truth mask. Green, orange, purple and dark pink colors are
used to indicate no-damage, minor-damage, major-damage and
destroyed classes respectively. See figure 5 for legend.
The baseline model available to our stakeholder shows F1
score of 0.64 on the test set for building segmentation, which
is inferior to our result of 0.74, shown in Table III. For the
stakeholder’s baseline damage classification performance, we
do not have access to the results for a comparable data split
to report here. We also show our model’s performance against
the baseline model presented in [26] in Table II. Our proposed
solution demonstrates a significant improvement in damage
classification task.
Model BLD DMG
Baseline 0.79 0.03
Ours 0.74 0.58
TABLE II
COMPARISON WITH THE BASELINE MODEL PRESENTED IN [26], BOTH
RESULTS ARE BASED ON TRAINING MODELS ON THE X BD TIER 1DATASET
VI. M ODEL ROBUSTNESS TO UNSEEN DISASTERS
To assess the robustness of the model performance to unseen
disasters, we conduct four additional experiments outlined in
Table III. To that end, each time, we leave either the Joplintornado or the Nepal flooding out for testing purposes and
we train and validate the model based on the random split of
the remaining data. Additionally, to see the impact of training
damage classification only based on a specific type of disaster,
we conduct two additional experiments: (I) when damage
classification is trained only on wind-caused data, and (II)
when damage classification is trained only on flood disasters.
In both cases, the building segmentation module is trained on
90% of the entire training data; not on a specific disaster type
as in the damage classifier module. In Table III, the second
row shows the results for the case when we leave out the
Joplin tornado for testing purposes and use a random split of
the remaining data for training and validation for both building
segmentation and damage classification tasks. For this unseen
disaster, the harmonic mean of F1 scores on the test set drops
by 4% compared to the completely random split of the dataset.
The drop in the performance is more significant, 0.54%,
when we leave Nepal flooding out as outlined in the fourth
row of Table III. Regression in performance is also notable
for the building segmentation task for Nepal flooding. This
observation can be associated to the geographical distribution
of the data. Unlike Nepal flooding, the majority of the disasters
in the dataset, used in the training phase, are concentrated
around North and Central America, which could explain the
dramatic decrease in the F1 score when testing the model on a
completely new geographical region. Test set results outlined
in rows three and five of Table III demonstrate that training
the damage classifier on the specific type of disasters boosts
the performance when testing on a completely unseen disaster
event.
VII. I NFERENCE SPEED BENCHMARKING
Inference speed (specifically, the number of pixels/second
that a model can process) is an important property of models
that will be deployed to run on imagery collected from future
disasters. Slow models will result in larger compute costs and
potentially delayed results in time-sensitive disaster response
applications.
We benchmark the inference speed of the top-ranked
solutions on the xView2 challenge with our proposed model
and find that our proposed model is three times faster than
the fastest winning solution and over 50 times faster than the
slowest first place solution. Table IV shows the performance
results of each solution (except for the 4th place solution which
was not reproducible). To benchmark each solution we use the
following setup:
•A NC6 virtual machine instance on Microsoft Azure
which contains a Tesla K80 GPU.
•The single input inference script provided in each
solution’s code release from the official “DIUx-xView”
GitHub account. If the inference script did not contain a
flag for enabling GPU acceleration we modified it to use
the GPU for model inference.
•Three pre- and post-disaster inputs from the xBD dataset.
Experiment Train Test BLD-1 DMG-0 DMG-1 DMG-2 DMG-3 DMG-mean
Random splits 80% at random 10% at random 0.74 0.89 0.43 0.54 0.73 0.60
Joplin held out 90% of non-Joplin Joplin only 0.76 0.89 0.50 0.36 0.81 0.56
Joplin held out (wind only damage classifier) 90% of non-Joplin Joplin only 0.74 0.89 0.42 0.54 0.77 0.60
Nepal held out 90% of non-Nepal Nepal only 0.63 0.42 0.17 0.23 0.02 0.06
Nepal held out (flood only damage classifier) 90% of non-Nepal Nepal only 0.64 0.54 0.12 0.27 0.07 0.14
TABLE III
PIXEL -WISE F1SCORE ACROSS VARIOUS SPLITS OF THE X BD DATASET . W E TEST GENERALIZATION PERFORMANCE OF MODELS ON THE JOPLIN WIND
AND NEPAL FLOODING EVENTS IN TWO SETTINGS :ONE IN WHICH WE TRAIN ON allAVAILABLE DATA THAT IS NOT FROM THE SPECIFIC EVENT ,AND
ANOTHER SETTING IN WHICH WE TRAIN THE DAMAGE CLASSIFICATION DECODER ON OTHER WIND -ONLY EVENTS (FOR TESTING ON THE JOPLIN
EVENT )AND OTHER FLOOD ONLY EVENTS (FOR TESTING ON THE NEPAL FLOODING EVENT ).
•The same Python virtual environment for all experiments
to remove the effect of different packages on the
performance.
Additionally, the inference times reported in Table IV
include the file I/O, model loading, pre-processing, and post-
processing costs associated with each approach and therefore
represent an upper bound on the time taken to process any
given 1024×1024 input (i.e. when running such approaches
over large amounts of input, the models would only need to
be loaded from the disk a single time).
As previously discussed, the xView2 challenge2encouraged
participants to optimize for leaderboard performance instead
of throughput. As such, many of the top-placed solutions used
techniques such as ensembling and test time augmentation, as
well as larger, more complex models in order to improve their
performance at the cost of inference speed. The top-performing
solution, for instance, consists of an ensemble of 12 models
that are run 4 times for each input (test time augmentation with
4 rotations). These solutions are prohibitively costly to run
on large inputs. For example, the Maxar Open Data program
released ∼20,000km2of pre- and post-disaster imagery
covering areas impacted by Hurricane Ida in 2021. Assuming
the inference times from Table IV, 0.3m/px spatial resolution
of the input imagery and $0.9/hr cost of running a Tesla K80
(based on current Azure pricing), the first place solution would
cost $6,500 to run, while our solution would only cost $100
to run. In this case, our solution would generate results for the
area affected by Hurricane Ida in 4.7 days while the first place
solution would take up to 301.4 days using a single NVIDIA
Tesla K80 GPU.
Finally, we benchmark our proposed solution in an
optimized setting compared to the above setting: we load data
with a parallel data-loader (vs. loading a single tile on the main
thread), we run pre- and post-processing steps on the GPU,
we maximize the amount of imagery that is run through the
model at once (vs. running on a single 1024×1024 tile of
imagery), and we use the most recent version of all relevant
packages (vs. the earliest version pinned in the environments
from the xView2 solution repositories). Here, we find that
our model is able to process 612.29 square kilometers per
hour compared to 89.35 square kilometers per hour under the
2Most machine learning competitions follow a similar format, whereby
participant solutions are only ranked in terms of the held-out test set
performance.same assumptions in the previous setup despite using the same
hardware. In this case, our model could process the Hurricane
Ida imagery in 2 days at a cost of $14.7. The stakeholder’s
baseline solution’s speed is 1000 square kilometers per hour
on Azure NC12 GPU. We project our runtime to be 20% faster
than their baseline solution on a similar GPU.
Method Inference time (s) sq. km/hr
xView2 1st place 245.75 (0.73) 1.38
xView2 2nd place 121.03 (0.36) 2.81
xView2 3rd place 108.21 (0.6) 3.14
xView2 4th place not reproducible not reproducible
xView2 5th place 10.94 (0.06) 31.07
Our method 3.8 (0.02) 89.35
TABLE IV
COMPARISON OF BUILDING DAMAGE MODEL INFERENCE TIMES ON A
SINGLE 1024 X1024 PIXEL TILE FOR DIFFERENT METHODS USING A
SINGLE TESLA K80 GPU ( ON AN AZURE NC6 MACHINE ). T IMES ARE IN
SECONDS AND ARE AVERAGED OVER THREE RUNS WITH A STANDARD
DEVIATION IN PARENTHESES . THE RESULTS FOR THE WINNING X VIEW2
SOLUTIONS ARE REPRODUCED THROUGH THE OFFICIAL GITHUB
REPOSITORIES PUBLISHED FOR EACH ,WHERE THE ONLY MODIFICATIONS
TO THE ORIGINAL CODE WAS TO ENABLE GPU PROCESSING FOR EACH
INFERENCE SCRIPT . THE RIGHTMOST COLUMN SHOWS THE INFERENCE
SPEED IN TERMS OF (SQ.KM)/HR ASSUMING A 0.3M/PIXEL INPUT SPATIAL
RESOLUTION .
VIII. W EBVISUALIZER TOOL
In contrast to standard vision applications, semantic
segmentation models that operate over satellite imagery need
to be applied over arbitrarily large scenes at inference-time. As
such, distributing the imagery and predictions made by such
models is non-trivial. First, high-resolution satellite imagery
scenes can be many gigabytes in size, difficult to visualize
(e.g. requiring GIS software and normalization steps), and may
require pre-processing to correctly align temporal samples.
Second, the predictions from a building damage model are
strongly coupled to the imagery itself. In other words,
only distributing georeferenced polygons of where damaged
buildings are predicted to be is not useful in a disaster response
setting. The corresponding imagery is necessary to interpret
and perform quality assessment on the predictions.
Considering these difficulties, we implement a web-based
visualizer to distribute the predictions made by our model over
satellite image scenes. This approach bypasses the need for any
specialized GIS software, allowing any modern web-browser
Fig. 6. Screenshot of the building damage visualizer instance for the August,
2021 Haiti Earthquakes. The left side of the map interface shows the pre-
disaster imagery while the right side shows the post-disaster imagery. The
slider in the middle of the interface allows a user to switch between the pre-
and post-disaster layers to quickly see the difference in the imagery. Finally,
the building damage predictions are shown as polygons with varying shades
of red corresponding to increasing damage. The visibility of these predictions
can be toggled in the interface so that a user can see the underlying imagery.
to view the imagery and predictions, and doesn’t require users
to have any formal GIS experience as all imagery is pre-
rendered. Specifically, users can:
1) Toggle back and forth between the pre- and post-disaster
imagery to easily see the differences;
2) Change the visibility of the damage predictions to see
the extent of the damage;
3) Show standard layers (e.g. OpenStreetMap or Esri World
Imagery) for additional spatial context.
This is implemented with open-source tools including:
GDAL3, leaflet4, and Docker5.
An instance of our visualizer is shown in Figure 6 for
a scene from Jeremie, Haiti after the Haiti Earthquake in
August, 2021. The tower of the Cathedral of Saint Louis Roi
of France (middle of the scene) is classified as damaged by the
model and can be seen to be destroyed. The code for running
inference with our final building damage model, as well as
setting up an instance of the building damage visualizer tool
is publicly available6.
Fig. 7. Full screenshots of pre- and post-disaster images shown partially in
the building damage visualizer instance in Figure 6 for better visibility. 2021
Haiti Earthquakes.
3https://gdal.org/programs/index.html
4https://leafletjs.com/
5https://www.docker.com/
6https://github.com/microsoft/Nonprofits/IX. D EPLOYMENT AND APPLICATIONS
Automating building detection and damage assessment has
the potential to tremendously speed up disaster response
processes [6], [27], which are of critical importance for
humanitarian organizations to estimate the geographical extent
and the severity of a disaster and plan accordingly [28]. To
ensure that such an assessment can be delivered in time,
this study’s stakeholder is implementing the proposed model
within a scalable, distributed computing system. Within this
system, satellite images are divided among many identical
instances of the model, which process them in parallel. This
guarantees a fixed computation time with any number and
size of input satellite images. The model’s output is then
shared with the wider humanitarian network in three ways: the
aforementioned web visualizer, the open data-sharing platform
“Humanitarian Data Exchange”, and via man-made maps
(in digital or printed format), which can be directly sent
to and used by first responders in the field. This ensures
the rapid diffusion of information among all stakeholders
involved in disaster response management. It is worth noting
that our stakeholder’s experience with applying such tools in
humanitarian settings and discussions with practitioners have
highlighted the importance of two aspects. First, the value
of automating damage assessment lies in speed, rather than
accuracy. Regardless of visible damages, detailed ground-level
inspections by trained personnel are still needed to assess
the structural integrity of a building [29] and it is unlikely
that remote sensing technology will replace that in the near
future. For this reason, the focus of satellite-based damage
assessments should be to provide broad numerical estimates
as fast as possible, rather than building-level prescriptions.
Secondly, while the immediate response is primarily informed
by disaster impact (which can be quantified by the number of
damaged buildings, among other metrics), long-term shelter
recovery programs must take into account several other
contextual factors, such as the socio-economic conditions of
affected people and land ownership [30]. Because of this,
information on building damage often needs to be combined
with other data to be useful. Providing raw data including
geo-referenced building footprint masks and corresponding
damage levels to the humanitarian community is necessary to
enable these analyses. Furthermore, as we discussed in section
VI, trained models based on the proposed approach might
not be robust to significant distribution shift across different
geographies; as such, domain adaptation techniques need to be
explored to address the data biases issues [31]. In this context,
active learning and human-machine collaboration approaches
have been discussed in [32] and [33].
X. C ONCLUSION
Natural disasters’ frequency is growing; thus, the impact
of such events on communities continues to increase. The
strategic response of humanitarian organizations to allocate
resources and save lives after disasters can be improved by
using AI tools. We propose a convolutional neural network
model that uses satellite images from before and after natural
disasters to localize buildings using the UNet model and
score their damage level on a scale of 1 (not-damaged) to 4
(destroyed) using a multi-class classifier. We showed that while
our proposed model demonstrates decent performance, it also
works three times faster than the fastest xView2 challenge
winning solution and over 50 times faster than the slowest
first place solution, which indicates a significant improvement
from an operational perspective. We also developed a web-
based visualizer that can display the before and after imagery
along with the model’s building damage predictions on a
custom map to allow better inspection of the impacted areas by
decision-makers. This paper outlines results of a collaboration
between Microsoft AI for Good/Humanitarian Action and 510
an initiative of the Netherlands Red Cross, to help inform
field deployments using satellite imagery and AI technologies.
Our solution outperforms stakeholder’s current baseline model
significantly in terms of inference speed and segmentation
accuracy. This study’s stakeholder is planning to deploy and
assess our proposed solution at the time of actual disasters.
REFERENCES
[1] OCHA-GHO, “Global humanitarian overview,”
https://reliefweb.int/sites/reliefweb.int/files/resources/GHO2019.pdf,
2019.
[2] W. M. O. WMO, “Climate and weather related disasters surge five-
fold over 50 years, but early warnings save lives - wmo report,”
https://news.un.org/en/story/2021/09/1098662, 2021.
[3] Defence-Innovation-Unit, “xview2: Assess building damage,”
https://xview2.org/challenge, 2019.
[4] R. C. Daudt, B. Le Saux, and A. Boulch, “Fully convolutional siamese
networks for change detection,” in 2018 25th IEEE International
Conference on Image Processing (ICIP) . IEEE, 2018, pp. 4063–4067.
[5] H. Hao, S. Baireddy, E. R. Bartusiak, L. Konz, K. LaTourette,
M. Gribbons, M. Chan, E. J. Delp, and M. L. Comer, “An attention-based
system for damage assessment using satellite imagery,” in 2021 IEEE
International Geoscience and Remote Sensing Symposium IGARSS .
IEEE, 2021, pp. 4396–4399.
[6] T. Valentijn, J. Margutti, M. van den Homberg, and J. Laaksonen,
“Multi-hazard and spatial transferability of a cnn for automated building
damage assessment,” Remote Sensing , vol. 12, no. 17, p. 2839, 2020.
[7] A. M. El Amin, Q. Liu, and Y . Wang, “Convolutional neural
network features based change detection in satellite images,” in
First International Workshop on Pattern Recognition , vol. 10011.
International Society for Optics and Photonics, 2016, p. 100110W.
[8] S. Kaur, S. Gupta, S. Singh, D. Koundal, and A. Zaguia, “Convolutional
neural network based hurricane damage detection using satellite images,”
Soft Computing , pp. 1–15, 2022.
[9] Y . Zhan, K. Fu, M. Yan, X. Sun, H. Wang, and X. Qiu, “Change
detection based on deep siamese convolutional network for optical aerial
images,” IEEE Geoscience and Remote Sensing Letters , vol. 14, no. 10,
pp. 1845–1849, 2017.
[10] F. Rahman, B. Vasu, J. Van Cor, J. Kerekes, and A. Savakis, “Siamese
network with multi-level features for patch-based change detection in
satellite imagery,” in 2018 IEEE Global Conference on Signal and
Information Processing (GlobalSIP) . IEEE, 2018, pp. 958–962.
[11] H. Chen, C. Wu, B. Du, and L. Zhang, “Change detection in multi-
temporal vhr images based on deep siamese multi-scale convolutional
networks,” arXiv preprint arXiv:1906.11479 , 2019.
[12] D. Peng, Y . Zhang, and H. Guan, “End-to-end change detection for high
resolution satellite images using improved unet++,” Remote Sensing ,
vol. 11, no. 11, p. 1382, 2019.
[13] J. Chen, Z. Yuan, J. Peng, L. Chen, H. Huang, J. Zhu, Y . Liu, and H. Li,
“Dasnet: Dual attentive fully convolutional siamese networks for change
detection in high-resolution satellite images,” IEEE Journal of Selected
Topics in Applied Earth Observations and Remote Sensing , vol. 14, pp.
1194–1206, 2020.[14] V . Oludare, L. Kezebou, O. Jinadu, K. Panetta, and S. Agaian,
“Attention-based two-stream high-resolution networks for building
damage assessment from satellite imagery,” in Multimodal Image
Exploitation and Learning 2022 , vol. 12100. SPIE, 2022, pp. 224–
239.
[15] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y . Zhao, D. Liu,
Y . Mu, M. Tan, X. Wang et al. , “Deep high-resolution representation
learning for visual recognition,” IEEE transactions on pattern analysis
and machine intelligence , vol. 43, no. 10, pp. 3349–3364, 2020.
[16] R. Gupta and M. Shah, “Rescuenet: Joint building segmentation and
damage assessment from satellite imagery,” in 2020 25th International
Conference on Pattern Recognition (ICPR) . IEEE, 2021, pp. 4405–
4411.
[17] H. Chen, E. Nemni, S. Vallecorsa, X. Li, C. Wu, and L. Bromley, “Dual-
tasks siamese transformer framework for building damage assessment,”
arXiv preprint arXiv:2201.10953 , 2022.
[18] A. Ismail and M. Awad, “Towards cross-disaster building damage
assessment with graph convolutional networks,” arXiv preprint
arXiv:2201.10395 , 2022.
[19] ——, “Bldnet: A semi-supervised change detection building damage
framework using graph convolutional networks and urban domain
knowledge,” arXiv preprint arXiv:2201.10389 , 2022.
[20] Y . Wang, A. W. Z. Chew, and L. Zhang, “Building damage detection
from satellite images after natural disasters on extremely imbalanced
datasets,” Automation in Construction , vol. 140, p. 104328, 2022.
[21] Z. Xia, Z. Li, Y . Bai, J. Yu, and B. Adriano, “Self-supervised learning
for building damage assessment from large-scale xbd satellite imagery
benchmark datasets,” arXiv preprint arXiv:2205.15688 , 2022.
[22] J. Block, D. Crawl, T. Artes, C. Cowart, R. de Callafon, T. DeFanti,
J. Graham, L. Smarr, T. Srivas, and I. Altintas, “Firemap: A web tool
for dynamic data-driven predictive wildfire modeling powered by the
wifire cyberinfrastructure,” in AGU Fall Meeting Abstracts , vol. 2016,
2016, pp. PA23B–2234.
[23] M. Madaio, S.-T. Chen, O. L. Haimson, W. Zhang, X. Cheng, M. Hinds-
Aldrich, D. H. Chau, and B. Dilkina, “Firebird: Predicting fire risk and
prioritizing fire inspections in atlanta,” in Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining , 2016, pp. 185–194.
[24] R. Gupta, B. Goodman, N. Patel, R. Hosfelt, S. Sajeev, E. Heim,
J. Doshi, K. Lucas, H. Choset, and M. Gaston, “Creating xbd: A dataset
for assessing building damage from satellite imagery,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) Workshops , June 2019.
[25] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
for biomedical image segmentation,” in International Conference
on Medical Image Computing and Computer-assisted Intervention .
Springer, 2015, pp. 234–241.
[26] R. Gupta, R. Hosfelt, S. Sajeev, N. Patel, B. Goodman, J. Doshi,
E. Heim, H. Choset, and M. Gaston, “xbd: A dataset for
assessing building damage from satellite imagery,” arXiv preprint
arXiv:1911.09296 , 2019.
[27] A. Elia, S. Balbo, and P. Boccardo, “A quality comparison between
professional and crowdsourced data in emergency mapping for potential
cooperation of the services,” European Journal of Remote Sensing ,
vol. 51, no. 1, pp. 572–586, 2018.
[28] D. P. Coppola, Introduction to International Disaster Management .
Elsevier, 2006.
[29] GFDDR, “Post-disaster needs assessments guidelines volume b
housing,” 2017.
[30] H. Shelter, “Settlements guidelines,” 2017.
[31] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and
J. W. Vaughan, “A theory of learning from different domains,” Machine
learning , vol. 79, no. 1, pp. 151–175, 2010.
[32] D. Tuia, M. V olpi, L. Copa, M. Kanevski, and J. Munoz-Mari, “A
survey of active learning algorithms for supervised remote sensing image
classification,” IEEE Journal of Selected Topics in Signal Processing ,
vol. 5, no. 3, pp. 606–617, 2011.
[33] N. Jojic, N. Malkin, C. Robinson, and A. Ortiz, “From local algorithms
to global results: Human-machine collaboration for robust analysis of
geographically diverse imagery,” in 2021 IEEE International Geoscience
and Remote Sensing Symposium IGARSS . IEEE, 2021, pp. 270–273.
PERSPECTIVES88 
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE     JUNE 2021DEVIS TUIA, RIBANA ROSCHER, JAN DIRK WEGNER, NATHAN JACOBS,  
XIAO XIANG ZHU, AND GUSTAU CAMPS-VALLS
In past years, we have witnessed the fields of geosci -
ences and remote sensing and artificial intelligence 
(AI) become closer. Thanks to the massive availability 
of observational data, improved simulations, and algo -
rithmic advances, these disciplines have found common 
objectives and challenges to help advance the modeling 
and understanding of the Earth system. Despite such 
great opportunities, we have also observed a worrisome 
tendency to remain in disciplinary comfort zones, ap -
plying recent advances from AI on well-resolved remote 
sensing problems. Here, we take a position on the re -
search directions for which we think the interface be -
tween these fields will have the most significant impact 
and become potential game changers. In our declared 
agenda for AI in Earth sciences, we aim to inspire re -
searchers, especially the younger generations, to tackle 
these challenges for a real advance of remote sensing 
and the geosciences.
DATA-DRIVEN APPROACHES IN THE 
GEOSCIENCES, THEIR PITFALLS AND 
OPPORTUNITIES
AI promises to change the way we do science. To -
day it is widely accepted, and almost a mantra, that 
data, along with faster computers and advanced ma -
chine learning (ML) algorithms, can solve any data 
science problem. Approaches from ML, computer vi -
sion, applied mathematics, or big data, in general, 
are undoubtedly revolutionizing the way we tackle 
challenges in remote sensing and geoscience. This is 
particularly visible as deep learning has entered the 
arena [ 1]: the promise of a technology that can pro -
cess large amounts of data and learn the complex 
structures of environmental processes, thus leading 
to improved modeling, is making ML an unavoidable  approach. For a multidisciplinary overview, see the 
work of Camps-Valls et al. [ 2].
We are convinced that the rise of data-driven ap -
proaches in the geosciences is beneficial and will lead to 
important discoveries. However, we want to raise aware -
ness of pitfalls and the fundamental questions that are 
currently understudied by the community. 
The first risk is that of seeing everything as an 
opportunity to apply ML and then deploy massive 
technology regardless of whether such technology is 
necessary and adapted to the problem at hand. Were 
that to become common practice, one would miss the 
opportunity to use (and improve) such new technol -
ogy to tackle new challenges that could not be solved 
without it. For example, remote sensing is a data-hun -
gry discipline that embraced ML very early on: after 
an exploratory phase [ 3]–[5], we now see the need for 
an impulse to embrace this technology to unlock new, 
difficult problems that will, in turn, create value for 
these geospatial data. Some of the concepts presented 
in this article illustrate these directions (see Table 1 ). 
First is the question of introducing reasoning in the al -
gorithms as a way to mimic cognitive processes about 
space and time (direction 1 in Table 1 ). Second is the 
need for exploring unconventional data modalities to 
capture the complexity of the visual world (direction 2 
in Table 1 ). And third are the new methods of human 
interaction with remote sensing models, for instance, 
via question answering as a way to retrieve image con -
tent on demand (direction 3 in Table 1 ).
A second pitfall is the blind faith in data science. 
Data science has, regrettably, been a misleading term. 
Should it be replaced with “data for science,” as some 
researchers have suggested? Science is about contrast -
ing hypotheses, understanding physical phenomena, 
and validating causal and explanatory models. If those 
objectives were to be achieved through data analysis, a 
new science would be born.  Toward a Collective Agenda on AI  
for Earth Science Data Analysis
Digital Object Identifier 10.1 109/MGRS.2020.3043504
Date of current version: 17 June 2021
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
89 
JUNE 2021    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINEDriven by the impressive results obtained in ML and 
computer vision, it is tempting to believe that everything 
can be solved using only data and algorithms. We believe 
that domain knowledge and model assumptions are of 
prime importance and that models must be challenged 
1) to respect the reality of the physical/biological/chemi -
cal processes governing the system under study and 2) 
to be accountable—by the transparency of their internal 
reasoning—for the decisions to which they lead.  This is 
important, especially when models are intended to be 
used for actual decision making and can affect balanc -
es of power or society-changing decisions. Later in this 
article, we present ideas that are aligned with these di -
rections (see Table 1 ) and centered on the injection of 
domain knowledge, but for different purposes. First we 
discuss physics-aware ML, which has the goal of using 
domain knowledge to restrict the solution spaces of the 
models so that the outcome is physically plausible (di -
rection 4 in Table 1 ). This will ensure that  the physical 
consistency of the solutions is maintained while avoid -
ing aberrant outcomes that break physics (e.g., mass and 
energy conservation).   
We then discuss how best to obtain human-under -
standable interpretations and explanations of the inner  functioning of the models, to understand why and how 
models make decisions (direction 5 in Table 1 ). This has the 
advantage of making the model trustable and nonfalsifi -
able and of avoiding situations where the right conclusions 
are reached for the wrong reasons.  
Explainability also enhances the potential for testing nov -
el hypotheses and acquiring new scientific knowledge from 
the analysis of the model’s functioning. Directions 4 and 5 
can be combined: they use domain knowledge in various 
ways with different goals. The transparency of the models’ 
weights is not absolutely necessary at this stage, as interpre -
tations can be achieved by the analysis of inputs (e.g., Local 
Interpretable Model-agnostic Explanations, i.e., LIME [ 23]) 
and physics awareness realized by modified loss functions.  
Yet, as mentioned previously, science is about under -
standing the world in which we live, not just approximat -
ing it. We argue that, without learning causal relationships  
from observational data and assumptions, this ambitious 
goal of understanding the Earth system will not be possible 
(direction 6 in Table 1 ). In this case, learning of cause-and-
effect relationships is a mix of the previous ingredients, as 
domain knowledge is needed to design the model in such 
a way that it can reveal (maybe novel) cause-and-effect 
relationships that can be then explained using domain 
TABLE 1. A SUMMARY OF THE SIX RESEARCH DIRECTIONS PRESENTED IN THIS ARTICLE. 
DIRECTION IN A NUTSHELL REFERENCES CURRENT ISSUES 10 YEARS FROM NOW 
1 Go beyond recognition  
toward induction, deduction, 
spatial and temporal  
reasoning, and structural 
inference.[6], [7] Missing or very limited  
benchmarks and novel tasks as 
well as reasoning models; the 
interpretability is unsolved.Intelligent systems linking meaningful 
transformation of entities, e.g., over space 
or time, and deriving knowledge as the 
way people understand the visual world 
and its processes. 
2 Think beyond the raster and 
consider all the possible 
inputs and sources of supervi -
sion, in particular, geotagged 
social media data. [8]–[10] The presence of data set biases 
and of label noise; a spatiotem -
poral mismatch between data 
sources and scalability, with an 
increasing number of sources. Systems that use a wide variety of sources 
to enable a fine-grained understanding of 
the world, all with minimal human effort 
required for data set building and system 
design. 
3 Query the world by asking 
questions about images and 
create descriptions. [11], [12] Simplistic language model, 
limited choice of thematic  
interactions, and a lack of  
large-scale infrastructure. Visual search engines have an understand -
ing of questions about images and are able 
to adapt to different types of requests and 
are usable for everyone.
4 Make models learned  
using deep neural networks 
consistent with domain-  
specific knowledge, like  
equations from physics.[1], [13]–[16] Networks’ outputs are not 
physically consistent; networks 
are often used as emulators of 
simulations but do not explore 
beyond current simulators’  
constraints: they cannot  
discover new physical rules.Systems trainable with much fewer data 
because they constrain output space 
via physical knowledge; systems that 
learn a new hypothesis for new science 
generation.  
5 Enhancing interpretability  
and explainability to  
understand processes in ML 
models in a better way. [17], [18] A lack of human-understand -
able interpretation, with a  
tendency toward confirmation 
bias (e.g., with attention maps) Models that are more understandable and 
therefore more reliable and trustworthy; 
models that can be queried (and  
challenged) by humans about their inner 
reasoning.
6 Learn cause-and-effect 
relationships—not just correla -
tions—from observations and  
assumptions about the  
underlying generating process 
and system.[19]–[22] Models cannot work with  
unevenly sampled time series 
or nonstationary/noisy  
processes; they extrapolate 
poorly.Machines that automatically blend domain 
knowledge, observational data, and as -
sumptions to learn the causal graph and 
generate causal-narrative explanations of 
the problem.
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
90 
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE     JUNE 2021 knowledge. Here, the transparency of the model is achieved 
by construction because the product is a self-explanatory 
causal graph. All three fields (explainability, hybrid model -
ing, and causal inference) also have in common the need 
of an active and tight interaction among domain experts 
and computer scientists to make a decisive, nonincremen -
tal leap in Earth sciences.
With this position article, we present six research direc -
tions that we think hold particular promise for the future of 
Earth observation data analysis. In the following, we argue 
their potential and relevance and provide some pointers to 
pertinent resources. Our goal is to trigger curiosity and fos -
ter successful research to truly advance the field of Earth 
sciences with AI.
DIRECTION 1: REASONING AND  
HUMAN–MACHINE DIALOGS
Current research on the interface between ML and remote 
sensing focuses largely on the direct recognition of mate -
rials and objects or on estimating geophysical parameters. 
Reasoning goes beyond the concept of recognition and 
aims at mimicking how people think and learn. It is cen -
tered around tasks such as induction, deduction, spatial 
and temporal reasoning, and structural inference [ 24].
To date, only a few pioneering studies have been pub -
lished on reasoning for remote sensing tasks. In computer 
vision, reasoning is mostly interpreted as the capability to 
link meaningful transformations of entities over space or 
time. This is a fundamental property of intelligent species 
and is also the way people understand visual data. Recent -
ly, papers implementing reasoning in convolutional neural 
networks (CNNs) have started to appear. Santoro et al. pro -
posed a relational reasoning network as a simple plug-and-
play module to solve problems requiring the understand -
ing of arbitrary relationships between objects (ordering or 
comparisons of relative positions/sizes) and applied it to 
the problem of visual question answering (VQA) [ 25] (for 
more information on VQA, see direction 3 in Table 1 ). A 
second pioneering work concerns temporal relationships 
in video sequences. When it comes to understanding what 
takes place between two sampled video frames, humans 
can easily infer the temporal relationships and transfor -
mations between observations, unlike neural networks. 
In [26], the authors proposed a temporal relationship net -
work, which learns intuitive and interpretable common-
sense knowledge in videos.
WHY SHOULD RELATIONAL REASONING  
MATTER IN REMOTE SENSING?
Earth observation images carry strong spatial and temporal 
information because each pixel is precisely referenced and 
connected to neighbors in space and time. When consider -
ing land processes (and, in particular, the geophysical ones), 
relevant relationships can be learned by using models. In 
[6], the authors explicitly modeled long-range relationships 
for semantic segmentation in aerial scenes. With the goal of increasing the representation capacity of a fully convo -
lutional network, two tailored relationship modules were 
used: one describing relationships between observations 
in convolved images and another producing relationship-
augmented feature representations. Given that convolutions 
operate by blending spatial and cross-channel information 
together, they captured relationships in both the spatial and 
channel domains.
PERSPECTIVES
The work mentioned in the previous section showcases how 
spatial-relational reasoning helps with improving semantic 
understanding of remote sensing images, and many other 
problems may also benefit from visual reasoning. One excit -
ing example is temporal reasoning for the analysis of multi -
temporal data/aerial videos, e.g., for event recognition. This 
is a new and exciting field where one is concerned with un -
derstanding complex events being imaged or filmed, such 
as cultural events, manifestations, or locating people in dis -
tress. Using reasoning enables knowing whether a person 
on a roof during a flood is in need of actual help or whether 
the video of a crowd of people is related to a peaceful or 
violent demonstration. This could be of interest to various 
stakeholders, including local authorities. To foster this re -
search direction, Mou et al. [ 7] have introduced the data set 
Event Recognition in Aerial  videos (ERA), which consists of 
3,000 unmanned aerial vehicle videos manually annotated 
into dozens of types of events ( Figure 1 ).
In addition to videos, other interesting examples in -
clude VQA (see direction 3 in Table 1 ), captioning [ 27], and 
audiovisual reasoning, i.e., linking remote sensing images 
to in situ audio signals [ 28]. In the long run, we hope that 
reasoning Earth observation systems will be capable of de -
ducing clues and making structural inferences to explain 
processes (see direction 5 in Table 1 ) and understand causal 
structures in the Earth system (see direction 6 in Table 1 ). 
DIRECTION 2: EXTREMELY MULTIMODAL  
REMOTE SENSING
Remote sensing is no longer restricted to observation with 
airborne or satellite sensors. Today, we can monitor our 
planet’s health and status with social media data and socio -
economic indicators as well as all kinds of imagery, audio, 
and text, in addition to satellite imagery [ 29]. This direc -
tion raises several questions related to the importance of 
the different sources and their adequacy for specific tasks. 
For this research direction, we discuss some of these aspects 
at the crossroads between (sometimes) extremely different 
data sources.
THE TRADITIONAL APPROACH
The first step in creating a traditional remote sensing system 
is to identify a property of interest y (e.g., land use or snow 
depth). We typically restrict the properties to a set of loca -
tions l, often described as a grid of points within a polygon , and 
times t. We can formalize this as modeling a  conditional 
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
91 
JUNE 2021    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINEdistribution, (| ,), Py lt over property y for a given set of 
locations and times. Using the traditional approach, one 
would resort to a pixel- or object-based classifier learning 
input–output relationships from a labeled set of image pix -
els. The goal is to provide an accurate estimate of the uncer -
tainty over property y and to have this be generalized to new 
locations and times.
The traditional approach is limited in several ways: 
1) It requires a person able to label the training satellite im -
agery, either in the field or through manual image inter -
pretation. This can be expensive and limiting, especially 
when the annotator is asked label difficult, fine-grained 
label spaces. 
2) It is able to make distinctions only among phenomena 
that are easily visible from an overhead perspective. It 
cannot, for example, see inside buildings. 
3) It is tightly coupled to the geographic region, task, and 
source of data. This approach has led to a profusion of 
remote sensing papers that use minor variations of the 
same computational methods.
MULTIMODAL APPROACHES WITH SOCIAL MEDIA
Social media data can be used to address our fundamental 
task, the estimation of (| ,). Py lt We begin by considering 
how social media can be incorporated as an input, by using 
l and t to query for nearby media content, and then how it 
can be used to expand the types of properties, y, that can 
be estimated.SOCIAL MEDIA AS AN ALTERNATIVE-INPUT MODALITY
Traditional remote sensing largely ignores social media and 
uses only l and t to index the image; however, today count -
less photographic, audio, and textual data from cellphones 
are being collected. These data are often associated with 
the location and time of capture, making each a potentially 
useful source of information about the state of the world 
(see Figure 2 , where a system based on both remote sens -
ing and ground-level images uses both modalities in syn -
ergy to provide likelihoods about the presence of objects). 
People use these data to make decisions on a daily basis 
(e.g., when reading reviews and looking at photographs 
while trying to make travel plans). With rapid advances in 
the automatic interpretation of imagery, audio, and text, we 
can start thinking of these data as potential inputs for re -
mote sensing. Recent work [ 30]–[32] has begun to explore 
the use of social media imagery, especially for fine-grained 
land-use classification. A new methodological framework 
of information fusion with ML is actually emerging [ 33]. 
These methods tend to use black-box models to extract vec -
tor-valued features from ground images and combine them 
with remote sensing features. They also tend to ignore the 
rich geometric information the images contain.
SOCIAL MEDIA AS A SOURCE OF SUPERVISION
An untrained person can easily interpret a wide array of 
properties from a single social media object. The interpreta -
tion will be fine grained and include subjective properties 
FIGURE 1.  An overview of the ERA data set, a benchmark for event recognition from aerial videos [7]. For each class, the first (top left) and 
last (bottom right) frames of an example video are shown. 
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
92 
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE     JUNE 2021such as “dangerousness” or “scenicness,” which are gener -
ally not visible in overhead images. With CNNs approach -
ing, and sometimes exceeding, human-level performance 
for ground-level image interpretation, we can now consider 
using the output of CNNs as a semantic description of a 
given place and time. We can then use this description to 
train a remote sensing model, which might take only sat -
ellite imagery as input. In this way, we can extend the in -
formation received from social media to areas where these 
media are absent and, simultaneously, reduce the need to 
manually annotate satellite images. This approach has been 
applied for a variety of tasks, including for mapping scenes 
categories [ 34] and for time-varying visual attributes [ 35]. 
However, there remain significant issues to address:
 ◗The data to be included : Each source must add value, being 
correlated to the task and independent from each other. 
In an extremely multimodal setting, the volume and 
velocity of data acquired from each source cannot be 
directly controlled because they depend on completely 
independent acquisition systems. Given that, it becomes 
important to understand how the spatial coverage and 
quality of each source vary. For example, clouds have 
a significant impact on optical imagery but do not af -
fect synthetic aperture radar (SAR). Similarly, the cover -
age and quality of various social media sources depend 
on a variety of conditions, including their proximity to  tourist landmarks, population density, and differences 
in the culture of a given social network (interested read -
ers are referred to discussions about social media data 
quality in [ 36] and [ 37]). Therefore, we must choose 
among not only satellite sources, but also social media 
platforms, and must often apply further filtering, e.g., 
including only the data collected from cellphone cam -
eras [ 38] or only certain types of scenes [ 34]. 
 ◗The quality of the matching between sources : Each source 
must be georegistered to reduce spatial uncertainty dur -
ing training. The satellite/aerial-to-ground matching at 
the scene level is highly challenging due to the large se -
mantic gap between the ground and overhead scene, but 
it is still resolvable. For instance, Lin et al. [ 39] proposed 
a dual-adversarial solution for an unsupervised satellite/
aerial-to-ground scene-adaptation solution. However, 
it becomes very crucial when object-level matching is 
concerned, e.g., when approaching automatic geolocal -
ization [ 40], [41], and, in particular, when considering 
image synthesis [ 42]–[44], where strong geometric mod -
els of the various modalities, with the ability to model 
uncertainty, are strongly needed.
PERSPECTIVES
Considering social media data as extra sources for remote sens -
ing analysis is gaining momentum. Aside from  classification 
FusionQuer y Location
FIGURE 2.  An example of a system that provides the likelihood of the presence of an object with multiple modalities. The ground and 
satellite images provide hard and weak (picture-based) labels, respectively, to create a heatmap that shows the presence of objects in urban 
areas. The location information is used to perform the fusion. (Sources: Google Maps;  geoawesomeness.com; Wikipedia; Wikimedia.) 
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
93 
JUNE 2021    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINEand automatic geolocalization, using these additional data 
could unlock new applications, such as modeling sound -
scapes [ 45], landscape scenicness [ 46], or place-perception 
analysis [ 47]–[49]. Further, one could study phenomena 
closer to the consumer providing the data; for example, one 
could think of a system finding the most visually appealing 
driving route.
This will inevitably raise the question of data set biases 
because social media data are personalized views of space: 
photographers tend to take pictures from places that are 
easy to reach; they are biased in the types of subjects they 
prefer and usually take more pictures in the daytime and 
in good weather conditions (see, for instance, the overs -
ampling of particular photographic forms and scenes in 
the Instagram account insta_repeat (https://www.instagram  
.com/insta\_repeat/). This problem was recently considered 
when observations collected by citizen scientists were 
used for species distribution mapping [ 50]. In general, 
biases in learning models is a growing topic of study in 
both the ML (see, for example, [ 51] and [ 52]) and social 
media (see the reviews in [ 53] and [ 54]) communities. 
There is ample room for such studies in remote sens -
ing and biases issued by fusion in multimodal settings 
or in hallucinations when using generative adversar -
ial networks. 
Using social media also implies the development 
of models that are robust to differences in the appear -
ance of classes, which becomes critical when predict -
ing in new geographies or in time moments. As acquir -
ing new labeled data is not always an option, one could 
envision using these alternative sources as a form of 
weekly supervised training data or even as an unsu -
pervised supervisory signal for knowledge discovery, 
as Law and Neira have proposed [ 55] to explore the ur -
ban latent space of London’s streetscapes. Considering 
that the data acquired by autonomous vehicles and In -
ternet of Things devices will push this need even fur -
ther, it will also unlock the potential of mapping on  
demand with extremely multimodal remote sensing. 
Finally, further integration could make it possible to 
use social media as an early detection system for events, 
such as natural disasters [ 56]. For instance, social me -
dia imagery could be used to detect damaged structures 
or people in distress [ 57]. Such a system could even be 
used to cue satellite image acquisition over areas of in -
terest based on image content, location-data densities, 
or tweets.
DIRECTION 3: INTERACTIVE AND SEMANTIC ML
With their massive increase in availability, remote sensing 
images are now used beyond scientific research. First, im -
ages are available worldwide and with a high update rate. 
But they are also much more accepted by the general public: 
no one is surprised anymore when they are shown a sat -
ellite view from Google Maps; consumer-level drones can 
be used by virtually anyone for all kinds of tasks, such as farmers monitoring crops, ecologists surveying animals, or 
architects keeping track of construction sites.
But, despite the massive potential for image acquisition 
and updating, the use of images remains static in the sense 
that images are mostly used for visualization or, at best, to 
compute standard indices such as the normalized differ -
ence vegetation index, which is then assumed to represent 
vegetation status. Moreover, models answering the specific 
needs of users are scarce and more often limited to classical 
processing tasks (e.g., car detection or land cover mapping) 
and cannot cover the variety of tasks in which different us -
ers could be interested. Another limitation is that end users 
rarely have the technical skills necessary to design and run 
ML models and would like to receive an answer to a spe -
cific question of interest asked in a natural language (e.g., 
in English).
Fortunately, many of these questions boil down to the 
presence of objects, to counting, or to some kind of re -
lational attribute (e.g., whether there was an increase of 
forest area or whether there are buildings in risk zones): 
a model capable of pursuing some kind of reasoning 
about the image content (see direction 1 in Table 1 ) but 
taking into account a specific question (in English) by 
a user could open the door to a new type of interaction 
with remote sensing. Similar to what search engines do 
on the Internet, a remote sensing VQA (RSVQA) [ 12] en-
gine could allow anyone, from scientists to laymen and 
journalists, to retrieve relevant information contained in  
the images.
Research into VQA is a vivid topic in computer vision 
[11], where it has had considerable impact on creating sys -
tems that support vision-impaired people with everyday 
tasks [ 58]. A traditional VQA system, in this context, can in -
deed be used to help people when buying groceries, cross -
ing the street, and so on.
DIALOGS AMONG USERS AND EARTH OBSERVATION 
IMAGES REQUIRE BOTH REMOTE SENSING AND 
NATURAL LANGUAGE PROCESSING
In remote sensing, the first VQA system was proposed by 
Lobry et al. [ 12] and is summarized in Figure 3 . To become 
truly general purpose, such a model needs to be trained us -
ing a large quantity of data from several areas and different 
thematic objectives: in [ 12], two models were designed, one 
for Sentinel-2  data and another for subdecimeter-resolution 
aerial images. The models were trained with large sets of 
image/answer pairs spanning tasks of classification, relative 
position reasoning, and object counting. As a large quantity 
of labels was necessary, OpenStreetMap (OSM) vector data 
were used to automatically generate labels: following the 
Compositional Language and Elementary Visual Reason -
ing (CLEVR) protocol [ 59], 100 questions per image involv -
ing objects occurring in the image (as informed by OSM) 
were generated. For each image/question pair, the answer 
(i.e., the label) was automatically obtained by querying 
OSM directly. The data and models are openly available 
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
94 
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE     JUNE 2021at https://rsvqa.sylvainlobry.com/. Examples of RSVQA 
model predictions are reported in Figure 4  for both resolu -
tion images. Note that, for a single image, several questions 
are possible and the same model is used to answer them.PERSPECTIVES
This first work opens a wide range of possibilities for 
a new line of research toward the next level of human/  
image interactions. Nonetheless, all the blocks of the 
FIGURE 3.  An example of an RSVQA system [12]. (a) A remote sensing image and (b) a question in natural language enter two source-specific 
neural networks, each one outputting a vector representing their information, and (c) both vectors are combined and become (d) the input of 
a classifier that outputs all the possible answers as separate classes. RNN: recurrent neural network. 
“How many cars are
there?”CNN
RNN6
7
8Yes
No
Peri Urban
Cropland
City Center
Increasing4) Question Answering 3) Feature
Fusion 1) Image Representation
2) Textual Representation(a)
(b) (c) (d)
Q: “Is there a road?”
A: “Yes (GT: yes)”
Q: “Is there a farm? ”
A: “Yes (GT: yes)”
Q: “How many small residential
 buildings are there?”
A: “Between 10 and 100 (GT: 147)”
Q: “Is there a residential building?”
A: “Yes (GT: yes)”
A: “Yes (GT: yes)”
Q: “How many small residential
 buildings are there?”Q: “Are there fewer commercial
 buildings than water areas?”
A: “Between 10 and 100  (GT: 18)”Q: “Is there a baseball field?”
A: “Yes (GT: yes)”
A: “0 (GT: 0)”
Q: “Is there a building south of a road?”Q: “How many commercial buildings are
 close to a baseball field?”
A: “No (GT: Yes)”
Q: “Is there a commercial building?”
A: “No (GT: no)”
A: “Yes (GT: yes)”
Q: “How many small buildings are there?”Q: “Are there fewer commercial buildings
 than roads?”
A: “0 (GT: 0)”
Aerial VHR Sentinel-2  Images
(a) (b)
FIGURE 4.  The RSVQA system predictions for (a) Sentinel-2  and (b) aerial images, respectively, and different questions for each. The same 
model is used to answer all of the questions related to one-resolution imagery. GT: ground truth; VHR: very high resolution. (Sources: (a) 
Copernicus and (b) USGS.)
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
95 
JUNE 2021    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINEmodel can (and must) be improved: for example, au -
tomatic data generation has its flaws, especially due to 
the very simplistic language model used, for which new 
models from natural language processing could help im -
prove the performance greatly. Also, fewer classical tasks 
(i.e., not reducible to classification, regression, or detec -
tion) should be imagined; for instance, when allowing for 
more complex output spaces, the lessons learned from 
image captioning in remote sensing [ 60] show that it is 
possible to move toward models that generate descrip -
tions of the image content, which could be used in, e.g., 
image retrieval [ 61].
DIRECTION 4: PHYSICS-AWARE ML
As seen from the eyes of a practitioner, a major drawback 
of deep learning models is that they can lead to implau -
sible results with scores that indicate high confidence 
in the outputs if no high-level constraints are imposed 
that check for consistency with theory. One possibility to 
compensate for this shortcoming is integrating domain 
knowledge into the modeling procedure. Particularly in 
the environmental and geosciences, the laws of physics, 
chemistry, or biology govern the underlying processes, 
and much theory exists. 
An interesting direction of research is thus how best 
to tightly couple ML, and especially deep learning, with 
physical laws. The hope is that this introduction of domain 
knowledge can help reduce the manual labeling effort  
for supervised learning, counter data set biases, lessen the 
influence of label noise, lead to good generalization ca -
pabilities, and, eventually, result in plausible outputs that 
adhere to the underlying physical principles. Machine 
learning needs to incorporate domain physical knowledge 
to become consistent, explainable (see direction 5 in Table 1), 
and causal (see direction 6 in Table 1), while still learning 
from observational data, which makes them amenable to 
backpropagation. In addition, physics-consistent ML ap -
proaches for remote sensing emphasize modeling natural 
phenomena with higher accuracy, which is not necessar -
ily the case for the two other research directions. In the 
following sections, we present several ideas clustered into 
three lines of thought: constrained optimization, physics 
layers in deep neural networks, and encoding and learn -
ing differential equations. A recent overview of the main 
families and approaches to the general field of the inter -
action between physics and ML for Earth observation is 
available in [ 62].
CONSTRAINED OPTIMIZATION
A first consideration when designing physics-consistent ML 
approaches is to impose constraints on the loss function 
[13], [63]. Loss functions that encode the physical princi -
ples of a particular problem while using otherwise mostly 
unchanged model architectures can ensure that the learned 
model respects the laws of physics; see Figure 5  for an ex -
ample for including a dependence-based regularizer [ 52]. In addition, this strategy can significantly reduce the num -
ber of necessary labels required for training, down to prac -
tically zero in some cases [ 14]. 
Designing custom-tailored loss functions and possibly 
combining them with models that are trained on simulat -
ed data represent another promising direction of research. 
However, this approach calls for very specific designs of 
loss functions that are not always straightforward and may 
simply not exist for many problems in remote sensing. For 
example, it seems very difficult to design a corresponding 
loss function for the semantic segmentation of cars in aerial 
images or the detection of building facades in street-level 
panoramas because the large, intraclass variability of the ap -
pearances would require a very large set of constraints. 
PHYSICS LAYERS IN DEEP NEURAL NETWORKS
An interesting idea, that of making use of well-established 
deep neural networks but still learning and constraining 
the underlying physics, is adding additional layers that 
encode physics [ 1], [64] (see Figure 6 ). The general back -
ground knowledge gained from physics can be encoded 
in the deeper network layers. Together with a custom-tailored 
loss  function, this approach enables the end-to-end 
RMSE11.051.11.15NHSIC
0.264 0.266 0.268 0.27Morel1
CalCOFI
OC2
OC4
FIGURE 5.  A standard family of hybrid modeling can be framed as 
a constrained optimization problem, where the physical rules are 
included as a particular form of regularizer [69]. The fair kernel learn -
ing [52] method forces model predictions to be not only accurate 
but also statistically dependent on a physical model, simulations, or 
ancillary observations. In this example, we forced the dependence 
of a data-driven model with respect to four standard ocean-color 
parametric models (Morel1, CalCOFI two-band linear, OC2, and OC4) 
and trained our constrained model to estimate the levels of ocean 
chlorophyll content from input radiances. We did so with increased 
dependency (as estimated by the NHSIC metric) between the ML 
and physical models. The results show that including the depen -
dence regularizer (i.e., for higher NHSIC values) helps reduce the 
root-mean-square error (RMSE) and that the OC2 and OC4 physical 
models, in particular, improve the error and consistency of the 
data-driven model. Morel1: Morel’s version 1 algorithm; OC2: ocean 
chlorophyll 2 version; OC4: ocean chlorophyll 4 version; NHSIC: 
normalized Hilbert–Schmidt Independence Criterion.
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
96 
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE     JUNE 2021 training of common deep networks that comply with 
physical constraints.
Although the idea of adding physical layers on top of 
common deep network architectures seems intuitive, im -
plementing it for a wide range of remote sensing tasks is far 
from trivial. One scheme could be to start with simplified 
versions that do not encode physics directly but rather some 
related, simplified constraints, for instance, for imposing 
maximum values for vegetation height mapping [ 65], tree 
stress [ 66], or flood water depth [ 57].
ENCODING AND LEARNING DIFFERENTIAL EQUATIONS
Probably the biggest advances toward deep neural networks 
that incorporate physics are so-called physics-informed 
neural networks, which directly encode nonlinear ordi -
nary differential equations (ODEs) and partial differential 
equations (PDEs) in deep learning architectures while al -
lowing for end-to-end training [ 15], [67]. Instead of using 
standard network layers, the authors proposed a framework 
to directly encode nonlinear differential equations in the 
network, which is fully end-to-end trainable. This idea al -
lows for learning yet-unknown correlations and coming up 
with novel research hypotheses in a data-driven way, a cen -
tral point also raised in the previous research directions on  
interpretability. Probabilistic models like Gaussian process -
es also allow for the encoding of ODEs as a form of con -
volutional process [ 68] and report additional advantages: 
aside from the uncertainty of quantification and propaga -
tion, they also learn the explicit form of the driving force 
and the ODE parameters, offering a solid basis for model 
understanding and interpretability (see the “Interpretable 
and Explainable ML” section).
PERSPECTIVES
When translated to remote sensing, physics-informed ML 
models enable the encoding and learning of  radiative-transfer equations and further physical laws, such as the backscat -
tering of SAR signals. Although starting directly with a full 
set of forward-modeling equations, e.g., for simulation 
engines, seems very hard, one could start with simplified 
versions and a subset of the most important components. 
Another idea would be the encoding of a simplified ver -
sion of the spectral-property changes of vegetation as a 
function of seasonality. Similar to the warping model 
proposed in [ 64], one could encode a change of spectral 
canopy properties in the infrared domain to ease domain 
transfer between the summer and winter scenes. In the 
broader context of geosciences and climate sciences, learn -
ing ODEs/PDEs from observational data and simulations 
is the direct way to explain the problem and variable rela -
tionships mechanistically while still resorting to empiri -
cal data. The main challenges are the needed simplicity of 
ODEs/PDEs so that scientists can understand (so sparsity 
gets involved here) and validating the plausibility of such 
equations (therefore, domain experts and computer sci -
entists should work together). This strongly links physics-
based deep learning to the explainable ML discussed in 
the next section.
DIRECTION 5: INTERPRETABLE AND  
EXPLAINABLE ML
Using ML for scientific applications aims at acquiring new 
scientific knowledge from observational data. In addition 
to the accuracy of results, their scientific consistency, reli -
ability, and explainability are of central importance. A pre -
requisite for achieving these is to design models that can be 
challenged, in other words, to create models whose inner 
functioning can be visualized, queried, or interpreted. In 
this section, we discuss the foundations of explainable AI 
(Figure 7 ) and its exciting perspectives and make links with 
the physics-aware/informed ML discussed in the previous 
section on physics-based ML. 
FROM TRANSPARENCY TO EXPLAINABILITY
Explainable ML has various definitions (see [ 17]), but they 
all revolve around the properties of transparency, interpret -
ability, and explainability.
1) A transparent model allows us to access its components 
and provides the motivation for choosing certain model 
components. This is in contrast to black-box models as 
traditional neural networks, for which one could indeed 
write the mathematical relationships explicitly (they are 
transparent in this sense), but their complexity makes 
them inaccessible to users.
2) An interpretable model counteracts the lack of transparen -
cy by presenting complex facts, like the processes in a neu -
ral network, in a space that can be understood by humans.
3) Sorting by increasing interpretability power, such a 
space can be made of localized image coordinates [ 71], 
semantic concepts [ 72], or understandable text [ 73].
4) To achieve explainability, domain knowledge is  exploited 
and used in combination with the interpretable model 
Model Super vision
Warping
Schemewt"
It+1"
It+1
It–k–1:t
FIGURE 6.  Another approach to hybrid ML modeling is that of 
including layers with physics motivation, which are learned from 
data end to end, into a deep neural network. The architecture 
learns a motion field with a convolution–deconvolution network, 
and the motion field is further processed using a warping physical 
model. The error is used to adjust the network weights, and, after 
training, the model can produce multiple time-step predictions 
recursively. (Adapted from [70].) 
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
97 
JUNE 2021    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINEand its components to understand, for example, why 
the model came to a certain decision. Therefore, expla -
nations become application dependent, and identical 
interpretations can lead to different explanations when 
linked to different domain knowledge.
HOW EXPLAINABLE AI MAY HELP REMOTE SENSING
Recently, many tools have been proposed for increasing 
interpretability and explainability when combined with 
domain knowledge [ 18]. Two major groups have emerged.
1) Posthoc interpretability : In this group, the outcomes and 
decisions of the model are interpreted and explained 
by looking at the input. The most common visualiza -
tions for interpretations are heatmaps and prototypes. 
Heatmaps highlight the parts of the input data that are 
prominent, important, or occlusion sensitive; for ex -
ample, they are created using the gradient flows in the 
neural network. Prototypes are optimized input data 
that, given a model, maximize the targeted output. Both 
of these approaches help to explain what a model bases 
its decisions on, what influences the output, or what is a 
typical input for the learned input–output relationships. 
In all cases, attention must be paid to the confirmation 
bias, which is defined as the tendency to try to explain interpretations that are consistent with our existing 
knowledge even if the explanation does not apply to the 
given case (see [ 74] for an example of the overinterpreta -
tion of saliency maps).
2) Interpretability by design : In this case, the model is inher -
ently designed so that it can be interpreted. Interpret -
ability is achieved by representing model components 
or obtained latent variables such that they can be ex -
plained with knowledge from a certain application 
domain. For instance, the units in hidden layers can 
be designed in such a way that the underlying factors 
of variation, such as the driving forces in Earth system 
data, become disentangled and are captured in separate 
units. This could be seen, for example, by the fact that 
simple correlations exist between variations of the input 
and the activation of the neurons. Interesting applica -
tions of this idea are proposed in [ 75], where the authors 
disentangle the physical forces applied between objects 
in videos, or in [ 76], for explaining human perceptions 
of beauty in landscapes. 
To ensure the scientific value of the output, interpreta -
tion tools can be used to check its reliability. Besides the 
inherently existing output score of the neural network, 
for example, visualizations of the processes within the 
Explainability
Interpretability
TransparencyScientific
Discoveries
and Insights
Model OutputML Model
Earth DataDomain Knowledge
(Reference Labels, Exper t
Knowledge, and  so on)
FIGURE 7.  Explainable ML can be used to gain scientific discoveries and insights by explaining a learned model and/or results (shown in 
the light gray box). The prerequisites are interpretability and potential transparency, which lead to scientific explanations when combined 
with domain knowledge. A feedback loop allows for extending and improving the known domain knowledge. One potential application 
is the derivation of improved definitions, for example, for certain land-use classes, which are currently only vaguely, incompletely, or not 
uniformly described. 
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
98 
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE     JUNE 2021neural network can be used to check whether correct deci -
sions have been made for the wrong reasons (the so-called 
Clever Hans effect [ 78]). This can be seen as an additional 
test for the reliability of the output due to the fact that a 
high score for the network does not always mean a correct 
result. In summary, these tools can increase confidence 
by improving traceability, as estimates are generated and 
reveal biases in the data through human-understand -
able visualization.
PERSPECTIVES
Explainable ML has thus far received comparatively little 
attention in remote sensing, partly because of the still-
predominant opinion that explainability is tightly coupled 
with the complexity of a model and, therefore, an increase 
in explainability leads directly to a decrease in accuracy 
(e.g., in [ 79]). In the meantime, however, several applica -
tions have shown that this is no longer the case.
Most of the approaches considered thus far are post hoc 
interpretations, but initial approaches that consider inter -
pretability by design are appearing. In [ 76], for instance, the 
model is forced to predict human-interpretable concepts 
before predicting the final task ( Figure 8 ). Such approaches 
have the potential to provide both reliability checks and human-understandable explanations and could be used to 
move toward physics-explicit models, similar to those dis -
cussed in the next section.
As a further step, explanations going beyond today’s 
knowledge could be sought. New insights could be gained 
by using the explainable ML model and the outcomes used 
to formulate hypotheses for explanations that are not yet 
known to us. These hypotheses could then be tested us -
ing simulation software, for example, and confirmed or 
rejected. To show the potential, previous neural network 
approaches discover presumed scientific laws from obser -
vations without providing the complete underlying prior 
knowledge to the learning method (e.g., in [ 80]). As illus -
trated in Figure 7 , the next promising step would be to re -
veal new hypotheses from remote sensing data. This can, 
for instance, be accomplished by searching for patterns in 
interpretations, which can be assigned to novel discoveries 
and insights when combined with domain knowledge. This 
approach may point us to previously undiscovered spatio -
temporal input–output relationships or data biases; for ex -
ample, interpretation tools and their resulting insights on 
how certain land-use classes are recognized could be used 
to derive improved class definitions and help with a tar -
geted acquisition of the training data.  
2.29 –2.14 1.22
–0.84 0.86 –0.62Rugged Scene/
Natural/ClimbingMan-Made/
Natural Light/MetalDiving/Swimming/
Climbing
Transpor ting Things
or People/Asphalt/MatteIce/Snow/Climbing Wire/Constructing
Building/Rusty0.2
0.15
0.1
0.05
0Urban
Vegetation
Rocky
Water
(a) (b)
FIGURE 8.  Explaining the factors behind landscape beauty in the United Kingdom [77] (a) The maps of landscape attributes contributing 
to the estimation of beauty in a series of landscape images; these maps are learned from ground-based photos and are used to predict the 
landscape attributes that are observed in single images.  Such attributes are then combined end to end in a neural network that predicts  
the beauty scores. The number above the single-factor maps correspond to the contribution of the factor to landscape beauty, on average.  
(b) The land cover map of the United Kingdom, which is used for visual comparison of the single-factor maps learned automatically.
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
99 
JUNE 2021    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINELEARNING CAUSE-AND-EFFECT  
RELATIONSHIPS FROM DATA
Earth is a highly complex, dynamic, and networked system, 
where very different physical, chemical, and biological  
processes interact in several spheres and at diverse spatio -
temporal scales. Despite the great predictive capabilities 
of current machine and deep learning methods, there 
is still little actual learning—understanding is harder 
than predicting. ML algorithms excel at fitting arbitrary,  
functional data relationships but do not have a clear 
notion of the underlying causal relationships. ML is far 
from problem understanding and even moreso from ma -
chine intelligence (see the critical perspectives in blogs 
by Gary Marcus and Michael Jordan and the articles in 
[1] and [ 22]). 
Earth system data analysis aims to extract information 
from multivariate, nongridded data sets, where missing 
data, nonlinearities, and nonstationarities are present in 
the wild. Variables and physical processes are coupled in 
space and time, and (tele)connections can be large range, 
discontinuous, and variant in strength and intensity.  Ad-
dressing this problem will allow us to identify the right set 
of predictors, develop robust models, and avoid getting the 
right answer for the wrong reasons. The links with physics 
(direction 4 in Table 1 ) and interpretability (direction 5 in 
Table 1 ) are very strong. CAUSALITY AS THE WAY FORWARD
Causal inference aims at discovering and explaining the 
causal structure of a system [ 20], [81], [82]. Very often, in -
terventions in the system are not possible because of ethi -
cal, practical, or economic reasons. Then, observational 
causal inference comes into play to extract cause-and-effect 
relationships from multivariate data sets, going beyond the 
commonly adopted correlation approach, which merely 
captures associations between variables.
Today, the science of causal inference [ 19], [83] is advanc -
ing fast and, under reasonable assumptions, can unravel 
causal relationships among two or more coupled variables 
even in the presence of nonlinearities and nonstationarities 
and even when time is not involved. Several rigorous algo -
rithms have been developed in the last decade that enable 
us to make inferences across multiple variables to discover 
plausible causal relationships from observations. Causal 
inference is, of course, very relevant for scientific endeav -
ors, but it also has impactful practical implications. For ex -
ample, learning causal structures enables us to build more 
parsimonious and robust models: that means faster, more 
fault-tolerant, and interpretable models.
A TAXONOMY OF CAUSAL-DISCOVERY METHODS
Causal-discovery methods can be divided into four main 
families. First, Granger causality (GC) [ 84] is the most 
FIGURE 9.  Examples of causal-inference approaches in remote sensing and the geosciences. (a) Nonlinear GC, showing a time series of 
ENSO4 (ENSO in region 4) that captures sea-surface temperature anomalies in the central equatorial Pacific, SM, and vegetation optical 
depth (VOD) to explore their causal relationships extracted using the nonlinear principal component analysis  in [91]. (b) The five-day lagged 
correlation (top) and causal (bottom) maps of ENSO4 and SM interannual components using the kernel GC method in [86]. Results show 
that many of the correlations, even the highest ones (~ .),08t  are not causal, thus suggesting mere spurious associations. ( continued  )
ENSO4
SMreal
SMimaginar y
VODreal
VODimaginar y
2011 2012 2013 2014 2015 2016 201700.020.040.060.08–0.8–0.40.40.8
0
Corr (ENSO4, SM)
ENSO4 --> SM
(a) (b)
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
100 
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE     JUNE 2021FIGURE 9.  (continued  ) (c) Convergent cross mapping. An example of the unbiased CCM application used in [88] to derive causal rela -
tionships among the variables accounting for photosynthesis [the gross primary productivity (GPP) from FLUXCOM initiative (www  
.fluxcom.org)],  temperature (T air from ERA Interim), and SM (from the European Space Agency’s climate change initiative  [(v 2.0)]. Data 
cubes at 0.5° and eight-day spatial and temporal resolutions, respectively, spanning 2001–2002 were used. Reasonable spatial-causal 
patterns are observed for SM and T air on GPP; GPP drives T air mostly in cold ecosystems (probably due to changes in land surface albe -
do, such as snow/ice to vegetation changes); SM is mostly controlled by T air, which partially drives evaporation in water-limited  regions; 
and GPP dominates SM. (d) The additive noise model. Structural equation models in the form of an additive noise model using the 
kernels in [21] for hypothesis testing. Assessing cause-and-effect relationships is also possible when time is not involved. Here we rely 
on a look-up table generated by radiative transfer model (RTM),  which gives the right direction of causation: state vectors (parameters) 
cause radiances. The algorithms accurately detect this from pairs of data and can be used for retrieval model-data intercomparison and 
RTM assessment. 
1
0.8
0.6
0.4
0.2
0True Positive  Rate
0 0.2 0.4 0.6 0.8 1
False-Positive-Rate
(d)C (0.7667)"
Cs(0.7953)"
Photosynthesis
Strength of Forcing Over SM
50
–50
–150 –100 –50 05 0100 150Latitude (°)0GPP
Temperature
Longitude (°)Strength of Forcing Over Air Temperature
50
–50
–150 –100–50 05 0100 150Latitude (°)0GPP
SM
Longitude (°)
Strength of Forcing Over GPP
50
–50
–150 –100 –50 05 0100 150Latitude (°)0Temperature
SM
Longitude (°)Air Temperature SM
(c)
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
101 
JUNE 2021    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINEwidely used approach in Earth and climate sciences to 
quantitatively identify relationships between time series. It 
tests whether including past states of a variable, X, improves 
the prediction of an output variable, Y, more than consider -
ing other covariates. GC is a linear test, but nonlinear (ker -
nel) versions have also been proposed [ 85]. In [ 86], a gen -
eralized kernel GC is presented, able to discover footprints 
of El Niño-Southern Oscillation (ENSO) on soil moisture 
(SM) and vegetation optical depth records [see Figure 9 (a)]. 
However, GC approaches have problems in nonstationary, 
nonlinear, and deterministic relationships, especially in 
dynamic systems with weak-to-moderate coupling. 
The second family considers nonlinear state-space meth -
ods, such as convergent cross mapping (CCM) [ 87]. CCM 
attempts to address GC problems by reconstructing the 
variable’s state spaces, ( Mx, My), using time embeddings 
and concludes, based on XY", whether points on Mx can 
be predicted more accurately using nearest neighbors in My 
as more points are used for prediction. However, CCM is 
very sensitive to noise and time-series length. Recent works 
have included bootstrap resampling to alleviate such prob -
lems and shown good results in identifying causal links 
in long global records of carbon and water fluxes [ 88] [see  
Figure 9 (b)]. 
The third family, collectively known as causal network  
learning algorithms , relies heavily on conditional-inde -
pendence tests. Its methods iteratively remove the links 
between pairs of variables, ( X, Y), if they are found to be 
independently conditioned on any subset of the other vari -
ables.  The PC  algorithm (named after its inventors Peter 
and Clark) allows us to identify parents and can be flexibly 
implemented with different kinds of conditional-indepen -
dence tests, which can handle nonlinear dependencies and 
variables that are discrete or continuous and are univariate 
or multivariate. Finally, structural causal models (SCMs) 
are used when time is not involved or the sampling frequen -
cy is too low. SCMs search for the causal direction within 
Markov-equivalent classes by exploiting the asymmetries 
between cause and effect. Additive noise models rely on the 
principle of independence between the cause and the gen -
erating mechanism and have recently shown good results 
in remote sensing and geosciences in cases where time is 
not involved and only two variables are observed [ 21].
PERSPECTIVES
Although the field of machine and deep learning has tra -
ditionally progressed very rapidly, we observe that this is 
not the case in tackling the challenge of learning causal 
relationships from Earth observation data. The role that 
deep learning will play on causal discovery is, at best, uncer -
tain because deep learning models focus mostly on fitting 
and are largely overparameterized, which is (apparently) 
against causal, sparse, reasoning. Only very recently have 
we witnessed efforts toward either incorporating or under -
standing deep models causally: the authors in [ 89] imple -
mented a metalearning objective that maximizes the speed of  domain transfer, which, under certain assumptions, can 
be seen as a way to localize changes in causal mechanisms.  
In [90], the authors learned individual-level causal effects 
from observational data that can efficiently handle con -
founding (hidden) factors. Both methods are, in principle, 
well suited to the problems in remote sensing and geosci -
ence data sets, which exhibit spatiotemporal relationships 
to be exploited but have not (thus far) been considered.
Yet we will have to face a more important challenge: 
cognitive barriers. Domain knowledge is elusive and dif -
ficult to encode, interaction between computer scientists 
and physicists is still a barrier, and education in synergistic 
concepts still needs to become a reality in coming years. 
Causal inference is believed to be the best approach to de -
velop Earth sciences, but this will be possible with a strong 
and continuous interaction between domain knowledge 
experts and computer scientists.
CONCLUSIONS
This article described the six ideas and six directions 
in which geosciences, Earth observation, and AI can 
achieve a lot if synergistically combined. With this ar -
ticle, we have provided our appreciation for research av -
enues that are new, refreshing, and exciting for scientists 
willing to evolve at the interface between AI and the geo -
sciences. We hope that they will spark curiosity and that 
the community, especially the younger generations, will 
embrace them.
ACKNOWLEDGMENTS
Xiao Xiang Zhu is jointly supported by the European Re -
search Council (ERC) under grant ERC-2016-StG-714087, 
by the Helmholtz Association through the Framework of 
Helmholtz Artificial Intelligence Cooperation Unit and 
Helmholtz Excellent Professorship Data Science in Earth 
Observation—Big Data Fusion for Urban Research, and by 
the German Federal Ministry of Education and Research 
in the framework of the international future AI lab AI4EO. 
Gustau Camps-Valls was partly funded by the ERC un -
der the ERC-SyG-2019 USMILE project (grant agreement 
855187). Nathan Jacobs was partly funded by a National 
Science Foundation CAREER Award (IIS-1553116). Devis 
Tuia is the corresponding author. 
Some of the ideas presented in this article originated 
from discussions during the first workshop of the ELLIS 
Program ML for Earth and Climate Science (Germany) a 
few days before the COVID-19 lockdown in Europe.
AUTHOR INFORMATION
Devis Tuia  (devis.tuia@epfl.ch) is with Ecole polytechnique 
fédérale de Lausanne, Sion, 1950, Switzerland. He is the 
corresponding author for this article. He is a Senior Mem -
ber of IEEE.  
Ribana Roscher  (ribana.roscher@uni-bonn.de) is with 
the University of Bonn, Bonn, 53115, Germany. She is a 
Member of IEEE.
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
102 
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE     JUNE 2021Jan Dirk Wegner  (jan.wegner@geod.baug.ethz.ch) is 
with ETH Zürich, Zürich, 8093, Switzerland. 
Nathan Jacobs  (nathan.jacobs@uky.edu) is with the 
University of Kentucky, Lexington, Kentucky, 40506-0633, 
USA. He is a Senior Member of IEEE.  
Xiao Xiang Zhu  (xiaoxiang.zhu@dlr.de) is with the Techni -
cal University of Munich, Munich, 80333, Germany, and the 
German Aerospace Center, Wessling, Bavaria, 82234, Ger -
many. She is a Senior Member of IEEE. He is a Fellow of IEEE.
Gustau Camps-Valls  (gustau.camps@uv.es) is with the 
Universitat de València, Paterna, València, 46980, Spain. 
He is a Fellow of IEEE. 
REFERENCES
[1] M. Reichstein , G. Camps-Valls , B. Stevens , M. Jung, and J. Den-
zler, “Deep learning and process understanding for data-driven 
earth system science ,” Nature , vol. 566, no. 7743 , pp. 195–204, 
2019 . doi: 10.1038/s41586-019-0912-1 .
[2] G. Camps-Valls , D. Tuia, X. X. Zhu, and M. Reichstein , Deep Learn -
ing for Earth Sciences: A Comprehensive Approach to Remote Sensing, 
Climate Science and Geosciences . Oxford , U.K.: Wiley , 2021 . 
[3] X. Zhu et al., “Deep learning in remote sensing: A comprehensive 
review and list of resources ,” IEEE Geosci. Remote Sens. Mag. , vol. 
5, no. 4, pp. 8–36, 2017. doi: 10.1109/MGRS.2017.2762307 .
[4] N. Audebert , B. Le Saux , and S. Lefevre , “Deep learning for clas -
sification of hyperspectral data: A comparative review ,” IEEE 
Geosci. Remote Sens. Mag. , vol. 7, no. 2, pp. 159–173, 2019 . doi: 
10.1109/MGRS.2019.2912563 .
[5] Q. Yuan  et al., “Deep learning in environmental remote sensing: 
Achievements and challenges ,” Remote Sens. Environ. , vol. 241, p. 
111,716,  May 2020 . doi: 10.1016/j.rse.2020.111716 .
[6] L. Mou, Y. Hua, and X. X. Zhu, “A relation-augmented fully con -
volutional network for semantic segmentation in aerial scenes ,” 
in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , 2019 , pp. 
12,416 –12,425 . doi: 10.1109/CVPR.2019.01270 .
[7] L. Mou, Y. Hua, P. Jin, and X. X. Zhu, “ERA: A dataset and deep 
learning benchmark for event recognition in aerial videos ,” IEEE 
Geosci. Remote Sens. Mag. , vol. 8, no. 4, pp. 125–133, Dec. 2020, 
doi: 10.1109/MGRS.2020.3005751. 
[8] Y. Aytar , C. Vondrick , and A. Torralba , “See, hear, and read: Deep 
aligned representations ,” 2017, arXiv:1706.00932.
[9] J.-M. Perez-Rua , V. Vielzeuf , S. Pateux , M. Baccouche , and F. Jurie, 
“MFAS: Multimodal fusion architecture search ,” in Proc. IEEE/CVF 
Conf. Comput. Vis. Pattern Recognit. (CVPR),  June 2019 , pp. 6966 –6975.
[10] A. R. Zamir  et al., “Robust learning through cross-task consisten -
cy,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2020 , 
pp. 11,197 –11,206 .
[11] A. Agrawal  et al., “VQA: Visual question answering ,” in Proc. Int. 
Conf. Comput. Vis. (ICCV) , 2015 , pp. 2425 –2433 .
[12] S. Lobry , D. Marcos , J. Murray , and D. Tuia, “RSVQA: Visual 
question answering for remote sensing data ,” IEEE Trans. Geosci. 
Remote Sens. , vol. 58, no. 12, pp. 8555–8566, Dec. 2020, doi: 
10.1109/TGRS.2020.2988782. 
[13] A. Karpatne , W. Watkins , J. Read , and V. Kumar , “Physics-guid -
ed neural networks (pgnn): An application in lake temperature 
modeling ,” 2017, arXiv:1710.11431.[14] R. Stewart  and S. Ermon , “Label-free supervision of neural net -
works with physics and domain knowledge ,” in Proc. 35th AAAI 
Conf. Artif. Intell., AAAI Press , 2017, pp. 2576 –2582 .
[15] M. Raissi , P. Perdikaris , and G. Karniadakis , “Physics-informed 
neural networks: A deep learning framework for solving forward 
and inverse problems involving nonlinear partial differential 
equations ,” Comput. Phys. , vol. 378, pp. 686–707, Feb. 2019 . doi: 
10.1016/j.jcp.2018.10.045 .
[16] G. Camps-Valls  et al., “Physics-aware Gaussian processes in re -
mote sensing ,” Appl. Soft Comput. , vol. 68, pp. 69–82, July 2018 . 
doi: 10.1016/j.asoc.2018.03.021 .
[17] R. Roscher , B. Bohn , M. F. Duarte , and J. Garcke , “Explainable ma -
chine learning for scientific insights and discoveries ,” IEEE Access , vol. 
8, pp. 42,200 –42,216 , Feb. 2020 . doi: 10.1109/ACCESS.2020.2976199 .
[18] W. Samek , G. Montavon , S. Lapuschkin , C. J. Anders , and K.-
R. Múller , “Toward interpretable machine learning: Transparent 
deep neural networks and beyond ,” 2020 , arXiv:2003.07631.
[19] J. Pearl  and D. Mackenzie , The Book of Why . New York : Basic 
Books , 2018 .
[20] J. Peters , D. Janzing , and, and B. Schölkopf , Elements of Causal In -
ference - Foundations and Learning Algorithms, ser. Adaptive Computa -
tion and Machine Learning Series . Cambridge, MA : MIT Press , 2017.
[21] A. Pérez-Suay  and G. Camps-Valls , “Causal inference in geosci -
ence and remote sensing from observational data ,” IEEE Trans. 
Geosci. Remote Sens. , vol. 57, no. 3, pp. 1502 –1513 , 2019 . doi: 
10.1109/TGRS.2018.2867002 .
[22] J. Runge  et al., “Inferring causation from time series with perspec -
tives in Earth system sciences ,” Nature Commun. , vol. 10, no. 1, 
p. 2533, 2019. doi: 10.1038/s41467-019-10105-3 . 
[23] M. T. Ribeiro , S. Singh , and C. Guestrin , “‘Why should I trust 
you?’: Explaining the predictions of any classifier ,” in Proc. 22nd 
ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining , 2016 , 
pp. 1135 –1144 . doi: 10.1145/2939672.2939778 .
[24] B. M. Lake , T. D. Ullman , J. B. Tenenbaum , and S. J. Gershman , 
“Building machines that learn and think like people ,” Nov. 2016 , 
arXiv: 1604.00289. 
[25] A. Santoro  et al., “A simple neural network module for relational rea -
soning ,” in Proc. Adv. Neural Inf. Process. Syst. , 2017, pp. 4967–4976 . 
[26] B. Zhou , A. Andonian , A. Oliva , and A. Torralba , “Temporal rela -
tional reasoning in videos ,” in Proc. Eur. Conf. Comput. Vis. (ECCV) , 
2018 , pp. 803–818.
[27] X. Zhang , X. Wang , X. Tang, H. Zhou , and C. Li, “Description genera -
tion for remote sensing images using attribute attention mechanism ,” 
Remote Sens. , vol. 11, no. 6, p. 612, Mar 2019 . doi: 10.3390/rs11060612 .
[28] D. Hu et al., “Cross-task transfer for multimodal aerial scene rec -
ognition ,” 2020 , arXiv:2005.08449.
[29] S. Lefèvre , D. Tuia, J. D. Wegner , T. Produit , and A. S. Nassar , 
“Towards seamless multi-view scene analysis from satellite to 
street-level ,” Proc. IEEE , vol. 105, no. 10, pp. 1884 –1899 , 2017. 
doi: 10.1109/JPROC.2017.2684300 .
[30] S. Workman , M. Zhai, D. Crandall , and N. Jacobs , “A unified 
model for near and remote sensing ,” in Proc. IEEE Int. Conf. Com -
put. Vis. (ICCV) , 2017, pp. 1–10.
[31] J. Kang , M. Körner , Y. Wang , H. Taubenböck , and X. X. Zhu, 
“Building instance classification using street view images ,” 
ISPRS J. Photogram. Remote Sens. , vol. 145, pp. 44–59, Nov. 2018 . 
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
103 
JUNE 2021    IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE [Online]. Available: http://www.sciencedirect.com/science/arti -
cle/pii/S0924271618300352 . doi: 10.1016/j.isprsjprs.2018.02.006 .
[32] S. Srivastava , J. E. Vargas-Muñoz , and D. Tuia, “Understanding 
urban landuse from the above and ground perspectives: A deep 
learning, multimodal solution ,” Remote Sens. Environ. , vol. 228, 
pp. 129–143, July 2019 . doi: 10.1016/j.rse.2019.04.014 .
[33] S. Salcedo-Sanz  et al., “Machine learning information fusion in 
earth observation: A comprehensive review of methods, applica -
tions and data sources ,” Inf. Fusion , vol. 63, pp. 256–272, Nov. 
2020 . doi: 10.1016/j.inffus.2020.07.004 .
[34] S. Workman , R. Souvenir , and N. Jacobs , “Wide-area image 
geolocalization with aerial reference imagery ,” in Proc. IEEE Int. 
Conf. Comput. Vis. (ICCV) , 2015 , pp. 3961 –3969 .
[35] T. Salem , S. Workman , and N. Jacobs , “Learning a dynamic map 
of visual appearance ,” in Proc. IEEE Conf. Comput. Vis. Pattern 
Recognit. (CVPR) , 2020 , pp. 12,435 –12,444 .
[36] R. Kitchin , “Big data and human geography: Opportunities, 
challenges and risks ,” Dialogues Human Geogr. , vol. 3, no. 3, 
pp. 262–267, 2013 . doi: 10.1177/2043820613513388 .
[37] J. E. Vargas , S. Srivastava , D. Tuia, and A. X. Falcão , “OpenStreet -
Map: Challenges and opportunities in machine learning and 
remote sensing ,” IEEE Geosci. Remote Sens. Mag. , vol. 9, no. 1, 
pp. 184–199, 2021. 
[38] M. Zhai, T. Salem , C. Greenwell , S. Workman , R. Pless , and N. Ja-
cobs, “Learning geo-temporal image features ,” in Proc. Brit. Mach. 
Vis. Conf. (BMVC) , 2018 , pp. 1–12. 
[39] J. Lin, L. Mou, T. Yu, X. Zhu, and Z. J. Wang , “Dual adversarial 
network for unsupervised ground/satellite-to-aerial scene ad -
aptation ,” in Proc. 28th ACM Int. Conf. Multimedia , ACM, 2020 , 
pp. 10–18. doi: 10.1145/3394171.3413893 .
[40] Y. Shi, X. Yu, L. Liu, T. Zhang , and H. Li, “Optimal feature 
transport for cross-view image geo-localization ,” in Proc. Assoc. 
Adv. Artif. Intell. , 2020 , pp. 11,990 –11,997 . doi: 10.1609/aaai.
v34i07.6875 .
[41] Y. Shi, X. Yu, D. Campbell , and H. Li, “Where am I looking at? 
joint location and orientation estimation by cross-view match -
ing,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2020 , 
pp. 4064 –4072 .
[42] X. Deng , Y. Zhu, and S. Newsam , “What is it like down there?: 
Generating dense ground-level views and image features from 
overhead imagery using conditional generative adversarial net -
works ,” in Proc. 26th ACM SIGSPATIAL Int. Conf. Adv. Geogr. Inf. 
Syst., 2018 , pp. 43–52. doi: 10.1145/3274895.3274969 .
[43] K. Regmi and M. Shah , “Bridging the domain gap for ground-
to-aerial image matching ,” in Proc. IEEE Int. Conf. Comput. Vis. , 
2019 , pp. 470–479.
[44] X. Lu, Z. Li, Z. Cui, M. R. Oswald , M. Pollefeys , and R. Qin, “Ge-
ometry-aware satellite-to-ground image synthesis for urban ar -
eas,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2020 , 
pp. 859–867.
[45] T. Salem , M. Zhai, S. Workman , and N. Jacobs , “A multimodal 
approach to mapping soundscapes ,” in Proc. IEEE Int. Geosci. Re -
mote Sens. Symp. (IGARSS) , 2018 , pp. 1–4.
[46] S. Workman , R. Souvenir , and N. Jacobs , “Understanding and 
mapping natural beauty ,” in Proc. IEEE Int. Conf. Comput. Vis. 
(ICCV) , 2017, pp. 5589 –5598 .[47] Y. Zhu and S. Newsam , “Spatio-temporal sentiment hotspot 
detection using geotagged photos ,” in Proc. 24th ACM SIG -
SPATIAL Int. Conf. Adv. Geogr. Inf. Syst. , 2016 , pp. 1–4. doi: 
10.1145/2996913.2996978 .
[48] F. Zhang  et al ., “Measuring human perceptions of a large-
scale urban region using machine learning ,” Landscape Urban 
Plan, vol. 180, pp. 148–160, Dec. 2018 . doi: 10.1016/j.landurb -
plan.2018.08.020 .
[49] F. Zhang, B. Zhou , C. Ratti , and Y. Liu, “Discovering place-infor -
mative scenes and objects using social media photos ,” Roy. Soc. 
Open. Sci. , vol. 6, no. 3, p. 181,375 , 2019 . doi: 10.1098/rsos.181375 .
[50] T. J. Bird et al., “Statistical solutions for error and bias in global 
citizen science datasets ,” Biol. Conserv. , vol. 173, pp. 144–154, 
May 2014 . [Online]. Available: http://www.sciencedirect.com/
science/article/pii/S0006320713002693  doi: 10.1016/j.biocon.  
2013.07.037 .
[51] H. Bahng , S. Chun , S. Yun, J. Choo , and S. J. Oh, “Learning de-
biased representations with biased representations ,” in Proc. 37th 
Int. Conf. Mach. Learn. , 2020 , pp. 528–539.
[52] A. Pérez-Suay , V. Laparra , G. Mateo-García , J. Muñoz-Marí , L. 
Gómez-Chova , and G. Camps-Valls , “Fair kernel learning ,” in 
Machine Learning and Knowledge Discovery in Databases, ser. Lecture 
Notes in Computer Science , M. Ceci, J. Hollmén , L. Todorovski , 
C. Vens , and S. Džeroski , Eds. New York : Springer-Verlag , 2017, 
pp. 339–355.
[53] P. Cihon  and T. Yasseri , “A biased review of biases in twitter stud -
ies on political collective action ,” Front. Phys. , vol. 4, p. 34, Aug. 
2016 . doi: 10.3389/fphy.2016.00034 .
[54] F. Morstatter  and H. Liu, “Discovering, assessing, and miti -
gating data bias in social media ,” Online Soc. Netw. Media , 
vol. 1, pp.  1–13, June 2017 . [Online]. Available: http://www  
.scien cedirect.com/science/article/pii/S2468696416300040 . 
doi: 10.1016/j.osnem.2017.01.001 .
[55] S. Law and M. Neira , “An unsupervised approach to geographi -
cal knowledge discovery using street level and street network 
images ,” in Proc. SIGSPATIAL Workshop GEOAI , 2019 , pp. 56–65. 
doi: 10.1145/3356471.3365238 .
[56] M. Imran , F. Ofli, D. Caragea , and A. Torralba , “Using AI and 
social media multimodal content for disaster response and man -
agement: Opportunities, challenges, and future directions ,” Inf. 
Process. Manage. , vol. 57, no. 5, p. 102,261,  2020 . doi: 10.1016/j.
ipm.2020.102261 .
[57] P. Chaudhary , S. D’Aronco , J. Leitão , K. Schindler , and J. D. We-
gner, “Water level prediction from social media images with a 
multi-task ranking approach ,” ISPRS J. Photogram. Remote Sens. , 
vol. 167, pp. 252–262, Sept. 2020 . doi: 10.1016/j.isprsjprs.2020  
.07.003 .
[58] D. Gurari  et al., “Vizwiz grand challenge: Answering visual ques -
tions from blind people ,” in Proc. Comput. Vis. Pattern Recognit. 
(CVPR) , 2019 , pp. 3608 –3617.
[59] J. Johnson , L. Fei-Fei , B. Hariharan , C. L. Zitnick , L. van der 
Maaten , and R. Girshick , “CLEVR: A diagnostic dataset for com -
positional language and elementary visual reasoning ,” in Proc. 
Compt. Vis. Pattern Recognit. (CVPR) , 2017, pp. 2901 –2910 .
[60] Z. Shi and Z. Zou, “Can a machine generate humanlike language 
descriptions for a remote sensing image? ” IEEE Trans. Geosci. 
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
104 
IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE     JUNE 2021Remote Sens. , vol. 55, no. 6, pp. 3623 –3634 , 2017. doi: 10.1109/
TGRS.2017.2677464 .
[61] G. Sumbul , S. Nayak , and B. Demir , “SD-RSIC: Summarization 
driven deep remote sensing image captioning ,” IEEE Trans. Geosci. 
Remote Sens. , early access, 2020 . doi: 10.1109/TGRS.2020.3031111 .
[62] G. Cam ps-Valls  et al., “Living in the physics and machine learn -
ing interplay for earth observation ,” in Proc. AAAI Fall Series 2020 
Symp. Phys.-Guided AI Accelerating Sci. Discovery , to be published. 
[63] A. Karpatne  et al., “Theory-guided data science: A new para -
digm for scientific discovery from data ,” IEEE Trans. Knowl. 
Data Eng. , vol. 29, no. 10, pp. 2318 –2331 , 2017. doi: 10.1109/
TKDE.2017.2720168 .
[64] E. de Bézenac , A. Pajot , and P. Gallinari , “Deep learning for phys -
ical processes: Incorporating prior scientific knowledge ,” in Proc. 
6th Int. Conf. Learn. Representations (ICLR) , 2018 . 
[65] N. Lang , K. Schindler , and J. D. Wegner , “Country-wide high-
resolution vegetation height mapping with Sentinel-2 ,” Remote 
Sens. Environ. , vol. 233, p. 111,347 , Nov. 2019 . doi: 10.1016/j.
rse.2019.111347 .
[66] U. Kälin , N. Lang, C. Hug, A. Gessler , and J. D. Wegner , “Defoliation 
estimation of forest trees from ground-level images ,” Remote Sens. En -
viron. , vol. 223, pp. 143–153, Mar. 2019. doi: 10.1016/j.rse.2018.12.021 .
[67] M. Raissi , “Deep hidden models: Deep learning of nonlinear 
partial differential equations ,” J. Mach. Learn. Res. , vol. 19, 
pp. 1–24, Jan. 2018 . doi: 10.5555/3291125.3291150 .
[68] D. Svendsen , M. Piles , J. Muñoz-Marí , D. Luengo , L. Martino , 
and G. Camps-Valls , “Integrating domain knowledge in data-
driven earth observation with process convolutions ,” submitted 
for publication.
[69] L. Von Rueden , S. Mayer , J. Garcke , C. Bauckhage , and J. Schueck -
er, “Informed machine learning–towards a taxonomy of explicit 
integration of knowledge into machine learning ,” Learning , vol. 
18, pp. 19–20, Mar. 2019 .
[70] E. de Bézenac , A. Pajot , and P. Gallinari , “Deep learning for 
physical processes: Incorporating prior scientific knowledge ,” J. 
Statist. Mech., Theory Exp. , vol. 2019 , no. 12, p. 124,009 , 2019 . doi: 
10.1088/1742-5468/ab3195 .
[71] R. R. Selvaraju , M. Cogswell , A. Das, R. Vedantam , D. Parikh , 
and D. Batra , “Grad-cam: Visual explanations from deep net -
works via gradient-based localization ,” in Proc. IEEE Int. Conf. 
Comput. Vis. (ICCV) , Oct. 2017 , pp. 618–626.
[72] M. M . Losch , M. Fritz, and B. Schiele , “Interpretability beyond 
classification output: Semantic bottleneck networks ,” 2019 . [On -
line]. Available: http://arxiv.org/abs/1907.10882
[73] D. Huk Park  et al., “Multimodal explanations: Justifying deci -
sions and pointing to the evidence ,” in Proc. IEEE Conf. Comput. 
Vis. Pattern Recognit. (CVPR) , June 2018 , pp. 1–10.
[74] J. Adebayo , J. Gilmer , M. Muelly , I. J. Goodfellow , M. Hardt , and 
B. Kim, “Sanity checks for saliency maps ,” in Proc. Neural Inf. Pro -
cess. Syst. , 2018 . 
[75] T. Ye, X. Wang , J. Davidson , and A. Gupta , “Interpretable in -
tuitive physics model ,” in Proc. Eur. Conf. Comput. Vis. (ECCV) , 
2018 , pp. 87–102.
[76] D. Marcos , S. Lobry , and D. Tuia, “Semantically interpre -
table activation maps: What-where-how explanations  within CNNs ,” in Proc. Int. Conf. Comput. Vision Workshop , 2019 , 
pp. 4207 –4215 .
[77] D. Marcos , S. Lobry , R. Fong , N. Courty , R. Flamary , and D. Tuia, 
“Contextual semantic interpretability ,” in Proc. Asian Conf. Com -
put. Vis. (ACCV) , 2020 . 
[78] S. Lapuschkin , S. Wäldchen , A. Binder , G. Montavon , W. Samek , 
and K.-R. Müller , “Unmasking Clever Hans predictors and as -
sessing what machines really learn ,” Nature Commun. , vol. 10, 
no. 1, p. 1096,  2019 . doi: 10.1038/s41467-019-08987-4 .
[79] C. Rudin , “Stop explaining black box machine learning models 
for high stakes decisions and use interpretable models instead ,” 
Nature Mach. Intell. , vol. 1, no. 5, pp. 206–215, 2019 . doi: 10.1038/
s42256-019-0048-x .
[80] R. Iten, T. Metger , H. Wilming , L. del Rio , and R. Renner , “Dis-
covering physical concepts with neural networks ,” Phys. Rev. 
Lett., vol. 124, no. 1, p. 010508,  Jan. 2020 . doi: 10.1103/PhysRev -
Lett.124.010508 .
[81] K. Zhang , B. Schölkopf , P. Spirtes , and C. Glymour , “Learning cau -
sality and causality-related learning: Some recent progress ,” Nat. 
Sci. Rev. , vol. 5, no. 1, pp. 26–29, 2018 . doi: 10.1093/nsr/nwx137 .
[82] J. Runge  et al., “Inferring causation from time series with per -
spectives in Earth system sciences ,” Nature Commun. , vol. 10, no. 
1, p. 2553 , 2019 . doi: 10.1038/s41467-019-10105-3 .
[83] J. Pearl , Causality: Models, Reasoning and Inference , 2nd ed . New 
York: Cambridge Univ. Press , 2009 . 
[84] C. Granger , “Investigating causal relations by econometric mod -
els and cross-spectral methods ,” Econometrica , vol. 37, no. 3, pp. 
424–438, 1969 . doi: 10.2307/1912791 .
[85] D. Marinazzo , M. Pellicoro , and S. Stramaglia , “Kernel method 
for nonlinear granger causality ,” Phys. Rev. Lett. , vol. 100, no. 14, 
p. 144,103,  Apr. 2008 . doi: 10.1103/PhysRevLett.100.144103 .
[86] D. Bueso , M. Piles , and, and G. Camps-Valls , “Cross-information 
kernel causality: Revisiting global teleconnections of ENSO over 
soil moisture and vegetation ,” in Proc. Climate Informatics , Paris, 
France, Oct. 2 –4, 2019 , pp. 1–5.
[87] G. Sugihara  et al., “Detecting causality in complex ecosystems ,” 
Science , vol. 338, no. 6106 , pp. 496–500, 2012 . doi: 10.1126/  
science.1227079 .
[88] G. Camps-Valls  et al., “Inferring causal graphs from observation -
al long-term carbon and water fluxes records ,” presented at the 
AGU Fall Meeting , San Francisco, Dec. 9 –13, 2019 .
[89] Y. Bengio  et al., “A meta-transfer objective for learning to disen -
tangle causal mechanisms ,” 2019 , arXiv:1901.10912.
[90] C. Louizos , U. Shalit , J. M. Mooij , D. Sontag , R. Zemel , and M. 
Welling , “Causal effect inference with deep latent-variable mod -
els,” in Proc. Adv. Neural Inf. Process. Syst. 30 , I. Guyon , U. V.  Lux-
burg, S. Bengio , H. Wallach , R. Fergus , S. Vishwanathan , and R. 
Garnett , Eds. Curran Associates , 2017, pp. 6446 –6456 . [Online]. 
Available: http://papers.nips.cc/paper/7223-causal-effect-infer -
ence-with-deep-latent-variable-models.pdf
[91] D. Bueso , M. Piles , and G. Camps-Valls , “Nonlinear PCA for spa -
tio-temporal analysis of earth observation data ,” IEEE Trans. Geos -
ci. Remote Sens. , vol. 58, no. 8, pp. 5752 –5763 , 2019 . doi: 10.1109/  
TGRS.2020.2969813 .
 GRS
Authorized licensed use limited to: ASU Library. Downloaded on March 07,2024 at 22:07:36 UTC from IEEE Xplore.  Restrictions apply. 
Segmenting across places: The need for fair transfer learning with
satellite imagery
Miao Zhang Harvineet Singh Lazarus Chok Rumi Chunara
New York University
{miaozhng, hs3673, lcc9673, rumi.chunara }@nyu.edu
Abstract
The increasing availability of high-resolution satellite
imagery has enabled the use of machine learning to support
land-cover measurement and inform policy-making. How-
ever, labelling satellite images is expensive and is available
for only some locations. This prompts the use of transfer
learning to adapt models from data-rich locations to others.
Given the potential for high-impact applications of satel-
lite imagery across geographies, a systematic assessment of
transfer learning implications is warranted. In this work,
we consider the task of land-cover segmentation and study
the fairness implications of transferring models across lo-
cations. We leverage a large satellite image segmentation
benchmark with 5987 images from 18 districts (9 urban
and 9 rural). Via fairness metrics we quantify disparities
in model performance along two axes – across urban-rural
locations and across land-cover classes. Findings show that
state-of-the-art models have better overall accuracy in ru-
ral areas compared to urban areas, through unsupervised
domain adaptation methods transfer learning better to ur-
ban versus rural areas and enlarge fairness gaps. In analy-
sis of reasons for these findings, we show that raw satellite
images are overall more dissimilar between source and tar-
get districts for rural than for urban locations. This work
highlights the need to conduct fairness analysis for satel-
lite imagery segmentation models and motivates the devel-
opment of methods for fair transfer learning in order not
to introduce disparities between places, particularly urban
and rural locations.
1. Introduction
Satellite imagery is becoming readily available with
around 1030 active satellites that are dedicated to earth ob-
servation [36]. Out of the different spectra of imagery avail-
able from such satellites, visible spectrum imagery is partic-
ularly relevant for many applications based on the extremelyhigh resolution and according ability to resolve specific ob-
jects of interest [5]. Consequently, satellite images com-
bined with semantic segmentation, the task of clustering
parts of an image together which belong to the same ob-
ject class, can be used to detect objects ranging from natu-
ral features (water bodies, forests) to human land-use types
(buildings, roads). The extracted information is being ap-
plied in a wide range of settings including urban planning
[34], modelling disease spread [1], aiding disaster relief
efforts [16, 57], and detecting and mapping environmental
phenomena [24,55]. However, because segmentation mod-
els employ supervised learning, availability of ground truth
data is a major bottleneck for their training. Annotation for
the segmentation task is particularly labor intensive as it re-
quires fine-grained labels at the level of pixels which results
in incomplete or noisy ground truth data [45]. In such sit-
uations, generalizing existing models to non-annotated data
bytransfer learning is a widely applied solution [43, 48].
Transfer learning uses knowledge learnt from the same
or related tasks to improve learning on the task at hand (see
Pan and Yang [39] for a survey). We will focus on a type of
transfer learning setting called domain adaptation , where
we have a single task but the train and test domains may
differ. The key challenge here is the discrepancy in data
distributions between domains. In the case of satellite im-
agery, the discrepancies commonly result from transferring
models to new geographies where the landscapes are dis-
similar to where the model was trained. For example, Islam
[22] finds that a well-trained seagrass detection model from
satellite images fails when tested at other locations with dif-
ferent seagrass density. To mitigate the degrading effects of
domain discrepancies on segmentation accuracy, previous
work has re-designed network architectures [28], loss func-
tions [18,50], and batch normalization methods [38] to im-
prove model generalization. Other approaches include us-
ing labels at a coarser granularity for the target domain (e.g.
image-level labels) as weak supervision [21] and learning
latent representations shared between source and target do-
2916

mains to help in adaptation [26, 29].
Simultaneously, while machine learning approaches
have been used to improve prediction in a variety of tasks,
recent studies have highlighted concerns towards model
fairness, exhibited by performance disparities across sen-
sitive groups based on geography, demographics, and eco-
nomic indicators [31,58]. A push for model fairness aligns
with the ideal of equity defined by World Health Organiza-
tion as “Equity is the absence of unfair, avoidable or reme-
diable differences among groups of people, whether those
groups are defined socially, economically, demographically,
or geographically or by other dimensions of inequality (e.g.
sex, gender, ethnicity, disability, or sexual orientation).”
Real-world examples have demonstrated the harmful effects
of unfair machine learning models, such as facial recogni-
tion software that performs worse on darker women [4] and
advertisement systems that deliver economic opportunity-
related ads less often to women than men [25]. Indeed,
discriminatory issues persist even in state-of-the-art learn-
ing methods [32]. Expanding types of data used in ma-
chine learning tasks, such as satellite imagery, enables in-
creased use in a wide range of daily-life applications and
ever-increasing social impacts. Accordingly, broader as-
pects and viewpoints of performance, such as fairness, need
to be ascertained in multiple machine learning subareas.
In this work we study the fairness impacts of transfer
learning with satellite imagery . To accomplish this goal,
we test multiple semantic segmentation models across dif-
ferent geographies. We then assess if such models made
fair predictions on both the source and the target data. We
focus on differences between urban and rural areas (i.e. ur-
ban/rural categorization is the sensitive attribute) due to per-
sistent and striking disparities between urban and rural ar-
eas, especially for poor populations [2, 3]. The unfairness
criterion in this work is based on differences in error rates
across protected groups where the error rate is computed
using Intersection-over-Union (IoU), a standard segmenta-
tion metric. We also examine model performance disparity
across different land-cover classes. Results show that ex-
isting domain adaptation methods do not maintain fairness
properties on transfer, either across protected groups or fea-
ture classes. This work serves as a valuable demonstration
of fairness being an critical issue in transfer learning using
a large freely-available satellite imagery dataset.
Important takeaways are as follows.
• Studied models have better overall accuracy (via mean
IoU over the 7 classes) on rural districts as compared to
urban districts.
• For common unsupervised domain adaptation methods,
transfer accuracy is improved, but at the cost of fairness;
the performance gap between rural and urban group is
enlarged indicating the need to design new methods thattransfer well for both the groups.
• Investigating reasons for the above findings, we find that
images from rural districts differ more across locations
than those of urban districts.
2. Related Work
Before discussing prior work on transfer learning for
satellite images, we describe some of the alternative ways
to address label scarcity and their shortcomings. Lastly, we
summarize work in the nascent area of fair transfer learning.
Approaches to tackle annotation burden for satellite
images Given the difficulty in labelling data for semantic
segmentation of satellite images, Schmitt, et al. [45] de-
veloped weakly-supervised learning methods, where noisy,
limited, or imprecise data sources are used to provide su-
pervision signal. Previous work has leveraged the spatial
context to develop unsupervised losses which, for exam-
ple, penalize nearby pixels with different predicted labels
[35, 50]. In another approach, Castillo-Navarro, et al. [6]
proposed auxiliary losses based on self-supervised image
reconstruction to improve the performance on the main task
of image segmentation. To improve efficiency of label-use,
Wang et al. [53] transferred classification models trained
with image-level labels to image segmentation tasks and
achieved high accuracy. While these approaches demon-
strate successful combination of labeled and unlabeled im-
ages, they assume that the images are from the same domain
(or distribution). However, the assumption of a consistent
domain is not realistic for problems involving satellite im-
ages which are often from different geographies. Thus, such
approaches are not straight-forwardly applicable in our set-
ting.
Transfer learning for satellite images Transferability
of satellite image segmentation models across different ge-
ographic locations has been studied in Ghorbanzadeh et
al.[14]. Using train and test sets across 3 different ge-
ographies (Taiwan, China, and Japan), they show consis-
tent decrease in evaluation scores upon transfer. Previous
work has incorporated domain adaptation methods to deal
with the challenge. For instance, Tran et al. [49] pro-
posed a two-stage transfer learning structure which gen-
erated pseudo-ground truth segmentation labels for target
data. Algorithms to improve the quality of such pseudo la-
bels were studied in [33,59]. Data augmentation is another
strategy for domain adaptation. Ji et al. augments images
to simulate perturbations due to atmospheric radiation and
demonstrate improved generalization of CNN-based mod-
els [23]. These studies show the promise of adapting mod-
els to data from different locations. But, the transfer is only
evaluated based on overall accuracy for the domain, such as
using Intersection-over-Union (IoU) to measure the over-
lap between predicted segmentation maps and ground-truth
2917

masks. Past work does not study fine-grained measures of
model performance on transfer, like how is the performance
for different subgroups in the domain (based on sensitive
attributes or land-use types) impacted. The risk that dis-
crepancy between domains in transfer learning may impact
subgroups unfairly remains unexplored.
Fairness in transfer learning Following the work in fair
machine learning literature [32], we will narrowly classify
the study of performance differences between subgroups
asmodel fairness analysis. Compared to fairness analysis
within the same domain, little work has studied transfer of
fair models across domains. The two objectives–improving
transfer accuracy and maintaining fairness–can be at odds
with each other [47, 56]. Schumann et al. [46] formalized
the problem of fair transfer learning which sets the learn-
ing objective to improve accuracy as well as fairness in the
target domain. Multiple approaches to fair transfer learning
have been proposed [8, 27, 30, 37, 41, 42, 47] that make var-
ious assumptions on how the domains differ and what data
is available. Even when labels are not available for the tar-
get domain, like in our setting, methods typically make the
covariate shift assumption which says that the labeling rule
remains the same between the domains and only the fea-
ture distribution changes. In this setting, Coston et al. [8]
propose a method for fair transfer even in cases where sen-
sitive attributes are absent from one of the domains. Other
approaches do not require access to target domain data al-
together and instead either make causal assumptions on the
discrepancy [47] or hypothesize a set of target domains and
optimize against them [10, 30]. Finally, Szab ´oet al. [19]
conduct a fairness evaluation of segmentation methods as-
suming a single domain.
However, none of the existing works study fair transfer
learning for semantic segmentation models. The task differs
considerably from the above settings as the input data is
high-dimensional, and the model output and loss function
for segmentation are different. We take the first step in this
direction by demonstrating the need for such methods via a
thorough empirical study on a relevant application.
3. Dataset
We use the publicly available, high spatial resolution
land-cover dataset called LoveDA [52] in this study. Com-
pared to other popular satellite image datasets, such as
Zurich Summer [50], DeepGlobe [9], and DSTL [20], the
recently released LoveDA has more annotated images and
includes images from diverse locations. The dataset con-
sists of 5987 images of size 1024 ×1024 and spatial reso-
lution 0.3m. The images are collected from 18 adminis-
trative districts from three cities in China, namely Nanjing,
Changzhou, and Wuhan. Out of these, there are 9 urban dis-
tricts and 9 rural districts, categorized based on their popu-
lation density and level of economic development. We use
Figure 1. Sample images from urban and rural scenes. For each
scene, one image from source domain districts, one from target
domain districts, and the network’s segmentation predictions (Seg-
map) for the 7 land-cover classes on that target image are shown.
satellite images from the 12 districts for which ground-truth
masks are available: Gulou, Qinhuai, Qixia, Yuhuatai, Jin-
tan, and Jianghan (urban); Pukou, Lishui, Liuhe, Huangpi,
Gaochun, and Jiangxia (rural). The remaining 6 districts
are not availanble as they are held out for the benchmark
challenge. The dataset contains segmentation masks, which
are pixel-level labels, for 7 land-cover classes: Background,
Building, Road, Water, Barren, Forest, and Agricultural.
The Background class consists of any pixel not belonging
to the other classes. Statistics of the dataset, given in Figure
3 in [52], show that the pixel counts across the 7 classes are
imbalanced. Further, distribution of classes and of building
scales differ between urban and rural scenes. Thus the rural
and urban groups of images, which we use in our fairness
analysis, have different characteristics.
4. Methods
We study three tasks, namely semantic segmentation
within the same domain, across districts, and across rural-
urban areas. Next, we describe the setup for each task.
2918

4.1. Task A - Semantic segmentation
Our task is to train multi-class semantic segmentation
models for detecting the 7 land-cover classes from a given
image. Sample images and predicted segmentation maps
are shown in Figure 1. Same as the models studied in
the LoveDA study [52], we use two commonly used deep
learning-based segmentation methods – U-Net [44] and
DeepLabV3+ [7] network, both with pre-trained ResNet50
[17] as the backbone model for the encoder [13].
4.1.1 Training-testing details
Images from the 12 districts with labeled data are shuffled
and split into training ( ≈80%) and testing sets ( ≈20%).
Training set has 3148 images with a mix of 1377 urban and
1771 rural images, and the rest of the images comprise the
testing set with a mix of 368 urban images and 473 rural
images. Images are augmented during training by mirroring
and rotation. Dimension of the input image to the network is
512×512×3 where 3 indicates the RGB bands. The output
dimension of the network is 512 ×512×7, where 7 repre-
sents the probability of each pixel belonging to each land-
cover class. We use cross-entropy (CE) loss, and stochastic
gradient descent (SGD) as the optimizer with a momentum
of 0.9 and a weight decay of 10−4. The batch size is set to
16 and the total training iterations are 15000, during which
the learning rate is decayed using a polynomial learning rate
scheduler implemented in PyTorch [40].
4.1.2 Evaluation metrics
We test the models on either the whole test set or the ur-
ban and rural subsets in the test set separately, and evaluate
model performance using the following metrics:
Accuracy metrics: Intersection-over-Union (IoU), also
called Jaccard index, is used to measure segmentation ac-
curacy which is a common method to evaluate the quality
of image segmentation [11, 54]. IoU for a class is defined
as the intersection of class-wise ground-truth masks and the
predicted segmentation divided by their union,
IoU :=TP
TP+FP+FN,
where TP,FP, andFN are pixel-wise true positives, false
positives, and false negatives. We report IoU score of the
model on each land-cover class as well as mean over class-
wise IoU (referred to as Mean ) over the 7 classes.
Fairness metrics: Besides looking at IoU on the rural
and urban subsets and comparing the two values, we de-
vise three additional metrics that quantify how accuracy is
distributed across the classes. The metrics have been used
in existing fairness analysis of segmentation models [19].
These are:1.Class-std . Standard deviation of IoUs across the 7
classes;
2.Worst . IoU of the worst-performing class (Worst);
3.Sorted 30% (bottom, top) . Mean of the bottom 30%
and top 30% classes of the sorted class IoUs. In our
case, 30% is 2 classes out of 7.
Next, we describe the setup for the two transfer tasks.
4.2. Transfer learning
As mentioned earlier, we consider the setting of unsu-
pervised domain adaption (UDA) that is we have a single
task (image segmentation) on the two domains. We assume
access to images and labels for the source (train) domain
and only the images for the target (test) domain. This is
a practical setting in satellite imagery since collecting im-
ages is inexpensive due to advancements in remote sensing,
however, annotating the segmentation labels is expensive.
Thus, we want to be able to use the labelled source images
to segment known but as yet unlabelled target domain.
We consider two UDA methods which performed the
best on the LoveDA benchmark [52] – class-balanced self-
training ( CBST ) [59] and instance adaptive self-training
(IAST ) [33]. CBST optimizes the generation of pseudo-
labels used during self-training to be more balanced among
the classes by using class-wise confidence scores. IAST
adaptively adjusts the pseudo-label generation to improve
the diversity of pseudo-labels and saves useful information
from hard instances. We also use a natural method which
ignores transfer learning and trains only with data from the
source domain ( No adaptation ).
For all transfer learning experiments, we use a
DeepLabV3+ [7] network with ResNet50 encoder. An
Adam optimizer is used with a momentum of 0.9. The batch
size is 8 and the total training iterations are 15000. Other ex-
perimental setup parameters are the same as in the semantic
segmentation task.
4.2.1 Task B - Transfer across districts
First, we consider the scenario where a model is transferred
across different geographical locations, which in our case
are administrative districts. Source domain comprises of
8 districts: Gulou, Qinhuai, Qixia, Jinghan (urban), and
Pukou, Gaochun, Lishui, Jingxia (rural); and Target do-
main has 4 districts: Yuhuatai, Jintan (urban), and Liuhe,
Huangpi (rural). The ”No adaptation” and two UDA meth-
ods (CBST, IAST) are applied to train the network on the
source domain, and are tested on urban and rural images
from the unseen target domain, separately. The same accu-
racy and fairness metrics listed in Section 4.1.2 are used for
evaluation.
2919

Mean Class-std
Model rural urban rural−urban (%) rural urban rural−urban (%)
UNet 0.639 0.595 0.044 (6.9%) 0.106 0.0946 0.0114 (10.8%)
DeepLabV3+ 0.632 0.597 0.035 (5.5%) 0.0982 0.0896 0.0086 (8.8%)
Worst Sorted 30% (bottom, top)
Model rural urban rural−urban (%) rural urban rural−urban (%)
UNet 0.453 0.474 −0.021 (-4.4%) (0.491, 0.742) (0.480, 0.705) (0.011 , 0.037)(2.2%, 5.0%)
DeepLabV3+ 0.473 0.473 0 (n/a) (0.504, 0.740) (0.489, 0.706) (0.015, 0.034)(3.0%, 4.6%)
Table 1. Task A: Evaluation on single-domain semantic segmentation. Two networks, UNet and DeepLabV3+, are tested on rural and
urban districts from the same domain as training set. For metrics, Mean, Class-std, and Worst, the better performing group (between rural
and urban) is in bold. The difference in performance between rural and urban is shaded. Typically, performance is better for rural than
urban.
Mean Class-std
Method rural urban rural−urban (%) rural urban rural−urban (%)
No adaptation 0.364 0.486 −0.122 ( −25%) 0.200 0.135 0.065 (33%)
CBST 0.374 0.523 −0.149 ( −28%) 0.215 0.105 0.110 (51%)
IAST 0.376 0.493 −0.117(−24%) 0.223 0.135 0.088 (39%)
Worst Sorted 30% (bottom, top)
Method rural urban rural−urban (%) rural urban rural−urban (%)
No adaptation 0.0609 0.244 −0.183 ( −75%) (0.098, 0.581) (0.317, 0.630) (−0.219,−0.049) ( −69%,−7.7%)
CBST 0.0172 0.362 −0.345 ( −95%) (0.0943, 0.609) (0.398, 0.647) (−0.304,−0.038) ( −76%,−5.9%)
IAST 0.0304 0.232 −0.202 ( −87%) (0.0772, 0.598) (0.327, 0.640) (−0.250,−0.042) ( −76%,−6.6%)
Table 2. Task B: Evaluation of transfer across districts. Three methods (No adaptation, CBST, IAST) are trained source districts, and
evaluated on target rural and target urban districts. For the metrics, Mean, Class-std, and Worst, the better performing group (between rural
and urban) is in bold. The differences in performance between rural and urban are shaded. Models have high unfairness upon transfer.
4.2.2 Task C - Transfer across urban and rural areas
Second, we consider the scenario where the segmentation
model is transferred either from urban to rural areas or from
rural to urban areas. The source and target domain consists
of data from the same set of districts. So for this task, the
only source for domain discrepancy is rural and urban dis-
crepancy. The no- adaptation method and two UDA meth-
ods are trained on the source domain, and tested on the tar-
get domain. Evaluation metrics are the same as earlier.
5. Results
We summarize results for the single-domain in Table 1.
Both the networks (UNet, DeepLabV3+) have better over-
all accuracy, shown with Mean IoU over the 7 classes, for
the rural districts compared to the urban districts. Fairness
metrics such as IoU of the worst class and mean IoU of
30% bottom classes are comparable between rural and ur-
ban. The worst class is Barren for both rural and urban. The
30% bottom classes include Barren and Road for rural, and
Barren and Forest for urban (see Table A.1 in Appendix
for class-wise results). Rural results show higher mean IoU
Figure 2. Task B: Mean IoU upon transfer across districts.
Mean IoU on the union of rural and urban data from the source
(S-overall) and target (T-overall), urban data from the source (S-
urban) and target (T-urban), rural data from the source (S-rural)
and target (T-rural) is plotted when transferring models across dis-
tricts. No adaptation is the source-only method, CBST and IAST
are UDA methods. While overall accuracy drops on transfer, UDA
methods have smaller accuracy drop.
2920

Sub-task Method Mean Class-std Worst Sorted 30% (bottom, top)
No adaptation 0.437 0.108 0.301 (0.322, 0.566)
Rural→Urban CBST 0.469 0.123 0.326 (0.332 , 0.617)
IAST 0.443 0.175 0.205 (0.211, 0.638 )
No adaptation 0.426 0.108 0.226 (0.271, 0.531)
Urban→Rural CBST 0.467 0.129 0.228 (0.283, 0.599 )
IAST 0.454 0.120 0.229 (0.307 , 0.592)
Table 3. Task C: Evaluation of urban-rural transfer. Three methods (No adaptation, CBST, IAST) are trained on rural districts and
evaluated on urban districts, and vice versa. Results with the most improvements are marked in bold. UDA methods improve Mean IoU
compared to No adaptation but increase standard deviation of IoUs across classes.
from top 30% classes than urban results, but higher class-
wise standard deviation. Overall, we observe rural-urban
disparities in all four metrics.
For Task B on transfer learning across districts, we sum-
marize the results in Figure 2 and Table 2. Figure 2 visual-
izes the mean IoU metric for both source and target districts.
Based on the first two bar plots (dark and light orange) for
No adapation, CBST, and IAST, we conclude that UDA
methods improve overall segmentation accuracy on the tar-
get domain (T-overall) compared to No adaptation (a de-
crease of 22% and 24% vs that of 26%). Similar trend is ob-
served for each of the source-target pairs for rural and urban
separately. However, the performance gap between the ru-
ral and urban data from target (T-rural and T-urban) remains
large. For instance, from Table 2 we observe that CBST ob-
tains mean IoU of 0.523 on urban area which is better than
the ”No adaptation” 0.486, and IAST obtains 0.376 on rural
area better than the ”No adaptation” 0.364. However, the
networks remain unfair across rural-urban groups after the
transfer (large values in the rural −urban columns). UDA
methods further lower fairness: CBST increases the differ-
ence of mean IoU between urban and rural by 22% (-0.122
to -0.149), increases the difference of standard deviation by
69% (0.065 to 0.11), and increases the difference of worst-
performing class’ IoU by 89% (-0.183 to -0.345).
Next, we examine Task C on transfer learning from ur-
ban domain to rural domain and vice versa. Results are
summarized in Table 3. For both the transfer directions,
the two UDA methods improve the overall accuracy, shown
as higher mean IoU, higher IoU on the worst-preforming
class, and higher mean IoU on bottom and top 30% classes.
However, compared to ”No adaptation”, UDA methods dis-
perse model performance across the classes, measured by
higher standard deviation. For example, CBST increases
Class-std from 0.108 to 0.123 on rural to urban transfer and
from 0.108 to 0.129 on urban to rural transfer. Looking
more closely into the CBST method which obtains the best
overall transfer accuracy (0.469 and 0.467), its performance
on each class is visualized in Figure 3. We observe that
the IoU changes for each class upon transfer are highly un-
Figure 3. Task C: Mean IoU upon transfer across rural-urban.
Mean IoU for 7 landscape classes on source and target domain
when transferring from rural area to urban area, and from ur-
ban area to rural area, with the UDA method CBST. Performance
changes vary substantially by class.
equal. For example, in transferring from Rural →Urban, the
network retains accuracy on the Water and Road classes, but
lost significant accuracy ( 53%) on the Forest class. In trans-
ferring from Urban →Rural, accuracy drops significantly on
Road, Water, and Barren classes, but increases by 33% on
the Forest class.
2921

6. Discussion
For the locations included in this study, segmentation re-
sults showed a disparity in performance between rural and
urban areas. Though the two groups obtain similar accu-
racy on the respective bottom 30% performing land-cover
classes, rural areas obtain better accuracy on the top 30%
performing classes. Moreover, performance distribution
across classes are different between rural and urban im-
ages. Specifically, the segmentation model detects Forest
and Agriculture classes well in their rural form, and detects
Road and Water classes well in their urban form (detailed
results are reported in Table 5 in the Appendix). Due to ur-
banization, rural and urban areas have clear landscape dif-
ferences. For example, roads are typically wider in the ur-
ban scenes and narrower in rural scenes and water takes on
larger shapes like lakes in urban scenes, and smaller shapes
like ditches in rural scenes [52]. This may explain why the
networks show advantages in urban images on Road and
Water classes. Moreover, agricultural land covers large area
and is continuously distributed in rural scenes. The per-
centage of pixels with Agriculture and Forest elements is
also higher [52]. This can facilitate learning on these two
classes in rural areas as compared to urban areas.
We considered two practical transfer learning tasks with
satellite images and assess network fairness while trans-
ferring across geographical locations, and across rural and
urban areas. Broadly, we observed that when transferring
across districts, networks made more unfair predictions on
data from the new domain than data from the same domain
as training. For the network trained without any adapta-
tion, the mean IoU accuracy difference between rural and
urban images on the target domain is around 64% higher
than the difference reported in the single-domain task. Sim-
ilarly, all other fairness metrics show much higher differ-
ences between groups on transferring to the target domain.
Notably, when applying UDA methods, CBST and IAST,
transfer accuracy was improved, but at the cost of fairness
damage. These methods further enlarged the performance
gap between rural and urban groups measured in all four
metrics. These findings indicate a need for new domain
adaptation methods that tackle the challenge of fair trans-
fer learning.
One of the possible reasons why the network can bet-
ter adapt to urban images than rural images is the unequal
domain discrepancy. To estimate how similar the source
and target images are in the rural and urban groups, we use
two metrics – Proxy-A-distance (PAD) [12] and Maximum
mean discrepancy (MMD) [15]. Both measure the dissim-
ilarity between data distributions of different domains. We
randomly sample 100 images from each domain at a time
and ran 30 trials to compute the two measures. The mean
and standard deviation of distance across the trials are re-
ported in Table 4. We observe that the raw satellite imagesGroup Source Target PAD MMD
Urban Gulou Yuhuatai 0.26 0.207
Qinhuai Jintan (±0.12) (±0.0235)
Qixia
Jinghan
Rural Pukou Liuhe 0.64 0.262
Gaochun Huangpi (±0.21) (±0.0437)
Lishui
Jingxia
Table 4. Shift in raw image distribution. Measurement of
source-target domain distance using two metrics – Proxy-A-
distance (PAD) and Maximum mean discrepancy (MMD). The
implementation is based on the online codebase in [51]. Rural
images shift more than urban images.
Figure 4. Shifts in class distribution. Class distribution in terms
of proportion of pixels per class is plotted for urban images from
source domain (S-urban), urban images from target domain (T-
urban), rural images from source domain (S-rural), and rural im-
ages from target domain (T-rural). Class distribution is substan-
tially different for all subsets.
are overall more dissimilar between source and target dis-
tricts for rural than for urban, which is a likely cause of
the unequal transfer learning performance between the two
groups. Figure 1 illustrates example images from both do-
mains and the segmentation predictions our network made
on the target. We see that Buildings across source and tar-
get districts are of similar shape and arrangement for urban,
but they are disordered and dissimilar for rural. Accord-
ingly, the model segmentation map shows that the model
segments urban buildings well but fails to detect most of
the rural buildings. This observation indicates the impor-
tance of checking class differences besides overall differ-
ences across the whole image.
Along these lines, we define pixel-wise class distribu-
tion as the proportion of pixels belonging to each class. We
assess shifts in the class distribution between source and
target for both rural and urban images. Class distributions
are shown in Figure 4. For urban locations, the Water and
2922

Agriculture classes have the largest shifts from source to
target. For rural, the Background and Forest classes have
the largest shifts. Indeed there are large class shifts between
source and target for both the urban and rural data. For
example, the class distribution of urban target data seems
more rural-like with an increased proportion in the Agricul-
ture class. This emphasizes the internal variation in rural
and urban categories. Moreover, as our data consists of im-
ages from just one set of locations, data from different loca-
tions are needed for more generalizable conclusions. How-
ever, based on the selection of classes examined in our data
which are common land-use classes globally, some results
(such as in Table 4 indicate common threads that can be
applicable to rural-urban disparities in general.
In the second transfer learning task, the networks show
unfairness on rural to urban domain transfer. Differences
between rural and urban scenes provide explanation for
why almost all classes lost accuracy on the target do-
main. Some classes show opposite transfer performance
in the two sub-tasks of Task C. The Road class lost only
4% accuracy on Rural →Urban transfer but lost 41% ac-
curacy on Urban →Rural transfer. The Water class shows
similar patterns, whereas the Forest class lost 53% accu-
racy on Rural →Urban transfer but gains 33% accuracy on
Urban→Rural transfer. These observations indicate that for
some classes, the features learnt by the networks from rural
scenes can be easily adapted to interpret urban scenes, and
some classes have the opposite case. From this perspective,
features of different classes can have very different general-
ization ability, which will cause the transfer performance to
be highly unequal across classes. This feature-specific char-
acteristic may be leveraged in the design of future transfer
learning methods.
7. Conclusion
Transfer learning models for semantic segmentation are
often evaluated based on overall accuracy metrics. Here,
we expand the scope of their evaluation by conducting a
systematic fairness evaluation when models are transferred
across domains. We examine the performance of two unsu-
pervised domain adaptation methods on a large-scale public
satellite imagery dataset. Model fairness is evaluated be-
tween rural and urban locations as the models are trained
and tested across administrative districts. Based on the ex-
periments, we conclude that the domain adaptation meth-
ods we study can be improved in terms of retaining model
fairness across rural and urban data. Domain adaptation im-
proves overall accuracy at the cost of decreasing fairness on
test domain. Further, more shifts in the raw image distri-
bution and pixel-wise class distribution result in more per-
formance drop. Broadly, our findings demonstrate potential
fairness problems when working with satellite image data
sourced from different locations. Also, the findings indicate
the need for developing methods focused on fair transferlearning, such as through new model architectures or loss
functions.
References
[1] Nabeel Abdur Rehman, Umar Saif, and Rumi Chunara. Deep
landscape features for improving vector-borne disease pre-
diction. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops , pages
44–51, 2019. 1
[2] Michiel Bakker, Humberto River ´on Vald ´es, Patrick D Tu,
Krishna P Gummadi, Kush R Varshney, Adrian Weller, and
AS Pentland. Fair enough: Improving fairness in budget-
constrained decision making using confidence thresholds.
2020. 2
[3] Ritika Brahmadesam and Kush R Varshney. Fairly estimat-
ing socioeconomic status under costly feature acquisition,
2021. 2
[4] Joy Buolamwini and Timnit Gebru. Gender shades: Inter-
sectional accuracy disparities in commercial gender classifi-
cation. In Sorelle A. Friedler and Christo Wilson, editors,
Proceedings of the 1st Conference on Fairness, Accountabil-
ity and Transparency , volume 81 of Proceedings of Machine
Learning Research , pages 77–91. PMLR, 23–24 Feb 2018.
2
[5] AP Carleer, Olivier Debeir, and El ´eonore Wolff. Assess-
ment of very high spatial resolution satellite image segmen-
tations. Photogrammetric Engineering & Remote Sensing ,
71(11):1285–1294, 2005. 1
[6] Javiera Castillo-Navarro, Bertrand Le Saux, Alexandre
Boulch, Nicolas Audebert, and S ´ebastien Lef `evre. Semi-
supervised semantic segmentation in earth observation: The
minifrance suite, dataset analysis and multi-task network
study. Machine Learning , pages 1–36, 2021. 2
[7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian
Schroff, and Hartwig Adam. Encoder-decoder with atrous
separable convolution for semantic image segmentation. In
Proceedings of the European conference on computer vision
(ECCV) , pages 801–818, 2018. 4
[8] Amanda Coston, Karthikeyan Natesan Ramamurthy, Dennis
Wei, Kush R Varshney, Skyler Speakman, Zairah Mustahsan,
and Supriyo Chakraborty. Fair transfer learning with missing
protected attributes. In Proceedings of the 2019 AAAI/ACM
Conference on AI, Ethics, and Society , pages 91–98, 2019. 3
[9] Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan
Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia,
and Ramesh Raskar. Deepglobe 2018: A challenge to parse
the earth through satellite images. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition Workshops , pages 172–181, 2018. 3
[10] Wei Du and Xintao Wu. Fair and Robust Classification Un-
der Sample Selection Bias , page 2999–3003. Association for
Computing Machinery, New York, NY , USA, 2021. 3
[11] Mark Everingham, SM Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
2923

pascal visual object classes challenge: A retrospective. Inter-
national journal of computer vision , 111(1):98–136, 2015. 4
[12] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. The journal of machine learning
research , 17(1):2096–2030, 2016. 7
[13] Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea,
Victor Villena-Martinez, Pablo Martinez-Gonzalez, and Jose
Garcia-Rodriguez. A survey on deep learning techniques for
image and video semantic segmentation. Applied Soft Com-
puting , 70:41–65, 2018. 4
[14] Omid Ghorbanzadeh, Alessandro Crivellari, Pedram
Ghamisi, Hejar Shahabi, and Thomas Blaschke. A com-
prehensive transferability evaluation of u-net and resu-net
for landslide detection from sentinel-2 data (case study
areas from taiwan, china, and japan). Scientific Reports ,
11(1):1–20, 2021. 2
[15] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bern-
hard Sch ¨olkopf, and Alexander Smola. A kernel two-sample
test. The Journal of Machine Learning Research , 13(1):723–
773, 2012. 7
[16] Ananya Gupta, Elisabeth Welburn, Simon Watson, and Hu-
jun Yin. Cnn-based semantic change detection in satellite
imagery. In International Conference on Artificial Neural
Networks , pages 669–684. Springer, 2019. 1
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 4
[18] Corentin Henry, Seyed Majid Azimi, and Nina Merkle. Road
segmentation in sar satellite images with deep fully convolu-
tional neural networks. IEEE Geoscience and Remote Sens-
ing Letters , 15(12):1867–1871, 2018. 1
[19] Szab ´o, A., Jamali-Rad, H. & Mannava, S. Tilted cross-
entropy (TCE): Promoting fairness in semantic segmenta-
tion. Proceedings Of The IEEE/CVF Conference On Com-
puter Vision And Pattern Recognition . pp. 2305-2310 (2021)
3, 4
[20] Vladimir Iglovikov, Sergey Mushinskiy, and Vladimir Osin.
Satellite imagery feature detection using deep convolutional
neural network: A kaggle competition, 2017. 3
[21] Javed Iqbal and Mohsen Ali. Weakly-supervised domain
adaptation for built-up region segmentation in aerial and
satellite imagery. ISPRS Journal of Photogrammetry and Re-
mote Sensing , 167:263–275, 2020. 1
[22] Kazi Aminul Islam. Deep Learning Approaches for Seagrass
Detection in Multispectral Imagery . PhD thesis, Old Domin-
ion University, 2021. 1
[23] Shunping Ji, Shiqing Wei, and Meng Lu. A scale robust con-
volutional neural network for automatic building extraction
from aerial and satellite imagery. International journal of
remote sensing , 40(9):3308–3322, 2019. 2[24] Marios Krestenitis, Georgios Orfanidis, Konstantinos Ioan-
nidis, Konstantinos Avgerinakis, Stefanos Vrochidis, and
Ioannis Kompatsiaris. Oil spill identification from satel-
lite images using deep neural networks. Remote Sensing ,
11(15):1762, 2019. 1
[25] Anja Lambrecht and Catherine Tucker. Algorithmic bias?
an empirical study of apparent gender-based discrimination
in the display of stem career ads. Management Science ,
65(7):2966–2981, 2019. 2
[26] Ke Li, Mingju Wang, Yixin Liu, Nan Yu, and Wei Lan. A
novel method of hyperspectral data classification based on
transfer learning and deep belief network. Applied Sciences ,
9(7), 2019. 2
[27] Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia
Smith. Ditto: Fair and robust federated learning through per-
sonalization. In ICML , pages 6357–6368, 2021. 3
[28] Yikun Li and Timo R Bretschneider. Semantic-sensitive
satellite image retrieval. IEEE Transactions on Geoscience
and Remote Sensing , 45(4):853–860, 2007. 1
[29] Bing Liu, Xuchu Yu, Anzhu Yu, and Gang Wan. Deep con-
volutional recurrent neural network with transfer learning for
hyperspectral image classification. Journal of Applied Re-
mote Sensing , 12(2):1 – 17, 2018. 2
[30] Debmalya Mandal, Samuel Deng, Suman Jana, and Daniel
Hsu. Ensuring fairness beyond the training data. Advances
in neural information processing systems , 2020. 3
[31] Amy McGovern, Imme Ebert-Uphoff, David John Gagne II,
and Ann Bostrom. The need for ethical, responsible, and
trustworthy artificial intelligence for environmental sciences.
arXiv preprint arXiv:2112.08453 , 2021. 2
[32] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina
Lerman, and Aram Galstyan. A survey on bias and fair-
ness in machine learning. ACM Computing Surveys (CSUR) ,
54(6):1–35, 2021. 2, 3
[33] Ke Mei, Chuang Zhu, Jiaqi Zou, and Shanghang Zhang. In-
stance adaptive self-training for unsupervised domain adap-
tation. In European conference on computer vision , pages
415–430. Springer, 2020. 2, 4
[34] Khaled Moghalles, Heng-Chao Li, Zaid Al-Huda, and
Essa Abdullah Hezzam. Multi-task deep network for seman-
tic segmentation of building in very high resolution imagery.
In2021 International Conference of Technology, Science and
Administration (ICTSA) , pages 1–6. IEEE, 2021. 1
[35] Anton Obukhov, Stamatios Georgoulis, Dengxin Dai, and
Luc Van Gool. Gated crf loss for weakly supervised seman-
tic image segmentation. arXiv preprint arXiv:1906.04651 , 6,
2019. 2
[36] Union of Concerned Scientists. Ucs satellite database.
https : / / www . ucsusa . org / resources /
satellite-database , 2022. [Accessed 23-March-
2022]. 1
[37] Luca Oneto, Michele Donini, Massimiliano Pontil, and An-
dreas Maurer. Learning fair and transferable representa-
tions with theoretical guarantees. In 2020 IEEE 7th Interna-
tional Conference on Data Science and Advanced Analytics
(DSAA) , pages 30–39, 2020. 3
2924

[38] Anthony Ortiz, Caleb Robinson, Dan Morris, Olac Fuentes,
Christopher Kiekintveld, Md Mahmudulla Hassan, and
Nebojsa Jojic. Local context normalization: Revisiting local
normalization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 11276–
11285, 2020. 1
[39] Sinno Jialin Pan and Qiang Yang. A survey on transfer learn-
ing. IEEE Transactions on Knowledge and Data Engineer-
ing, 22(10):1345–1359, 2010. 1
[40] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai-
son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im-
perative style, high-performance deep learning library. In H.
Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E.
Fox, and R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems 32 , pages 8024–8035. Curran Asso-
ciates, Inc., 2019. 4
[41] Ashkan Rezaei, Anqi Liu, Omid Memarrast, and Brian D.
Ziebart. Robust fairness under covariate shift. Proceedings of
the AAAI Conference on Artificial Intelligence , 35(11):9419–
9427, May 2021. 3
[42] Yuji Roh, Kangwook Lee, Steven Whang, and Changho Suh.
Sample selection for fair and robust training. Advances in
Neural Information Processing Systems , 34, 2021. 3
[43] Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bol-
liger, Vaishaal Shankar, Miyabi Ishihara, Benjamin Recht,
and Solomon Hsiang. A generalizable and accessible ap-
proach to machine learning with global satellite imagery. Na-
ture communications , 12(1):1–11, 2021. 1
[44] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-
puting and computer-assisted intervention , pages 234–241.
Springer, 2015. 4
[45] Michael Schmitt, Jonathan Prexl, Patrick Ebel, Lukas Liebel,
and Xiao Xiang Zhu. Weakly supervised semantic segmen-
tation of satellite images for land cover mapping–challenges
and opportunities. arXiv preprint arXiv:2002.08254 , 2020.
1, 2
[46] Candice Schumann, Xuezhi Wang, Alex Beutel, Jilin Chen,
Hai Qian, and Ed H Chi. Transfer of machine learning
fairness across domains. arXiv preprint arXiv:1906.09688 ,
2019. 3
[47] Harvineet Singh, Rina Singh, Vishwali Mhasawade, and
Rumi Chunara. Fairness violations and mitigation under
covariate shift. In Proceedings of the 2021 ACM Confer-
ence on Fairness, Accountability, and Transparency , FAccT
’21, page 3–13, New York, NY , USA, 2021. Association for
Computing Machinery. 3
[48] Lisa Torrey and Jude Shavlik. Transfer learning. In Hand-
book of research on machine learning applications and
trends: algorithms, methods, and techniques , pages 242–
264. IGI global, 2010. 1[49] An Tran, Ali Zonoozi, Jagannadan Varadarajan, and Hannes
Kruppa. Pp-linknet: Improving semantic segmentation of
high resolution satellite imagery with multi-stage training.
InProceedings of the 2nd Workshop on Structuring and Un-
derstanding of Multimedia heritAge Contents , pages 57–64,
2020. 2
[50] Michele V olpi and Vittorio Ferrari. Semantic segmentation
of urban scenes by learning local class interactions. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops , pages 1–9, 2015. 1, 2, 3
[51] Jindong Wang et al. Everything about transfer learning and
domain adapation. http://transferlearning.xyz .
7
[52] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and
Yanfei Zhong. Loveda: A remote sensing land-cover dataset
for domain adaptive semantic segmentation. arXiv preprint
arXiv:2110.08733 , 2021. 3, 4, 7
[53] Sherrie Wang, William Chen, Sang Michael Xie, George Az-
zari, and David B Lobell. Weakly supervised deep learning
for segmentation of remote sensing imagery. Remote Sens-
ing, 12(2):207, 2020. 2
[54] Zhaobin Wang, E Wang, and Ying Zhu. Image segmenta-
tion evaluation: a survey of methods. Artificial Intelligence
Review , 53(8):5637–5674, 2020. 4
[55] Renzhe Wu, Guoxiang Liu, Rui Zhang, Xiaowen Wang,
Yong Li, Bo Zhang, Jialun Cai, and Wei Xiang. A deep learn-
ing method for mapping glacial lakes from the combined use
of synthetic-aperture radar and optical satellite images. Re-
mote Sensing , 12(24):4020, 2020. 1
[56] Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang.
To be robust or to be fair: Towards fairness in adversarial
training. In Marina Meila and Tong Zhang, editors, Pro-
ceedings of the 38th International Conference on Machine
Learning , volume 139 of Proceedings of Machine Learning
Research , pages 11492–11501. PMLR, 18–24 Jul 2021. 3
[57] Fei Zhao and Chengcui Zhang. Building damage evaluation
from satellite imagery using deep learning. In 2020 IEEE
21st International Conference on Information Reuse and In-
tegration for Data Science (IRI) , pages 82–89. IEEE, 2020.
1
[58] Yuyin Zhou, Shih-Cheng Huang, Jason Alan Fries, Alaa
Youssef, Timothy J Amrhein, Marcello Chang, Imon Baner-
jee, Daniel Rubin, Lei Xing, Nigam Shah, et al. Radfusion:
Benchmarking performance and fairness for multimodal pul-
monary embolism detection from ct and ehr. arXiv preprint
arXiv:2111.11665 , 2021. 2
[59] Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang.
Unsupervised domain adaptation for semantic segmentation
via class-balanced self-training. In Proceedings of the Eu-
ropean conference on computer vision (ECCV) , pages 289–
305, 2018. 2, 4
2925

JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1
An Empirical Study of Remote Sensing Pretraining
Di Wang, Jing Zhang, Bo Du, Senior Member, IEEE, Gui-Song Xia, Senior Member, IEEE,
and Dacheng Tao, Fellow, IEEE
Abstract —Deep learning has largely reshaped remote sensing
(RS) research for aerial image understanding and made a
great success. Nevertheless, most of the existing deep models
are initialized with the ImageNet pretrained weights. Since
natural images inevitably present a large domain gap relative
to aerial images, probably limiting the ﬁnetuning performance
on downstream aerial scene tasks. This issue motivates us to
conduct an empirical study of remote sensing pretraining (RSP)
on aerial images. To this end, we train different networks from
scratch with the help of the largest RS scene recognition dataset
up to now — MillionAID, to obtain a series of RS pretrained
backbones, including both convolutional neural networks (CNN)
and vision transformers such as Swin and ViTAE, which have
shown promising performance on computer vision tasks. Then,
we investigate the impact of RSP on representative downstream
tasks including scene recognition, semantic segmentation, object
detection, and change detection using these CNN and vision
transformer backbones. Empirical study shows that RSP can
help deliver distinctive performances in scene recognition tasks
and in perceiving RS related semantics such as “Bridge” and
“Airplane”. We also ﬁnd that, although RSP mitigates the data
discrepancies of traditional ImageNet pretraining on RS images,
it may still suffer from task discrepancies, where downstream
tasks require different representations from scene recognition
tasks. These ﬁndings call for further research efforts on both
large-scale pretraining datasets and effective pretraining meth-
ods. The codes and pretrained models will be released at
https://github.com/ViTAE-Transformer/RSP.
Index Terms —Remote Sening Pretraining, CNN, Vision Trans-
former, Classiﬁcation, Detection, Semantic Segmentation.
I. I NTRODUCTION
WITH the development of geoinformatics technology,
the earth observation ﬁelds have witnessed signiﬁcant
progress, where various remote sensing (RS) sensors and
devices have been widely used. Among them, with the advan-
tages of real-time, abundant amount, and easy access, the aerial
image has become one of the most important data sources in
earth vision to serve the requirements of a series of practical
tasks, such as precision agriculture [1], [2] and environmental
monitoring [3]. In these applications, aerial scene recognition
is a fundamental and active research topic over the past years.
However, because of the own characteristics of aerial images,
it is still challenging to efﬁciently understand the aerial scene.
The aerial images are usually obtained by a camera in a bird-
view perspective lying on the planes or satellites, perceiving a
D. Wang, B. Du and Gui-Song Xia are with the School of
Computer Science, Wuhan University, Wuhan 430072, China (e-mail:
wd74108520@gmail.com; dubo@whu.edu.cn; guisong.xia@whu.edu.cn). B.
Du is the corresponding author.
J. Zhang is with the School of Computer Science, Faculty of Engineering,
The University of Sydney, Australia (jing.zhang1@sydney.edu.au).
D. Tao is with the JD Explore Academy, China and is also with the School
of Computer Science, Faculty of Engineering, The University of Sydney,
Australia (dacheng.tao@gmail.com).
Playground
Swimming pool
Tennis court
Building
(c)
(a)
(d)(b)
Playground
PlaygroundFig. 1. The challenges of aerial scene recognition. (a) and (b) are the natural
image and aerial image belonging to the “park” category. (c) and (d) are two
aerial images from the “school” category. Despite the distinct view difference
between (a) and (b), (b) contains the playground that is unusual in the park
scenes but usually exists in the school scenes like (d). On the other hand,
(c) and (d) show different colors as well as signiﬁcantly different spatial
distributions of land objects like the playground and swimming pool. Here, (a)
is obtained from http://travel.qunar.com/p-oi24486013-townhill country park
by searching “park” on internet, while (b), (c), and (d) are the aerial images
from the AID dataset.
large scope of land uses and land covers. The obtained aerial
scene is usually difﬁcult to be interpreted since the interference
of the scene-irrelevant regions and the complicated spatial
distribution of land objects. Speciﬁcally, it causes the issue of
inter-class similarity in aerial scene understanding, i.e., some
different scenes present similar characteristics, as well as the
issue of large intra-class variations, where the scenes in the
same category have discrepancies, as shown in Figure 1.
To tackle the above problems, it is necessary to obtain dis-
criminative feature representations for different categories of
aerial scenes. According to the difference in feature extraction
methods, they can be divided into three types, i.e., the hand-
crafted features, the unsupervised learning features, and the
supervised deep learning (DL) features. Initially, researchers
directly utilize simple properties, such as color [4], texture
[5], contour [6], spectral or their combination [7] to recognize
different aerial scenes. Besides these intuitive attributes, there
are also some well-designed feature descriptors. For instance,
the scale-invariant feature transformation and histogram of
oriented gradients. These handcrafted features usually perform
well in simple scenes while being ineffective in complex
scenes. They are usually regarded as shallow features from
a modern view in the DL era, while interpreting complex
scenes requires more semantic information, which can not be
efﬁciently extracted by shallow-layer methods [8]. Compared
with the above approaches, the unsupervised learning methodsarXiv:2204.02825v4  [cs.CV]  4 May 2023
2 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
provide a feasible way to automatically extract the suitable
features by adaptively learning the mapping functions or ﬁlters
based on a set of handcrafted features or the raw pixel intensity
values. The typical unsupervised learning methods include
potential latent semantic analysis [9] and bag-of-visual-words
[10]. Some simple feature enhancement methods such as the
principal component analysis also belong to this category.
Nonetheless, the encoded unsupervised features still have
limited performance since no category supervision is explicitly
used, which is useful for feature discrimination.
In recent years, with the superiority of automatically extract-
ing deep features that reﬂect the inherent properties of objects,
DL has achieved an impressive breakthrough in the computer
vision (CV) ﬁeld [11]–[17], as well as the RS ﬁeld [18]–
[20]. In the aerial scene recognition domain, the most com-
monly used deep models are the convolutional neural networks
(CNN), which have a good ability for local perception and
global perception, where the former is achieved via applying
sliding window convolutions on the input image to extract the
local features while the latter is achieved by stacking multiple
convolutional layers to increase the receptive ﬁeld. According
to the training scheme, the ways of using CNN in aerial scene
recognition methods can be divided into three categories, i.e.,
training from scratch, ﬁnetuning, and adoption as the feature
extractor. The ﬁrst scheme does not involve any external data,
meaning there is no prior knowledge that can be leveraged.
To remedy this issue, the ﬁnetuning scheme uses the networks
pretrained on a large-scale dataset as the start point for further
training (i.e., ﬁnetuning). The last scheme directly extracts the
feature from the pretrained CNN without further ﬁnetuning,
therefore lacking the ﬂexibility of adapting to aerial images
from different downstream tasks.
The existing literature [21] reveals that the ﬁnetuning strat-
egy performs better than the other ones. We attribute it to the
capacity of the used pretraining dataset, including the sample
size and the number of categories. In the current RS ﬁeld
including the aerial scene recognition task, almost all ﬁnetuned
models are pretrained on the ImageNet-1K dataset [22], which
is the most famous image dataset in the CV ﬁeld. Its millions
of real-world images from 1,000 different categories enable
the models to learn a powerful representation. Usually, off-
the-shelf deep models like VGG [11] and ResNet [12] are
used as backbone networks for aerial scene recognition, since
training a new network on the ImageNet from scratch is
time-consuming and requires a large number of computational
resources. To further improve the classiﬁcation performance,
some methods [23], [24] adopt the ImageNet pretrained mod-
els as the backbone network and employ multi-level features
from it. In addition, many other components or strategies are
specially designed for the aerial recognition task, such as the
distillation [25] and feature partitioning [26].
Although the aforementioned methods have achieved re-
markable performance for aerial scene recognition, there are
still some issues needed to be further investigated. Intuitively,
when considering the characteristics of aerial images, there
exists a large domain gap compared with the natural images
in terms of view, color, texture, layout, object, etc. Previous
methods attempt to narrow this gap by further ﬁnetuning thepretrained model on the RS image dataset. Nevertheless, the
systematic bias introduced by ImageNet Pretraining (IMP)
has a non-negligible side impact on the performance [27].
On the other hand, we notice that there are abundant aerial
images captured by diverse multiple sensors with the progress
of RS technology, which can be used for pretraining. As a
representative example, MillionAID [28] is so far the largest
aerial image dataset and has a million-level volume similar to
ImageNet-1K, making the Remote Sensing Pretraining (RSP)
become possible.
RSP enables training a deep model from scratch, implying
that the candidate model is not necessary to be limited to
the off-the-shelf CNN. In this paper, we also investigate the
impact of RSP together with vision transformers, which have
shown surprisingly good performance in the CV domain.
Compared with convolutions in CNN which are skilled in
modeling locality, the multi-head self-attention (MHSA) in
transformers, e.g., Swin transformer [13], can ﬂexibly capture
diverse global contexts. Recently, ViTAE [14], [29] explores
both convolutions and MHSA for modelling locality and
long-range dependency simultaneously, achieving state-of-the-
art (SOTA) performance on the ImageNet classiﬁcation task
and downstream vision tasks. In addition, it also extracts
multi-scale features through a dilated convolution module and
the stage-wise design, which have been shown effective in
previous works, especially for aerial image interpretation [30].
Since both the CNN and aforementioned vision transformers
can also produce intermediate features in different stages,
which are useful for many downstream tasks, we also in-
vestigate their ﬁnetuning performance of them after RSP on
semantic segmentation, object detection, and change detection.
To achieve these goals, we conduct extensive experiments on
nine popular datasets and have some ﬁndings. The RSP is
an emerging research direction in aerial image understanding,
which however is still underexplored, especially in the context
of vision transformers. We hope this study could ﬁll this gap
and provide useful insights for future research.
The main contribution of this paper is three-fold.
(1) We investigate the impact of remote sensing pretraining
by training on a large-scale remote sensing dataset using
three types of backbone networks, including traditional
CNN, competitive visual transformer models, and the
advanced ViTAE transformers.
(2) We further ﬁnetune the above models that are initialized
with the remote sensing or ImageNet pretraining weights
on four kinds of tasks including scene recognition, se-
mantic segmentation, object detection, and change detec-
tion using a total of nine datasets, and compare them with
other methods.
(3) Experimental results show that typical vision transformer
models can obtain competitive performance or perform
better than CNN. Especially, the ViTAE achieves the
best performance on almost all settings even if compared
with the existing state-of-the-art methods. In addition, a
series of ﬁndings of remote sensing pretraining will be
presented, including the comparison with the traditional
ImageNet pretraining and the performances on different
downstream tasks. These ﬁndings provide useful insights
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 3
for future research.
The remainder of this paper is organized as follows. Section
II introduces the related works, including the aerial scene
recognition methods, especially CNN and vision transformer
related ones, and the existing works of RSP. Section III
describes the implementations of RSP, as well as the employed
large capacity MillionAID dataset and the adopted ViTAE
network. The experiment results on the four tasks and the
related comprehensive analyses are presented in section IV .
Finally, Section V concludes this paper.
II. R ELATED WORK
A. Aerial Scene Recognition
There are a large number of CNN-based approaches to
aerial scene recognition. Many off-the-shelf CNN classiﬁca-
tion models that are pretrained on ImageNet, such as the
VGG [11], ResNet [12], and DenseNet [31], have been used
and further ﬁnetuned on aerial images. Nonetheless, the chal-
lenging aerial scene that possesses inter-class similarity and
intra-class diversity can not be easily interpreted by only
using features of the last layer, which are also considered as
the “global features” compared with the features in previous
layers, since it is also useful to highlight the important scene-
relevant local regions for scene understanding. To address this
issue, [23], [32] jointly exploits the multi-level CNN features,
where the high-level features from deep layers usually have
abundant semantic information, while the low-level features
of the shallow layers tend to provide visual structures. For
example, [32] conducts varied dilated convolutions on multiple
features of VGG to obtain the more effective multiscale
features. In addition, they optimize the category probabilities
by preserving the local maximum and revising others with
the two-dimensional Gaussian-like distribution in a window
to strengthen the local regions. [23] separately applies graph
convolutions on multilayer VGG features, where each pixel
representation is regarded as a node, and the extracted graph
features are concatenated with the last global visual features.
Apart from feature fusion, the attention mechanisms have
also been commonly used in aerial scene recognition since
they can enhance the local features by simulating the human
vision that directly assigns different weights to various areas of
the current scene. The attention modules can be easily inserted
into the CNN [33]. For example, [34] adopts channel attention
and spatial attention modules in parallel like [35] to form a
complementary attention. [32] also employs spatial attention
to further adjust the optimized category probabilities. Another
point for aerial scene recognition is to model the relationships
of different regions. For example, [23] captures the topological
relations of different objects, where the adjacency matrices are
carefully designed. Besides, some interesting topics such as
the multiple instance learning [36], self-distillation [25] and
feature partition [26] have also been explored in aerial scene
recognition research.
Networks before linear layers in scene recognition task can
be used as feature encoders for many downstream tasks, where
the most representative ones for aerial images are semantic
segmentation, object detection, and change detection. Whilesemantic segmentation and object detection are common tasks
in CV , change detection is a speciﬁc task in RS. So far, a large
number of related approaches in the aforementioned ﬁelds
have been developed. Please refer to [37]–[40] for details.
B. Vision Transformer
Transformer is ﬁrst proposed in [41], and has been widely
used in the natural language processing (NLP) ﬁeld [42],
[43]. Besides NLP, the recently proposed vision transformers
inspire a wave of researches [13], [14], [29], [44]–[51] in
the CV ﬁeld. The core component of the vision transformer
is the MHSA, which is the extension of self-attention (SA).
Compared with the convolution operation, the SA can capture
long-range context and the relationships between any different
positions. On the foundation of SA, the MHSA separately
conducts the SAs in different projected subspaces, possess-
ing more powerful representative abilities. ViT [44] is the
pioneer vision transformer, where the input image is split
into ﬁxed-size patches to form tokens, which are then fed
into the MHSA. However, the ﬁxed receptive ﬁeld restricts
its applications on downstream tasks, and the global MHSA
brings high computational complexities. To address the former
issue, PVT [46] adopts the classical pyramid structure, improv-
ing the model transferability with the generated hierarchical
multiscale features. Swin [13] further substitutes the global
MHSA to the shiftable window MHSA (WMHSA), it reduces
the computational overhead signiﬁcantly, achieving excellent
performance on many CV tasks. Nonetheless, it still suffers
from the common issues of vision transformers, such as being
inefﬁcient in modeling locality and scale invariance, which are
exactly the advantages of CNN. Thus, besides Swin, we also
employ another advanced vision transformer named ViTAE
[14], [29], where the intrinsic biases of CNN are introduced
into the transformer. It also adopts a pyramid structure to
generate hierarchical features and local window attention,
achieving better performance on many CV tasks while having
a reasonable computational overhead and memory footprint.
Using vision transformers as the backbone for aerial scene
recognition is still under-explored, while the existing method
merely takes ViT as a branch parallel to CNN [52], implying
an urgent need to further explore the applications of vision
transformers on aerial scene tasks.
C. Remote Sensing Pretraining
Pretraining using RS dataset for aerial scene recognition is
a very intuitive idea. However, to our best knowledge, there
are few explorations in this direction since the insufﬁciency
of large-scale RS datasets like ImageNet. Nevertheless, re-
searchers have attempted to obtain the RS representations from
other resources. For example, GeoKR [53] leverages the global
land cover product as the labels, and they use the mean-teacher
framework to alleviate the inﬂuences of imaging time and res-
olution differences between RS images and geographical ones.
However, forcing alignment of different datasets inevitably
brings errors due to the intrinsic different data distributions.
The scarcity of large capacity RS dataset is mainly in the
aspect of category labels instead of images. In this case, it
4 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
Reduction CellReduction CellReduction CellNormal CellNormal CellP
1/4 1/8 1/16
Class tokenConcatLinear
AdditionSchool
Reduction Cell
1/4 1/8 1/16Linear
School
Normal CellReduction CellNormal CellReduction CellNormal CellReduction CellNormal Cell
1/32GAP(a)
(b)×𝑵𝟏 ×𝑵𝟐 ×𝑵𝟑 ×𝑵𝟒𝑵
Fig. 2. The diagram of the adopted ViTAE models. (a) Original ViTAE [14]. (b) ViTAEv2 [29].
is promising to develop self-supervised pretraining methods
[54]–[57] and some related methods have been developed in
the RS area [27], [58]–[60]. For instance, SeCo [58] leverages
the seasonal changes to enforce consistency between positive
samples, which are the unique characteristics of aerial scenes,
while [59] simultaneously fuses the temporal information and
geographical location into the MoCo-V2 [55] framework.
Moreover, the channel properties [60] and spatial variance
[27] are also explored in some approaches. In this study,
since the adopted MillionAID dataset has the ground truth
labels annotated by experts and does not contain any temporal
information, we directly conduct the supervised pretraining
like the conventional IMP
III. R EMOTE SENSING PRETRAINING
In this section, we ﬁrst provide a brief introduction of
the adopted large-scale RS dataset—MillionAID. Then, we
describe the details of the utilized ViTAE transformer. The
whole RSP procedure will be presented ﬁnally.
A. MillionAID
To our best knowledge, the MillionAID is by far the largest
dataset in the RS area. It contains 100,0848 non-overlapping
scenes, exceeding the competitive counterpart fMoW [61]
and BigEarthNet [62], which separately includes 132,716
and 590,326 scenes. Note that the fMoW contains 1,047,691
images since they provide multiple temporal views for each
scene. In addition, it should be noted that the fMoW and
BigEarthNet are multispectral datasets, while the MillionAID
is an RGB dataset, which is more suitable for existing deep
vision models. The categories of MillionAID consist of a
hierarchical tree that has 51 leaves, which locate on 28 parent
nodes on the second level, while the 28 groups are belonging
to 8 base classes: agriculture land, commercial land, industrial
land, public service land, residential land, transportation land,
unutilized land, and water area, and each leaf category has
about 2,000 ∼45,000 images. This dataset is obtained from the
Google Earth that is made up of diverse sensors including but
not limited to SPOT, IKONOS, WorldView, and Landsat series,resulting in different resolutions. The maximum resolution can
reach 0.5m, while the smallest is 153m. The image size ranges
from 110 ×110 to 31,672 ×31,672.
B. ViTAE
The original ViTAE [14] follows the deep-narrow design
of T2T-ViT [63], which found that simply decreasing channel
dimensions and increasing layer depth can improve the feature
richness of ViT, boosting the performance while reducing the
model size and computational cost. Thus, the original ViTAE
ﬁrstly downsamples the input image to 1/16 size by three
reduction cells. Similar to ViT, a class token is concatenated
with the output of the third reduction cell before adding
an element-wise sinusoid position encoding. Then, multiple
normal cells are stacked, and the feature size is always kept
till the end. The class token feature of the last normal cell is
used for classiﬁcation through a linear layer.
Although the original ViTAE performs well on ImageNet
classiﬁcation, it is unsuitable for transferring to other tasks
like segmentation, detection, pose estimation, and so on, since
it cannot generate abundant intermediate features at different
scales. Thus, the authors propose the ViTAEv2 variant [29],
which adopts the classic stage-wise design of popular back-
bone networks such as ResNet and Swin. Figure 2 shows
the comparison between the original ViTAE and ViTAEv2.
In ViTAEv2, the network is split into multiple stages, usually,
the number is 4. In each stage, the ﬁrst cell is a reduction cell
for downsampling, which is followed by the stacked normal
cells. An average pooling layer is used after the last normal cell
to replace the class tokens. When ﬁnetuning on downstream
tasks, this pooling layer is removed and the remained network
is connected with corresponding task decoders.
In this paper, we employ the ViTAEv2 model for RSP.
Concretely, inspired by Swin [13], some MHSAs in ViTAE are
replaced by the WMHSA to reduce computational cost. Specif-
ically, considering the feature size becomes smaller in later
stages, it is unnecessary to partition the feature for WMHSA.
Thus, only the MHSAs in the ﬁrst two stages are substituted by
WMHSA. It should be noticed that the adopted WMHSA does
not need to be shifted as the original implementation, since the
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 5
D-Conv D-Conv
ConcatGELU GELU
LN
MHSA
LN
FFNG-Conv
BN
SiLU
G-Conv
BN
SiLU Img2Seq
Img2Seq
Seq2Img…PRMPCMD-Conv D-Conv
ConcatGELU GELU
LN
WMHSA
LN
FFNImg2Seq
Img2Seq
Seq2Img…PRMPCM
G-Conv
SiLUG-Conv
BN
SiLU
G-Conv
BN
SiLU
G-Conv
LN
MHSA
LN
FFNG-Conv
BN
SiLUImg2Seq
Img2Seq
Seq2ImgPCM
G-Conv
SiLUG-Conv
BN
SiLULN
WMHSA
LN
FFNG-Conv
BN
SiLUImg2Seq
Img2Seq
Seq2ImgPCM
G-ConvG-Conv
BN
SiLU(a) (b) 
(c) (d) 
Fig. 3. The structures of different cells in ViTAE models. (a) and (c) are the
reduction cell and normal cell in the original ViTAE, while (b) and (d) are
the corresponded variants in the ViTAEv2.
WMHSA is conducted on the merged multiscale feature from
the pyramid reduction module (PRM), where different regions
have communicated with each other through the overlapped
receptive ﬁelds of the sliding dilated convolutions. Besides, it
is also not necessary to use relative positional encoding since
the convolutions already encode the positional information.
Additionally, the SiLU [64] in the last convolutional layer
of the parallel convolutional module (PCM) is also removed
to reduce nonlinearity. The structures and comparisons of
different cells in the original ViTAE and ViTAEv2 have been
shown in Figure 3. For reduction cell, normal cell, PRM and
PCM, readers can refer to [14] and [29] for more details.
In our implementation, we mainly evaluate the “small”
version of the original ViTAE, named ViTAE-S. In addition,
we also adopt the ViTAEv2-S model due to its excellent
representation ability and transferability to downstream tasks.
Table I lists the details of ViTAE-S and ViTAEv2-S. Here,
the length of the corresponded list equals the number of
stages. “Embedding Dim” means the encoding dimension in
PRM, while “Stage Dim” is the channel number of the feature
through the corresponding stage, which is useful for aligning
the related downstream task decoders. The “RC” and “NC”
separately represent the reduction cell and normal cell, where
“Head” is the head number in MHSA or WMHSA, “Group”
represents the number of group convolutions in PCM, andTABLE I
THE HYPERPARAMETER SETTINGS OF DIFFERENT “SMALL ”VERSION
VITAE MODELS . “P” DENOTES PERFORMER ATTENTION [63] WHILE “L”
MEANS THE REDUCTION CELL HAS NO PCM AND ATTENTION . “F”
DENOTES THE ORIGINAL MHSA WHILE “W” DENOTES THE WMHSA.
Network ViTAE-S [14] ViTAEv2-S [29]
Stage 3 4
Downsampling Ratio [4, 2, 2] [4, 2, 2, 2]
Embedding Dim [64, 64, 192] [64, 64, 128, 256]
Stage Dim [96, 192, 384] [64, 128, 256, 512]
RCHead [1, 1, 1] [1, 1, 2, 4]
Group [1, 1, 1] [1, 16, 32, 64]
Type [P, P, L] [W, W, F, F]
NCHead [1, 1, 6] [1, 2, 4, 8]
Group [1, 1, 96] [1, 32, 64, 128]
Type [F, F, F] [W, W, F, F]
Depth [0, 0, 14] [2, 2, 8, 2]
“Type” is the speciﬁc attention types. The ViTAE-S adopts
the performer of T2T-ViT [63] in the ﬁrst two reduction cells.
“L” means this reduction cell does not use PCM and attention.
“F” and “W” separately denote the MHSA and WMHSA in
ViT and Swin. At last, the “Depth” is the number of stacked
normal cells, which is also the Niin Figure 2.
C. Implementation
1) Determine the Pretraining Network: We ﬁrst determine
the type of deep models to be used for RSP. To this end, from
the ofﬁcial training set, we construct a mini-training set and a
mini-evaluation set, which have 9,775 and 225 images, respec-
tively. Note the latter set is formed by randomly selecting 5
images from each category to balance the classes. For CNN,
the classic ResNet-50 [12] is employed. Since this research
mainly explores the performance of vision transformer models
with RSP, a series of typical vision transformer based networks
including DeiT-S [45], PVT-S [46], and Swin-T [13], are also
evaluated. The selection of a speciﬁc version is to guarantee
these models have a similar amount of parameters compared
with the ViTAE-S model. In addition, we also include ViT-B
[44] for reference since ViT is the most basic model of vision
transformers.
All models are trained with 300 epochs and batch size 16.
We adopt the AdamW optimizer where the momentum is set
to 0.9 and the weight decay is set to 0.05. The initial learning
rate is 1e-3, which is adjusted by the cosine scheduling
policy:currentlr=minlr+1
2(initiallr−minlr)(1 +
cos(iter
max iterπ)), whereminlris 5e-6. In addition, we set
the warming up epochs to 5, where the learning rate is set
to 5e-7. Following the typical IMP, the input image is resized
to 224 ×224 by randomly cropping during training, while
during testing, the image at the same size is obtained through
“center crop”. In addition, a series of data argumentations
including AutoAugment [65], Random Erasing [66], Mixup
[67], CutMix [68], and color jitter are applied to improve the
training performance. The top-1 accuracy and top-5 accuracy
are used as the evaluation metrics. In addition, all models are
implemented on a single NVIDIA Tesla V100 GPU, and the
results are shown in Table II.
6 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
TABLE II
RESULTS OF DIFFERENT MODELS ON THE MINI -EVALUATION SET . THEY ARE TRAINED ON THE MINI -TRAINING SET FROM MILLION AID.
Model Acc@1 Acc@5 Param(M) Training Time (hh:mm:ss)
ResNet-50 [12] 80.8 96.1 23.6 16:42:58
DeiT-S [45] 72.9 92.9 21.7 16:57:39
ViT-B [44] 78.4 92.1 114.4 17:41:03
PVT-S [46] 80.0 94.1 23.9 18:06:42
Swin-T [13] 84.7 93.7 27.6 17:49:06
ViTAE-S [14] 85.9 96.9 22.8 24:00:35
ViTAEv2-S [29] 88.2 96.1 18.8 23:57:30
It can be seen that despite the ViT-B has the most param-
eters, it performs not better than the classic ResNet-50. The
DeiT-S performs the worst since we do not adopt the teacher
model. Because our task is pretraining using RS images,
obtaining the corresponding teacher model is our target instead
of the prerequisite. By introducing the design paradigm of the
feature pyramid, PVT-S improves the accuracy compared with
ViT-B. On this foundation, the original ViTAE-S further con-
siders the locality and scale-invariance modeling, which are the
inductive biases of conventional CNN models. However, it cost
much training time since the token number in the early RCs is
large, requiring more computations. The Swin-T addresses this
issue by restricting the MHSA in ﬁxed windows and adopts
the image shifting to implicitly promote the communications
between windows. By taking the advantage of WMHSA, the
ViTAEv2-S achieves the best performance and it exceeds the
second place by 2.3% top-1 accuracy.
The procedure of model determination is shown as follows.
For the ViTAE models, we choose the strongest one to expect
good performance in downstream tasks such as the aerial
scene recognition when adopting RSP, i.e., the ViTAEv2-S.
For comparison, the ResNet-50 is selected as the representative
network in conventional CNN, and the RS pretrained ResNet-
50 can also provide a group of new CNN related baselines on a
series of aerial datasets. The DeiT-S and ViT-B are eliminated
because of the low accuracy and a large number of parameters,
and they are difﬁcult to be transferred into downstream tasks
because of the design of stacking transformers. The Swin
can be regarded as building on the foundation of PVT by
substituting the global MHSA with the shiftable WMHSA.
Since the top-1 accuracy of Swin is larger than PVT, and
Swin-T requires less training time, we also choose Swin-T
in the subsequent experiments.
2) Obtain the Suitable Weights: After determining the
model candidates, we conduct the RSP to obtain the pretrained
weights. Concretely, maintaining the category balance, we
randomly choose 1,000 images in each category of the Million-
AID dataset to form the validation set that has 51,000 images,
achieving a similar volume compared with the ImageNet
validation set, which contains 50,000 images. The remaining
949,848 images are used for training. Although the number
of images and categories for RSP are less than those of the
ImageNet training set, it still can perform to be competitive
or even achieves SOTA results on aerial scene tasks, whose
details will be presented later.
To obtain suitable pretraining weights, we separately train
the ViTAEv2-S model under the conﬁguration of different
epochs. The basic learning rate is 5e-4, and the batch sizeTABLE III
RESULTS OF VITAE V2-S WITH DIFFERENT SETTINGS OF TRAINING
EPOCH ON THE MILLION AID VALIDATION SET .
Epoch Acc@1 Acc@5
5 94.53 99.41
10 96.45 99.64
15 97.38 99.74
20 98.00 99.81
40 98.64 99.86
60 98.87 99.83
80 98.90 99.85
100 98.97 99.88
TABLE IV
RESULTS OF THE CANDIDATE MODELS FOR THE SUBSEQUENT
FINETUNING EXPERIMENTS ON THE MILLION AID VALIDATION SET .
Epoch Acc@1 Acc@5
ResNet-50
40 97.99 99.81
120 98.76 99.83
300 98.99 99.82
Swin-T
40 97.80 99.84
120 98.63 99.89
300 98.59 99.88
ViTAEv2-S
40 98.64 99.86
100 98.97 99.88
is set to 384. The remained settings are the same as the
previous experiment. All experiments are conducted with 4
V100 GPUs, and the results are shown in Table III. According
to the results, it can be observed that the model starts saturation
after about 40 epochs, since it only improves 0.64% top-1
accuracy compared with training 20 epochs, while the next
20 epochs only bring a gain of 0.23%. Thus, the network
weights trained with 40 epochs are ﬁrstly chosen as the RSP
parameters of ViTAEv2-S to be applied to the subsequent
tasks. Intuitively, the model achieving good performance on
the large-scale pretraining dataset will also perform well on
the downstream tasks. Therefore, we also use the network
weights trained with 100 epochs in the downstream tasks.
These models are separately denoted with the sufﬁx “E40”
and “E100”.
For ResNet-50 and Swin-T, we follow [13] to conﬁgure
the training settings, where the networks are trained for 300
epochs. In the experiments, we observe that the top-1 accuracy
of Swin-T-E120 on the validation set is roughly equivalent
to ViTAEv2-S-E40. Thus, the training weights of Swin-T-
E120 are selected. Similarly, we also choose the ﬁnal network
weights Swin-T-E300 as a comparison with ViTAEv2-S-E100.
To make the experiments fair, the weights of ResNet-50 and
Swin-T that are trained with 40 epochs are also considered,
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 7
since they are trained using the same number of epochs with
the ViTAEv2-S-E40.
The ﬁnal pretraining models are listed in Table IV. It can be
seen that the validation accuracies are almost increasing with
the increase of training epochs. However, the performance of
Swin-T-E300 is not as well as Swin-T-E120. Nonetheless, we
still keep it since it may have stronger generalization by seeing
more diverse samples.
IV. F INETUNING ON DOWNSTREAM TASKS
In this section, the pretrained models are further ﬁnetuned
on a series of downstream tasks, including recognition, seman-
tic segmentation, object detection in aerial scenes as well as
change detection. It should be clariﬁed that models for scene
recognition in this section are trained and evaluated on com-
monly used aerial scene datasets rather than the MillionAID
engaging for RSP.
A. Aerial Scene Recognition
We ﬁrst introduce the used scene recognition datasets and
the implementation details, then present the experimental
results and analyses.
1) Dataset: The three most popular scene recognition
datasets including the UC Merced Land Use (UCM) dataset
[69], the Aerial Image Dataset (AID) [70], and the benchmark
for RS Image Scene Classiﬁcation that is created by North-
western Polytechnical University (NWPU-RESISC) [21], are
used to comprehensively evaluate the impact of RSP and the
representation ability of the above adopted backbones.
•UCM: This is the most important dataset for scene
recognition. It contains 2,100 images whose sizes are
all 256 ×256 and have a pixel resolution of 0.3m. The
2,100 images equally belong to 21 categories. Thus, each
category has 100 images. All samples are manually ex-
tracted from the large images in the USGS National Map
Urban Area Imagery Database collected from various
urban areas around the country.
•AID: This is a challenging dataset, which is generated by
collecting the images from multi-source sensors on GE.
It has high intra-class diversities since the images are
carefully chosen from different countries. And they are
extracted at different times and seasons under different
imaging conditions. It has 10,000 images at the size of
600×600, belonging to 30 categories.
•NWPU-RESISC: This dataset is characterized by a great
number of samples. It contains 31,500 images and 45
categories in total, where each category has 700 samples.
Each image has 256 ×256 pixels. The spatial resolutions
are varied from 0.2m to 30m. Some special landforms,
such as islands, lakes, regular mountains, and snow
mountains, maybe in lower resolutions.
2) Implementation Detail and Experimental Setting: The
training settings are the same as previous experiments. The
training epoch and batch size are set to 200 and 64, respec-
tively. These experiments are conducted on a single V100
GPU. Following [34], ﬁve settings of these three datasets are
adopted to comprehensively evaluate the RS pretrained modelsand make the experiments become convincible, including
UCM (8:2), AID (2:8), AID (5:5), NWPU-RESISC (1:9), and
NWPU-RESISC (2:8). Note the m:nmeans 10m%samples
are used for training, while the others form the testing set.
Similar to the previous section, the images in each category
are proportionally divided into two groups that are separately
used for training and evaluation, respectively. Besides the
above three backbones we selected, the ImageNet pretrained
ResNet-50 and the ResNet-50 pretrained by SeCo [58] – an RS
self-supervised method considering seasonal variation, are also
adopted for a fair comparison. When implementing ﬁnetuning
on each scene recognition task, only the neuron number of
the last linear layer is changed to match the categories of
the target dataset. The overall accuracy (OA), which is the
most commonly used criterion in the aerial scene recognition
community by counting the proportion of the correct classiﬁed
images relative to all images in the testing set, is utilized in the
experiments. The models are repeatedly trained and evaluated
ﬁve times at each setting, and the average value µand standard
deviationσof the results in different trials are recorded as
µ±σ.
3) Experimental Results: Quantitative Results and Anal-
yses: Table V presents the results of the above selected
backbones pretrained using different methods and other SOTA
methods. Since this research only focuses on the pretraining
of deep networks, especially the vision transformers. We only
lists the DL based aerial scene recognition methods. For
convenience, the “IMP” and “RSP” are used to represent
“ImageNet Pretraining” and “Remote Sensing Pretraining”,
respectively. It can be seen that the methods are split into ﬁve
groups. The ﬁrst group is the methods that adopt ResNet-50
as the backbone network, where the ResNet-50 is initialized
by the ImageNet pretrained weights. This group can be used
to compare with the third group. The second group includes
the recent existing advanced methods whose backbone is
other popular networks except for ResNet-50, such as the
ImageNet pretrained VGG-16, ResNet-101, DenseNet-121,
and so on. Then, the ResNet-50, Swin-T, and ViTAEv2-S
networks, whose pretrained weights are obtained by IMP, RSP,
or SeCo, form the last three groups, respectively. In addition,
it should be noted that besides the network types, the weights
pretrained for different epochs are also considered. The bold
fonts in the last three groups mean the best results in each
group, while “ *” denotes the best among all models (same
meanings in other tasks).
On the foundation of ImageNet pretrained ResNet-50, many
methods are developed, which have been shown in the ﬁrst
group. Among these methods, many ﬂexible and advanced
modules have been explored. For example, the attention mech-
anisms (CBAM [35], EAM [71], MBLANet [34]), where
speciﬁc channels or spatial positions of the features are high-
lighted, and multiscale features (F2BRBM [72] and GRMANet
[73]), where the intermediate features are also employed.
In addition, the self-distillation technology combined with
specially designed loss functions (ESD-MBENet [25]) and
the multibranch siamese networks (IDCCP [74]) have also
been applied. While in the second group, the more diverse
frameworks with various backbones are presented. Besides
8 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
TABLE V
RESULTS OF THE SELECTED MODELS AND SOTA METHODS ON THE THREE SCENE RECOGNITION DATASETS UNDER DIFFERENT SETTINGS . THE BOLD
FONTS IN THE LAST THREE GROUPS MEAN THE BEST RESULTS ,WHILE “*”DENOTES THE BEST AMONG ALL MODELS .
Model Publication UCM (8:2) AID (2:8) AID (5:5) NWPU-RESISC (1:9) NWPU-RESISC (2:8)
CBAM [35] ECCV2018 99.04±0.23 94.66±0.39 96.90±0.04 92.10±0.04 94.26±0.12
EAM (IMP-ResNet-50) [71] GRSL2021 98.98±0.37 93.64±0.25 96.62±0.13 90.87±0.15 93.51±0.12
F2BRBM (IMP-ResNet-50) [72] JSTARS2021 99.58±0.23 96.05±0.31 96.97±0.22 92.74±0.23 94.87±0.15
MBLANet (IMP-ResNet-50) [34] TIP2021 99.64±0.12 95.60±0.17 97.14±0.13 92.32±0.15 94.66±0.11
GRMANet (IMP-ResNet-50) [73] TGRS2021 99.19±0.10 95.43±0.32 97.39±0.24 93.19±0.42 94.72±0.25
IDCCP (IMP-ResNet-50) [74] TGRS2021 99.05±0.20 94.80±0.18 96.95±0.13 91.55±0.16 93.76±0.12
ESD-MBENet-v1 (IMP-ResNet-50) [25] TGRS2021 99.81±0.10 96.00±0.15 98.54±0.17 92.50±0.22 95.58±0.08
ESD-MBENet-v2 (IMP-ResNet-50) [25] TGRS2021 99.86±0.12 95.81±0.24 98.66±0.20 93.03±0.11 95.24±0.23
ARCNet (IMP-VGG-16) [75] TGRS2019 99.12±0.40 88.75±0.40 93.10±0.55 — —
SCCov (IMP-VGG-16) [76] TNNLS2019 99.05±0.25 93.12±0.25 96.10±0.16 89.30±0.35 92.10±0.25
KFBNet (IMP-DenseNet-121) [77] TGRS2020 99.88±0.12 95.50±0.27 97.40±0.10 93.08±0.14 95.11±0.10
GBNet (IMP-VGG-16) [24] TGRS2020 98.57±0.48 92.20±0.23 95.48±0.12 — —
MG-CAP (IMP-VGG-16) [78] TIP2020 99.00±0.10 93.34±0.18 96.12±0.12 90.83±0.12 92.95±0.13
EAM (IMP-ResNet-101) [71] GRSL2021 99.21±0.26 94.26±0.11 97.06±0.19 91.91±0.22 94.29±0.09
IMP-ViT-B [44] ICLR2021 99.28±0.23 93.81±0.21 96.08±0.14 90.96±0.08 93.96±0.17
MSANet (IMP-ResNet-101) [79] JSTARS2021 98.96±0.21 93.53±0.21 96.01±0.43 90.38±0.17 93.52±0.21
CTNet (IMP-MobileNet-V2+IMP-ViT-B) [52] GRSL2021 — 96.25±0.10 97.70±0.11 93.90±0.14 95.40±0.15
LSENet (IMP-VGG-16) [32] TIP2021 99.78±0.18 94.41±0.16 96.36±0.19 92.23±0.14 93.34±0.15
DFAGCN (IMP-VGG-16) [23] TNNLS2021 98.48±0.42 — 94.88±0.22 — 89.29±0.28
MGML-FENet (IMP-DenseNet-121) [26] TNNLS2021 99.86±0.12 96.45±0.18 98.60±0.04 92.91±0.22 95.39±0.08
ESD-MBENet-v1 (IMP-DenseNet-121) [25] TGRS2021 99.86±0.12 96.20±0.15 98.85±0.13* 93.24±0.15 95.50±0.09
ESD-MBENet-v2 (IMP-DenseNet-121) [25] TGRS2021 99.81±0.10 96.39±0.21 98.40±0.23 93.05±0.18 95.36±0.14
IMP-ResNet-50 [12] CVPR2016 98.81±0.23 94.67±0.15 95.74±0.10 90.09±0.13 94.10±0.15
SeCo-ResNet-50 [58] ICCV2021 97.86±0.23 93.47±0.08 95.99±0.13 89.64±0.17 92.91±0.13
RSP-ResNet-50-E40 Ours 99.43±0.24 95.88±0.07 97.29±0.07 92.86±0.09 94.40±0.05
RSP-ResNet-50-E120 Ours 99.52±0.15 96.60±0.04 97.78±0.08 93.76±0.03 94.97±0.07
RSP-ResNet-50-E300 Ours 99.48±0.10 96.81±0.03 97.89±0.08 93.93±0.10 95.02±0.06
IMP-Swin-T [13] ICCV2021 99.62±0.19 96.55±0.03 98.10±0.06 92.73±0.09 94.70±0.10
RSP-Swin-T-E40 Ours 99.24±0.18 95.95±0.06 97.52±0.04 91.22±0.18 93.30±0.08
RSP-Swin-T-E120 Ours 99.52±0.00 96.73±0.07 98.20±0.02 92.02±0.14 93.84±0.07
RSP-Swin-T-E300 Ours 99.52±0.00 96.83±0.08 98.30±0.04 93.02±0.12 94.51±0.05
IMP-ViTAEv2-S [29] arXiv2022 99.71±0.10 96.61±0.07 98.08±0.03 93.90±0.07 95.29±0.12
RSP-ViTAEv2-S-E40 Ours 99.71±0.10 96.72±0.06 97.92±0.06 94.12±0.07 95.35±0.03
RSP-ViTAEv2-S-E100 Ours 99.90±0.13* 96.91±0.06* 98.22±0.09 94.41±0.11* 95.60±0.06*
the traditional CNN, the recent ViT has also been applied in
some works. Compared with the IMP-ViT-B, the RSP-Swin-
T-E300 performs better, although the former model has more
trainable parameters. It can be observed that the backbones are
changing over time. The VGG-16 used in the early years is
gradually replaced by the deeper networks such as ResNet-101
or DenseNet-121 due to their better representation ability.
In the implemented networks, the SeCo-ResNet-50 performs
the worst compared with its counterparts, it may be because
there still exists a gap between the Sentinel-2 multispectral
images where the SeCo trained on with the RGB images
for aerial scenes recognition. Compared with the ImageNet
pretrained ResNet-50, our RS pretrained ResNet-50 improves
the accuracy on all settings. These results imply that RSP
brings a better starting point for the optimization of the
subsequent ﬁnetuning process, attributing to the aerial im-
ages used for pretraining compared with the natural images
in ImageNet. Similarly, the RSP-Swin-T outperforms IMP-
Swin-T on three settings and achieves comparable results
on the other two settings. In addition, the ResNet-50 and
Swin-T can perform to be competitive compared to other
complicated methods by only using the RSP weights without
changing the network structures. Besides, when comparing
the ImageNet pretrained ResNet-50 and Swin-T, we can ﬁnd
that the IMP-Swin-T performs better in all settings since the
vision transformers have stronger context modeling capability.
While being initialized by RSP weights, the ResNet becomes
to be more competitive and surpasses the IMP-Swin-T on
the AID (2:8), NWPU-RESISC (1:9), and NWPU-RESISC(2:8) settings, showing the beneﬁt of RSP again. Owing to
the excellent representation ability of ViTAEv2-S, which has
both the locality modeling ability and long-range dependency
modeling ability, it outperforms both ResNet-50 and Swin-
T on almost all the settings, regardless of IMP and RSP.
Moreover, the RSP-ViTAEv2-S achieves the best performance
compared with all other methods on almost all settings except
for the AID (5:5), though on which it also delivers comparable
performance with the best one, i.e., RSP-Swin-T-E300.
In our experiments, RSP helps the networks obtain bet-
ter performance on small datasets, it may be because the
models are easier to converge when adopting the RS pre-
trained weights. While for the case where training samples
are abundant, like AID (5:5), the representation ability of
deeper models can be fully exploited. For example, the
DenseNet-121 based ESD-MBENeT obtain the best accuracy.
Nevertheless, it should be noted that only the feature output
from the last layer of RSP-ResNet-50, RSP-Swin-T, or RSP-
ViTAEv2-S is used for classiﬁcation, and it is expected that
their performance can be further improved when employing
the multilayer intermediate features. In this sense, these RS
pretrained models can serve as effective backbones for future
research in the aerial recognition ﬁeld. Furthermore, Table
V also shows that the models pretraining with more epochs
will probably have stronger representation abilities. Since
RSP-ResNet-50-E40 and RSP-Swin-T-E40 fall behind their
counterparts with more epochs, we only evaluate the “E120”
and “E300” pretrained weights for these two types of networks
in the rest experiments, while for ViTAEv2-S, both the “E40”
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 9
Terrace
Mountain
River
Bridge
Church
Airplane
Stadium
Airport
Thermal 
power 
station
Sparse
residential
Medium
residential
Dense
residential
School-1
School-2
(b) (c) (d) (e) (f) (g) (h)
 (a)
Fig. 4. Response maps of the evaluated models on different scenes. (a)
Original image. (b) IMP-ResNet-50. (c) SeCo-ResNet-50. (d) RSP-ResNet-50.
(e) IMP-Swin-T. (f) RSP-Swin-T. (g) IMP-ViTAEv2-S. (h) RSP-ViTAEv2-S.
and “E100” weights are still used.
Qualitative Results and Analyses: Figure 4 shows the
response maps of the above evaluated models using Grad-
CAM++ [80] on images from various scenes. The warmer the
color is, the higher the response is. To better show the impact
of RSP, we use the pretrained weights of “E300” for ResNet-
50 and Swin-T, and the weights of “E100” for ViTAEv2-S.
The ﬁrst three rows are the natural landscapes, and the scenes
in 4-8 rows mainly contain speciﬁc foreground objects, while
the next six rows present some scenes with different artiﬁcial
constructions. For example, the “Thermal power station” scene
includes not only chimneys but also cooling towers.
Corresponding to the quantitative results in Table V, the
response maps of SeCo-ResNet-50 are scattered and they can
not precisely capture the semantic-relevant areas, especially
in the natural landscapes or complex scenes with artiﬁcial
constructions. Compared with the IMP-ResNet-50, the RSP-
ResNet-50 pays more attention to the important targets. It
(a)
 (b)
(c)
 (d)
Fig. 5. (a), (b) and (c) are separately the training loss curves of ResNet-50,
Swin-T and ViTAEv2-S, where the loss is recorded every 10 iterations. (d)
is the testing accuracy curves of these models. All curves are obtained by
training in the setting of UCM (8:2). The curves in (a), (b), and (c) have been
smoothed by moving average.
implies that RSP facilitates ResNet-50 to learn better semantic
representations, probably by seeing semantic-similar images
in the MillionAID dataset. Compared to the ResNet-50, the
Swin-T has a better context modeling ability by attending
faraway regions with the help of MHSA. Thus, their range
of high response areas is wider. Surprisingly, the IMP-Swin-T
mainly concentrates on background context, but the foreground
responses have been enhanced when adopting the RSP. By
combining the advantages of CNN and vision transformers, the
ViTAEv2-S achieves a comprehensive perception of the whole
scene. Especially, the RSP-ViTAEv2-S can better recognize
the typical natural RS scenes, such as terrace, mountain and
river. In the foreground object based scenes, compared with
RSP-ResNet-50, the RSP-ViTAEv2-S not only focuses on the
primary objects, but it also considers the related regions in
the background. While on the objects, the RSP-ViTAEv2-S
assigns higher attention, such as the airplane with a warmer
color compared with IMP-ViTAEv2-S. In the residential areas
with complex object distributions, the RSP-ViTAEv2-S can
correctly capture the sparse buildings and connect these re-
gions to form a holistic representation, effectively perceiving
the overall information of the scene. In the ﬁrst image of the
school scene, the RSP-ViTAEv2-S simultaneously focuses on
the playground and the surroundings, surpassing the SeCo-
ResNet-50. As for the “school-2” image that is even difﬁcult
to be recognized by humans, these models show different
recognition priorities. For example, the RSP-ViTAEv2-S not
only considers the campus (it can be possibly distinguished by
the irregular shape of buildings) like the IMP-ResNet-50, but
also notices the surrounding roads. The results in Table V and
Figure 4 validate the effectiveness of RSP and the superiority
of vision transformers in the aerial scene recognition task.
We also provide training loss curves and testing accuracy
curves to investigate the performances of different pretraining
10 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
methods during the training phase. Here, the setting of UCM
(8:2) is chosen as our example. The corresponding results
have been displayed in Figure 5, where the loss curves of
three kinds of networks are separately plotted. It can be seen
that SeCo-ResNet-50 performs the worst and it has the largest
initial loss, further conﬁrming our aforementioned hypothesis
that there is a large gap between Sentinel-2 multispectral
images and the used RGB aerial images, although they are both
RS images. Compared with the IMP, it can be observed that the
RS pretrained models have better starting points, proving our
intuition that the RS pretrained weights are easier to transfer
between aerial scenes. It is also noteworthy that the shapes
of these curves are similar for the same network, implying
the unique characteristics of different network structures. We
can also ﬁnd the advanced structures enable ViTAEv2-S to
reduce the performance gap indicated by the different starting
points between IMP and RSP, while other networks failed.
In addition, we can also ﬁnd that the RSP accelerates the
learning of ResNet-50 and makes the corresponding accuracy
curve similar to Swin-T compared with IMP-ResNet-50. For
Swin-T, the RSP also helps it converge fast. When adopting
RSP, among all models, the advanced transformer network
ViTAEv2-S can simultaneously achieve the best accuracy and
the fastest convergence speed.
B. Aerial Semantic Segmentation
Aerial semantic segmentation is also a classiﬁcation task
like aerial scene recognition but at the pixel-level rather than
the scene-level. We then evaluate the above three models on
the aerial semantic segmentation task, including the scene
parsing and object segmentation subtasks, where the former
focuses on labeling each pixel of the whole scene, while the
latter emphasizes the segmentation of foreground objects.
1) Dataset: We use the ISPRS Potsdam dataset1and the
large-scale segmentation benchmark — iSAID [81], to serve
as the testbed of the corresponded subtasks, respectively.
•Potsdam: This dataset is released by ISPRS Commission
WG II/4. It covers a large scene that is collected over
3.42 km2area of the Potsdam city. It contains 38 images,
whose average size is 6,000 ×6,000 pixels, and the
resolution is 0.5m. Among them, the training and testing
sets separately have 24 and 14 images. There are 6
categories included in these scenes, namely impervious
surface, building, low vegetation, tree, car, and clutter.
•iSAID: This is a large-scale dataset that is mainly for in-
stance segmentation. Also, it provides the semantic mask
including 15 foregrounds and 1 background category over
the aerial objects. It consists of 2,806 high-resolution
images that range from 800 ×800 to 4,000 ×13,000
pixels. The training, validation, and test set separately
have 1,411, 458, and 937 images. In this paper, only the
validation set is used for evaluation since the testing set
is unavailable.
1https://www.isprs.org/education/benchmarks/UrbanSemLab/2d-sem-label-
potsdam.aspx
(a) (b) (c)Fig. 6. Visual samples from different datasets: (a) MillionAID, (b) Potsdam,
and (c) iSAID.
2) Implementation Detail and Experimental Setting: We
adopt different settings for the above backbone networks by
following the common practice. Concretely, the ResNet based
networks are trained with the mini-batch stochastic gradient
descent with momentum (SGDM) strategy, where the initial
learning rate, weight decay, and momentum are separately set
to 0.01, 0.0005, and 0.9. The learning rate is optimized by the
polynomial scheduler: currentlr=minlr+initiallr·(1−
iter
max iter)power, whereminlr= 0.0001,power = 0.9. While
the vision transformers, such as the Swin-T and ViTAEv2-
S, are trained using the AdamW optimizer, whose learning
rate and weight decay are 6e-5 and 0.01. The learning rate
schedule adopts the polynomial decay policy with a power
of 1.0 and minlrof 0. They also have a linear warming
up stage at the ﬁrst 1,500 iterations with the initial learning
rate of 1e-6. For a fair comparison, all networks are training
80k iterations with a batch size of 8. Following [13], [14], we
use the UperNet [15] as the uniﬁed segmentation framework
for all the pretrained backbones since the output stride equals
32. All methods are implemented based on mmsegmentation
[82]. For the convenience of training, the Potsdam and iSAID
are separately sampled and cropped into patches with a size
of 512 ×512 and 896 ×896 with a stride of 384 and
512, respectively. We use the random horizontal ﬂipping data
augmentation strategy. Following the evaluation protocol in the
aerial segmentation community, for the Potsdam dataset, we
report the OA, mean F1 score (mF1), and per class F1 score.
Note that the clutter category is regarded as the background
and ignored during computing loss and evaluation metrics.
While for the iSAID dataset, the intersection over union (IoU)
of foreground categories and the average IOU of all classes
(including background) are calculated. All evaluations are
conducted on a single scale for a fair comparison.
3) Experimental Results: Quantitative Results and Anal-
yses: Table VI-VII present the segmentation results of our
methods and other SOTA methods. It can be seen that when
changing the backbone from ResNet-50 to Swin-T, and then
to ViTAEv2-S, the performance is increased. The results are
consistent with the aforementioned scene recognition results,
showing the better representation ability of vision transform-
ers. Although the ViTAEv2-S obtains the highest OA on the
Potsdam dataset, its mF1 is not as well as LANet [92]. From
Table VII, we can ﬁnd that the scores of the “Car” category of
the selected models are worse than other methods. We suspect
that it may be because of the encoder-decoder structure and
the rough feature fusion strategy in the UperNet, where the
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 11
TABLE VI
RESULTS OF THE UPERNET SEGMENTATION MODEL WITH DIFFERENT BACKBONES AND SOTA METHODS ON THE TESTING SET OF THE POTSDAM
DATASET .
Method Backbone OA mF1F1 score per category
Imper. surf. Building Low veg. Tree Car
FCN [83] IMP-VGG-16 85.59 87.61 88.61 93.29 83.29 79.83 93.02
S-RA-FCN [84] IMP-VGG-16 88.59 90.17 91.33 94.70 86.81 83.47 94.52
Multi-ﬁlter CNN [85] IMP-VGG-16 90.65 85.23 90.94 96.98 76.32 73.37 88.55
FCN [83] IMP-ResNet-50 89.42 88.66 91.46 96.63 85.99 86.94 82.28
DANet [86] IMP-ResNet-50 89.72 89.14 91.61 96.44 86.11 88.04 83.54
PSPNet [87] IMP-ResNet-50 89.45 90.51 91.61 96.30 86.41 86.84 91.38
DeeplabV3+ [88] IMP-ResNet-50 89.74 90.94 92.35 96.77 85.22 86.79 93.58
ResT [89] IMP-ResNet-50 89.13 90.89 91.14 95.11 86.30 87.27 94.63
EaNet [90] IMP-ResNet-50 90.15 91.73 92.87 96.30 86.16 87.99 95.30 *
BotNet [91] IMP-ResNet-50 90.42 91.77 92.34 96.30 87.32 * 88.74 * 94.17
LANet [92] IMP-ResNet-50 90.84 91.95 * 93.05 97.19 * 87.30 88.04 94.19
UperNet IMP-ResNet-50 90.64 89.96 92.30 96.14 85.93 85.66 89.76
UperNet SeCo-ResNet-50 89.64 89.03 91.21 94.92 85.12 84.89 89.02
UperNet RSP-ResNet-50 90.61 89.94 92.42 96.15 85.75 85.49 89.87
UperNet IMP-Swin-T 91.17 90.60 92.94 96.66 86.54 85.87 90.98
UperNet RSP-Swin-T 90.78 90.03 92.65 96.35 86.02 85.39 89.75
UperNet IMP-ViTAEv2-S 91.60* 91.00 93.34* 96.84 87.28 86.38 91.18
UperNet RSP-ViTAEv2-S 91.21 90.64 93.05 96.62 86.62 85.89 91.01
TABLE VII
RESULTS OF THE UPERNET SEGMENTATION MODEL WITH DIFFERENT BACKBONES AND SOTA METHODS ON THE VALIDATION SET OF THE I SAID
DATASET .
Method Backbone mIOUIOU per category1
Ship ST BD TC BC GTF Bridge LV SV HC SP RA SBF Plane Harbor
FCN [83] IMP-VGG-16 41.7 51.7 22.9 26.4 74.8 30.2 27.9 8.2 49.3 37.0 0 30.7 51.9 52.1 62.9 42.0
UNet [93] - 39.2 49.0 0 36.5 78.6 22.9 5.5 7.5 49.9 35.6 0 38.0 46.5 9.7 74.7 45.6
DenseASPP [94] IMP-DenseNet-121 56.8 61.1 50.0 67.5 86.1 56.6 52.3 29.6 57.1 38.4 0 43.3 64.8 74.1 78.1 51.1
DenseUNet [95] IMP-DenseNet-121 58.7 66.1 50.4 76.1 86.2 57.7 49.5 33.9 54.7 46.2 0 45.1 65.9 71.9 82.2 54.6
Semantic FPN [96] IMP-ResNet-50 59.3 63.7 59.5 71.8 86.6 57.8 51.6 34.0 59.2 45.1 0 46.4 68.7 73.6 80.8 51.3
ReﬁneNet [97] IMP-ResNet-50 60.2 63.8 58.6 72.3 85.3 61.1 52.8 32.6 58.2 42.4 23.0 43.4 65.6 74.4 79.9 51.1
PSPNet [87] IMP-ResNet-50 60.3 65.2 52.1 75.7 85.6 61.1 60.2 32.5 58.0 43.0 10.9 46.8 68.6 71.9 79.5 54.3
DeeplabV3 [98] IMP-ResNet-50 59.0 59.7 50.5 77.0 84.2 57.9 59.6 32.9 54.8 33.7 31.3 44.7 66.0 72.1 75.8 45.7
DeeplabV3+ [88] IMP-ResNet-50 60.8 63.9 52.5 72.8 84.9 56.5 58.9 32.2 59.1 42.9 31.4 46.1 67.7 72.9 79.8 52.6
EMANet [99] IMP-ResNet-50 55.4 63.1 68.4 66.2 82.7 56.0 18.8 42.1 58.2 41.0 33.4 38.9 46.9 46.4 78.5 47.5
ASP-OCNet [100] IMP-ResNet-50 40.2 47.3 40.2 44.4 65.0 24.1 29.9 27.1 46.3 13.6 10.3 34.6 37.9 41.4 68.1 38.0
DANet [86] IMP-ResNet-50 57.5 60.2 63.0 71.4 84.7 50.9 52.5 28.6 57.5 42.1 30.4 46.1 40.6 63.3 80.9 48.8
CCNet [101] IMP-ResNet-50 58.3 61.4 65.7 68.9 82.9 57.1 56.8 34.0 57.6 38.3 31.6 36.5 57.2 75.0 75.8 45.9
EncNet [102] IMP-ResNet-50 58.9 59.7 64.9 70.0 84.2 55.2 46.3 36.8 57.2 38.7 34.8 42.4 59.8 69.8 76.1 48.0
HRNet [103] IMP-HRNetW-18 61.5 65.9 68.9 74.0 86.9 59.4 61.5 33.8 62.1 46.9 14.9 44.2 52.9 75.6 81.7 52.2
RANet [104] IMP-ResNet-50 62.1 67.1 61.3 72.5 85.1 53.2 47.1 45.3* 60.1 49.3 38.1 41.8 70.5 58.8 83.1 55.6
AlignSeg [105] IMP-ResNet-50 62.1 67.4 68.9 76.2 86.2 62.1 52.0 28.7 60.7 50.3 31.2 45.7 56.2 71.2 82.9 54.8
OCR [106] IMP-HRNet-W48 62.6 67.8 70.7 73.6 87.9 63.4 47.7 33.1 61.4 49.6 30.4 48.4 59.5 72.8 83.3 53.3
HMANet [107] IMP-ResNet-50 62.6 65.4 70.9 74.7 88.7 60.5 54.6 29.0 59.7 50.3 32.6 51.4 62.9 70.2 83.8 51.9
FarSeg [108] IMP-ResNet-50 63.7 65.4 61.8 77.7 86.4 62.1 56.7 36.7 60.6 46.3 35.8 51.2 71.4* 72.5 82.0 53.9
FactSeg [109] IMP-ResNet-50 64.8 68.3 56.8 78.4* 88.9* 64.9* 54.6 36.3 62.7 49.5 42.7 51.5* 69.4 73.6 84.1 55.7
UperNet IMP-ResNet-50 61.9 65.9 73.9 68.1 70.7 57.3 52.5 39.2 61.2 48.8 34.3 44.5 62.1 76.8 83.8 52.2
UperNet SeCo-ResNet-50 57.2 63.9 71.7 66.9 69.9 54.5 45.9 38.9 58.2 44.8 33.2 9.3 52.3 71.6 83.3 51.4
UperNet RSP-ResNet-50 61.6 64.2 75.9 68.8 69.9 58.5 54.4 40.2 59.6 47.5 32.1 43.8 65.4 76.5 82.8 51.5
UperNet IMP-Swin-T 64.6 69.2 76.5 74.1 69.9 56.3 60.1 41.9 62.3 51.6 44.7* 45.8 64.5 75.9 85.7 56.7
UperNet RSP-Swin-T 64.1 67.0 74.6 73.7 70.7 59.0 60.1 44.3 62.0 50.6 37.6 46.8 64.9 76.2 85.2 53.8
UperNet IMP-ViTAEv2-S 65.3* 71.4* 77.5* 68.2 71.0 60.8 61.9 43.0 63.8* 53.6* 43.4 44.8 65.1 77.9* 86.4* 57.7*
UperNet RSP-ViTAEv2-S 64.3 71.3 74.3 72.2 70.4 57.4 63.0* 44.0 62.5 51.6 35.4 47.0 62.2 77.7 85.2 54.7
1ST: storage tank. BD: baseball diamond. TC: tennis court. BC: baseball court. GTF: ground track ﬁeld. LV: large vehicle. SV: small vehicle. HC: helicopter.
SP: swimming pool. RA: roundabout. SBF: soccer ball ﬁeld.
high-resolution features have not encoded sufﬁcient high-level
semantics, while LANet [92] not only simultaneously enhance
the high and low-level features, it also enriches the semantics
of the high-resolution features. Thus, the segmentation perfor-
mance of the evaluated models based on UperNet on small
objects, such as cars, needs to be improved. On the other
hand, the IMP-Swin-T performs to be competitive and the
IMP-ViTAEv2-S achieves the best performance on the iSAID
dataset, outperforming the SOTA methods such as the HRNet
[103] and OCR [106] as well as a series of methods that
specially designed for aerial semantic segmentation, e.g., the
FarSeg [108] and FactSeg [109].
Table VII also shows the advantages of RSP models lying
in the “Bridge” category, which conforms to the ﬁnding in
the previous scene recognition task. Nevertheless, we can also
see from Table VI-VII that, on the segmentation task, theperformances of RSP are not as good as the classical IMP.
In our considerations, there may be two reasons. The ﬁrst
one is the difference between the pretraining dataset and the
evaluation one. Besides the dataset volume (note that the train-
ing sample and category numbers of MillionAID are smaller
than ImageNet-1k), the spectral disparities also have a side
impact on the performance, especially on the Potsdam dataset,
which adopts the IR-R-G channels instead of the ordinary
RGB image (See Figure 6). Another reason we attribute to
the difference between tasks. The representation used for scene
recognition needs to have a global understanding of the whole
scene as Figure 4 shows, while the segmentation task requires
the features to be more detailed while possessing high-level
semantic information simultaneously since they separately
conduct the scene-level or pixel-level classiﬁcation. To prove
this conjecture, we then evaluate these networks on the aerial
12 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
(a) (b) (c) (d)
(e) (f) (g) (h)Imper. surf. 
Building 
Low veg. 
Tree
Car
Ignore
Fig. 7. Segmentation maps of the UperNet with different backbones on the Potsdam dataset. (a) Ground Truth. (b) IMP-ResNet-50. (c) SeCo-ResNet-50. (d)
RSP-ResNet-50. (e) IMP-Swin-T. (f) RSP-Swin-T. (g) IMP-ViTAEv2-S. (h) RSP-ViTAEv2-S.
object detection task in the next section. The granularity of
the representation needed for detection probably lies between
those for the segmentation and recognition tasks, since one of
the aims in the detection task is the object-level classiﬁcation.
Qualitative Results and Analyses: We present some visual
segmentation results of the UperNet with different backbones
on the Potsdam dataset in Figure 7. As can be seen, only
the ViTAEv2-S successfully connects the long strip low veg-
etations (see the red boxes), while IMP-ViTAEv2-S performs
slightly better than RSP-ViTAEv2-S, which is consistent with
the quantitative results in Table VI.
C. Aerial Object Detection
Since the aerial images are top-down photoed in the sky,
the objects can be presented in any direction in the birdview.
Thus, the aerial object detection is the oriented bounding
box (OBB) detection, which is distinguished from the usual
horizontal bounding box (HBB) task on natural images [111],
[117], [134]. In this paper, similar to segmentation, we also
use different detection datasets in the experiments. Concretely,
we evaluated on the multi-category RS objects detection and
the single-category ship detection subtasks, respectively.
1) Dataset: Two datasets including the large-scale DOTA
[135] scenes and the commonly used HRSC2016 [136] dataset
are separately utilized for the above objectives.
•DOTA: This is the most famous large-scale dataset for
OBB detection. It totally contains 2,806 images whose
size ranges from 800 ×800 to 4,000 ×4,000, where
188,282 instances belonging to 15 categories are in-
cluded. The training, validation, and testing set separately
have 1,411/458/937 tiles. It should be noticed that the cat-
egories are completely the same with the iSAID dataset,since the two datasets share the same set of scenes. The
difference lies in the annotations for different tasks.
•HRSC2016: This is a specialized ship detection dataset,
where the bounding boxes are annotated in arbitrary ori-
entations. 1,061 images with the size ranging from 300 ×
300 to 1,500 ×900 are included. In the ofﬁcial division,
436/181/444 images are used for training, validation, and
testing, respectively. The dataset only has one category,
since there is no need to recognize the type of ships.
2) Implementation Detail and Experimental Setting: Simi-
lar to segmentation, the ResNet models are trained using the
SGDM algorithm with a learning rate of 0.005, a momentum
of 0.9, and a weight decay of 0.0001, while the vision
transformers are trained with the AdamW optimizer, where the
learning rate and weight decay are separately set to 0.0001 and
0.05. These models are trained for 12 and 36 epochs with a
batch size of 2 on DOTA and HRSC2016 scenes, respectively.
The learning rate is adjusted by a multi-step scheduler. On the
DOTA dataset, the learning rate will be separately reduced by
10×after the 8th epoch and the 11th epoch, while on the
HRSC2016 scene, the corresponded settings are epoch 24 and
epoch 33. We use one of the SOTA OBB detection frameworks
— ORCN [126] to evaluate the performance of different pre-
trained backbones. We adopt the default hyper-parameters of
ORCN, which is implemented in OBBDetection2. Following
[126], the DOTA dataset is sampled and cropped to 1,024
×1,024 patches with a stride of 824, while the HRSC2016
images are scaled keeping the aspect ratio with the shorter side
equals to 800, and the length of the longer side is less than
or equal to 1333. Data augmentations during training include
random horizontal and vertical ﬂipping. For convenience, the
original training and validation sets are merged for training,
2https://github.com/jbwang1997/OBBDetection
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 13
TABLE VIII
RESULTS OF THE ORCN DETECTION MODEL WITH DIFFERENT BACKBONES AND SOTA METHODS ON THE TESTING SET OF THE DOTA DATASET .†: THE
RESULT IS FROM AERIAL DETECTON [110]. ‡: THE RESULT IS FROM THE ORIGINAL ORCN PAPER .
Method Backbone mAPAP per category1
Ship ST BD TC BC GTF Bridge LV SV HC SP RA SBF Plane Harbor
One-stage
RetinaNet [111]† IMP-ResNet-50-FPN 68.43 79.11 74.32 77.62 90.29 82.18 58.17 41.81 71.64 74.58 60.64 69.67 60.60 54.75 88.67 62.57
DAL [112] IMP-ResNet-50-FPN 71.44 79.74 78.45 76.55 90.84 79.54 66.80 45.08 76.76 67.00 60.11 73.14 62.27 57.71 88.68 69.05
RSDet [113] IMP-ResNet-101-FPN 72.20 70.20 83.40 82.90 90.50 85.60 65.20 48.60 70.10 69.50 68.00 * 67.20 63.90 62.50 89.80 65.60
R3Det [114] IMP-ResNet-101-FPN 71.69 77.54 83.54 81.99 90.80 81.39 62.52 48.46 74.29 70.48 60.05 67.46 59.82 61.97 89.54 65.44
R3Det [114] IMP-ResNet-152-FPN 73.74 78.21 84.23 81.17 90.81 85.26 66.10 50.53 78.66 70.92 67.17 69.83 63.77 61.81 89.49 68.16
S2ANet [115] IMP-ResNet-50-FPN 74.12 87.25 85.64 82.84 90.83 84.90 71.11 48.37 78.39 78.11 57.94 69.13 62.60 60.36 89.11 65.26
S2ANet [115] IMP-ResNet-101-FPN 76.11 88.04 86.22 81.41 90.69 84.75 69.75 54.28 80.54 78.04 58.86 73.37 * 65.81 65.03 88.70 76.16
Two-stage
ICN [116] IMP-ResNet-101-FPN 68.16 69.98 78.20 74.30 90.76 79.06 70.32 47.70 67.82 64.89 50.23 64.17 62.90 53.64 81.36 67.02
Faster R-CNN [117] † IMP-ResNet-50-FPN 69.05 77.11 83.90 73.06 90.84 78.94 59.09 44.86 71.49 73.25 56.18 64.91 62.95 48.59 88.44 62.18
CAD-Net [118] IMP-ResNet-101-FPN 69.90 76.60 73.30 82.40 90.90 * 79.20 73.50 49.40 63.50 71.10 62.20 67.00 60.90 48.40 87.80 62.00
ROI Transformer [119] IMP-ResNet-101-FPN 69.56 83.59 81.46 78.52 90.74 77.27 75.92 43.44 73.68 68.81 47.67 58.93 53.54 58.39 88.64 62.83
SCRDet [120] IMP-ResNet-101-FPN 72.61 72.41 86.86 * 80.65 90.85 87.94 68.36 52.09 60.32 68.36 65.21 68.24 66.68 65.02 89.98 66.25
ROI Transformer†[119] IMP-ResNet-50-FPN 74.61 86.87 82.51 82.60 90.71 83.83 70.87 52.53 76.67 77.93 61.03 68.75 67.61 53.95 88.65 74.67
Gliding Vertex [121] IMP-ResNet-101-FPN 75.02 86.82 86.81 85.00 90.74 79.02 77.34 * 52.26 73.14 73.01 57.32 70.86 70.91 * 59.55 89.64 72.94
FAOD [122] IMP-ResNet-101-FPN 73.28 79.56 84.68 79.58 90.83 83.40 76.41 45.49 68.27 73.18 64.86 69.69 65.42 53.40 90.21 * 74.17
CenterMap-Net [123] IMP-ResNet-50-FPN 71.74 78.10 83.61 81.24 88.83 77.80 60.65 53.15 66.55 78.62 58.70 72.36 66.19 49.36 88.88 72.10
FR-Est [124] IMP-ResNet-101-FPN 74.20 86.44 83.56 81.17 90.82 84.13 70.19 50.44 77.98 73.52 60.55 66.72 66.59 60.64 89.63 70.59
Mask OBB [125] IMP-ResNet-50-FPN 74.86 85.57 85.05 85.09 * 90.37 82.08 72.90 51.85 73.23 75.28 66.33 69.87 68.39 55.73 89.61 71.61
ORCN‡[126] IMP-ResNet-50-FPN 75.87 88.20 * 84.68 82.12 90.90 * 87.50 70.86 54.78 83.00 78.93 52.28 68.84 67.69 63.97 89.46 74.94
ORCN‡[126] IMP-ResNet-101-FPN 76.28 87.52 85.33 83.48 90.90 * 85.56 76.92 55.27 82.10 74.27 57.28 70.15 66.82 65.51 * 88.86 74.36
ORCN IMP-ResNet-50-FPN 76.14 88.16 84.91 81.35 90.90* 87.43 71.35 54.86 83.03 79.04 58.14 69.05 66.67 63.39 89.58 74.19
ORCN SeCo-ResNet-50-FPN 70.02 86.33 81.31 73.32 90.88 79.46 67.07 49.94 76.48 76.15 49.71 65.32 58.55 41.31 88.64 65.90
ORCN RSP-ResNet-50-FPN 76.50 88.17 85.72 81.88 90.84 86.17 70.91 54.39 83.01 78.67 62.22 72.21 67.45 62.22 89.78 73.99
ORCN IMP-Swin-T-FPN 76.07 88.02 84.92 82.23 90.90* 87.42 74.37 52.25 83.55 77.99 63.07 69.30 65.99 57.70 89.48 73.88
ORCN RSP-Swin-T-FPN 76.12 87.83 84.84 79.74 90.86 85.90 74.50 52.91 84.02 78.96 57.36 70.61 67.33 62.90 89.54 74.45
ORCN IMP-ViTAEv2-S-FPN 77.38 88.14 86.35 83.50 90.90 * 87.51 75.38 53.42 85.15* 79.99* 66.03 66.12 70.91* 61.02 89.27 76.95*
ORCN RSP-ViTAEv2-S-FPN 77.72* 88.04 85.58 83.04 90.90 * 88.17* 75.16 55.85* 84.34 79.95 67.89 67.15 70.60 62.64 89.66 76.77
1ST: storage tank. BD: baseball diamond. TC: tennis court. BC: baseball court. GTF: ground track ﬁeld. LV: large vehicle. SV: small vehicle. HC: helicopter.
SP: swimming pool. RA: roundabout. SBF: soccer ball ﬁeld.
TABLE IX
RESULTS OF THE ORCN DETECTION MODEL WITH DIFFERENT
BACKBONES AND SOTA METHODS ON THE TESTING SET OF THE
HRSC2016 DATASET .†: THE RESULT IS FROM THE ORIGINAL ORCN
PAPER .
Method Backbone mAP
R2PN [127] IMP-VGG-16 79.6
RRD [128] IMP-VGG-16 84.3
FoRDet [129] IMP-VGG-16 89.9
R2CNN [130] IMP-ResNet-101 73.1
Rotated RPN [131] IMP-ResNet-101 79.1
ROI Transformer [119] IMP-ResNet-101-FPN 86.2
Gliding Vertex [121] IMP-ResNet-101-FPN 88.2
GRS-Det [132] IMP-ResNet-50-FPN 88.9
GRS-Det [132] IMP-ResNet-101-FPN 89.6
R3Det [114] IMP-ResNet-101-FPN 89.3
DAL [112] IMP-ResNet-101-FPN 89.8
AproNet [115] IMP-ResNet-101-FPN 90.0
S2ANet [115] IMP-ResNet-101-FPN 90.2
ORCN†[126] IMP-ResNet-50-FPN 90.4
CHPDet [133] Hourglass104 90.6*
ORCN IMP-ResNet-50-FPN 90.4
ORCN SeCo-ResNet-50-FPN 88.9
ORCN RSP-ResNet-50-FPN 90.3
ORCN IMP-Swin-T-FPN 89.7
ORCN RSP-Swin-T-FPN 90.0
ORCN IMP-ViTAEv2-S-FPN 90.4
ORCN RSP-ViTAEv2-S-FPN 90.4
while the original testing sets of DOTA and HRSC2016 are
separately used for evaluation. We report the mean average
precision (mAP) of all categories and the average precision
(AP) of each class on the corresponding testing set. All models
are trained on a single V100 GPU.
3) Experimental Results: Quantitative Results and Anal-
yses: Table VIII-IX show the results of OBB detection
experiments. On the challenging DOTA dataset, it can be
seen that using the advanced ORCN framework, the models
whose backbone is either ResNet-50 or Swin-T performs well,
although the mAPs of Swin-T models are slightly lower than
the ResNet models. The ViTAEv2-S, which is a kind of vision
transformer network that is introduced the inductive biasesincluding the locality and scale-invariance characteristics of
CNN, obtains amazing performance that improves the ORCN
baseline by nearly 2% mAP. Another point needed to be
noticed is the performance of RSP weights on these three
backbones all outperforms their ImageNet pretrained coun-
terparts. These results support our previous claims that the
granularity of the representation required for the detection
task is closer to that for the scene recognition task compared
with the segmentation task. Thus, the performance difference
between RSP and IMP in the detection experiments aligns with
the results in the scene recognition experiments.
In addition, we observe that compared with IMP-ViTAEv2-
S, the APs of most categories obtained by RSP-ViTAEv2-S
are smaller, implying the universality of IMP. Nevertheless,
the mAP of RSP-ViTAEv2-S is higher than IMP-ViTAEv2-
S, since RSP has signiﬁcant advantages in the categories
of “Bridge” and aerial vehicles including “Helicopter” or
“Plane”, echoing the previous ﬁnding in the segmentation
experiments. While on the other categories, the gaps between
these two models are not very large. Combining the above two
points, RSP-ViTAEv2-S delivers better overall performance
than IMP-ViTAEv2-S. On HRSC2016 dataset, the CHPDet
[133] that performs the best is a speciﬁcally designed detector
by considering the ship characteristics. For ORCN [126]
related networks, the results of RSP and IMP are roughly
the same, where there are wins or losses on both sides.
Compared with CNN, the vision transformer models have not
demonstrated the advantages. We think that on this relatively
easy subtask, where only one category needed to be detected
and the ship sizes in HRSC2016 are relatively larger than
DOTA, the performance is probably saturated.
Qualitative Results and Analyses: We visualize some
detection results of the ORCN model with the ViTAEv2-S
backbones on the DOTA testing set in Figure 8. The red boxes
show that, when objects are densely distributed, the RSP-
14 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
IMP
RSPLV & SV BR
Fig. 8. Visual detection results of the ORCN model with the ViTAEv2-S
backbones on the DOTA testing set. LV: large vehicle. SV: small vehicle.
BR: Bridge. IMP: IMP-ViTAEv2-S. RSP: RSP-ViTAEv2-S.
ViTAEv2-S can still predict correct object categories, while
the IMP-ViTAEv2-S is confused by the dense context and
makes wrong predictions. For the “Bridge” category, the IMP-
ViTAEv2-S produces missing detections (see yellow boxes),
while the RSP-ViTAEv2-S model successfully detects the long
bridge with a high conﬁdence score.
D. Aerial Change Detection
We then apply the above models on a typical application
in the RS ﬁeld, i.e., change detection, which aims to ﬁnd the
changes between two aerial images of a same region captured
at different times. It is formulated as a pixel-level binary
classiﬁcation task, where “1” indicates change.
1) Dataset: We adopt the commonly used CDD [137] and
LEVIR [138] datasets to comprehensively evaluate the above
models on this task since they separately involve the natural
and artiﬁcial changes.
•CDD: The original dataset contains 11 pairs of multi-
source real season-varying RS images collecting from
GE, where 7 pairs of images are at the size of 4,725
×2,200 and 4 image pairs are at the size of 1,900 ×
1,000 pixels. The resolutions are ranging from 0.03m
to 1m. Then, [139] clipped the images to a series of
256×256 patches and generated a dataset, where
the sizes of the training, validation and testing set are
10,000/3,000/3,000, respectively.
•LEVIR: This dataset is collected using the GE API on 20
different regions in Texas, the USA, from 2002 to 2018. It
contains 637 image pairs at the size of 1,024 ×1,024 and
with a high resolution of 0.5m, where most changes are
from man-made structures, including 31,333 independent
building change entities. The training, validation, and
testing set contain 445/64/128 image pairs, respectively.TABLE X
RESULTS OF THE BIT CHANGE DETECTION MODEL WITH DIFFERENT
BACKBONES AND SOTA METHODS ON THE TESTING SET OF CDD AND
LEVIR DATASETS .†: THE RESULT IS FROM THE ORIGINAL BIT PAPER .
Method BackboneF1 score
CDD LEVIR
FC-EF [141] — 77.11 62.32
FC-Siam-conc [141] — 82.50 68.21
FC-Siam-diff [141] — 83.73 63.09
CLNet [142] — 92.10 90.00
SNUNet-c48 [143] — 96.20 —
IFN [144] IMP-VGG-16 90.30 83.57
DASNet [145] IMP-VGG-16 91.93 82.83
SRCDNet [146] IMP-ResNet-18 90.02 —
STANet [138] IMP-ResNet-18 90.75 87.34
BSFNet [147] IMP-ResNet-18 91.90 88.00
DSAMNet [148] IMP-ResNet-18 93.69 —
HRTNet [149] IMP-HRNet-W18 93.71 88.48
CDNet+IAug [150] IMP-ResNet-18 — 89.00
BIT†[140] IMP-ResNet-18 — 89.31
ChangeFormer [151] IMP-MiT-B2 [47] — 90.40
CS-HSNet [152] IMP-ResNet-50 94.95 90.79
LSS-Net [153] IMP-SE-ResNet-50 96.30 —
ChangeStar [154] IMP-ResNext-101-32 ×4d — 91.25
BIT IMP-ResNet-50 95.09 89.19
BIT SeCo-ResNet-50 95.95 90.14
BIT RSP-ResNet-50 96.00 90.10
BIT IMP-Swin-T 94.77 90.25
BIT RSP-Swin-T 95.21 90.10
BIT IMP-ViTAEv2-S 97.02* 91.26*
BIT RSP-ViTAEv2-S 96.81 90.93
2) Implementation Detail and Experimental Setting: In this
section, we adopt a SOTA framework — BIT [140], which
uses the transformer to capture the contextual information
between different temporal images for change detection. If
BIT is equipped with the ResNet backbone, it is optimized
by the SGDM optimizer, where the learning rate, momentum,
and weight decay are 0.001, 0.99, and 0.0005. While the Swin
or ViTAE based BIT models are trained with the AdamW
optimizer with the learning rate of 6e-5 and weight decay
of 0.01. These models are trained for 200 epochs with a
batch size of 8, while the learning rate is linearly decayed
until the end of training. Following [140], the LEVIR dataset
is clipped to the patches at the size of 256 ×256 with no
overlaps. Thus, the sizes of the training, validation and testing
set are 7,120/1,024/2,048. The ﬁnal performance of different
models is evaluated on the testing set, while the results on the
validation set are only used to select the best model during
training. We use the F1 score as the evaluation metric and the
experiments are conducted on a single V100 GPU.
3) Experimental Results: Quantitative Results and Anal-
yses: The quantitative results are summarized in Table X.
Without surprise, the self-supervised SeCo pretrained weights
perform well on this task, e.g., the SeCo-ResNet-50 based
BIT performs better than the IMP counterpart. Although the
SeCo weights are trained to achieve seasonal invariance, the
change features can be encoded via the multi-head sub-space
embedding [58]. Nevertheless, ViTAEv2-S pretrained either by
IMP or RSP performs better than SeCo-ResNet-50, showing
the beneﬁt of using the advanced backbone.
Compared with other methods, it is no doubt that the
ViTAEv2-S achieves the best performance, showing the po-
tentiality of applying an advanced vision transformer on RS
ﬁeld. As before, we analyze the performance difference be-
tween the RSP with the IMP through the perspective of task
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 15
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 17
(a)
 (b)
 (c)
 (d)
 (e)
 (f)
 (g)
 (h)
 (i)
 (j)
(k)
 (l)
 (m)
 (n)
 (o)
 (p)
 (q)
 (r)
 (s)
 (t)
Fig. 9. Visualization of the change detection maps. The ﬁrst and second row separately show the change detection results of a sample image from CDD and
LEVIR dataset. Here, (a)(k), (b)(l) are the ﬁrst and second temporals of the same regions. (c)(m) are the corresponded change annotations. (d)(n) are generated
by the IMP-ResNet-50 based BIT, while (e)(o), (f)(p), (f)(o), (g)(q), (h)(r), (i)(s), (g)(t) are separately the results from the SeCo-ResNet-50, RSP-ResNet-50,
IMP-Swin-T, RSP-Swin-T, IMP-ViTAE-S-Stage-Win and RSP-ViTAE-S-Stage-Win backbones.
We hope this study can drive the works on aerial image ﬁeld
using vision transformers based on remote sensing pretraining.
ACKNOWLEDGEMENT
The authors would like to thank PhD candidate Yang Long
and Prof. Gui-Song Xia for providing the MillionAID dataset
and PhD candidate Qiming Zhang for offering the ViTAE
series models. This work was done by Di Wang as the research
intern in JD Explore Academy.
REFERENCES
[1] X. Zhang, Y . Sun, K. Shang, L. Zhang, and S. Wang, “Crop clas-
siﬁcation based on feature band set construction and object-oriented
approach using hyperspectral images,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens. , vol. 9, no. 9, pp. 4117–4128, Sep. 2016.
[2] X. Yang and Y . Yu, “Estimating soil salinity under various moisture
conditions: An experimental study,” IEEE Trans. Geosci. Remote Sens. ,
vol. 55, no. 5, pp. 2525–2533, May 2017.
[3] M. J. Swain and D. H. Ballard, “Color indexing,” Int. J.
Comput. Vis. , vol. 7, no. 1, pp. 11–32, 1991. [Online]. Available:
https://doi.org/10.1007/BF00130487
[4] R. M. Haralick, K. Shanmugam, and I. Dinstein, “Textural features
for image classiﬁcation,” IEEE Trans. Syst. Man Cybern. , vol. SMC-3,
no. 6, pp. 610–621, 1973.
[5] A. Oliva and A. Torralba, “Modeling the shape of the scene:
A holistic representation of the spatial envelope,” Int. J. Comput.
Vis., vol. 42, no. 3, pp. 145–175, 2001. [Online]. Available:
https://doi.org/10.1023/A:1011139631724
[6] A. Avramovi ´c and V . Risojevi ´c, “Block-based semantic classiﬁcation
of high-resolution multispectral aerial images,” Signal, Image Video
Process. , vol. 10, no. 1, pp. 75–84, 2016.
[7] O. A. Penatti, K. Nogueira, and J. A. Dos Santos, “Do deep features
generalize from everyday objects to remote sensing and aerial scenes
domains?” in CVPRW , 2015, pp. 44–51.
[8] D. G. Lowe, “Distinctive image features from scale-invariant key-
points,” Int. J. Comput. Vis. , vol. 60, no. 2, pp. 91–110, 2004.
[9] N. Dalal and B. Triggs, “Histograms of oriented gradients for human
detection,” in CVPR , vol. 1. Ieee, 2005, pp. 886–893.
[10] Y . Bengio, A. Courville, and P. Vincent, “Representation learning: A
review and new perspectives,” IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 35, no. 8, pp. 1798–1828, Aug 2013.
[11] T. Hofmann, “Unsupervised learning by probabilistic latent semantic
analysis,” Mach. Learn. , vol. 42, no. 1, pp. 177–196, 2001.
[12] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object
retrieval with large vocabularies and fast spatial matching,” in CVPR .
IEEE, 2007, pp. 1–8.
[13] Q. ZHANG, J. Zhang, W. Liu, and D. Tao, “Category anchor-
guided unsupervised domain adaptation for semantic segmentation,”
inNeurIPS , vol. 32. Curran Associates, Inc., 2019.
[14] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in CVPR , 2016, pp. 770–778.[15] L. Gao, J. Zhang, L. Zhang, and D. Tao, DSP: Dual Soft-Paste for
Unsupervised Domain Adaptive Semantic Segmentation . New York,
NY , USA: Association for Computing Machinery, 2021, p. 28252833.
[Online]. Available: https://doi.org/10.1145/3474085.3475186
[16] D. Wang, B. Du, L. Zhang, and Y . Xu, “Adaptive spectralspatial
multiscale contextual feature extraction for hyperspectral image classi-
ﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 59, no. 3, pp. 2461–
2477, 2021.
[17] L. Zhang, M. Lan, J. Zhang, and D. Tao, “Stagewise unsupervised
domain adaptation with adversarial self-training for road segmentation
of remote-sensing images,” IEEE Trans. Geosci. Remote Sens. , vol. 60,
pp. 1–13, 2022.
[18] D. Wang, B. Du, and L. Zhang, “Fully contextual network for hyper-
spectral scene parsing,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp.
1–16, 2022.
[19] Y . Chen, Z. Lin, X. Zhao, G. Wang, and Y . Gu, “Deep Learning-Based
Classiﬁcation of Hyperspectral Data,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens. , vol. 7, no. 6, pp. 2094–2107, June 2014.
[20] T. Li, J. Zhang, and Y . Zhang, “Classiﬁcation of hyperspectral image
based on deep belief networks,” in ICIP , Oct 2014, pp. 5132–5136.
[21] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classiﬁ-
cation: Benchmark and state of the art,” Proc. IEEE , vol. 105, no. 10,
pp. 1865–1883, 2017.
[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in CVPR , 2009, pp. 248–
255.
[23] K. Simonyan and A. Zisserman, “Very Deep Convolutional Net-
works for Large-Scale Image Recognition,” arXiv e-prints , p.
arXiv:1409.1556, Sep. 2014.
[24] K. Xu, H. Huang, P. Deng, and Y . Li, “Deep feature aggregation frame-
work driven by graph convolutional network for scene classiﬁcation in
remote sensing,” IEEE Trans. Neural Netw. Learn. Syst. , 2021.
[25] H. Sun, S. Li, X. Zheng, and X. Lu, “Remote sensing scene classi-
ﬁcation by gated bidirectional network,” IEEE Trans. Geosci. Remote
Sens. , vol. 58, no. 1, pp. 82–96, 2019.
[26] Q. Zhao, Y . Ma, S. Lyu, and L. Chen, “Embedded self-distillation
in compact multibranch ensemble network for remote sensing scene
classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1–15,
2022.
[27] Q. Zhao, S. Lyu, Y . Li, Y . Ma, and L. Chen, “Mgml: Multigranularity
multilevel feature ensemble network for remote sensing scene classiﬁ-
cation,” IEEE Trans. Neural Netw. Learn. Syst. , 2021.
[28] J. Kang, R. Fernandez-Beltran, Z. Wang, X. Sun, J. Ni, and A. Plaza,
“Rotation-invariant deep embedding for remote sensing images,” IEEE
Trans. Geosci. Remote Sens. , vol. 60, pp. 1–13, 2022.
[29] Y . Long, G.-S. Xia, S. Li, W. Yang, M. Y . Yang, X. X. Zhu,
L. Zhang, and D. Li, “On creating benchmark dataset for aerial image
interpretation: Reviews, guidances and million-aid,” IEEE J. Sel. Topics
Appl. Earth Observ. Remote Sens. , vol. 14, pp. 4205–4230, 2021.
[30] Y . Long, G.-S. Xia, L. Zhang, G. Cheng, and D. Li, “Aerial scene
parsing: From tile-level scene classiﬁcation to pixel-wise semantic
labeling,” 2022.
[31] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,
“Swin Transformer: Hierarchical Vision Transformer using Shifted
Windows,” arXiv e-prints , p. arXiv:2103.14030, Mar. 2021.
Fig. 9. Visual change detection results. The ﬁrst and second row separately show the change detection results of a sample image from the CDD and LEVIR
datasets. Here, (a) and (k), (b) and (l) are the ﬁrst and second temporals of the same regions. (c) and (m) are ground truth change annotations. (d) and (n)
are the results of the IMP-ResNet-50 based BIT, while (e) and (o), (f) and (p), (f) and (o), (g) and (q), (h) and (r), (i) and (s), (g) and (t) are the results from
the SeCo-ResNet-50, RSP-ResNet-50, IMP-Swin-T, RSP-Swin-T, IMP-ViTAEv2-S, and RSP-ViTAEv2-S backbones, respectively.
TABLE XI
OVERALL COMPARISONS OF DIFFERENT BACKBONES TRAINED WITH
DIFFERENT EPOCHS ON ALL DOWNSTREAM TASKS .
BackboneScene
RecognitionSemantic
SegmentationObject
DetectionChange
Detection
RSP-ResNet-50-E120 96.53 75.70 82.51 92.82
RSP-ResNet-50-E300 96.63 75.68 83.39 93.05
RSP-Swin-T-E120 96.06 76.17 83.08 92.28
RSP-Swin-T-E300 96.44 77.07 82.80 92.66
RSP-ViTAEv2-S-E40 96.76 76.95 83.65 93.08
RSP-ViTAEv2-S-E100 97.01 77.45 83.98 93.87
characteristics. Given the analyses in previous sections, we can
infer that the granularity of the required representation for the
change detection lies in between those of the segmentation
and detection, since it is also a segmentation task, although
there only are two categories and there is no need to recognize
speciﬁc object category.
Qualitative Results and Analyses: We present some visual
change detection results in Figure 9. As can be seen, ResNet-
50 and Swin-T by IMP can not well detect the changes in
roads inside the ﬁelds in the natural scene. This issue could
be partly alleviated by adopting RSP. It is consistent with the
results in Table X that SeCo-ResNet-50 further improves the
detection in the road areas. Compared with the above models,
the ViTAEv2-S model effectively captures the road details. In
the artiﬁcially changed scene, the ViTAEv2-S model greatly
overcomes the problem of object adhesion in the results of
all other models, demonstrating that the ViTAEv2-S features
are more discriminative for distinguishing objects from the
background.
E. Overall Comparison of Different Backbones on All Tasks
In this part, we comprehensively compare the performance
of different backbones by RSP on all tasks. Speciﬁcally, the
scores in Table XI are calculated by averaging the scores
across all datasets for each task. For example, we calculate
the average score of the mean values on ﬁve settings in the
scene recognition task, so the overall accuracy of RSP-ResNet-
50-E120 is (99.52+96.60+97.78+93.76+94.97)/5≈96.53,
while the overall score of RSP-Swin-T-E300 on the segmenta-
tion task is obtained by averaging the mF1 on Potsdam datasetand the mIOU on iSAID dataset: (90.03 + 64.10)/2≈77.07.
From Table XI, we can ﬁnd that the backbones pretraining
with more epochs generally perform better on downstream
tasks, since they obtain stronger representations, although
there is an exception, i.e., the Swin-T on the object detection
task, implying the task discrepancy also matters. In general,
ViTAEv2-S takes advantage of CNN and transformers and
delivers better performance when training with more epochs,
and outperforms ResNet-50 and Swin-T on all the tasks.
V. C ONCLUSION
In this study, we investigate the remote sensing pretraining
problem based on both CNN and vision transformers on
the largest remote sensing dataset MillionAID, and compre-
hensively evaluate their performance on four related tasks,
including scene recognition, semantic segmentation, object
detection, and change detection, as well as compare them
with the ImageNet pretraining and other SOTA methods. By
synthetically analyzing the experiment results, we draw the
following conclusions:
•Compared with the traditional CNN model, the vision
transformers perform competitively on a series of remote
sensing tasks, and they can obtain better performance on
some more challenging datasets, such as the iSAID and
DOTA. Particularly, the ViTAEv2-S, an advanced model
introducing the inductive biases of CNN into the vision
transformers, achieves the best performance on almost all
settings of these tasks.
•Beneﬁtting from the large capacity of ImageNet-1K
dataset, the classical IMP enables deep models to learn
more universal representations that generalize well to
almost all categories in the downstream tasks. Thus, the
IMP can produce competitive baseline results despite
aerial scenes. RSP is comparable with IMP and performs
extremely well on some speciﬁc categories, such as the
“Bridge” and “Airplane”, owing to mitigating the data-
level discrepancy between the upstream pretraining task
and downstream task.
•The task-level discrepancy also has a side impact on
the performance of RSP. If the granularity of the rep-
resentation required for a speciﬁc downstream task is
16 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
closer to that of the upstream pretraining task, i.e., scene
recognition, RSP usually leads to better performances.
We hope this study can provide useful insights to the com-
munity about using advanced vision transformers and remote
sensing pretraining. In future work, we will investigate the
RSP on large-scale datasets for downstream tasks as well as the
unsupervised pretraining considering the abundant unlabelled
data in this area.
REFERENCES
[1] X. Zhang, Y . Sun, K. Shang, L. Zhang, and S. Wang, “Crop clas-
siﬁcation based on feature band set construction and object-oriented
approach using hyperspectral images,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens. , vol. 9, no. 9, pp. 4117–4128, Sep. 2016.
[2] J. Zhang and D. Tao, “Empowering things with intelligence: a survey
of the progress, challenges, and opportunities in artiﬁcial intelligence
of things,” IEEE Internet of Things Journal , vol. 8, no. 10, pp. 7789–
7817, 2020.
[3] X. Yang and Y . Yu, “Estimating soil salinity under various moisture
conditions: An experimental study,” IEEE Trans. Geosci. Remote Sens. ,
vol. 55, no. 5, pp. 2525–2533, May 2017.
[4] M. J. Swain and D. H. Ballard, “Color indexing,” Int. J. Comput. Vis. ,
vol. 7, no. 1, pp. 11–32, 1991.
[5] R. M. Haralick, K. Shanmugam, and I. Dinstein, “Textural features
for image classiﬁcation,” IEEE Trans. Syst. Man Cybern. , vol. SMC-3,
no. 6, pp. 610–621, 1973.
[6] A. Oliva and A. Torralba, “Modeling the shape of the scene: A holistic
representation of the spatial envelope,” Int. J. Comput. Vis. , vol. 42,
no. 3, pp. 145–175, 2001.
[7] O. A. Penatti, K. Nogueira, and J. A. Dos Santos, “Do deep features
generalize from everyday objects to remote sensing and aerial scenes
domains?” in CVPRW , 2015, pp. 44–51.
[8] Y . Bengio, A. Courville, and P. Vincent, “Representation learning: A
review and new perspectives,” IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 35, no. 8, pp. 1798–1828, Aug 2013.
[9] T. Hofmann, “Unsupervised learning by probabilistic latent semantic
analysis,” Mach. Learn. , vol. 42, no. 1, pp. 177–196, 2001.
[10] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object
retrieval with large vocabularies and fast spatial matching,” in CVPR .
IEEE, 2007, pp. 1–8.
[11] K. Simonyan and A. Zisserman, “Very deep convolutional networks
for large-scale image recognition,” in ICLR , May 2015.
[12] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in CVPR , 2016, pp. 770–778.
[13] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and
B. Guo, “Swin transformer: Hierarchical vision transformer using
shifted windows,” in ICCV , 2021, pp. 10 012–10 022.
[14] Y . Xu, Q. Zhang, J. Zhang, and D. Tao, “Vitae: Vision transformer
advanced by exploring intrinsic inductive bias,” NeurIPS , vol. 34, 2021.
[15] T. Xiao, Y . Liu, B. Zhou, Y . Jiang, and J. Sun, “Uniﬁed perceptual
parsing for scene understanding,” in ECCV , 2018, pp. 418–434.
[16] Q. Zhang, J. Zhang, W. Liu, and D. Tao, “Category anchor-guided un-
supervised domain adaptation for semantic segmentation,” in NeurIPS ,
vol. 32, 2019.
[17] L. Gao, J. Zhang, L. Zhang, and D. Tao, “Dsp: Dual soft-paste
for unsupervised domain adaptive semantic segmentation,” in ACM
Multimedia , 2021, pp. 2825–2833.
[18] D. Wang, B. Du, L. Zhang, and Y . Xu, “Adaptive spectral–spatial
multiscale contextual feature extraction for hyperspectral image classi-
ﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 59, no. 3, pp. 2461–
2477, 2021.
[19] L. Zhang, M. Lan, J. Zhang, and D. Tao, “Stagewise unsupervised
domain adaptation with adversarial self-training for road segmentation
of remote-sensing images,” IEEE Trans. Geosci. Remote Sens. , vol. 60,
pp. 1–13, 2022.
[20] D. Wang, B. Du, and L. Zhang, “Fully contextual network for hyper-
spectral scene parsing,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp.
1–16, 2022.
[21] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classiﬁ-
cation: Benchmark and state of the art,” Proc. IEEE , vol. 105, no. 10,
pp. 1865–1883, 2017.[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in CVPR , 2009, pp. 248–
255.
[23] K. Xu, H. Huang, P. Deng, and Y . Li, “Deep feature aggregation frame-
work driven by graph convolutional network for scene classiﬁcation in
remote sensing,” IEEE Trans. Neural Netw. Learn. Syst. , 2021.
[24] H. Sun, S. Li, X. Zheng, and X. Lu, “Remote sensing scene classi-
ﬁcation by gated bidirectional network,” IEEE Trans. Geosci. Remote
Sens. , vol. 58, no. 1, pp. 82–96, 2019.
[25] Q. Zhao, Y . Ma, S. Lyu, and L. Chen, “Embedded self-distillation
in compact multibranch ensemble network for remote sensing scene
classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1–15,
2022.
[26] Q. Zhao, S. Lyu, Y . Li, Y . Ma, and L. Chen, “Mgml: Multigranularity
multilevel feature ensemble network for remote sensing scene classiﬁ-
cation,” IEEE Trans. Neural Netw. Learn. Syst. , 2021.
[27] J. Kang, R. Fernandez-Beltran, P. Duan, S. Liu, and A. J. Plaza,
“Deep Unsupervised Embedding for Remotely Sensed Images Based
on Spatially Augmented Momentum Contrast,” IEEE Trans. Geosci.
Remote Sens. , vol. 59, no. 3, pp. 2598–2610, Mar. 2021.
[28] Y . Long, G.-S. Xia, S. Li, W. Yang, M. Y . Yang, X. X. Zhu,
L. Zhang, and D. Li, “On creating benchmark dataset for aerial image
interpretation: Reviews, guidances and million-aid,” IEEE J. Sel. Topics
Appl. Earth Observ. Remote Sens. , vol. 14, pp. 4205–4230, 2021.
[29] Q. Zhang, Y . Xu, J. Zhang, and D. Tao, “Vitaev2: Vision transformer
advanced by exploring inductive bias for image recognition and be-
yond,” arXiv preprint arXiv:2202.10108 , 2022.
[30] X. Wang, S. Wang, C. Ning, and H. Zhou, “Enhanced feature pyramid
network with deep semantic embedding for remote sensing scene
classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 59, no. 9, pp.
7918–7932, 2021.
[31] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely
connected convolutional networks,” in CVPR , 2017, pp. 4700–4708.
[32] Q. Bi, K. Qin, H. Zhang, and G.-S. Xia, “Local semantic enhanced
convnet for aerial scene recognition,” IEEE Trans. Image Process. ,
vol. 30, pp. 6498–6511, 2021.
[33] B. Ma, J. Zhang, Y . Xia, and D. Tao, “Auto learning attention,” in
NeurIPS , vol. 33, 2020, pp. 1488–1500.
[34] S.-B. Chen, Q.-S. Wei, W.-Z. Wang, J. Tang, B. Luo, and Z.-Y . Wang,
“Remote sensing scene classiﬁcation via multi-branch local attention
network,” IEEE Trans. Image Process. , vol. 31, pp. 99–109, 2021.
[35] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam: Convolutional
block attention module,” in ECCV , 2018, pp. 3–19.
[36] Q. Bi, K. Qin, Z. Li, H. Zhang, K. Xu, and G.-S. Xia, “A multiple-
instance densely-connected convnet for aerial scene classiﬁcation,”
IEEE Trans. Image Process. , vol. 29, pp. 4911–4926, 2020.
[37] G. Cheng, X. Xie, J. Han, L. Guo, and G.-S. Xia, “Remote sensing
image scene classiﬁcation meets deep learning: Challenges, methods,
benchmarks, and opportunities,” IEEE J. Sel. Topics Appl. Earth
Observ. Remote Sens. , vol. 13, pp. 3735–3756, 2020.
[38] L. Ma, Y . Liu, X. Zhang, Y . Ye, G. Yin, and B. A. Johnson, “Deep
learning in remote sensing applications: A meta-analysis and review,”
ISPRS-J. Photogramm. Remote Sens. , vol. 152, pp. 166–177, 2019.
[39] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, “Object detection
in optical remote sensing images: A survey and a new benchmark,”
ISPRS-J. Photogramm. Remote Sens. , vol. 159, pp. 296–307, 2020.
[40] W. Shi, M. Zhang, R. Zhang, S. Chen, and Z. Zhan, “Change detection
based on artiﬁcial intelligence: State-of-the-art and challenges,” Remote
Sens. , vol. 12, no. 10, p. 1688, 2020.
[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in
NeurIPS , 2017, pp. 5998–6008.
[42] D. W. Otter, J. R. Medina, and J. K. Kalita, “A survey of the usages
of deep learning for natural language processing,” IEEE Trans. Neural
Netw. Learn. Syst. , vol. 32, no. 2, pp. 604–624, 2021.
[43] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,
C. Xu, Y . Xu, Z. Yang, Y . Zhang, and D. Tao, “A survey on vision
transformer,” IEEE Trans. Pattern Anal. Mach. Intell. , pp. 1–1, 2022.
[44] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:
Transformers for image recognition at scale,” ICLR , 2021.
[45] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and
H. J ´egou, “Training data-efﬁcient image transformers & distillation
through attention,” in ICML . PMLR, 2021, pp. 10 347–10 357.
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 17
[46] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,
and L. Shao, “Pyramid vision transformer: A versatile backbone for
dense prediction without convolutions,” in ICCV , 2021, pp. 568–578.
[47] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,
“Segformer: Simple and efﬁcient design for semantic segmentation
with transformers,” NeurIPS , vol. 34, 2021.
[48] W. Wang, Y . Cao, J. Zhang, F. He, Z.-J. Zha, Y . Wen, and D. Tao,
“Exploring sequence feature alignment for domain adaptive detection
transformers,” in ACM Multimedia , 2021, pp. 1730–1738.
[49] W. Wang, Y . Cao, J. Zhang, and D. Tao, “FP-DETR: Detection
transformer advanced by fully pre-training,” in ICLR , 2022.
[50] W. Wang, J. Zhang, Y . Cao, Y . Shen, and D. Tao, “Towards data-
efﬁcient detection transformers,” arXiv preprint arXiv:2203.09507 ,
2022.
[51] Z. Chen, J. Zhang, and D. Tao, “Recurrent glimpse-based decoder for
detection with transformer,” arXiv preprint arXiv:2112.04632 , 2021.
[52] P. Deng, K. Xu, and H. Huang, “When cnns meet vision transformer: A
joint framework for remote sensing scene classiﬁcation,” IEEE Geosci.
Remote Sens. Lett. , vol. 19, pp. 1–5, 2022.
[53] W. Li, K. Chen, H. Chen, and Z. Shi, “Geographical knowledge-
driven representation learning for remote sensing images,” IEEE Trans.
Geosci. Remote Sens. , vol. 60, pp. 1–16, 2022.
[54] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple frame-
work for contrastive learning of visual representations,” in ICML .
PMLR, 2020, pp. 1597–1607.
[55] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with
momentum contrastive learning,” arXiv preprint arXiv:2003.04297 ,
2020.
[56] Y . Xu, Q. Zhang, J. Zhang, and D. Tao, “Regioncl: Can simple
region swapping contribute to contrastive learning?” arXiv preprint
arXiv:2111.12309 , 2021.
[57] K. He, X. Chen, S. Xie, Y . Li, P. Doll ´ar, and R. Girshick, “Masked
autoencoders are scalable vision learners,” arXiv:2111.06377 , 2021.
[58] O. Ma ˜nas, A. Lacoste, X. Giro-i Nieto, D. Vazquez, and P. Rodriguez,
“Seasonal contrast: Unsupervised pre-training from uncurated remote
sensing data,” in ICCV , 2021, pp. 9414–9423.
[59] K. Ayush, B. Uzkent, C. Meng, K. Tanmay, M. Burke, D. Lobell, and
S. Ermon, “Geography-aware self-supervised learning,” in ICCV , 2021,
pp. 10 181–10 190.
[60] S. Vincenzi, A. Porrello, P. Buzzega, M. Cipriano, P. Fronte, R. Cuccu,
C. Ippoliti, A. Conte, and S. Calderara, “The color out of space:
learning self-supervised representations for earth observation imagery,”
inICPR . IEEE, 2021, pp. 3034–3041.
[61] G. Christie, N. Fendley, J. Wilson, and R. Mukherjee, “Functional map
of the world,” in CVPR , 2018, pp. 6172–6180.
[62] G. Sumbul, M. Charfuelan, B. Demir, and V . Markl, “Bigearthnet: A
large-scale benchmark archive for remote sensing image understand-
ing,” in IGARSS . IEEE, 2019, pp. 5901–5904.
[63] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z.-H. Jiang, F. E. Tay,
J. Feng, and S. Yan, “Tokens-to-token vit: Training vision transformers
from scratch on imagenet,” in ICCV , 2021, pp. 558–567.
[64] P. Ramachandran, B. Zoph, and Q. V . Le, “Searching for activation
functions,” arXiv preprint arXiv:1710.05941 , 2017.
[65] E. D. Cubuk, B. Zoph, D. Mane, V . Vasudevan, and Q. V . Le, “Au-
toaugment: Learning augmentation policies from data,” arXiv preprint
arXiv:1805.09501 , 2018.
[66] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y . Yang, “Random erasing
data augmentation,” in AAAI , vol. 34, no. 07, 2020, pp. 13 001–13 008.
[67] H. Zhang, M. Cisse, Y . N. Dauphin, and D. Lopez-Paz, “mixup: Beyond
empirical risk minimization,” ICLR , 2018.
[68] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y . Yoo, “Cutmix: Reg-
ularization strategy to train strong classiﬁers with localizable features,”
inICCV , 2019, pp. 6023–6032.
[69] Y . Yang and S. Newsam, “Bag-of-visual-words and spatial extensions
for land-use classiﬁcation,” in GEOProcessing , 2010, p. 270–279.
[70] G.-S. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y . Zhong, L. Zhang, and X. Lu,
“Aid: A benchmark data set for performance evaluation of aerial scene
classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 55, no. 7, pp.
3965–3981, 2017.
[71] Z. Zhao, J. Li, Z. Luo, J. Li, and C. Chen, “Remote Sensing Image
Scene Classiﬁcation Based on an Enhanced Attention Module,” IEEE
Geosci. Remote Sens. Lett. , vol. 18, no. 11, pp. 1926–1930, Nov. 2021.
[72] X. Zhang, W. An, J. Sun, H. Wu, W. Zhang, and Y . Du, “Best Represen-
tation Branch Model for Remote Sensing Image Scene Classiﬁcation,”
IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. , vol. 14, pp.
9768–9780, Jan. 2021.[73] B. Li, Y . Guo, J. Yang, L. Wang, Y . Wang, and W. An, “Gated
Recurrent Multiattention Network for VHR Remote Sensing Image
Classiﬁcation,” IEEE Trans. Geosci. Remote Sens. , vol. 60, p. 3093914,
Jan. 2022.
[74] S. Wang, Y . Ren, G. Parr, Y . Guan, and L. Shao, “Invariant Deep
Compressible Covariance Pooling for Aerial Scene Categorization,”
IEEE Trans. Geosci. Remote Sens. , vol. 59, no. 8, pp. 6549–6561,
Aug. 2021.
[75] Q. Wang, S. Liu, J. Chanussot, and X. Li, “Scene Classiﬁcation With
Recurrent Attention of VHR Remote Sensing Images,” IEEE Trans.
Geosci. Remote Sens. , vol. 57, no. 2, pp. 1155–1167, Feb. 2019.
[76] N. He, L. Fang, S. Li, J. Plaza, and A. Plaza, “Skip-connected
covariance network for remote sensing scene classiﬁcation,” IEEE
Trans. Neural Netw. Learn. Syst. , vol. 31, no. 5, pp. 1461–1474, 2020.
[77] F. Li, R. Feng, W. Han, and L. Wang, “High-Resolution Remote
Sensing Image Scene Classiﬁcation via Key Filter Bank Based on
Convolutional Neural Network,” IEEE Trans. Geosci. Remote Sens. ,
vol. 58, no. 11, pp. 8077–8092, Nov. 2020.
[78] S. Wang, Y . Guan, and L. Shao, “Multi-Granularity Canonical Appear-
ance Pooling for Remote Sensing Scene Classiﬁcation,” IEEE Trans.
Image Process. , vol. 29, pp. 5396–5407, Jan. 2020.
[79] G. Zhang, W. Xu, W. Zhao, C. Huang, E. N. Yk, Y . Chen, and J. Su,
“A Multiscale Attention Network for Remote Sensing Scene Images
Classiﬁcation,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. ,
vol. 14, pp. 9530–9545, Jan. 2021.
[80] A. Chattopadhay, A. Sarkar, P. Howlader, and V . N. Balasubramanian,
“Grad-cam++: Generalized gradient-based visual explanations for deep
convolutional networks,” in WACV . IEEE, 2018, pp. 839–847.
[81] S. Waqas Zamir, A. Arora, A. Gupta, S. Khan, G. Sun, F. Shah-
baz Khan, F. Zhu, L. Shao, G.-S. Xia, and X. Bai, “isaid: A large-scale
dataset for instance segmentation in aerial images,” in CVPRW , 2019,
pp. 28–37.
[82] M. Contributors, “MMSegmentation: Openmmlab semantic seg-
mentation toolbox and benchmark,” https://github.com/open-mmlab/
mmsegmentation, 2020.
[83] E. Shelhamer, J. Long, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 39, no. 4, pp. 640–651, 2017.
[84] L. Mou, Y . Hua, and X. X. Zhu, “Relation matters: Relational context-
aware fully convolutional network for semantic segmentation of high-
resolution aerial images,” IEEE Trans. Geosci. Remote Sens. , vol. 58,
no. 11, pp. 7557–7569, 2020.
[85] Y . Sun, X. Zhang, Q. Xin, and J. Huang, “Developing a multi-ﬁlter
convolutional neural network for semantic segmentation using high-
resolution aerial imagery and LiDAR data,” ISPRS J. Photogramm.
Remote Sens. , vol. 143, pp. 3–14, Sep. 2018.
[86] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu, “Dual attention
network for scene segmentation,” in CVPR , 2019, pp. 3141–3149.
[87] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing
network,” in CVPR , 2017, pp. 6230–6239.
[88] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam,
“Encoder-decoder with atrous separable convolution for semantic im-
age segmentation,” in ECCV , 2018, pp. 801–818.
[89] Q. Zhang and Y .-B. Yang, “Rest: An efﬁcient transformer for visual
recognition,” NeurIPS , vol. 34, 2021.
[90] X. Zheng, L. Huan, G.-S. Xia, and J. Gong, “Parsing very high
resolution urban scene images by learning deep ConvNets with edge-
aware loss,” ISPRS J. Photogramm. Remote Sens. , vol. 170, pp. 15–28,
Dec. 2020.
[91] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani,
“Bottleneck transformers for visual recognition,” in CVPR , 2021, pp.
16 519–16 529.
[92] L. Ding, H. Tang, and L. Bruzzone, “LANet: Local Attention Em-
bedding to Improve the Semantic Segmentation of Remote Sensing
Images,” IEEE Trans. Geosci. Remote Sens. , vol. 59, no. 1, pp. 426–
435, Jan. 2021.
[93] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional net-
works for biomedical image segmentation,” in MICCAI . Springer,
2015, pp. 234–241.
[94] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, “Denseaspp for
semantic segmentation in street scenes,” in CVPR , 2018, pp. 3684–
3692.
[95] R. Dong, X. Pan, and F. Li, “Denseu-net-based semantic segmentation
of small objects in urban remote sensing images,” IEEE Access , vol. 7,
pp. 65 347–65 356, 2019.
[96] A. Kirillov, R. Girshick, K. He, and P. Doll ´ar, “Panoptic feature
pyramid networks,” in CVPR , 2019, pp. 6392–6401.
18 JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
[97] G. Lin, A. Milan, C. Shen, and I. Reid, “Reﬁnenet: Multi-path
reﬁnement networks for high-resolution semantic segmentation,” in
CVPR , 2017, pp. 5168–5177.
[98] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking
atrous convolution for semantic image segmentation,” arXiv preprint
arXiv:1706.05587 , 2017.
[99] X. Li, Z. Zhong, J. Wu, Y . Yang, Z. Lin, and H. Liu, “Expectation-
maximization attention networks for semantic segmentation,” in ICCV ,
2019, pp. 9166–9175.
[100] Y . Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang,
“Ocnet: Object context network for scene parsing,” arXiv preprint
arXiv:1809.00916 , 2018.
[101] Z. Huang, X. Wang, L. Huang, C. Huang, Y . Wei, and W. Liu, “Ccnet:
Criss-cross attention for semantic segmentation,” in ICCV , 2019, pp.
603–612.
[102] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and
A. Agrawal, “Context encoding for semantic segmentation,” in CVPR ,
2018, pp. 7151–7160.
[103] K. Sun, Y . Zhao, B. Jiang, T. Cheng, B. Xiao, D. Liu, Y . Mu, X. Wang,
W. Liu, and J. Wang, “High-resolution representations for labeling
pixels and regions,” arXiv preprint arXiv:1904.04514 , 2019.
[104] L. Mou, Y . Hua, and X. X. Zhu, “A relation-augmented fully convolu-
tional network for semantic segmentation in aerial scenes,” in CVPR ,
2019, pp. 12 416–12 425.
[105] Z. Huang, Y . Wei, X. Wang, W. Liu, T. S. Huang, and H. Shi, “Alignseg:
Feature-aligned segmentation networks,” IEEE Trans. Pattern Anal.
Mach. Intell. , vol. 44, no. 1, pp. 550–557, 2022.
[106] Y . Yuan, X. Chen, and J. Wang, “Object-contextual representations for
semantic segmentation,” in ECCV . Springer, 2020, pp. 173–190.
[107] R. Niu, X. Sun, Y . Tian, W. Diao, K. Chen, and K. Fu, “Hybrid multiple
attention network for semantic segmentation in aerial images,” IEEE
Trans. Geosci. Remote Sens. , vol. 60, pp. 1–18, 2022.
[108] Z. Zheng, Y . Zhong, J. Wang, and A. Ma, “Foreground-aware relation
network for geospatial object segmentation in high spatial resolution
remote sensing imagery,” in CVPR , 2020, pp. 4095–4104.
[109] A. Ma, J. Wang, Y . Zhong, and Z. Zheng, “Factseg: Foreground
activation-driven small object semantic segmentation in large-scale
remote sensing imagery,” IEEE Trans. Geosci. Remote Sens. , vol. 60,
pp. 1–16, 2022.
[110] “Aerialdetection,” https://github.com/dingjiansw101/AerialDetection.
[111] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar, “Focal loss
for dense object detection,” IEEE Trans. Pattern Anal. Mach. Intell. ,
vol. 42, no. 2, pp. 318–327, 2020.
[112] Q. Ming, Z. Zhou, L. Miao, H. Zhang, and L. Li, “Dynamic anchor
learning for arbitrary-oriented object detection,” in AAAI , vol. 35, no. 3,
2021, pp. 2355–2363.
[113] W. Qian, X. Yang, S. Peng, J. Yan, and Y . Guo, “Learning modulated
loss for rotated object detection,” AAAI , vol. 35, no. 3, pp. 2458–2466,
May 2021.
[114] X. Yang, J. Yan, Z. Feng, and T. He, “R3det: Reﬁned single-stage
detector with feature reﬁnement for rotating object,” AAAI , vol. 35,
no. 4, pp. 3163–3171, May 2021.
[115] J. Han, J. Ding, J. Li, and G.-S. Xia, “Align Deep Features for Oriented
Object Detection,” IEEE Trans. Geosci. Remote Sens. , vol. 60, p.
3062048, Jan. 2022.
[116] S. M. Azimi, E. Vig, R. Bahmanyar, M. K”orner, and P. Reinartz,
“Towards multi-class object detection in unconstrained remote sensing
imagery,” in ACCV . Springer, 2018, pp. 150–165.
[117] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” IEEE Trans. Pattern
Anal. Mach. Intell. , vol. 39, no. 6, pp. 1137–1149, June 2017.
[118] G. Zhang, S. Lu, and W. Zhang, “CAD-Net: A Context-Aware Detec-
tion Network for Objects in Remote Sensing Imagery,” IEEE Trans.
Geosci. Remote Sens. , vol. 57, no. 12, pp. 10 015–10 024, Dec. 2019.
[119] J. Ding, N. Xue, Y . Long, G.-S. Xia, and Q. Lu, “Learning roi
transformer for oriented object detection in aerial images,” in CVPR ,
2019, pp. 2844–2853.
[120] X. Yang, J. Yang, J. Yan, Y . Zhang, T. Zhang, Z. Guo, X. Sun, and
K. Fu, “Scrdet: Towards more robust detection for small, cluttered and
rotated objects,” in ICCV , 2019, pp. 8231–8240.
[121] Y . Xu, M. Fu, Q. Wang, Y . Wang, K. Chen, G.-S. Xia, and X. Bai,
“Gliding vertex on the horizontal bounding box for multi-oriented
object detection,” IEEE Trans. Pattern Anal. Mach. Intell. , vol. 43,
no. 4, pp. 1452–1459, 2021.
[122] C. Li, C. Xu, Z. Cui, D. Wang, T. Zhang, and J. Yang, “Feature-
attentioned object detection in remote sensing imagery,” in ICIP , 2019,
pp. 3886–3890.[123] S. Zhang, C. Chi, Y . Yao, Z. Lei, and S. Z. Li, “Bridging the gap
between anchor-based and anchor-free detection via adaptive training
sample selection,” in CVPR , 2020, pp. 9756–9765.
[124] K. Fu, Z. Chang, Y . Zhang, and X. Sun, “Point-based estimator
for arbitrary-oriented object detection in aerial images,” IEEE Trans.
Geosci. Remote Sens. , vol. 59, no. 5, pp. 4370–4387, 2021.
[125] J. Wang, J. Ding, H. Guo, W. Cheng, T. Pan, and W. Yang, “Mask
OBB: A Semantic Attention-Based Mask Oriented Bounding Box
Representation for Multi-Category Object Detection in Aerial Images,”
Remote Sens. , vol. 11, no. 24, p. 2930, Dec. 2019.
[126] X. Xie, G. Cheng, J. Wang, X. Yao, and J. Han, “Oriented r-cnn for
object detection,” in ICCV , October 2021, pp. 3520–3529.
[127] Z. Zhang, W. Guo, S. Zhu, and W. Yu, “Toward Arbitrary-Oriented
Ship Detection With Rotated Region Proposal and Discrimination
Networks,” IEEE Geosci. Remote Sens. Lett. , vol. 15, no. 11, pp. 1745–
1749, Nov. 2018.
[128] M. Liao, Z. Zhu, B. Shi, G.-s. Xia, and X. Bai, “Rotation-sensitive
regression for oriented scene text detection,” in CVPR , 2018, pp. 5909–
5918.
[129] T. Zhang, X. Zhang, P. Zhu, P. Chen, X. Tang, C. Li, and L. Jiao,
“Foreground reﬁnement network for rotated object detection in remote
sensing images,” IEEE Trans. Geosci. Remote Sens. , vol. 60, pp. 1–13,
2022.
[130] Y . Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu, and
Z. Luo, “R2cnn: Rotational region cnn for orientation robust scene
text detection,” arXiv preprint arXiv:1706.09579 , 2017.
[131] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y . Zheng, and X. Xue,
“Arbitrary-oriented scene text detection via rotation proposals,” IEEE
Trans. Multimedia , vol. 20, no. 11, pp. 3111–3122, 2018.
[132] X. Zhang, G. Wang, P. Zhu, T. Zhang, C. Li, and L. Jiao, “Grs-det: An
anchor-free rotation ship detector based on gaussian-mask in remote
sensing images,” IEEE Trans. Geosci. Remote Sens. , vol. 59, no. 4, pp.
3518–3531, 2021.
[133] F. Zhang, X. Wang, S. Zhou, Y . Wang, and Y . Hou, “Arbitrary-
oriented ship detection through center-head point extraction,” IEEE
Trans. Geosci. Remote Sens. , vol. 60, pp. 1–14, 2022.
[134] Z. Chen, J. Zhang, and D. Tao, “Recursive context routing for object
detection,” Int. J. Comput. Vis. , vol. 129, no. 1, pp. 142–160, 2021.
[135] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu,
M. Pelillo, and L. Zhang, “Dota: A large-scale dataset for object
detection in aerial images,” in CVPR , June 2018.
[136] Z. Liu, L. Yuan, L. Weng, and Y . Yang, “A high resolution optical
satellite image dataset for ship recognition and some new baselines,”
inICPRAM , 2017, pp. 324–331.
[137] M. A. Lebedev, Y . V . Vizilter, O. V . Vygolov, V . A. Knyaz, and
A. Y . Rubis, “Change Detection in Remote Sensing Images Using
Conditional Adversarial Networks,” Int. Arch. Photogrammetry, Remote
Sens. Spatial Inf. Sci. , vol. 422, pp. 565–571, May 2018.
[138] H. Chen and Z. Shi, “A spatial-temporal attention-based method and a
new dataset for remote sensing image change detection,” Remote Sens. ,
vol. 12, no. 10, 2020.
[139] S. Ji, S. Wei, and M. Lu, “Fully convolutional networks for multisource
building extraction from an open aerial and satellite imagery data set,”
IEEE Trans. Geosci. Remote Sens. , vol. 57, no. 1, pp. 574–586, 2019.
[140] H. Chen, Z. Qi, and Z. Shi, “Remote Sensing Image Change Detection
With Transformers,” IEEE Trans. Geosci. Remote Sens. , vol. 60, p.
3095166, Jan. 2022.
[141] R. Caye Daudt, B. Le Saux, and A. Boulch, “Fully convolutional
siamese networks for change detection,” in ICIP , 2018, pp. 4063–4067.
[142] Z. Zheng, Y . Wan, Y . Zhang, S. Xiang, D. Peng, and B. Zhang,
“CLNet: Cross-layer convolutional neural network for change detection
in optical remote sensing imagery,” ISPRS J. Photogramm. Remote
Sens. , vol. 175, pp. 247–267, May 2021.
[143] S. Fang, K. Li, J. Shao, and Z. Li, “SNUNet-CD: A Densely Connected
Siamese Network for Change Detection of VHR Images,” IEEE Geosci.
Remote Sens. Lett. , vol. 19, p. 3056416, Jan. 2022.
[144] C. Zhang, P. Yue, D. Tapete, L. Jiang, B. Shangguan, L. Huang,
and G. Liu, “A deeply supervised image fusion network for change
detection in high resolution bi-temporal remote sensing images,” ISPRS
J. Photogramm. Remote Sens. , vol. 166, pp. 183–200, Aug. 2020.
[145] J. Chen, Z. Yuan, J. Peng, L. Chen, H. Huang, J. Zhu, Y . Liu, and H. Li,
“DASNet: Dual Attentive Fully Convolutional Siamese Networks for
Change Detection in High-Resolution Satellite Images,” IEEE J. Sel.
Topics Appl. Earth Observ. Remote Sens. , vol. 14, pp. 1194–1206, Jan.
2021.
WANG et al. : EMPIRICAL STUDY OF REMOTE SENSING PRETRAINING 19
[146] M. Liu, Q. Shi, A. Marinoni, D. He, X. Liu, and L. Zhang, “Super-
Resolution-Based Change Detection Network With Stacked Attention
Module for Images With Different Resolutions,” IEEE Trans. Geosci.
Remote Sens. , vol. 60, p. 3091758, Jan. 2022.
[147] H. Du, Y . Zhuang, S. Dong, C. Li, H. Chen, B. Zhao, and L. Chen,
“Bilateral Semantic Fusion Siamese Network for Change Detection
From Multitemporal Optical Remote Sensing Imagery,” IEEE Geosci.
Remote Sens. Lett. , vol. 19, p. 3082630, Jan. 2022.
[148] Q. Shi, M. Liu, S. Li, X. Liu, F. Wang, and L. Zhang, “A deeply
supervised attention metric-based network and an open aerial image
dataset for remote sensing change detection,” IEEE Trans. Geosci.
Remote Sens. , vol. 60, pp. 1–16, 2022.
[149] X. Hou, Y . Bai, Y . Li, C. Shang, and Q. Shen, “High-resolution
triplet network with dynamic multiscale feature for change detection
on satellite images,” ISPRS J. Photogramm. Remote Sens. , vol. 177,
pp. 103–115, Jul. 2021.
[150] H. Chen, W. Li, and Z. Shi, “Adversarial Instance Augmentation for
Building Change Detection in Remote Sensing Images,” IEEE Trans.
Geosci. Remote Sens. , vol. 60, p. 3066802, Jan. 2022.
[151] W. G. C. Bandara and V . M. Patel, “A transformer-based siamese
network for change detection,” arXiv preprint arXiv:2201.01293 , 2022.
[152] Q. Ke and P. Zhang, “CS-HSNet: A Cross-Siamese Change Detection
Network Based on Hierarchical-Split Attention,” IEEE J. Sel. Topics
Appl. Earth Observ. Remote Sens. , vol. 14, pp. 9987–10 002, Jan. 2021.
[153] H. Lee, K. Lee, J. H. Kim, Y . Na, J. Park, J. P. Choi, and J. Y . Hwang,
“Local Similarity Siamese Network for Urban Land Change Detection
on Remote Sensing Images,” IEEE J. Sel. Topics Appl. Earth Observ.
Remote Sens. , vol. 14, pp. 4139–4149, Jan. 2021.
[154] Z. Zheng, A. Ma, L. Zhang, and Y . Zhong, “Change is everywhere:
Single-temporal supervised object change detection in remote sensing
imagery,” in ICCV , 2021, pp. 15 173–15 182.
PersP ective
https://doi.org/10.1038/s41586-019-0912-1
Deep learning and process understanding 
for data-driven Earth system science
Markus r eichstein1,2*, Gustau camps-v alls3, Bjorn stevens4, Martin Jung1, Joachim Denzler2,5, Nuno carvalhais1,6 & Prabhat7
Machine learning approaches are increasingly used to extract patterns and insights from the ever-increasing stream of 
geospatial data, but current approaches may not be optimal when system behaviour is dominated by spatial or temporal 
context. Here, rather than amending classical machine learning, we argue that these contextual cues should be used as 
part of deep learning (an approach that is able to extract spatio-temporal features automatically) to gain further process 
understanding of Earth system science problems, improving the predictive ability of seasonal forecasting and modelling 
of long-range spatial connections across multiple timescales, for example. The next step will be a hybrid modelling 
approach, coupling physical process models with the versatility of data-driven machine learning.
Humans have always striven to predict and understand the world, 
and the ability to make better predictions has given competi -
tive advantages in diverse contexts (such as weather, diseases or 
financial markets). Y et the tools for prediction have substantially changed 
over time, from ancient Greek philosophical reasoning to non-scientific 
medieval methods such as soothsaying, towards modern scientific dis -
course, which has come to include hypothesis testing, theory develop -
ment and computer modelling underpinned by statistical and physical 
relationships, that is, laws1. A success story in the geosciences is weather 
prediction, which has greatly improved through the integration of better 
theory, increased computational power, and established observational 
systems, which allow for the assimilation of large amounts of data into the 
modelling system2. Nevertheless, we can accurately predict the evolution 
of the weather on a timescale of days, not months. Seasonal meteorolog-
ical predictions, forecasting extreme events such as flooding or fire, and 
long-term climate projections are still major challenges. This is especially 
true for predicting dynamics in the biosphere, which is dominated by 
biologically mediated processes such as growth or reproduction, and is 
strongly controlled by seemingly stochastic disturbances such as fires and 
landslides. Such predictive problems have not seen much progress in the 
past few decades3.
At the same time, a deluge of Earth system data has become available, 
with storage volumes already well beyond dozens of petabytes and rapidly 
increasing transmission rates exceeding hundreds of terabytes per day4. 
These data come from a plethora of sensors measuring states, fluxes and 
intensive or time/space-integrated variables, representing fifteen or more 
orders of temporal and spatial magnitude. They include remote sensing 
from a few metres to hundreds of kilometres above Earth as well as in situ 
observations (increasingly from autonomous sensors) at and below the 
surface and in the atmosphere, many of which are further being comple -
mented by citizen science observations. Model simulation output adds to 
this deluge; the CMIP-5 dataset of the Climate Model Intercomparison 
Project, used extensively for scientific groundwork towards periodic  
climate assessments, is over 3 petabytes in size, and the next generation, 
CMIP-6, is estimated to reach up to 30 petabytes5. The data from models 
share many of the challenges and statistical properties of observational 
data, including many forms of uncertainty. In summary, Earth system 
data are exemplary of all four of the ‘four Vs’ of ‘big data’: volume, velocity, variety and veracity (see Fig.  1). One key challenge is to extract interpret -
able information and knowledge from this big data, possibly almost in 
real time and integrating between disciplines.
Taken together, our ability to collect and create data far outpaces our 
ability to sensibly assimilate it, let alone understand it. Predictive ability in 
the last few decades has not increased apace with data availability. To get 
the most out of the explosive growth and diversity of Earth system data, 
we face two major tasks in the coming years: (1) extracting knowledge 
from the data deluge, and (2) deriving models that learn much more 
from data than traditional data assimilation approaches can, while still 
respecting our evolving understanding of nature’s laws.
The combination of unprecedented data sources, increased computa-
tional power, and the recent advances in statistical modelling and machine 
learning offer exciting new opportunities for expanding our knowledge 
about the Earth system from data. In particular, many tools are available 
from the fields of machine learning and artificial intelligence, but they 
need to be further developed and adapted to geo-scientific analysis. Earth 
system science offers new opportunities, challenges and methodological  
demands, in particular for recent research lines focusing on spatio-  
temporal context and uncertainties (Box  1; see https://developers.
google.com/machine-learning/glossary/ and http://www.wildml.com/
deep-learning-glossary/ for more complete glossaries).
In the following sections we review the development of machine learn -
ing in the geoscientific context, and highlight how deep learning—that 
is, the automatic extraction of abstract (spatio-temporal) features—has 
the potential to overcome many of the limitations that have, until now, 
hindered a more wide-spread adoption of machine learning. We further 
lay out the most promising but also challenging approaches in combining 
machine learning with physical modelling.
State-of-the-art geoscientific machine learning
Machine learning is now a successful part of several research-driven 
and operational geoscientific processing schemes, addressing the 
atmosphere, the land surface and the ocean, and has co-evolved with 
data availability over the past decade. Early landmarks in classifica -
tion of land cover and clouds emerged almost 30 years ago through 
the coincidence of high-resolution satellite data and the first revival 
of neural networks6,7. Most major machine learning methodological 
1Department of Biogeochemical Integration, Max Planck Institute for Biogeochemistry, Jena, Germany. 2Michael-Stifel-Center Jena for Data-driven and Simulation Science, Jena, Germany. 3Image 
Processing Laboratory (IPL), University of València, Valencia, Spain. 4Max Planck Institute for Meteorology, Hamburg, Germany. 5Computer Vision Group, Computer Science, Friedrich Schiller 
University, Jena, Germany. 6CENSE, Departamento de Ciências e Engenharia do Ambiente, Faculdade de Ciências e Tecnologia, Universidade NOVA de Lisboa, Lisbon, Portugal. 7National Energy 
Research Supercomputing Center, Lawrence Berkeley National Laboratory, Berkeley, CA, USA. *e-mail: mreichstein@bgc-jena.mpg.de
14 F eBrUA rY 2019 | vOL 566 | NA tUre  | 195
PersPective reseArcHdevelopment (for example, kernel methods or ‘random forests’) has 
subsequently been applied to geoscience and remote sensing problems, 
often when data suitable for pertinent methods became available8. 
Thus, machine learning has become a universal approach in geoscien -
tific classification, and change- and anomaly-detection problems9–12. In 
the past few years, geoscience has begun to use deep learning to better 
exploit spatial and temporal structures in the data, features that would 
normally be problematic for traditional machine learning to extract 
(see Table  1, and below).
Another class of problem where machine learning has been success -
ful is regression. An example is soil mapping, where measurements 
of soil properties and covariates exist at points sparsely distributed in 
space, and where a ‘random forest’ , a popular and efficient machine 
learning approach, is used to predict spatially dense estimates of soil 
properties or soil types13,14. In the past decade, machine learning has 
attained outstanding results in the regression estimation of biogeo -
physical parameters from remotely sensed reflectances at local and 
global scales15–17. These approaches emphasize spatial prediction, that 
is, prediction of properties that are relatively static over the observa -
tional time period.
Y et what makes the Earth system interesting is that it is not static, 
but dynamic. Machine learning regression techniques have also been 
used to study these dynamics by mapping temporally varying features 
onto temporally varying target variables in land, ocean and atmos -
phere domains. Since variables such as land–atmosphere or ocean–  
atmosphere carbon uptake cannot be observed everywhere, one challenge  
has been to infer continental or global estimates from point obser -
vations, by building models that relate climate and remote-sensing  
co-variates to the target variables. In this context, machine learning  
methods have proved more powerful and flexible than previous 
mechanistic or semi-empirical modelling approaches. For instance, 
an artificial neural network with one hidden layer was able to filter 
out noise, predict the diurnal and seasonal variation of carbon dioxide  
(CO 2) fluxes, and extract patterns such as an increased respiration 
in spring during root growth, which was formerly unquantified and 
not well represented in carbon cycle models18. Further developments 
have then allowed us to quantify global terrestrial photosynthesis and 
evapotranspiration of water in a purely data-driven way19,20. Spatial, seasonal, interannual or decadal variations of such machine-learning-  
predicted fluxes are even being used as important benchmarks for physical  
land-surface and climate model evaluation21–24. Similarly, ocean CO 2 
concentrations and fluxes have been mapped spatio-temporally with 
neural networks, where classification and regression approaches have 
been combined, both for stratifying the data and for prediction25. 
Recently the random forest method has also been used to predict 
spatio-temporally varying precipitation26. Overall, we conclude that a 
diversity of influential machine learning approaches have already been 
applied across all the major sub-domains of Earth system science and 
are increasingly being integrated into operational schemes and being 
used to discover patterns, to improve our understanding and to evaluate 
comprehensive physical models.
Notwithstanding the success of machine learning in the geosciences, 
important caveats and limitations have hampered its wider adoption 
and impact. A few pitfalls such as the risk of naive extrapolation,  
sampling or other data biases, ignorance of confounding factors, inter -
pretation of statistical association as causal relation, or fundamental 
flaws in multiple hypothesis testing (‘P -fishing’)27–29 should be avoided 
by best practice and expert intervention. More fundamentally, there 
are inherent limitations of machine learning approaches as applied at 
present. It is in this realm that the techniques of deep learning promise 
breakthroughs.
Classical machine learning approaches benefit from domain-specific, 
hand-crafted features to account for dependencies in time or space (for 
example, cumulative precipitation derived from a daily time series), but 
rarely exploit spatio-temporal dependencies exhaustively. For instance, 
in ocean–atmosphere or land–atmosphere CO 2 flux prediction19,25, 
mapping of instantaneous, local environmental conditions (such as 
radiation, temperature and humidity) to instantaneous fluxes is per -
formed. In reality, processes at a certain point in time and space are 
almost always additionally affected by the state of the system, which is 
often not well observed and thus not available as a predictor. However, 
previous time steps and neighbouring grid cells contain hidden infor -
mation on the state of the system (for example, a long period without 
rainfall combined with sustained sunny days implies a drought). One 
example where both spatial and temporal context are highly relevant 
is the prediction of fire occurrence and characteristics such as burnt 
area and trace gas emissions. Fire occurrence and spread depends not 
only on instantaneous climatic drivers and sources of ignition (such 
as humans, lightning or both) but also on state variables, such as the 
state and amount of available fuel3. Fire spread and thus the burned 
area depends not only on the local conditions of each pixel but also on 
the spatial arrangement and connectivity of fuel, its moisture, terrain 
properties, and of course wind speed and direction. Similarly, classi -
fying a certain atmospheric situation as a hurricane or extratropical  
storm requires knowledge of the spatial context such as a storm’s 
geometry as constituted by pixels, their values, and their topology. For 
instance, detecting symmetric outflow and a visible ‘eye’ is important 
for detecting hurricanes and assessing their strength, and this cannot 
be determined by localized, single-pixel values alone.
Certainly, temporally dynamic properties (‘memory effects’) can be 
represented by hand-designed and domain-specific features in machine 
learning. Examples are cumulative sums of daily temperature, which are 
used to predict phenological phases of vegetation, and the standardized 
precipitation index30, which summarizes precipitation anomalies over 
the last months as a meteorological indicator of drought states. Very 
often, these approaches only consider memory in a single variable, 
ignoring the interactive effects of several variables, although excep -
tions exist22,31.
Machine learning can also use hand-designed features, such as  
terrain shape and topographical or texture features from satellite 
images, to incorporate spatial context6. This is analogous to earlier 
approaches in computer vision where objects were often characterized  
by a set of features describing edges, textures, shapes and colours. Such features were then fed into a standard machine learning algo
-
rithm for localization, classification or detection of objects in images. 
Volume
Data size
Velocity
Speed of
change
Variety
Diverse data
sources
Veracity
Uncertainty
of dataObserved and
simulated ‘big data’
Small and ‘digestible’
Real-time critical
in some ar eas, not all
Integrated acr oss
disciplines
Con/f_idence
robustnessPatter ns and
knowledge
‘Data from knowledge’‘Knowledge from data’
Fig. 1 | Big data challenges in the geoscientific context. Data size now 
exceeds 100 petabytes, and is growing quasi-exponentially (tapering of the figure to the right indicates decreasing data size.) The speed of change 
exceeds 5 petabytes a year; data are taken at frequencies of up to 10 Hz or 
more; reprocessing and versioning are common challenges. Data sources can be one- to four-dimensional, spatially integrated, from the organ level (such as leaves) to the global level. Earth has diverse observational systems, from remote sensing to in situ observation. The uncertainty of data can stem from observational errors or conceptual inconsistencies.
196 | NA tUre  | vOL 566 | 14 F eBrUA rY 2019
PersPective reseArcHSimilar approaches have been followed for decades in remote-sensing 
image classification8–10. Hand-designed features can be seen both as an 
advantage (control of the explanatory drivers) and as a disadvantage  
(tedious, ad hoc process, probably non-optimal), but certainly the  
concerns related to the use of a restricted and subjective choice of fea -
tures rather than an extensive and generic approach remain valid and 
important. New developments in deep learning, however, no longer 
limit us to such approaches.
Deep learning opportunities in Earth system science
Deep learning has achieved notable success in modelling ordered 
sequences and data with spatial context in the fields of computer 
vision, speech recognition and control systems32, as well as in related 
scientific fields in physics33–35, chemistry36 and biology37 (see also  ref. 38). Applications to problems in geosciences are in their infancy, 
but across the key problems (classification, anomaly detection, regres -
sion, space- or time-dependent state prediction) there are promising 
examples (see Table  1 and Supplementary Fig. 2)39,40. Two recent 
studies demonstrate the application of deep learning to the problem 
of extreme weather, for instance hurricane detection41,42—already 
mentioned as problematic for traditional machine learning to per -
form. The studies report success in applying deep learning archi -
tectures to objectively extract spatial features to define and classify 
extreme situations (for example, storms, atmospheric rivers) in 
numerical weather prediction model output. Such an approach ena -
bles rapid detection of such events and forecast simulations without 
using either subjective human annotation or methods that rely on 
predefined arbitrary thresholds for wind speed or other variables. Box 1 
Definition of terms
Term Explanation
Artificial intelligence, machine 
learning and deep learningArtificial intelligence is the capacity of an algorithm to assimilate information to perform tasks that are characteristic of human intelligence, such as recognizing objects and sounds, contextualizing language, learning from the environment, and 
problem solving.
Machine learning is a field of statistical research for training computational algorithms that split, sort and transform a set of 
data to maximize the ability to classify, predict, cluster or discover patterns in a target dataset.
Deep learning refers to machine learning algorithms that construct hierarchical architectures of increasing sophistication. 
Artificial neural networks with many layers are examples of deep learning algorithms.
Bayesian inference Bayesian inference is a framework in statistics and machine learning that develops methods for data analysis in which 
observational evidence is used to update the probability that an hypothesis is true. The framework is mostly concerned about 
treating uncertainty, encoding prior beliefs and estimating error propagation when dealing with data and models.
Causal inference Causal inference links events, processes or properties in a system via a cause-and-effect connection. Recent observational 
causal inference algorithms attempt to discover causal relationships in observational data.
Convolution Convolution is one of the most important operations in signal and image processing, and it can operate in objects that are 
one-dimensional (for example, speech), two-dimensional (for example, images) or three-dimensional (for example, video). 
A convolutional filter is essentially a weighting vector/matrix/cube that uses a sliding-window approach. Depending on the kernel structure, the convolution enhances some features of the data, such as edges, trends or flat regions. Convolution is 
embedded in convolutional neural networks at the neuron level, which extracts useful features from the previous layers.
Differentiable programming Differentiable programming refers to a programming paradigm to generate code that is automatically differentiated, such that 
its parameters can be seamlessly optimized. It generalizes current deep learning frameworks to arbitrary programs, which 
may include the hybrid modelling approaches that we discuss in ‘Integration with physical modelling’.
Feedforward versus recurrent 
networksAn artificial neural network is a computational algorithm that simulates how signals are transferred between a network of neurons, via synapses. In an artificial neural network, information is transferred only in the forward direction, whereas in a 
recurrent artificial neural network the information can cycle or loop between the different nodes, creating complex dynamics 
such as memory, as seen in data.
Generative adversarial 
networksThis is a family of unsupervised machine learning methods widely used to generate realistic samples from an unknown  
probability density function. Generative adversarial networks are formed by a neural network that generates plausible  
examples; these examples are then used to attempt to fool a discriminator network that should discern real from fake examples.
Memory effects This is a metaphorical term meaning that the current behaviour of a system cannot be explained without considering the effect of past states or forcing variables.
Nowcasting and forecasting To forecast a certain variable means to establish a prediction of its value in the future, days to centuries from now. Nowcasting 
refers to making that prediction for a very near future (for example, predicting whether it is going to rain in a couple of hours).
Probabilistic programming Probabilistic programming is a method of defining probabilistic models using a unified high-level programming language. 
Statistical inference is automatically achieved by built-in inference machines, freeing the developer from the difficulties of 
high-performance probabilistic inference.
Radiative transfer models These are mathematical models that describe how radiation at different wavelengths (such as visible light) propagates 
through different media (such as the atmosphere or a vegetation canopy) by simulating absorption, emission, transmission 
and scattering processes.
Remote sensing Most remote sensing deals with measuring the radiance at different wavelengths reflected or emitted from an object or 
surface. Remote sensing uses satellite or airborne sensors to detect and classify objects as well as to estimate geoscientific 
variables of interest (temperature, salinity or carbon dioxide concentrations), based on propagated reflectance signals (such 
as electromagnetic radiation).
Supervised and unsupervised 
learningIn supervised learning an algorithm learns the input-to-output relationship after being provided both the inputs and the respective outputs. For example, the input might be a set of photos and the output might be a set of corresponding labels. In unsupervised learning the algorithm does not have access to the output, so the goal is to infer the underlying structure of the 
data. For example, the algorithm could automatically separate pictures with different statistical or semantic properties (such 
as a set of images of cats and dogs).
Teleconnections Teleconnections refer to climate anomalies related to each other at large distances (typically thousands of kilometres).  
Quantifying teleconnection patterns allows the prediction of key patterns on Earth, which are distant in space and time.  
For example, predicting an El Niño event enables prediction of North American rainfall, snowfall, droughts or temperature 
patterns with lead times of a few weeks to months.
14 F eBrUA rY 2019 | vOL 566 | NA tUre  | 197
PersPective reseArcHIn particular, such an approach uses the information in the spatial 
shape of events, such as the typical spiral of hurricanes. Similarly, for 
classification of urban areas the automatic extraction of multi-scale 
features from remote-sensing data strongly improved the classifica -
tion accuracy (to almost always greater than 95%)43.
While deep learning approaches have classically been divided into 
spatial learning (for example, convolutional neural networks for 
object classification) and sequence learning (for example, speech 
recognition), there is a growing interest in blending these two 
perspectives. A prototypical example is video and motion predic -
tion44,45, a problem that has striking similarities to many dynamic 
geoscience problems. Here we are faced with time-evolving multi-  
dimensional structures, such as organized precipitating convection,  
which dominates patterns of tropical rainfall, and vegetation 
states that influence the flow of carbon and evapotranspiration. 
Studies are beginning to apply combined convolutional–recurrent 
approaches to geoscientific problems such as precipitation nowcast -
ing (Table  1)46. Modelling atmospheric and ocean transport, fire 
spread, soil movements or vegetation dynamics are other examples 
of problems where spatio-temporal dynamics are important, but 
that have yet to benefit from a concerted effort to apply these new 
approaches.
In short, the similarities between the types of data addressed with  
classical deep learning applications and geoscientific data make a compelling argument for the integration of deep learning into the 
geosciences (Fig.  2). Images are analogous to two-dimensional data 
fields containing particular variables in analogy to colour triplets 
(RGB values) in photographs, while videos can be linked to a sequence 
of images and hence to two-dimensional fields that evolve over time. 
Similarly, natural language and speech signals share the same multi-  
resolution characteristics of dynamic time series of Earth system  
variables. Furthermore, classification, regression, anomaly detection, 
and dynamic modelling are typical problems in both computer vision 
and the geosciences.
Deep-learning challenges in Earth system science
The similarities between classical deep learning applications and  
geoscience applications outlined above are striking. Y et numerous  differences exist. For example, while classical computer vision  
applications deal with photos which have three channels (red, green, 
blue) hyperspectral satellite images extend to hundreds of spectral channels well beyond the visible range, which often induce differ
-
ent statistical properties to those of natural images. This includes 
spatial dependence and interdependence of variables, violating the 
important assumption of identically, independently distributed data. 
Additionally, integrating multi-sensor data is not trivial since different  
sensors exhibit different imaging geometries, spatial and temporal  
resolution, physical meaning, content and statistics. Sequences of (multi-sensor) satellite observations also come with diverse noise 
sources, uncertainty levels, missing data and (often systematic) gaps 
(owing to the presence of clouds or snow, distortions in acquisition, 
storage and transmission, and so on).
In addition, spectral, spatial and temporal dimensionalities raise 
computational challenges. Data volume is increasing and soon it will 
be necessary to deal with petabytes per day globally. At present, the 
biggest meteorological agencies have to process terabytes per day in 
near real time, often at very high (32-bit, 64-bit) precision. Further, 
while typical computer vision applications have worked with image 
sizes of 512  × 512 pixels, a moderate-resolution (around 1 km) global 
field has sizes of approximately 40,000  × 20,000 pixels, that is, three 
orders of magnitude more.
Last but not least, unlike on ImageNet (a database of human-labelled 
images, with labels like, for example, ‘cat’ or ‘dog’47), large, labelled  
geoscientific datasets do not always exist in geoscience, not only 
because of the sizes of the datasets involved, but also owing to the con -
ceptual difficulty in labelling datasets; for example, determining that 
an image depicts a cat is much easier than determining that a dataset  
reflects a drought, given that droughts are contingent on intensity 
and extent and can change according to the methods used to collect 
and analyse the data, and that there are not enough labelled cases for 
training a machine learning system. Besides the challenge of working 
with a limited training set, geoscientific problems are often under -
constrained, leading to the possibility of models thought to be of high 
quality, which perform well in training and even test datasets, but deviate strongly for situations and data outside their valid domain 
(the extrapolation problem), which is true even for complex physical Table 1 |  Conventional approaches and deep learning approaches to geoscientific tasks
Analytical task Scientific task Conventional approaches Limitations of conventional approaches Emergent or potential approaches
Classification and anomaly detection
Finding extreme weather 
patternsMultivariate, threshold-based detectionHeuristic approach, ad hoc criteria usedSupervised and semi-supervised  
convolutional neural networks
41,42
Land-use and change 
detectionPixel-by-pixel spectral  
classificationShallow spatial context used, or none Convolutional neural networks43
Regression
Predict fluxes from  
atmospheric conditionsRandom forests, kernel 
methods, feedforward neural 
networksMemory and lag effects not  
consideredRecurrent neural networks, long-  
short-term-memories (LSTMs)89,99,100
Predict vegetation properties from atmospheric conditionsSemi-empirical algorithms  
(temperature sums, water 
deficits)Prescriptive in terms of functional 
forms and dynamic assumptionsRecurrent neural networks
90, possibly 
with spatial context
Predict river runoff in  
ungauged catchmentsProcess models or statistical 
models with hand-designed 
topographic features91Consideration of spatial context  
limited to hand-designed featuresCombination of convolutional neural network with recurrent networks
State prediction
Precipitation nowcasting Physical modelling with data assimilationComputational limits due to resolution,  
data used only to update statesConvolutional–LSTM nets short-range spatial context
92
Downscaling and bias-  
correcting forecastsDynamic modelling and  
statistical approachesComputational limits, subjective feature selectionConvolutional nets
72, conditional 
generative adversarial networks 
(cGANs)53,93,101
Seasonal forecasts Physical modelling with initial 
conditions from dataFully dependent on physical model, current skill relatively weakConvolutional–LSTM nets with  
long-range spatial context
Transport modelling Physical modelling of transport Fully dependent on physical model, computational limitsHybrid physical–convolutional network models
68,94
198 | NA tUre  | vOL 566 | 14 F eBrUA rY 2019
PersPective reseArcHEarth system models48. Overall, we identify five major challenges and 
avenues for the successful adoption of deep learning approaches in 
the geosciences, as follows.
(1) Interpretability
Improving predictive accuracy is important but insufficient. 
Certainly, interpretability and understanding are crucial, including 
visualization of the results for analysis by humans. Interpretability 
has been identified as a potential weakness of deep neural networks, 
and achieving it is a current focus in deep learning49. The field is 
still far from achieving self-explanatory models, and also far from 
causal discovery from observational data50,51. Y et we should note that, given their complexity, modern Earth system models are in practice 
often also not easily traceable back to their assumptions, limiting 
their interpretability too.
(2) Physical consistency
Deep learning models can fit observations very well, but predictions 
may be physically inconsistent or implausible, owing to extrapo -
lation or observational biases, for example. Integration of domain 
knowledge and achievement of physical consistency by teaching 
models about the governing physical rules of the Earth system can 
provide very strong theoretical constraints on top of the observa -
tional ones.
a
b
c
dMachine lear ning tasks Earth science tasks
Object classiﬁcation and localization
Super -resolution and fusion
Video pr edictio n
Language translationPatter n classiﬁcation
Statistical downscaling and blendin g
Short-term for ecastin g
Dynamic time series modelling
Dog: 0.994
Cat: 0.982
8 × 8
input32 × 32
samplesGround
truth
Time
Predict futur e visual
representation
Real vs pr edicted humidity valuesXt
φ (Xt)
φ (Xt+1)Xt+1
Encoder
Embe d
He loved to eat ..
S Null Er liebtez ue ssenEr liebtez ue ssen
Decode rSoftmax
TimeHumidity
Fig. 2  | Four examples of typical deep learning applications (left 
panels) and the geoscientific problems they can be applied to (right 
panels).  a, Object recognition in images links to classification of 
extreme weather patterns using a unified convolutional neural network 
on climate simulation data41. b, Super-resolution applications relate to 
statistical downscaling of climate model output72. c, Video prediction is similar to short-term forecasting of Earth system variables. Right image, 
courtesy of Sujan Koirala and Paul Bodesheim, Max Planck Institute for 
Biogeochemistry. d, Language translation links to modelling of dynamic 
time series (ref. 96 and figure 11 in ref. 97). Left image, courtesy of Stephen 
Merity (figure 1 in https://smerity.com/articles/2016/google_nmt_arch.
html ).
14 F eBrUA rY 2019 | vOL 566 | NA tUre  | 199
PersPective reseArcH(3) Complex and uncertain data
Deep learning methods are needed to cope with complex statistics, 
multiple outputs, different noise sources and high-dimensional spaces. 
New network topologies that not only exploit local neighbourhood 
(even at different scales), but also long-range relationships (for example, 
for teleconnections) are urgently needed, but the exact cause-and-effect 
relations between variables are not clear in advance and need to be  
discovered. Modelling uncertainties will be certainly an important aspect 
and will require concepts from Bayesian/probabilistic inference to be 
integrated, directly addressing such uncertainties (see Box  1 and ref. 52).
(4) Limited labels
Deep learning methods are needed to learn from few labelled exam -
ples while exploiting the wealth of information in related unlabelled 
observations. These methods include unsupervised density modelling, 
feature extraction, semi-supervised learning and domain adaptation53 
(see Box  1).
(5) Computational demand
There is a huge technical challenge regarding the high computational 
cost of current geoscience problems—an excellent example of how to 
address this is Google’s Earth Engine, which allowed the solution of real 
problems from deforestation54 to lake55 monitoring and is expected to 
follow up with deep learning applications in the future.
By addressing these challenges, deep learning could make an even 
bigger difference in the geosciences than in classical computer vision, 
because in computer vision hand-crafted features are derived from a 
clear understanding of the world (existence of surfaces, boundaries 
between objects, and so on), the mapping from the world to images, 
and assumptions about the (visual) appearance of world points (surface 
points) on two-dimensional images. Assumptions for successful pro -
cessing include the assumption of Lambertian surfaces (that is, intensity 
does not depend on the angle between surface and light source), which 
results in the classical assumption of the constant intensity of the obser -
vation of a three-dimensional point over time. In addition, changes in 
the world (the motion of objects) are in most cases modelled as rigid 
transformations, or non-rigid transformations that arise from physical 
assumptions and that are only valid locally (such as in registration of 
brain structures, before and after removal of a tumour). Even complex 
problems in computer vision have been solved by hand-crafted features 
that reflect the assumptions and expectations that arise from common 
world knowledge. In geoscience and climate science, such global, gen -
eral knowledge is still partly missing, and indeed, is exactly what we are 
seeking in research (hence, it cannot be an assumption). All problems, 
from segmentation in remote-sensing images to regression analysis of 
certain variables, have certain assumptions that are known to be valid 
or at least good approximations. Y et the less well processes are under -
stood, the fewer high-quality hand-crafted features for modelling can be 
expected to exist. Thus, deep learning methods, particularly since they 
find a good representation from data, represent an opportunity with 
which to tackle geoscience and climate research problems.
The most promising near-future applications include nowcasting (that 
is, prediction of the very near future, up to two hours in meteorology) 
and forecasting applications, anomaly detection and classification based 
on spatial and temporal context information (see examples in Table  1). A 
longer-term vision includes data-driven seasonal forecasting, modelling 
of spatial long-range correlations across multiple timescales, modelling 
spatial dynamics where spatial context is important (for example, fires), 
and detecting teleconnections and connections between variables that 
a human may not have thought about.
We infer that deep learning will soon be the leading method for clas -
sifying and predicting space-time structures in the geosciences. More 
challenging is to gain understanding in addition to optimal prediction, 
and to achieve models that have maximally learned from data, while still 
taking into account physical and biological knowledge. One promising 
but largely uncharted approach to achieving this goal is the integration 
of machine learning with physical modelling, which we discuss next.Integration with physical modelling
Historically, physical modelling and machine learning have often been 
treated as two different fields with very different scientific paradigms 
(theory-driven versus data-driven). Y et, in fact these approaches are 
complementary, with physical approaches in principle being directly 
interpretable and offering the potential of extrapolation beyond observed conditions, whereas data-driven approaches are highly 
 
flexible in adapting to data and are amenable to finding unexpected 
patterns (surprises). The synergy between the two approaches has been 
gaining attention56–58, expressed in benchmarking initiatives59,60 and 
in concepts such as emergent constraints27,61,62.
Here we argue that advances in machine learning and in obser -
vational and simulation capabilities within Earth sciences offer an opportunity to integrate simulation and data science approaches 
more intensively in multiple ways. From a systems modelling point of 
view there are five points of potential synergy (see Fig.  3, in which the  
numbered circles correspond to the following numbered list).
(1) Improving parameterizations
See Fig.  3 (circle 1). Physical models require parameters, but many of 
those cannot be easily derived from first principles. Machine learning 
can learn parameterizations to optimally describe the ground truth 
that can be observed or generated from detailed and high-resolution 
models through first principles. For example, instead of assigning 
parameters of the vegetation in an Earth system model to plant func -
tional types (a common ad hoc decision in most global land surface 
models), one can allow these parameterizations to be learned from 
appropriate sets of statistical covariates, allowing them to be more 
dynamic, interdependent and contextual. A prototypical approach has 
been taken already in hydrology where the mapping of environmental 
variables (for example, precipitation and surface slope) to catchment 
parameters (such as mean, minimum and maximum streamflow) has 
been learned from a few thousand catchments and applied globally to 
feed hydrological models63. Another example from global atmospheric 
modelling is learning the effective coarse-scale physical parameters 
of precipitating convection (for example, the fraction of water that 
is precipitating out of a cloud during convection) from data or high-  
resolution models64,65 (the high-resolution models are too expen -
sive to run, which is why coarse-scale parametrizations are needed). InputObservations Observations Observations
ObservationsForcing Ground truth Ground truth Ground truthInputSubmodel 12Parameters
Output
ML3Cost
functionCost
function
ML3Output
ML34Submodel 1
Input5
Parameters5
Parameterization
meta-model 11Parameterization
meta-model 2
Fig. 3 | Linkages between physical models and machine learning. An 
abstraction of a part of a physical system—for example, an Earth system model—is depicted here. The model consists of submodels; each submodel 
has parameters and forcing variables as inputs and produces output, 
which can be input (forcing) to another sub-model. Data-driven learning approaches can be helpful in various instances, as indicated by the circled numbers. For example, the circle labelled 2 represents hybrid modelling. See the text for more detail. ML, machine learning.
200 | NA tUre  | vOL 566 | 14 F eBrUA rY 2019
PersPective reseArcHThese learned parametrizations could lead to better representations of  
tropical convection66,67.
(2) Replacing a ‘physical’ sub-model with a machine learning 
model
See Fig.  3 (circle 2). If formulations of a submodel are of semi-empirical 
nature, where the functional form has little theoretical basis (for example,  
biological processes), this submodel can be replaced by a machine 
learning model if a sufficient number of observations are available. 
This leads to a hybrid model, which combines the strengths of physical 
modelling (theoretical foundations, interpretable compartments) and 
machine learning (data-adaptiveness). For example, we could couple 
well established physical (differential) equations of diffusion for trans -
port of water in plants with machine learning for the poorly understood 
biological regulation of water transport conductance. This results in a 
more ‘physical’ model that obeys accepted conservation of mass and 
energy laws, but its regulation (biological) is flexible and learned from 
data. Such principles have recently been taken to efficiently model 
motion of water in the ocean and specifically predict sea surface tem -
peratures. Here, the motion field was learned via a deep neural network, 
and then used to update the heat content and temperatures via phys -
ically modelling the movement implied by the motion field68. Also, 
a number of atmospheric scientists have begun experimenting with 
related approaches to circumvent long-standing biases in physically 
based parameterizations of atmospheric convection65,69.
The problem may become more complicated if physical model and 
machine learning parameters are to be estimated simultaneously while 
maintaining interpretability, especially when several sub-models are 
replaced with machine learning approaches. In the field of chemistry 
this approach has been used in calibration exercises and to describe 
changes in unknown kinetic rates while maintaining mass balance in 
biochemical reactor modelling70, which, although less complex, bears 
many similarities to hydrological and biogeochemical modelling.
(3) Analysis of model–observation mismatch
See Fig.  3 (circle 3). Deviations of a physical model from observations 
can be perceived as imperfect knowledge causing model error, assum -
ing no observational biases. Machine learning can help to identify, vis -
ualize and understand the patterns of model error, which allows us also 
to correct model outputs accordingly. For example, machine learning 
can extract patterns from data automatically and identify those which 
are not explicitly represented in the physical model. This approach 
helps to improve the physical model and theory. In practice, it can also 
serve to correct the model bias of dynamic variables, or it can facilitate 
improved downscaling to finer spatial scales compared to tedious and ad hoc hand-designed approaches
71,72.
(4) Constraining submodels
See Fig.  3 (circle 4). One can drive a submodel with the output from 
a machine learning algorithm, instead of another (potentially biased) 
submodel in an offline simulation. This helps to disentangle model 
error originating from the submodule of interest from errors of cou -
pled submodules. As a consequence, this simplifies and reduces biases 
and uncertainties in model parameter calibration or the assimilation 
of observed system state variables.
(5) Surrogate modelling or emulation
See Fig.  3 (circle 5). Emulation of the full (or specific parts of) a physical 
model can be useful for computational efficiency and tractability rea -
sons. Machine learning emulators, once trained, can achieve simulations 
orders of magnitude faster than the original physical model without  
sacrificing much accuracy. This allows for fast sensitivity analysis, 
model parameter calibration, and derivation of confidence intervals 
for the estimates. For example, machine learning emulators are used 
to replace computationally expensive, physics-based radiative-transfer 
models of the interactions between radiation, vegetation and atmos -
phere57,73,74, which are critical for the interpretation and assimilation of land-surface remote sensing in models. Emulators are also used in dynamic modelling, where states are evolving, for example, in 
 
climate modelling75 and more recently explored in vegetation dynamic 
models76. Further, given the complexity of physical models, emulation 
challenges are very good test beds in which to explore the potential of 
machine learning and deep learning approaches to extrapolate outside 
the range of training conditions.
Some of the concepts in Fig.  3 have already been adopted in a 
broad sense. For instance, linkage (3) relates to model benchmarking 
and statistical downscaling and model output statistics77,78. Here we 
argue that adopting a deep-learning approach will strongly improve 
the use of spatio-temporal context information for the modification 
of model output. Emulation (5) has been widely adopted in several 
branches of engineering and geosciences, mainly for the sake of effi -
cient modelling, but tractability issues have not yet been explored 
in depth. Other paths, such as the hybrid modelling (linkage (2)), 
appear to be much less explored. Conceptually, the hybrid approaches 
discussed above can be interpreted as deepening a neural network 
(Fig.  4) to make it more physically realistic, where the physical  
model comes on top of a neural network layers (see examples in 
Fig.  4b, c). It contrasts with the reverse approach discussed above 
where physical model output is produced and then corrected using 
additional layers of machine learning approaches. We believe that it is 
worthwhile pursuing both avenues of integrating physical modelling 
and machine learning.
Figure 3  presents a system-modelling view that seeks to integrate 
machine learning into a system model. As an alternative perspective, 
system knowledge can be integrated into a machine learning frame -
work. This may include design of the network architecture36,79, physical 
constraints in the cost function for optimization58, or expansion of the 
training dataset for undersampled domains (that is, physically based data 
augmentation)80. For instance, while usually a so-called cost function 
like ordinary least squares penalizes model–data mismatch, it can be 
modified to also avoid physically implausible predictions for lake tem -
perature modelling58. The integration of physics and machine learning 
models may not only achieve improved performance and generalizations 
but, perhaps more importantly, incorporates the consistency and credi -
bility of the machine learning models. As a byproduct, the hybridization 
has an interesting regularization effect, given that the physics discards 
implausible models. Therefore, physics-aware machine learning models 
should combat overfitting better, especially in low-to-medium sample 
sized datasets81. This notion is also related to the direction of attaining 
explainable and interpretable machine learning models (‘explainable 
AI’82), and to combining logic rules with deep neural networks83.
Recent advances in two fields of methodological approaches have 
potential in facilitating the fusion of machine learning and physical 
models in a sound way: probabilistic programming52 and differenti -
able programming. Probabilistic programming allows for accounting 
of various uncertainty aspects in a formal but flexible way. A proper 
accounting for data and model uncertainty along with integration of 
knowledge by priors and constraints is critical for optimally combining 
the data-driven and theory-driven paradigms, including logical rules, 
as done in statistical relational learning. In addition, error propagation 
is conceptually seamless, facilitating well founded uncertainty margins 
for model output. This capability is largely missing so far but is crucial 
for scientific purposes, and in particular for management or policy 
decisions. Differentiable programming allows for efficient optimiza -
tion owing to automated differentiation84,85. This helps in making the 
large, nonlinear and complex inversion problem computationally more 
tractable, and in addition allows for explicit sensitivity assessments, 
thus aiding interpretability.
Advancing science
There is no doubt, as exemplified in this Perspective, that modern 
machine learning methods greatly improve classification and predic -
tion skills. This alone has great value. Y et, beyond statistical prediction, 
the question is how data-driven approaches can improve fundamental 
14 F eBrUA rY 2019 | vOL 566 | NA tUre  | 201
PersPective reseArcHscientific understanding, given that the outcome of complex statisti -
cal models, in particular, is often hard to interpret. One basic answer 
is that observations have almost always been the basis for scientific 
progress. For example, the Copernican discoveries were enabled by 
the precise observation of planetary trajectories to infer and test the 
laws governing them.
Now, although the general cycle of exploration, hypotheses gener -
ation and testing remains the same, modern data-driven science and 
machine learning can extract arbitrarily complex patterns in obser -
vational data to challenge complex theories and Earth system models 
(Supplementary Fig. 3). For instance, spatially explicit global data-
driven estimates of photosynthesis based on machine learning have 
indicated an overestimation of photosynthesis in the tropical rainforest  
by climate models86. This mismatch has led scientists to develop 
hypotheses that enable a better description of the radiative transfer in 
vegetation canopies23, which has led to better photosynthesis estimates 
in other regions, and better consistency with leaf-level observations. 
Related data-driven carbon cycle estimates have enabled the calibra -
tion of vegetation models and helped to explain the conundrum of the 
increasing seasonal amplitude of CO 2 concentration at high latitudes87, 
which (according to these results) is caused by vegetation being more 
vigorous in the high latitudes.
In addition to data-driven theory and model building, such 
extracted patterns are increasingly being used as a way to explore 
improved parameterizations in Earth system models65,69, and model 
emulators are increasingly being used as a basis for model cali -
bration88. In this way, the scientific interplay between theory and 
observation, and between hypothesis generation and theory-driven 
hypothesis testing, will continue. Since the complexity of hypotheses 
and tests inferred from data and the pace of hypothesis generation 
are increasing by orders of magnitude via powerful machine learning 
techniques, we can expect unprecedented qualitative and quantita -
tive progress in the science of the complex Earth system.
Conclusion
Earth sciences need to process large and rapidly increasing amounts of 
data to provide more accurate, less uncertain, and physically consistent inferences in the form of prediction, modelling and understanding the 
complex Earth system. Machine learning in general and deep learning 
in particular offer promising tools to build new data-driven models for 
components of the Earth system and thus to build our understanding 
of Earth. Challenges specific to the Earth system will further stimulate 
the development of methodologies, and we have four major recom -
mendations, as follows.
(1) Recognition of the particularities of the data
Multi-source, multi-scale, high-dimensional, complex spatio-temporal 
relations, including non-trivial and lagged long-distance relationships 
(teleconnections) between variables need to be adequately modelled. 
Deep learning is well positioned to address these data challenges, and 
network architectures and algorithms need to be developed to produce 
approaches that address both spatial and temporal context at different 
scales (see Fig.  4).
(2) Plausibility and interpretability of inferences
Models should not only be accurate but also credible, incorporating 
the physics governing the Earth system. Wide adoption of machine 
learning in the Earth sciences will be facilitated if models become more 
transparent and interpretable: their parameters and feature rankings 
should have a minimal physical interpretation, and the model should 
be reducible to or explainable by a set of rules, descriptors and relations.
(3) Uncertainty estimation
Models should define their confidence and credibility. Bayesian/  
probabilistic inference should be integrated into models, because such 
inference allows for explicit representation and propagation of uncer -
tainties. In addition, identifying and treating extrapolation is a priority.
(4) Testing against complex physical models
The spatial and temporal prediction ability of machine learning should 
be, at least, consistent with the patterns observed in physical models. 
Therefore we recommend testing the performance of machine learning 
methods against synthetic data derived from physical models of the 
Earth system. For instance, the models in Fig.  4b, c, which are applied 
Physical output
Physical model
‘parameters’
More abstract
featur es
Simple featur es
InputPhysical
model
Learned
mapping n + 2
Learned
mapping 2... n  + 1
Learned
mapping 1a b
m
n
Input: past
temperatur e /f_ields 
Predicted futur e
temperatur e /f_ield
Convolutional–
deconvolutional
neural network
Motion /f_ieldc
Physical warping model
Input: sequence/time
series of driversPredicted transpiration
and photosynthesis
RNN layers
Recurr ent neural
networkSequence of
stomatal
apertur e
nPhysical dif fusion and
energy balance modelTranspirationPhotosynthesis
RadiationPrecipitation
Temperatur e
Fig. 4 | Interpretation of hybrid modelling as deepening a deep learning 
architecture by adding one or several physical layers after the multilayer neural network to make the model more physically realistic. a, The 
multilayer neural network, with n  the number of neural layers and m  the 
number of physical layers. b  and c  are concrete examples of hybrid modelling 
(circle 2 in Fig.  3). b, Prediction of sea-surface temperatures, where a motion 
field of the water is learned with a convolutional–deconvolutional neural network, and the motion field is further processed with a physical model 
 
to predict future states. Adapted from figure 1 of de Bezenac et al.68.  
c, A biological regulation process (opening of the stomatal ‘valves’ controlling 
water vapour flux from the leaves) is modelled with a recurrent neural network. Then a physical diffusion model is used to estimate transpiration, 
which in turn influences some of the drivers, such as soil moisture. The basic 
scheme in a  is inspired by figure 1.5 in Goodfellow et al.
98 and redrawn.
202 | NA tUre  | vOL 566 | 14 F eBrUA rY 2019
PersPective reseArcHto real data, should be tested across a broad range of dynamics as sim -
ulated by complex physical models. This is of particular relevance in 
conditions of limited training data and to assess extrapolation issues.
Overall, we suggest that future models should integrate process-  
based and machine learning approaches. Data-driven machine learning 
approaches to geoscientific research will not replace physical modelling, 
but strongly complement and enrich it. Specifically, we envision various 
synergies between physical and data-driven models, with the ultimate 
goal of hybrid modelling approaches: these should obey physical laws, 
feature a conceptualized and thus interpretable structure, and at the 
same time be fully data-adaptive where theory is weak. Importantly, 
machine learning research will benefit from plausible physically based 
relationships derived from the natural sciences. Among others, two 
major Earth system challenges in which little progress has been made 
recently—the parameterization of atmospheric convection and the 
description of the spatio-temporal dependency of ecosystems on  
climate and interacting geo-factors—could be addressed using the 
hybrid approaches discussed here.
Online content
Any methods, additional references, Nature Research reporting summaries, source 
data, statements of data availability and associated accession codes are available at 
https://doi.org/10.1038/s41586-019-0912-1.
Received: 28 July 2017; Accepted: 5 December 2018;  
Published online 13 February 2019.
 1. Howe, L. & Wain, A. Predicting the Future Vol. V, 1–195 (Cambridge Univ. Press, 
1993).
 2. Bauer, P., Thorpe, A. & Brunet, G. The quiet revolution of numerical weather prediction. Nature 525, 47–55 (2015).
 3. Hantson, S. et al. The status and challenge of global fire modelling. Biogeosciences 13, 3359–3375 (2016).
 4. Agapiou, A. Remote sensing heritage in a petabyte-scale: satellite data and heritage Earth Engine© applications. Int. J. Digit. Earth 10, 85–102 (2017).
 5. Stockhause, M. & Lautenschlager, M. CMIP6 data citation of evolving data. 
Data Sci. J. 16, 30 (2017).
 6. Lee, J., Weger, R. C., Sengupta, S. K. & Welch, R. M. A neural network approach 
to cloud classification. IEEE Trans. Geosci. Remote Sens. 28, 846–855 (1990).
 7. Benediktsson, J. A., Swain, P. H. & Ersoy, O. K. Neural network approaches 
versus statistical methods in classification of multisource remote sensing data. 
IEEE Trans. Geosci. Remote Sens. 28, 540–552 (1990).
 8. Camps-Valls, G. & Bruzzone, L. Kernel Methods for Remote Sensing Data 
Analysis 434 (John Wiley & Sons, Chichester, 2009).
 9. Gómez-Chova, L., Tuia, D., Moser, G. & Camps-Valls, G. Multimodal 
classification of remote sensing images: a review and future directions. 
 
Proc. IEEE 103, 1560–1584 (2015).
 10. Camps-Valls, G., Tuia, D., Bruzzone, L. & Benediktsson, J. A. Advances in 
hyperspectral image classification: Earth monitoring with statistical learning 
methods. IEEE Signal Process. Mag. 31, 45–54 (2014).  
This paper provides a comprehensive overview of machine learning for classification.
 11. Gislason, P. O., Benediktsson, J. A. & Sveinsson, J. R. Random forests for land cover classification. Pattern Recogn. Lett. 27, 294–300 (2006). 
 
This paper is one of the first machine learning papers for land-cover classification, a method now operationally used.
 12. Muhlbauer, A., McCoy, I. L. & Wood, R. Climatology of stratocumulus cloud morphologies: microphysical properties and radiative effects. Atmos. Chem. Phys. 14, 6695–6716 (2014).
 13. Grimm, R., Behrens, T., Märker, M. & Elsenbeer, H. Soil organic carbon 
concentrations and stocks on Barro Colorado Island—digital soil mapping 
using Random Forests analysis. Geoderma 146, 102–113 (2008).
 14. Hengl, T. et al. SoilGrids250m: global gridded soil information based on 
machine learning. PLoS ONE 12, e0169748 (2017). 
 
This paper describes machine learning used for operational global soil 
mapping.
 15. Townsend, P. A., Foster, J. R., Chastain, R. A. & Currie, W. S. Application of imaging spectroscopy to mapping canopy nitrogen in the forests of the central Appalachian Mountains using Hyperion and AVIRIS. IEEE Trans. Geosci. Remote Sens. 41, 1347–1354 (2003).
 16. Coops, N. C., Smith, M.-L., Martin, M. E. & Ollinger, S. V. Prediction of eucalypt foliage nitrogen content from satellite-derived hyperspectral data. IEEE Trans. Geosci. Remote Sens. 41, 1338–1346 (2003).
 17. Verrelst, J., Alonso, L., Camps-Valls, G., Delegido, J. & Moreno, J. Retrieval 
 
of vegetation biophysical parameters using Gaussian process techniques.  
IEEE Trans. Geosci. Remote Sens. 50, 1832–1843 (2012).
 18. Papale, D. & Valentini, R. A new assessment of European forests carbon 
exchanges by eddy fluxes and artificial neural network spatialization. Glob. 
Change Biol. 9, 525–535 (2003). 19. Jung, M. et al. Global patterns of land-atmosphere fluxes of carbon dioxide, latent heat, and sensible heat derived from eddy covariance, satellite and 
 
meteorological observations. J. Geophys. Res. Biogeo. 116, G00j07  
(2011).
 20. Tramontana, G. et al. Predicting carbon dioxide and energy fluxes across 
global FLUXNET sites with regression algorithms. Biogeosciences 13, 
4291–4313 (2016).
 21. Jung, M. et al. Recent decline in the global land evapotranspiration trend due 
to limited moisture supply. Nature 467, 951–954 (2010).  
This paper describes the first data-driven machine-learning-based spatio-temporal estimation of global water fluxes on land.
 22. Jung, M. et al. Compensatory water effects link yearly global land CO
2 sink 
changes to temperature. Nature 541, 516–520 (2017).
 23. Bonan, G. B. et al. Improving canopy processes in the Community Land Model version 4 (CLM4) using global flux fields empirically inferred from FLUXNET data. J. Geophys. Res. Biogeosci. 116, G02014 (2011).
 24. Anav, A. et al. Spatiotemporal patterns of terrestrial gross primary production: 
a review. Rev. Geophys. 53, 785–818 (2015).
 25. Landschützer, P. et al. A neural network-based estimate of the seasonal to 
inter-annual variability of the Atlantic Ocean carbon sink. Biogeosciences 10, 
7793–7815 (2013). 
 
This paper describes the first data-driven machine-learning-based carbon fluxes in the ocean.
 26. Kühnlein, M., Appelhans, T., Thies, B. & Nauss, T. Improving the accuracy of rainfall rates from optical satellite sensors with machine learning—a random forests-based approach applied to MSG SEVIRI. Remote Sens. Environ. 141, 
129–143 (2014).
 27. Caldwell, P. M. et al. Statistical significance of climate sensitivity predictors obtained by data mining. Geophys. Res. Lett. 41, 1803–1808 (2014).
 28. Reichstein, M. & Beer, C. Soil respiration across scales: the importance of a 
model-data integration framework for data interpretation. J. Plant Nutr. Soil Sci. 
171, 344–354 (2008).
 29. Wright, S. Correlation and causation. J. Agric. Res. 20, 557–585 (1921).
 30. Guttman, N. B. Accepting the standardized precipitation index: a calculation 
algorithm. J. Am. Water Resour. Assoc. 35, 311–322 (1999).
 31. Vicente-Serrano, S. M., Beguería, S. & López-Moreno, J. I. A multiscalar drought 
index sensitive to global warming: the standardized precipitation 
evapotranspiration index. J. Clim. 23, 1696–1718 (2010).
 32. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444 (2015).
 33. Lore, K. G., Stoecklein, D., Davies, M., Ganapathysubramanian, B. & Sarkar, S. 
Hierarchical feature extraction for efficient design of microfluidic flow patterns. 
Proc. Machine Learning Res. 44, 213–225 (2015).
 34. Baldi, P., Sadowski, P. & Whiteson, D. Searching for exotic particles in 
high-energy physics with deep learning. Nat. Commun. 5, 4308 (2014).
 35. Bhimji, W., Farrell, S. A., Kurth, T., Paganini, M. & Racah, E. Deep neural networks for physics analysis on low-level whole-detector data at the LHC. Preprint at https://arxiv.org/abs/1711.03573 (2017).
 36. Schütt, K. T., Arbabzadah, F., Chmiela, S., Muller, K. R. & Tkatchenko, A. 
Quantum-chemical insights from deep tensor neural networks. Nat. Commun. 
8, 13890 (2017).
 37. Alipanahi, B., Delong, A., Weirauch, M. T. & Frey, B. J. Predicting the sequence 
specificities of DNA- and RNA-binding proteins by deep learning. Nat. 
Biotechnol. 33, 831–838 (2015).
 38. Prabhat. A look at deep learning for science. O’Reilly Blog https://www.oreilly.
com/ideas/a-look-at-deep-learning-for-science (2017).
 39. Zhang, L. P., Zhang, L. F. & Du, B. Deep learning for remote sensing data: a technical tutorial on the state of the art. IEEE Geosci. Remote Sens. Mag. 4, 
22–40 (2016).
 40. Ball, J. E., Anderson, D. T. & Chan, C. S. Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community. J. Appl. Remote Sens. 11, 042609 (2017).
 41. Racah, E. et al. ExtremeWeather: a large-scale climate dataset for semi-
supervised detection, localization, and understanding of extreme weather 
events. Adv. Neural Inform. Process. Syst. 30, 3405–3416 (2017).
 42. Liu, Y. et al. Application of deep convolutional neural networks for detecting 
extreme weather in climate datasets. In ABDA'16-International Conference on 
Advances in Big Data Analytics 81–88 https://arxiv.org/abs/1605.01156 (2016). 
 
This paper is the first approach to detecting extreme weather automatically without any prescribed thresholds, using deep learning.
 43. Zhao, W. Z. & Du, S. H. Learning multiscale and deep representations for classifying remotely sensed imagery. ISPRS J. Photogramm. Remote Sens. 113, 
155–165 (2016).
 44. Mathieu, M., Couprie, C. & LeCun, Y. Deep multi-scale video prediction 
beyond mean square error. Preprint at https://arxiv.org/abs/1511.05440 
(2015).
 45. Oh, J., Guo, X., Lee, H., Lewis, R. L. & Singh, S. Action-conditional video 
prediction using deep networks in Atari games. Adv. Neural Inf. Process. Syst. 
28, 2863–2871 (2015).
 46. Shi, X. et al. Convolutional LSTM network: a machine learning approach for precipitation nowcasting. Adv. Neural Inf. Process. Syst. 28, 802–810 
(2015).
 47. Deng, J. et al. ImageNet: a large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition 248–255 
 
(IEEE, 2009).
 48. Friedlingstein, P. et al. Uncertainties in CMIP5 climate projections due to 
carbon cycle feedbacks. J. Clim. 27, 511–526 (2014).
14 F eBrUA rY 2019 | vOL 566 | NA tUre  | 203
PersPective reseArcH 49. Montavon, G., Samek, W. & Müller, K.-R. Methods for interpreting and 
understanding deep neural networks. Digit. Signal Process. 73, 1–15 (2017).
 50. Runge, J. et al. Identifying causal gateways and mediators in complex spatio-temporal systems. Nat. Commun. 6, 8502 (2015).
 51. Chalupka, K., Bischoff, T., Perona, P. & Eberhardt, F. in UAI'16 Proceedings 
 
of the Thirty-Second Conference on Uncertainty in Artificial Intelligence 72–81 (AUAI Press, 2016).
 52. Ghahramani, Z. Probabilistic machine learning and artificial intelligence. Nature 521, 452–459 (2015).
 53. Goodfellow, I. J. et al. Generative Adversarial Nets. Adv. Neural. Inf. Process. Syst. 27, 2672–2680 (2014). 
 
This is a fundamental paper on a deep generative modelling approach, allowing possible futures to be modelled from data.
 54. Hansen, M. C. et al. High-resolution global maps of 21st-century forest cover change. Science 342, 850–853 (2013).
 55. Pekel, J.-F., Cottam, A., Gorelick, N. & Belward, A. S. High-resolution mapping of global surface water and its long-term changes. Nature 540, 418–422 
(2016).
 56. Karpatne, A. et al. Theory-guided data science: a new paradigm for 
 
scientific discovery from data. IEEE Trans. Knowl. Data Eng. 29, 2318–2331 
(2017).
 57. Camps-Valls, G. et al. Physics-aware Gaussian processes in remote sensing. Appl. Soft Comput. 68, 69–82 (2018).
 58. Karpatne, A., Watkins, W., Read, J. & Kumar, V. Physics-guided Neural Networks (PGNN): an application in lake temperature modeling. Preprint at https://
arxiv.org/abs/1710.11431 (2017).
 59. Luo, Y. Q. et al. A framework for benchmarking land models. Biogeosciences 9, 
3857–3874 (2012).
 60. Eyring, V. et al. Towards improved and more routine Earth system model 
evaluation in CMIP. Earth Syst. Dyn. 7, 813–830 (2016).
 61. Klocke, D., Pincus, R. & Quaas, J. On constraining estimates of climate sensitivity with present-day observations through model weighting. J. Clim. 24, 
6092–6099 (2011).
 62. Cox, P. M. et al. Sensitivity of tropical carbon to climate change constrained by carbon dioxide variability. Nature 494, 341–344 (2013).
 63. Beck, H. E. et al. Global-scale regionalization of hydrologic model parameters. Wat. Resour. Res. 52, 3599–3622 (2016).
 64. Schirber, S., Klocke, D., Pincus, R., Quaas, J. & Anderson, J. L. Parameter estimation using data assimilation in an atmospheric general circulation 
model: from a perfect toward the real world. J. Adv. Model. Earth Syst. 5, 58–70 
(2013).
 65. Gentine, P., Pritchard, M., Rasp, S., Reinaudi, G. & Yacalis, G. Could machine learning break the convection parameterization deadlock? Geophys. Res. Lett. 
45, 5742–5751 (2018).
 66. Becker, T., Stevens, B. & Hohenegger, C. Imprint of the convective parameterization and sea-surface temperature on large-scale convective 
self-aggregation. J. Adv. Model. Earth Syst. 9, 1488–1505 (2017).
 67. Siongco, A. C., Hohenegger, C. & Stevens, B. Sensitivity of the summertime 
tropical Atlantic precipitation distribution to convective parameterization 
and model resolution in ECHAM6. J. Geophys. Res. Atmos. 122, 2579–2594 
(2017).
 68. de Bezenac, E., Pajot, A. & Gallinari, P. Deep learning for physical processes: 
incorporating prior scientific knowledge. Preprint at https://arxiv.org/abs/
 
1711.07970 (2017).
 69. Brenowitz, N. D. & Bretherton, C. S. Prognostic validation of a neural network 
unified physics parameterization. Geophys. Res. Lett. 45, 6289–6298 (2018).
 70. Willis, M. J. & von Stosch, M. Simultaneous parameter identification and 
discrimination of the nonparametric structure of hybrid semi-parametric models. Comput. Chem. Eng. 104, 366–376 (2017).
 71. McGovern, A. et al. Using artificial intelligence to improve real-time decision making for high-impact weather. Bull. Am. Meteorol. Soc. 98, 2073–2090 
(2017).
 72. Vandal, T. et al. Generating high resolution climate change projections through single image super-resolution: an abridged version. In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI-18)  
https://www.ijcai.org/proceedings/2018/0759.pdf (2018).
 73. Verrelst, J. et al. Emulation of leaf, canopy and atmosphere radiative transfer models for fast global sensitivity analysis. Remote Sens. 8, 673 (2016).
 74. Chevallier, F., Chéruy, F., Scott, N. & Chédin, A. A neural network approach for a 
fast and accurate computation of a longwave radiative budget. J. Appl. 
Meteorol. 37, 1385–1397 (1998).
 75. Castruccio, S. et al. Statistical emulation of climate model projections based on precomputed GCM runs. J. Clim. 27, 1829–1844 (2014).
 76. Fer, I. et al. Linking big models to big data: efficient ecosystem model calibration through Bayesian model emulation. Biogeosci. Disc. 2018, 1–30 
(2018).
 77. Glahn, H. R. & Lowry, D. A. The use of model output statistics (MOS) in objective weather forecasting. J. Appl. Meteorol. 11, 1203–1211 (1972).
 78. Wilks, D. S. Multivariate ensemble model output statistics using empirical copulas. Q. J. R. Meteorol. Soc. 141, 945–952 (2015).
 79. Tewari, A. et al. in Proc. IEEE Conf. on Computer Vision and Pattern Recognition  
2549–2559 (IEEE, 2018).
 80. Xie, Y., Franz, E., Chu, M. & Thuerey, N. tempoGAN: a temporally coherent, 
volumetric GAN for super-resolution fluid flow. Preprint at https://arxiv.org/
abs/1801.09710 (2018). 81. Stewart, R. & Ermon, S. in Proc. Thirty-First AAAI Conf. on Artificial Intelligence (AAAI-17) 2576–2582 (2017).
 82. Gunning, D. Explainable Artificial Intelligence (XAI) https://www.cc.gatech.
edu/~alanwags/DLAI2016/(Gunning)%20IJCAI-16%20DLAI%20WS.pdf (2017).
 83. Hu, Z., Ma, X., Liu, Z., Hovy, E. & Xing, E. in Proc. 54th Annual Meeting of the 
Association for Computational Linguistics Vol. 1, 2410–2420 (Association for 
Computational Linguistics, 2016).
 84. Pearlmutter, B. A. & Siskind, J. M. Reverse-mode AD in a functional framework: lambda the ultimate backpropagator. ACM Trans. Progr. Lang. 
Syst. 30, 7 (2008).
 85. Wang, F. & Rompf, T. in ICLR 2018 Workshop https://openreview.net/pdf?id
 
=SJxJtYkPG (2018).
 86. Beer, C. et al. Terrestrial gross carbon dioxide uptake: global distribution and covariation with climate. Science 329, 834–838 (2010).
 87. Forkel, M. et al. Enhanced seasonal CO
2 exchange caused by amplified plant 
productivity in northern ecosystems. Science 351, 696–699 (2016).
 88. Bellprat, O., Kotlarski, S., Lüthi, D. & Schär, C. Objective calibration of regional 
climate models. J. Geophys. Res. Atmos. 117, D23115 (2012).
 89. Reichstein, M. et al. in AGU Fall Meeting Abstracts 2016AGUFM.B2044A.2007R (AGU, 2016).
 90. Rußwurm, M. & Körner, M. Multi-temporal land cover classification with long short-term memory neural networks. Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci 42, 551–558 (2017). 
 
This paper describes the first use of the LSTM deep learning model for multi-temporal land-cover classification.
 91. Nash, J. E. & Sutcliffe, J. V. River flow forecasting through conceptual models. Part I—a discussion of principles. J. Hydrol. 10, 282–290 (1970).
 92. Shi, X. et al. Deep learning for precipitation nowcasting: a benchmark and a new model. Adv. Neural. Inf. Process. Syst. 30, 5617–5627 (2017). 
 
This paper describes the first approach to data-driven modelling of near-term precipitation using a combination of deep-learning concepts, that 
is, LSTMs and convolutional neural networks.
 93. Isola, P., Zhu, J.-Y., Zhou, T. & Efros, A. A. Image-to-image translation with 
conditional adversarial networks. Preprint at https://arxiv.org/abs/
 
1611.07004 (2016).  
This paper is a geoscience-related extension application of ref. 53, in which, for example, remote sensing images are transferred to thematic 
maps.
 94. Tompson, J., Schlachter, K., Sprechmann, P. & Perlin, K. Accelerating Eulerian 
fluid simulation with convolutional networks. Proc. Machine Learning Res. 70, 
3424–3433 (2017).
 95. University Corporation for Atmospheric Research (UCAR). Short-Term Explicit Prediction (STEP) Program Research Applications Laboratory 2013 Annual Report https://nar.ucar.edu/2013/ral/short-term-explicit-prediction-step-
program (NCAR/UCAR, 2013).
 96. Ren, S., He, K., Girshick, R. & Sun, J. Faster R-CNN: towards real-time object detection with region proposal networks. Adv. Neural Inf. Process. Syst. 28, 
91–99 (2015).
 97. Zaytar, M. A. & El Amrani, C. Sequence to sequence weather forecasting with long short term memory recurrent neural networks. Int. J. Comput. Appl. 143, 
7–11 (2016).
 98. Goodfellow, I., Bengio, Y. & Courville, A. Deep Learning Vol. xxii, 1–775 
 
(MIT Press, Cambridge, 2016).
 99. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput 9, 
1735–1780 (1997).
 100. Schmidhuber, J. Deep learning in neural networks: an overview. Neural Netw. 61, 85–117 (2015).
 101. Requena-Mesa, C., Reichstein, M., Mahecha, M., Kraft, B. & Denzler, J. 
Predicting landscapes as seen from space from environmental conditions. In 
IEEE International Geoscience and Remote Sensing Symposium (IGARSS)  
1768–1771 (IEEE, 2018).
Acknowledgements We thank D. Frank and L. Maack for proofreading, help with the literature and technical help, and F. Gans for programming help in 
Julia. This research was supported by a grant by the Alexander von Humboldt 
Foundation (Max Planck Research Prize) to M.R. G.C.-V. was supported by the European Research Council (ERC) under the ERC Consolidator Grant  
ERC-CoG-2014 SEDAL (grant agreement 647423).
Author contributions M.R. conceived the work, created a first outline with 
Prabhat and ran the numerical experiment. All authors wrote the manuscript.
Competing interests The authors declare no competing interests.Additional information
Supplementary information is available for this paper at https://doi.org/  
10.1038/s41586-019-0912-1.
Reprints and permissions information is available at http://www.nature.com/
reprints.Correspondence and requests for materials should be addressed to M.R.Publisher’s note: Springer Nature remains neutral with regard to jurisdictional 
claims in published maps and institutional affiliations.
© The Author(s), under exclusive licence to Springer Nature Limited 2019
204 | NA tUre  | vOL 566 | 14 F eBrUA rY 2019
Local Context Normalization: Revisiting Local Normalization
Anthony Ortiz∗1, 4, Caleb Robinson3, Dan Morris2, Olac Fuentes1, Christopher Kiekintveld1, Md
Mahmudulla Hassan1, and Nebojsa Jojic†2
1The University of Texas at El Paso
2Microsoft Research
3Georgia Institute of Technology
4Microsoft AI for Good Research Lab
Abstract
Normalization layers have been shown to improve con-
vergence in deep neural networks, and even add useful in-
ductive biases. In many vision applications the local spa-
tial context of the features is important, but most com-
mon normalization schemes including Group Normaliza-
tion (GN), Instance Normalization (IN), and Layer Normal-
ization (LN) normalize over the entire spatial dimension of
a feature. This can wash out important signals and degrade
performance. For example, in applications that use satel-
lite imagery, input images can be arbitrarily large; conse-
quently, it is nonsensical to normalize over the entire area.
Positional Normalization (PN), on the other hand, only nor-
malizes over a single spatial position at a time. A natural
compromise is to normalize features by local context, while
also taking into account group level information. In this pa-
per, we propose Local Context Normalization (LCN) : a nor-
malization layer where every feature is normalized based on
a window around it and the ﬁlters in its group. We propose
an algorithmic solution to make LCN efﬁcient for arbitrary
window sizes, even if every point in the image has a unique
window. LCN outperforms its Batch Normalization (BN),
GN, IN, and LN counterparts for object detection, seman-
tic segmentation, and instance segmentation applications in
several benchmark datasets, while keeping performance in-
dependent of the batch size and facilitating transfer learn-
ing.
1. Introduction
A variety of neural network normalization layers have
been proposed in the literature to aid in convergence and
∗Work partially done while author was interning at Microsoft Research
†Correspondence: jojic@microsoft.com, anthonymlortiz@gmail.comsometimes even add desirable inductive bias.
Batch Normalization (BN) is a subtractive and divisive
feature normalization scheme widely used in deep learning
architectures [12]. Recent research has shown that BN fa-
cilitates convergence of very deep learning architectures by
smoothing the optimization landscape [33]. BN normalizes
the features by the mean and variance computed within a
mini-batch. Using the batch dimension while calculating
the normalization statistics has two main drawbacks:
•Small batch sizes affect model performance because
the mean and variance estimates are less accurate.
•Batches might not exist during inference, so the mean
and variance are pre-computed from the training set
and used during inference. Therefore, changes in the
target data distribution lead to issues while performing
transfer learning, since the model assumes the statistics
of the original training set [28].
To address both of these issues, Group Normalization
(GN) was recently proposed by Wu and He [41]. GN di-
vides channels into groups and normalizes the features by
using the statistics within each group. GN does not exploit
the batch dimension so the computation is independent of
batch sizes and model performance does not degrade when
the batch size is reduced. GN shows competitive perfor-
mance with respect to BN when the batch size is small; con-
sequently, GN is being quickly adopted for computer vision
tasks like segmentation and video classiﬁcation, since batch
sizes are often restricted for those applications. When the
batch size is sufﬁciently large, BN still outperforms GN.
BN, GN, IN, and LN all perform “global” normalization
where spatial information is not exploited, and all features
are normalized by a common mean and variance value. We
argue that for the aforementioned applications, local con-
text matters. To incorporate this intuition we propose Lo-
1arXiv:1912.05845v3  [cs.CV]  9 May 2020
Figure 1: Proposed Local Context Normalization (LCN) layer . LCN normalizes each value in a channel according to
the values in its feature group and spatial neighborhood. The ﬁgure shows how our proposed method compares to other
normalization layers in terms of which features are used in normalization (shown in blue), where H,W, and Care the height,
width, and number of channels in the output volume of a convolutional layer.
cal Context Normalization (LCN) as a normalization layer
which takes advantage of the context of the data distribution
by normalizing each feature based on the statistics of its lo-
cal neighborhood and corresponding feature group. LCN
is in fact inspired by computational neuroscience, speciﬁ-
cally the contrast normalization approach leveraged by the
human vision system [21], as well as early generative mod-
eling approaches to co-segmentation [14, 15, 40], where
the reasoning about pixel labels is based on shared self-
similarity patterns within an image or image window, rather
than on shared features across images. LCN provides a per-
formance boost over all previously-proposed normalization
techniques, while keeping the advantages of being compu-
tationally agnostic to the batch size and suitable for trans-
fer learning. We empirically demonstrate the performance
beneﬁt of LCN for object detection as well as semantic and
instance segmentation.
Another issue with GN is that because it performs nor-
malization using the entire spatial dimension of the features,
when it is used for inference in applications where input
images need to be processed in patches, just shifting the
input patch for a few pixels produces different predictions.
This is a common scenario in geospatial analytics and re-
mote sensing applications where the input tends to cover
an immense area [24, 29, 23]. Interactive ﬁne-tuning ap-
plications like [30] become infeasible using GN, since a
user will not be able to recognize whether changes in the
predictions are happening because of ﬁne-tuning or simply
because of changes in the image input statistics. With LCN,
predictions depend only on the statistics within the feature
neighborhood; inference does not change when the input is
shifted.
2. Related Work
Normalization in Neural Networks. Since the early days
of neural networks, it has been understood that input nor-malization usually improves convergence [18, 17]. LeCun
et al. showed that convergence in neural networks is faster
if the average of each input variable to any layer is close to
zero and their covariances are about the same [17]. Many
normalization schemes have been proposed in the litera-
ture since then [21, 13, 16, 12, 19, 38, 41]. A Local Con-
trast Normalization Layer was introduced by [13], later re-
ferred to as Local Response Normalization (LRN). A mod-
iﬁcation of this original version of LRN was used by the
original AlexNet paper which won the Imagenet [7] chal-
lenge in 2012 [16], as well as the 2013 winning entry [43].
Most popular deep learning architectures until 2015 includ-
ing Overfeat and GoogLeNet [35, 37] also used LRN, which
normalizes based on the statistics in a very small window (at
most 9×9) around each feature.
After Ioffe et al. proposed BN in 2015, the community
moved towards global normalization schemes where the
statistics are computed along entire spatial dimensions [12].
BN normalizes the feature maps of a given mini-batch along
the batch dimension. For convolutional layers the mean and
variance are computed over both the batch and spatial di-
mensions, meaning that each location in the feature map is
normalized in the same way. Mean and variance are pre-
computed on the training set and used at inference time, so
when presented with any distribution shift in the input data,
BN produces inconsistency at the time of transfer or infer-
ence [28]. Reducing the batch size also affects BN perfor-
mance as the estimated statistics are less accurate.
Other normalization methods [38, 41, 19] have been pro-
posed to avoid exploiting the batch dimension. LN [19] per-
forms normalization along the channel dimension, IN [38]
performs normalization for each sample, and GN uses the
mean and variance from the entire spatial dimension and a
group of feature channels. See Figure 1 for a visual repre-
sentation of different normalization schemes. Instead of op-
erating on features, Weight Normalization (WN) normalizes
the ﬁlter weights [32]. These strategies do not suffer from
the issues caused by normalizing along the batch dimen-
sion, but they have not been able to approach BN perfor-
mance in most visual recognition applications. Wu and He
recently proposed GN, which is able to match BN perfor-
mance on some computer vision tasks when the batch size is
small [41]. All of these approaches perform global normal-
ization, which might wipe out local context. Our proposed
LCN takes advantages of both local context around the fea-
tures and improved convergence from global normalization
methods.
Contrast Enhancement. In general, contrast varies
widely across a typical image. Contrast enhancement is
used to boost contrast in the regions where it is low or mod-
erate, while leaving it unchanged where it is high. This re-
quires that the contrast enhancement be adapted to the local
image content. Contrast normalization is inspired by com-
putational neuroscience models [13, 21] and reﬂects certain
aspects of human visual perception. This inspired early nor-
malization schemes for neural networks, but contrast en-
hancement has not been incorporated into recent normal-
ization methods. Perin et al. showed evidence for synaptic
clustering, where small groups of neurons (a few dozen)
form small-world networks without hubs [26]. For exam-
ple, in each group, there is an increased probability of con-
nection to other members of the group, not just to a small
number of central neurons, facilitating inhibition or excita-
tion within a whole group. Furthermore, these cell assem-
blies are interlaced so that together they form overlapping
groups. Such groups could in fact implement LCN. These
groups could also implement more extreme color and fea-
ture invariance as in probabilistic index map (PIM) mod-
els [14, 40, 15], which assume that the spatial clustering
pattern of features (segmentation) is shared across images
but the palette (feature intensities in each cluster) can vary
freely. PIMs are naturally suited to co-segmentation ap-
plications. LCN also emphasizes local similarities among
pixel features, but preserves some intensity information, as
well.
Local contrast enhancement has been applied in com-
puter vision to pre-process input images [27, 34] ensur-
ing that contrast is normalized across a very small window
(7×7or9×9traditionally). Local contrast normalization
was essential for the performance of the popular Histogram
of Oriented Gradients (HOG) feature descriptors [6]. In this
work, we propose applying a similar normalization not only
at the input layer, but in all layers of a neural network, to
groups of neurons.3. Local Context Normalization
3.1. Formulation
Local Normalization In the LRN scheme proposed
by [13], every feature xi,h,w – where i refers to channel i
and h,w refer to spatial position of the feature – is normal-
ized by equation 1, where Wpqis a Gaussian weighting win-
dow of size 9×9,∑
pqWpq= 1,cis set to bemean (σhw),
andσhwis the weighted standard deviation of all features
over a small spatial neighborhood. handware spatial co-
ordinates, and iis the feature index.
ˆxihw=xihw−∑
pqWpqxi,h+p,w+q
max(c,σhw)(1)
Global Normalization Most recent normalization tech-
niques, including BN, LN, IN, and GN, apply global nor-
malization. In these techniques, features are normalized
following equation 2.
ˆxi=xi−µi
σi(2)
For a 2D image, i= (iB,iC,iH,iW)is a 4D vector in-
dexing the features in (B,C,H,W )order, where Bis the
batch axis,Cis the channel axis, and HandWare the
spatial height and width axes. µandσare computed as:
µi=1
m∑
k∈Sixk
σi=√
1
m∑
k∈Si(xk−µi)2+ϵ(3)
withϵas a small constant. Siis the set of pixels in which
the mean and standard deviation are computed, and mis
the size of this set. As shown by [41], most recent types of
feature normalization methods mainly differ in how the set
Siis deﬁned. Figure 1 shows graphically the corresponding
setSifor different normalization layers.
For BN, statistics are computed along ( B,H,W ):
BN=⇒Si={k|kC=iC} (4)
For LN, normalization is performed per-sample, within
each layer.µandσare computed along ( C,H,W ):
LN=⇒ Si={k|kB=iB}, (5)
For IN, normalization is performed per-sample, per-
channel.µandσare computed along (H,W):
IN=⇒ Si={k|kB=iB,kC=iC}, (6)
For GN, normalization is performed per-sample, within
groups of size Galong the channel axis:
GN=⇒ Si={k|kB=iB,⌊kC
C/G⌋=⌊iC
C/G⌋},(7)
All global normalization schemes (GN, BN, LN, IN)
learn a per-channel linear transformation to compensate for
the change in feature amplitude:
yi=γˆxi+β (8)
whereγandβare learned during training.
Local Context Normalization In LCN, the normaliza-
tion statistics µandγare computed following equation 2
using the set Sideﬁned by 9. We propose performing the
normalization per-sample, within a window of size p×q, for
groups of ﬁlters of size predeﬁned by the number of chan-
nels per group (cgroup )along the channel axis, as shown
in equation 9. instead of number of groups Glike com-
monly done for GN, we use (cgroup )as hyper-parameter.
We consider windows much bigger than the ones used in
LRN and can compute µandγin a computationally efﬁ-
cient manner. The size pandqshould be adjusted accord-
ing to the input size and resolution and can be different for
different layers of the network.
LCN =⇒ Si={k|kB=iB,⌊kC
cgroup⌋=⌊iC
cgroup⌋,
⌊kH
p⌋=⌊iH
p⌋,⌊kW
q⌋=⌊iW
q⌋},(9)
Relation to Previous Normalization Schemes LCN al-
lows an efﬁcient generalization of most previously proposed
mini-batch-independent normalization layers. Like GN, we
perform per-group normalization. If the chosen pis greater
than or equal to Hand the chosen qis greater than or equal
toW, LCN behaves exactly as GN, but keeping the number
of channels per group ﬁxed throughout the network instead
of the number or groups. If in that scenario the number of
channels per group (c group) is chosen as the total number
of channels (c group = C), LCN becomes LN. If the number
of channels per group (c group) is chosen as 1 (c group =
1), LCN becomes IN.
3.2. Implementation
LCN can be implemented easily in any framework with
support for automatic differentiation like PyTorch [25] and
TensorFlow [2]. For an efﬁcient calculation of mean and
variance, we used the summed area table algorithm, also
known in computer vision as the integral image trick [39],
along with dilated convolutions [42, 3]. Algorithm 1 shows
the pseudo-code for the implementation of LCN. We ﬁrst
create two integral images using the input features and the
square of the input features. Then, we apply dilated convo-
lution to both integral images with proper dilation (dilation
depends on c group, p, and q), kernel and stride of one. ThisAlgorithm 1 LCN pseudo-code
Input:x: input features of shape [B, C, H, W],
cgroup : number of channels per group ,
windowsize: spatial window size as a tuple (p, q),
γ,β: scale and shifting parameters to be learned
Output:{y=LCNγ,β(x)}
1S←dilatedconv(I(x),d,k) /*I(x) is integral
image of x, dilation d is (c group,p,q), kernel
k is a tensor with -1 and 1 to substract or add
dimension */
2Ssq←dilatedconv(I(xsq),d,k) // I( xsq) is
integral image of xsq
3µ←S
n// Compute Mean n=cgroup∗p∗q
4σ2←1
n(Ssq−S⊙S
n) // compute Variance
5ˆx←x−µ√
σ2+ϵ// Normalize activation
6y←γˆx+β // Apply affine transform
provides us the sum and sum of squares tensors for each
featurexihwwithin the corresponding window and group.
From the sums and sum of square tensors we obtain mean
and variance tensors needed to normalize the input features.
Note that the running time is constant with respect to the
window size making LCN efﬁcient for arbitrarily large win-
dows1.
4. Experimental Results
In this section we evaluate our proposed normalization
layer for the tasks of object detection, semantic segmen-
tation, and instance segmentation in several benchmark
datasets, and we compare its performance to the best pre-
viously known normalization schemes.
4.1. Semantic Segmentation on Cityscapes
Semantic segmentation consists of assigning a class label
to every pixel in an image. Each pixel is typically labeled
with the class of an enclosing object or region. We test for
semantic segmentation on the Cityscapes dataset [5] which
contains 5,000 ﬁnely-annotated images. The images are di-
vided into 2,975 training, 500 validation, and 1,525 testing
images. There are 30 classes, 19 of which are used for eval-
uation.
Implementation Details. We train state-of-the-art HR-
NetV2 [36] and HRNetV2-W18-Small-v1 networks as
baselines2. We follow the same training protocol as [36].
1A Python implementation of the proposed LCN normalization
layer using PyTorch can be found at: https://github.com/
anthonymlortiz/lcn
2We used the ofﬁcial implementation code from: https:
//github.com/leoxiaobin/deep-high-resolution-net.
pytorch
Table 1: Cityscapes Semantic Segmentation Performance
Method Normalization mIoU Class (%) Pixel Acc. (%) Mean Acc. (%)
HRNetV2 W48 BN 76.22 96.39 83.73
HRNetV2 W48 GN 75.08 95.84 82.70
HRNetV2 W48 LCN (ours) 77.49 96.14 84.60
HRNetV2 W18 Small v1 BN 71.27 95.36 79.49
HRNetV2 W18 Small v1 IN 69.74 94.92 77.77
HRNetV2 W18 Small v1 LN 66.81 94.51 75.46
HRNetV2 W18 Small v1 GN 70.31 95.03 78.99
HRNetV2 W18 Small v1 LCN (ours) 71.77 95.26 79.72
∆GN 1.46 0.23 0.73
The data is augmented by random cropping (from 1024 ×
2048 to 512×1024), random scaling in the range of [0.5,
2], and random horizontal ﬂipping. We use the Stochastic
Gradient Descent (SGD) optimizer with a base learning rate
of 0.01, momentum of 0.9, and weight decay of 0.0005. The
poly learning rate policy with the power of 0.9 is used for
reducing the learning rate as done in [36]. All the mod-
els are trained for 484 epochs. We train HRNetV2 using
four GPUs and a batch size of two per GPU. We then sub-
stitute sync-batch normalization layers by BN, GN, LCN
and compare results. We do exhaustive comparisons us-
ing HRNetV2-W18-Small-v1, which is a smaller version of
HRNetV2; all training details are kept the same except for
the batch size, which is increased to four images per GPU
for faster training.
Quantitative Results. Table 1 shows the performance of
the different normalization layers on the Cityscapes vali-
dation set. In addition to the mean of class-wise intersec-
tion over union (mIoU), we also report pixel-wise accuracy
(Pixel Acc.) and mean of class-wise pixel accuracy (Mean
Acc.).
We observe that our proposed normalization layer out-
performs all other normalization techniques including BN.
LCN is almost 1.5% better than the best GN conﬁguration
in terms of mIoU. For LCN, c group was chosen as 2, with
a window size of 227 ×227 (p=q= 227) for HRNetV2
W18 Small v1 and 255 ×255 for HRNetV2 W48. For GN,
we tested different numbers of groups as shown in Table
2, and we report the best (using 16 groups) for comparison
with other approaches in Table 1. Table 2 shows that GN is
somewhat sensitive to the number of groups, ranging from
67% to 70.3% mIoU. Table 2 also shows results for IN and
LN, both of which perform worse than the best GN perfor-
mance. These results were obtained using HRNetV2-W18-
Small-v1 network architecture. It is important to mention
that we used the same learning rate values to train all mod-
els, which implies that LCN still beneﬁts from the same fast
convergence as other global normalization techniques; thisis not true for local normalization schemes such as LRN,
which tend to require lower learning rates for convergence.
Sensitivity to Number of Channels per Group. We
tested the sensitivity of LCN to the number of channels per
group (c group) parameter by training models for different
values of c group while keeping the window size ﬁxed to
227×227 (p=q= 227). Table 3 shows the performance of
LCN for the different number of channels per group, which
is fairly stable among all conﬁgurations.
Sensitivity to Window Size. We also tested how LCN
performance varies with respect to changes in window size
while keeping the number of channels per group ﬁxed. The
results are shown in Table 4. The bigger the window size
is the closer LCN gets to GN. When the window size (p,
q) is equal to the entire spatial dimensions LCN becomes
GN. From Table 4 we see how performance decreases as
the window size gets closer to the GN equivalent.
Qualitative Results Figure 2 shows two randomly se-
lected examples of the semantic segmentation results ob-
tained from HRNetV2-W18-Small-v1 using GN (last col-
umn) and LCN (second-to-last column) as the normaliza-
tion layers. The second and fourth rows are obtained by
maximizing the orange area from the images above them.
By zooming in and looking at the details in the segmen-
tation results, we see that LCN allows sharper and more
accurate predictions. Carefully looking at the second row,
we can observe how using GN HRNet misses pedestrians,
which are recognized when using LCN. From the last row,
we can see that using LCN results in sharper and less dis-
continuous predictions. LCN allows HRNet to distinguish
between the bike and the legs of the cyclist while GN can-
not. LCN also provides more precise boundaries for the cars
in the background than GN.
Table 2: GN Performance for Different Numbers of Groups
Method Number of Groups mIoU Class (%) Pixel Acc. (%) Mean Acc. (%)
HRNetV2 W18 Small v1 1 (=LN) 66.81 94.51 75.46
HRNetV2 W18 Small v1 2 69.28 94.78 77.39
HRNetV2 W18 Small v1 4 67.00 94.50 76.13
HRNetV2 W18 Small v1 8 67.67 94.76 75.81
HRNetV2 W18 Small v1 16 70.31 95.03 78.99
HRNetV2 W18 Small v1 C (=IN) 69.74 94.92 77.77
Figure 2: Qualitative results on Cityscapes. Going from left to right, this ﬁgure shows: Input ,Ground Truth ,Group
Norm Predictions , and Local Context Norm Predictions . The second and fourth rows are obtained by maximizing the
orange area from the images above. We observe how LCN allows the model to detect small objects missed by GN and offers
sharper and more accurate predictions.
4.2. Object Detection and Instance Segmentation
on Microsoft COCO Dataset
We evaluate our LCN against previously-proposed nor-
malization schemes for object detection and instance seg-
mentation. Object detection involves detecting instances
of objects from a particular class in an image. Instance
segmentation involves detecting and segmenting each ob-
ject in an image. The Microsoft COCO dataset [20] is a
high-quality dataset which provides labels appropriate for
both detection and instance segmentation and is the stan-
dard dataset for both tasks. The annotations include both
pixel-level segmentation masks and bounding boxes for ob-jects belonging to 80 categories.
These computer vision tasks in general beneﬁt from
higher-resolution input. We experiment with the Mask R-
CNN baselines [9], implemented in the publicly available
Detectron codebase. We replace BN and/or GN by LCN
during ﬁnetuning, using the model pre-trained from Ima-
geNet using GN. We ﬁne-tune with a batch size of one im-
age per GPU and train the model using four GPUs.
The models are trained in the COCO [20] train2017 set
and evaluated in the COCO val2017 set (a.k.a. minival).
We report the standard COCO metrics of Average Precision
(AP),AP50, andAP75, for both bounding box detection
Table 3: LCN sensitivity to number of channels per group for a ﬁxed window size (227, 227)
Method Channels per Group mIoU Class (%) Pixel Acc. (%) Mean Acc. (%)
HRNetV2 W18 Small v1 2 71.77 95.26 79.72
HRNetV2 W18 Small v1 4 70.26 95.07 78.49
HRNetV2 W18 Small v1 8 70.14 94.97 78.11
HRNetV2 W18 Small v1 16 70.11 94.78 79.10
Table 4: LCN sensitivity to Window Size
Method Window Size mIoU Class (%) Pixel Acc. (%) Mean Acc. (%)
HRNetV2 Small v1 199 71.55 95.18 79.89
HRNetV2 Small v1 227 71.77 95.26 79.72
HRNetV2 Small v1 255 71.80 95.18 79.26
HRNetV2 Small v1 383 70.09 95.06 77.64
HRNetV2 Small v1 511 70.03 95.09 77.94
HRNetV2 Small v1 all/GN 70.30 95.04 78.97
Table 5: Detection and Instance Segmentation Performance on the Microsoft Coco Dataset
Method APbbox(%) APbbox
50(%) APbbox
75(%) APmask(%) APmask
50 (%) APmask
75
R50 BN 37.47 59.15 40.76 34.06 55.74 36.04
R50 GN 37.34 59.65 40.34 34.33 56.53 36.31
R50 LCN (Ours) 37.90 59.82 41.16 34.50 56.81 36.43
(APbbox) and instance segmentation ( APmask).
Table 5 shows the performance of the different normal-
ization techniques3. LCN outperforms both GN and BN by
a substantial margin in all experiments, even using hyper-
parameters tuned for the other schemes.
4.3. Image Classiﬁcation in ImageNet
We also experiment with image classiﬁcation using the
ImageNet dataset [7]. In this experiment, images must be
classiﬁed into one of 1000 classes. We train on all training
images and evaluate on the 50,000 validation images, using
the ResNet models [11].
Implementation Details. As in most reported results, we
use eight GPUs to train all models, and the batch mean and
variance of BN are computed within each GPU. We use
He’s initialization [10] to initialize convolution weights. We
train all models for 100 epochs, and decrease the learning
rate by 10×at 30, 60, and 90 epochs.
During training, we adopt the data augmentation of
Szegedy et al. [37] as used in [41]. We evaluate the top-
1 classiﬁcation error on the center crops of 224×224pixels
in the validation set. To reduce random variations, we report
the median error rate of the ﬁnal ﬁve epochs [8].
3Our results differ slightly from the ones reported in the original paper,
but this should not affect the comparison across normalization schemes.As in [41] our baseline is the ResNet trained with
BN [11]. To compare with GN and LCN, we replace BN
with the speciﬁc variant. We use the same hyper-parameters
for all models. We set the number of channels per group
for LCN as 32, and we used p=q= 127 for the win-
dow size parameters. Table 6 shows that LCN offers simi-
lar performance as GN, but we don’t see the same boost in
performance observed for object detection and image seg-
mentation. We hypothesize that this happens because image
classiﬁcation is a global task which might not beneﬁt from
local context.
4.4. Systematic Generalization on INRIA Aerial
Imagery Dataset
The INRIA Aerial Image Labeling Dataset was intro-
duced to test generalization of remote-sensing segmentation
models [22]. It includes imagery from 10 dissimilar urban
areas in North America and Europe. Instead of splitting
adjacent portions of the same images into training and test
sets, the splitting was done city-wise. All tiles of ﬁve cities
were included in the training set and the remaining ones are
used as the test set. The imagery is orthorectiﬁed [22] and
has a spatial resolution of 0.3m per pixel. The dataset cov-
ers 810km2(405km2for training and 405 km2for the
test set). Images were labeled for the semantic classes of
building and non-building.
Table 6: Image Classiﬁcation Error on Imagenet
Network Architecture Normalization Top 1 Err. (%) Top 5 Err. (%)
Resnet 50 BN 23.59 6.82
Resnet 50 GN 24.24 7.35
Resnet 50 LCN 24.23 7.22
Table 7: Performance in INRIA Aerial Image Labeling Dataset. LCN outperforms all the other normalization layers overall.
Method Bellingham Bloomington Innsbruck San Francisco East Tyrol Overall
IoU Acc. IoU Acc. IoU Acc. IoU Acc. IoU Acc. IoU Acc.
U-Net + BN 65.37 96.53 55.07 95.83 67.62 96.08 72.80 91.00 67.00 96.91 67.98 95.27
U-Net + GN 55.48 93.38 55.47 94.41 58.93 93.77 72.12 89.56 62.27 95.73 63.71 93.45
U-Net + LCN 63.61 96.26 60.47 96.22 68.99 96.28 75.01 91.46 68.90 97.19 69.90 95.48
Implementation Details. We trained different versions of
U-Net [31] where just the normalization layer was changed.
We trained all models in this set of experiments using 572×
572randomly sampled patches from all training image tiles.
We used the Adam optimizer with a batch size of 12. All
networks were trained from scratch with a starting learning
rate of 0.001. We keep the same learning rate for the ﬁrst
60 epochs and decay it to 0.0001 over the next 40 epochs.
Every network was trained for 100 epochs. In every epoch
8,000 patches are seen. Binary cross-entropy loss was used
as the loss function.
Table 7 summarizes the performance of the different
normalization layers in the INRIA aerial image labeling
dataset. Our proposed LCN outperforms all the other nor-
malization layers with an overall mIoU almost 2% higher
than the next-best normalization scheme, and more than 6%
better than GN in terms of overall IoU. LCN provides much
better performance than other methods in almost every test
city. LCN was trained using a 91×91window size and four
channels per group.
4.5. Land Cover Mapping
Table 8: Landcover Mapping Tested on Maryland 2013 Test
Tiles
Method mIoU (%) Pixel Acc. (%)
UNet + BN 76.69 87.15
UNet + GN 74.15 85.18
UNet + LCN 76.51 86.96
Finally, we evaluate LCN on a land cover mapping task
previously studied in [29, 1]. Land cover mapping is a
semantic image segmentation task where each pixel in an
aerial or satellite image must be classiﬁed as belonging to
one of a variety of land cover classes. This process of turn-
ing raw remotely sensed imagery into a summarized data
product is an important ﬁrst step in many downstream sus-tainability related applications. For example, the Chesa-
peake Bay Conservancy uses land cover data in a vari-
ety of settings including determining where to target plant-
ing riparian forest buffers [4]. The dataset can be found
at [1] and contains 4-channel (red, green, blue, and near-
infrared), 1m resolution imagery from the National Agri-
cultural Imagery Program (NAIP) and dense pixel labels
from the Chesapeake Conservancy’s land cover mapping
program over 100,000 square miles intersecting 6 states in
the northeastern US. We use the Maryland 2013 subset -
training on the 50,000 multi-spectral image patches, each
of size 256×256×4, from the train split. We test over
the 20 test tiles4. Each pixel must be classiﬁed as: water,
tree canopy / forest, low vegetation / ﬁeld, or impervious
surfaces.
Implementation Details We trained different versions of
U-Net architecture used on [29] for different normalization
layers without doing any data augmentation and compared
results. We used the Adam optimizer with a batch size of
96. All networks were trained from scratch for 100 epochs
with a starting learning rate of 0.001 with decay to 0.0001
after 60 epochs. The multi-class cross-entropy loss was
used as criterion. The best GN results are obtained using
8 groups. LCN results are obtained using 4 channels per
group and and a 31×31window.
Table 8 shows the mean IoU and Pixel Accuracy of the
different normalization layers for land cover mapping. LCN
outperforms GN for this task with performance slightly
lower than BN. We notice that LCN beneﬁts from larger in-
put images. When input images are small like in this setting
the performance boost from using LCN becomes smaller.
4Consisting of∼900,000,000pixels
5. Discussion and Conclusion
We proposed Local Context Normalization (LCN) as
a normalization layer where every feature is normalized
based on a window around it and the ﬁlters in its group. We
empirically showed that LCN outperforms all previously-
proposed normalization layers for object detection, seman-
tic segmentation, and instance image segmentation across a
variety of datasets. The performance of LCN is invariant
to batch size, and it is well-suited for transfer learning and
interactive systems.
We note that we used hyper-parameters which were al-
ready highly optimized for BN and/or GN without tuning,
so it is likely that we could obtain better results with LCN
by just searching for better hyper-parameters. In our exper-
iments we also do not consider varying the window size for
different layers in the network, but it is a direction worth ex-
ploring: adjusting the window size during training via gra-
dient descent may further improve performance for LCN.
Acknowledgement
The authors thank Lucas Joppa and the Microsoft
AI for Earth initiative for their support. A.O. was
supported by the Army Research Ofﬁce under award
W911NF-17-1-0370. We thank Nvidia Corporation for
the donation of two Titan Xp GPUs used for this re-
search.
References
[1] Chesapeake land cover. Maryland split. 8
[2] Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,
Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Is-
ard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Man-
junath Kudlur, Josh Levenberg, Dan Man ´e, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-
nanda Vi ´egas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorﬂow.org. 4
[3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,
Kevin Murphy, and Alan L Yuille. Semantic image seg-
mentation with deep convolutional nets and fully connected
CRFs. arXiv preprint arXiv:1412.7062 , 2014. 4
[4] Chesapeake Conservancy. Land cover data project
2013/2014. https://chesapeakeconservancy.
org/conservation-innovation-center/
high-resolution-data/
land-cover-data-project/ , 2016. 8
[5] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo
Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe
Franke, Stefan Roth, and Bernt Schiele. The cityscapesdataset for semantic urban scene understanding. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 3213–3223, 2016. 4
[6] Navneet Dalal and Bill Triggs. Histograms of oriented gra-
dients for human detection. In 2005 IEEE conference on
computer vision and pattern recognition , 2005. 3
[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.
ImageNet: A Large-Scale Hierarchical Image Database. In
CVPR09 , 2009. 2, 7
[8] Priya Goyal, Piotr Doll ´ar, Ross Girshick, Pieter Noord-
huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,
Yangqing Jia, and Kaiming He. Accurate, large mini-
batch SGD: Training Imagenet in 1 hour. arXiv preprint
arXiv:1706.02677 , 2017. 7
[9] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask R-CNN. In Proceedings of the IEEE inter-
national conference on computer vision , pages 2961–2969,
2017. 6
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Delving deep into rectiﬁers: Surpassing human-level perfor-
mance on imagenet classiﬁcation. In Proceedings of the
IEEE international conference on computer vision , pages
1026–1034, 2015. 7
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Cision and Pat-
tern Recognition , pages 770–778, 2016. 7
[12] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. Proceedings of the International Conference in
Machine Learning (ICML) , 2015. 1, 2
[13] Kevin Jarrett, Koray Kavukcuoglu, Yann LeCun, et al. What
is the best multi-stage architecture for object recognition?
In2009 IEEE 12th International Conference on Computer
Vision , pages 2146–2153. IEEE, 2009. 2, 3
[14] Nebojsa Jojic and Yaron Caspi. Capturing image structure
with probabilistic index maps. In Proceedings of the 2004
IEEE Computer Society Conference on Computer Vision and
Pattern Recognition, 2004. CVPR 2004. , volume 1, pages I–
I. IEEE, 2004. 2, 3
[15] Nebojsa Jojic, Alessandro Perina, Marco Cristani, Vittorio
Murino, and Brendan Frey. Stel component analysis: Mod-
eling spatial correlations in image class structure. In 2009
IEEE conference on computer vision and pattern recogni-
tion, pages 2044–2051. IEEE, 2009. 2, 3
[16] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In Advances in neural information processing sys-
tems, pages 1097–1105, 2012. 2
[17] Yann LeCun, L ´eon Bottou, Yoshua Bengio, Patrick Haffner,
et al. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE , 86(11):2278–2324, 1998.
2
[18] Yann A LeCun, L ´eon Bottou, Genevieve B Orr, and Klaus-
Robert M ¨uller. Efﬁcient backprop. In Neural networks:
Tricks of the trade , pages 9–48. Springer, 1998. 2
[19] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450 ,
2016. 2
[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
European conference on computer vision , pages 740–755.
Springer, 2014. 6
[21] Siwei Lyu and Eero P Simoncelli. Nonlinear image repre-
sentation using divisive normalization. In 2008 IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
1–8. IEEE, 2008. 2, 3
[22] Emmanuel Maggiori, Yuliya Tarabalka, Guillaume Charpiat,
and Pierre Alliez. Can semantic labeling methods generalize
to any city? the inria aerial image labeling benchmark. In
IEEE International Geoscience and Remote Sensing Sympo-
sium (IGARSS) . IEEE, 2017. 7
[23] Anthony Ortiz, Olac Fuentes, Dalton Rosario, and Christo-
pher Kiekintveld. On the defense against adversarial ex-
amples beyond the visible spectrum. In MILCOM 2018-
2018 IEEE Military Communications Conference (MIL-
COM) , pages 1–5. IEEE, 2018. 2
[24] Anthony Ortiz, Alonso Granados, Olac Fuentes, Christopher
Kiekintveld, Dalton Rosario, and Zachary Bell. Integrated
learning and feature selection for deep neural networks in
multispectral images. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition Work-
shops , pages 1196–1205, 2018. 2
[25] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
ban Desmaison, Luca Antiga, and Adam Lerer. Automatic
differentiation in PyTorch. In NIPS-W , 2017. 4
[26] Rodrigo Perin, Thomas K Berger, and Henry Markram.
A synaptic organizing principle for cortical neuronal
groups. Proceedings of the National Academy of Sciences ,
108(13):5419–5424, 2011. 3
[27] Nicolas Pinto, David D Cox, and James J DiCarlo. Why is
real-world visual object recognition hard? PLoS computa-
tional biology , 4(1):e27, 2008. 3
[28] Sylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea Vedaldi.
Learning multiple visual domains with residual adapters. In
Advances in Neural Information Processing Systems , pages
506–516, 2017. 1, 2
[29] Caleb Robinson, Le Hou, Kolya Malkin, Rachel Soobit-
sky, Jacob Czawlytko, Bistra Dilkina, and Nebojsa Jojic.
Large scale high-resolution land cover mapping with multi-
resolution data. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 12726–
12735, 2019. 2, 8
[30] Caleb Robinson, Anthony Ortiz, Kolya Malkin, Blake Elias,
Andi Peng, Dan Morris, Bistra Dilkina, and Nebojsa Jojic.
Human-machine collaboration for fast land cover mapping.
AAAI Conference on Artiﬁcial Intelligence (AAAI 2020) ,
2020. 2
[31] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In International Conference on Medical image com-puting and computer-assisted intervention , pages 234–241.
Springer, 2015. 8
[32] Tim Salimans and Durk P Kingma. Weight normalization: A
simple reparameterization to accelerate training of deep neu-
ral networks. In Advances in Neural Information Processing
Systems , pages 901–909, 2016. 2
[33] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and
Aleksander Madry. How does batch normalization help op-
timization? In Advances in Neural Information Processing
Systems , pages 2483–2493, 2018. 1
[34] Pierre Sermanet, Soumith Chintala, and Yann LeCun. Con-
volutional neural networks applied to house numbers digit
classiﬁcation. In 2012 21st International Conference on
Pattern Recognition (ICPR 2012) , pages 3288–3291. IEEE,
2012. 3
[35] Pierre Sermanet, David Eigen, Xiang Zhang, Micha ¨el Math-
ieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated
recognition, localization and detection using convolutional
networks. arXiv preprint arXiv:1312.6229 , 2013. 2
[36] Ke Sun, Yang Zhao, Borui Jiang, Tianheng Cheng, Bin Xiao,
Dong Liu, Yadong Mu, Xinggang Wang, Wenyu Liu, and
Jingdong Wang. High-resolution representations for labeling
pixels and regions. arXiv preprint arXiv:1904.04514 , 2019.
4, 5
[37] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1–9, 2015.
2, 7
[38] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-
stance normalization: The missing ingredient for fast styliza-
tion. arXiv preprint arXiv:1607.08022 , 2016. 2
[39] Paul Viola, Michael Jones, et al. Rapid object detection using
a boosted cascade of simple features. 2001. 4
[40] John Winn and Nebojsa Jojic. Locus: Learning object
classes with unsupervised segmentation. In Tenth IEEE In-
ternational Conference on Computer Vision (ICCV’05) Vol-
ume 1 , volume 1, pages 756–763. IEEE, 2005. 2, 3
[41] Yuxin Wu and Kaiming He. Group normalization. In Euro-
pean Conference on Computer Vision , pages 3–19. Springer,
2018. 1, 2, 3, 7
[42] Fisher Yu and Vladlen Koltun. Multi-scale context
aggregation by dilated convolutions. arXiv preprint
arXiv:1511.07122 , 2015. 4
[43] Matthew D Zeiler and Rob Fergus. Visualizing and under-
standing convolutional networks. In European conference on
computer vision , pages 818–833. Springer, 2014. 2
Transfer Learning from Deep Features for
Remote Sensing and Poverty Mapping
Michael Xie, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon
Department of Computer Science, Stanford University
{xie, nealjean, ermon}@cs.stanford.edu
Department of Earth System Science, Stanford University
{mburke,dlobell}@stanford.edu
Abstract
The lack of reliable data in developing countries is a major
obstacle to sustainable development, food security, and dis-
aster relief. Poverty data, for example, is typically scarce,
sparse in coverage, and labor-intensive to obtain. Remotesensing data such as high-resolution satellite imagery, on the
other hand, is becoming increasingly available and inexpen-
sive. Unfortunately, such data is highly unstructured and cur-rently no techniques exist to automatically extract useful in-
sights to inform policy decisions and help direct humanitar-
ian efforts. We propose a novel machine learning approachto extract large-scale socioeconomic indicators from high-
resolution satellite imagery. The main challenge is that train-
ing data is very scarce, making it difﬁcult to apply modern
techniques such as Convolutional Neural Networks (CNN).
We therefore propose a transfer learning approach wherenighttime light intensities are used as a data-rich proxy. We
train a fully convolutional CNN model to predict nighttime
lights from daytime imagery, simultaneously learning fea-tures that are useful for poverty prediction. The model learns
ﬁlters identifying different terrains and man-made structures,
including roads, buildings, and farmlands, without any su-pervision beyond nighttime lights. We demonstrate that these
learned features are highly informative for poverty mapping,
even approaching the predictive performance of survey datacollected in the ﬁeld.
Introduction
New technologies fueling the Big Data revolution are cre-
ating unprecedented opportunities for designing, monitor-ing, and evaluating policy decisions and for directing hu-manitarian efforts (Abelson, Varshney, and Sun 2014;Varshney et al. 2015). However, while rich countries arebeing ﬂooded with data, developing countries are suffer-ing from data drought. A new data divide is emerging, withhuge differences in the quantity and quality of data avail-able. For example, some countries have not taken a census indecades, and in the past ﬁve years an estimated 230 millionbirths have gone unrecorded (Independent Expert AdvisoryGroup Secretariat 2014). Even high-proﬁle initiatives suchas the Millennium Development Goals (MDGs) are affected(United Nations 2015). Progress based on poverty and infantmortality rate targets can be difﬁcult to track. Often, poverty
Copyright c⃝2016, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.measures must be inferred from small-scale and expensivehousehold surveys, effectively rendering many of the poor-est people invisible.
Remote sensing, particularly satellite imagery, is perhaps
the only cost-effective technology able to provide data at aglobal scale. Within ten years, commercial services are ex-pected to provide sub-meter resolution images everywhereat a fraction of current costs (Murthy et al. 2014). This levelof temporal and spatial resolution could provide a wealthof data towards sustainable development. Unfortunately, thisraw data is also highly unstructured, making it difﬁcult toextract actionable insights at scale.
In this paper, we propose a machine learning approach
for extracting socioeconomic indicators from raw satelliteimagery. In the past ﬁve years, deep learning approachesapplied to large-scale datasets such as ImageNet have rev-olutionized the ﬁeld of computer vision, leading to dramaticimprovements in fundamental tasks such as object recogni-tion (Russakovsky et al. 2014). However, the use of con-temporary techniques for the analysis of remote sensing im-agery is still largely unexplored. Modern approaches suchas Convolutional Neural Networks (CNN) can, in principle,be directly applied to extract socioeconomic factors, but theprimary challenge is a lack of training data. While such datais readily available in the United States and other developednations, it is extremely scarce in Africa where these tech-niques would be most useful.
We overcome this lack of training data by using a se-
quence of transfer learning steps and a convolutional neu-ral network model. The idea is to leverage available datasetssuch as ImageNet to extract features and high-level repre-sentations that are useful for the task of interest, i.e., ex-tracting socioeconomic data for poverty mapping. Similarstrategies have proven quite successful in the past. For ex-ample, image features from the Overfeat network trained
on ImageNet for object classiﬁcation achieved state-of-the-
art results on tasks such as ﬁne-grained recognition, image
retrieval, and attribute detection (Razavian et al. 2014).
Pre-training on ImageNet is useful for learning low-level
features such as edges. However, ImageNet consists onlyof object-centric images, while satellite imagery is capturedfrom an aerial, bird’s-eye view. We therefore employ a sec-ond transfer learning step, where nighttime light intensitiesare used as a proxy for economic activity. Speciﬁcally, weProceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16)
3929
start with a CNN model pre-trained for object classiﬁca-
tion on ImageNet and learn a modiﬁed network that pre-
dicts nighttime light intensities from daytime imagery. To
address the trade-off between ﬁxed image size and informa-tion loss from image scaling, we use a fully convolutionalmodel that takes advantage of the full satellite image. Weshow that transfer learning is successful in learning featuresrelevant not only for nighttime light prediction but also forpoverty mapping. For instance, the model learns ﬁlters iden-tifying man-made structures such as roads, urban areas, andﬁelds without any supervision beyond nighttime lights, i.e.,without any labeled examples of roads or urban areas (Fig-
ure 2). We demonstrate that these features are highly infor-
mative for poverty mapping and capable of approaching thepredictive performance of survey data collected in the ﬁeld.
Problem Setup
We begin by reviewing transfer learning and convolutionalneural networks, the building blocks of our approach.
Transfer Learning
We formalize transfer learning as in (Pan and Yang 2010): AdomainD={X,P(X)}consists of a feature space Xand
a marginal probability distribution P(X). Given a domain,
ataskT={Y,f(·)}consists of a label space Yand a pre-
dictive function f(·)which models P(y|x)fory∈Y and
x∈X . Given a source domain D
Sand learning task TS, and
a target domain DTand learning task TT,transfer learning
aims to improve the learning of the target predictive functionf
T(·)inTTusing the knowledge from DSandTS, where
DS̸=DT,TS̸=TT, or both. Transfer learning is particu-
larly relevant when, given labeled source domain data DS
and target domain data DT, we ﬁnd that |DT|≪| DS|.
In our setting, we are interested in more than two re-
lated learning tasks. We generalize the formalism by repre-
senting the multiple source-target relationships as a transfer
learning graph. First, we deﬁne a transfer learning prob-
lemP=(D,T)as a domain-task pair. The transfer learn-
ing graph is then deﬁned as follows: A transfer learning
graph G=(V,E)is a directed acyclic graph where ver-
ticesV={P1,···,Pv}are transfer learning problems and
E={(Pi1,Pj1),···,(Pie,Pje)}is an edge set. For each
transfer learning problem Pi=(Di,Ti)∈V , the aim is to
improve the learning of the target predictive function fi(·)
inTiusing the knowledge in ∪(j,i)∈EPj.
Convolutional Neural Networks
Deep learning approaches are based on automatically learn-
ing nested, hierarchical representations of data. Deep feed-forward neural networks are the typical example of deeplearning models. Convolutional Neural Networks (CNN) in-clude convolutional operations over the input and are de-signed speciﬁcally for vision tasks. Convolutional ﬁlters areuseful for encoding translation invariance, a key concept fordiscovering useful features in images (Bouvrie 2006).
A CNN is a general function approximator deﬁned by a
set of convolutional and fully connected layers ordered suchthat the output of one layer is the input of the next. For imagedata, the ﬁrst layers of the network typically learn low-levelfeatures such as edges and corners, and further layers learnhigh-level features such as textures and objects (Zeiler andFergus 2013). Taken as a whole, a CNN is a mapping fromtensors to feature vectors, which become the input for a ﬁnalclassiﬁer. A typical convolutional layer maps a tensor x∈
R
h×w×dtogi∈Rˆh×ˆw×ˆdsuch that
gi=pi(fi(Wi∗x+bi)),
where for the i-th convolutional layer, Wi∈Rl×l×ˆdis a
tensor of ˆdconvolutional ﬁlter weights of size l×l,(∗)is
the 2-dimensional convolution operator over the last two di-mensions of the inputs, b
iis a bias term, fiis an element-
wise nonlinearity function (e.g., a rectiﬁed linear unit orReLU), and p
iis a pooling function. The output dimensions
ˆhandˆwdepend on the stride and zero-padding parameters
of the layer, which control how the convolutional ﬁlters slideacross the input. For the ﬁrst convolutional layer, the inputdimensions h,w, and dcan be interpreted as height, width,
and number of color channels of an input image, respec-tively.
In addition to convolutional layers, most CNN models
have fully connected layers in the ﬁnal layers of the net-work. Fully connected layers map an unrolled version of theinputˆx∈R
hwd, which is a one-dimensional vector of the
elements of a tensor x∈Rh×w×d, to an output gi∈Rk
such that
gi=fi(Wiˆx+bi),
where Wi∈Rk×hwdis a weight matrix, biis a bias term,
andfiis typically a ReLU nonlinearity function. The fully
connected layers encode the input examples as feature vec-tors, which are used as inputs to a ﬁnal classiﬁer. Since thefully connected layer looks at the entire input at once, thesefeature vectors “summarize” the input into a feature vec-tor for classiﬁcation. The model is trained end-to-end usingminibatch gradient descent and backpropagation.
After training, the output of the ﬁnal fully connected layer
can be interpreted as an encoding of the input as a featurevector that facilitates classiﬁcation. These features often rep-resent complex compositions of the lower-level features ex-tracted by the previous layers (e.g., edges and corners) andcan range from grid patterns to animal faces (Zeiler and Fer-gus 2013; Le et al. 2012).
Combining Transfer Learning and Deep Learning
The low-level and high-level features learned by a CNN on asource domain can often be transferred to augment learningin a different but related target domain. For target problemswith abundant data, we can transfer low-level features, suchas edges and corners, and learn new high-level features spe-ciﬁc to the target problem. For target problems with limitedamounts of data, learning new high-level features is difﬁcult.However, if the source and target domain are sufﬁcientlysimilar, the feature representation learned by the CNN onthe source task can also be used for the target problem. Deepfeatures extracted from CNNs trained on large annotateddatasets of images have been used as generic features very
3930
effectively for a wide range of vision tasks (Donahue et al.
2013; Oquab et al. 2014).
Transfer Learning for Poverty Mapping
In our approach to poverty mapping using satellite imagery,we construct a linear chain transfer learning graph withV={P
1,P2,P3}andE={(P1,P2),(P2,P3)}. The ﬁrst
transfer learning problem P1is object recognition on Im-
ageNet (Russakovsky et al. 2014); the second problem P2
is predicting nighttime light intensity from daytime satelliteimagery; the third problem P
3is predicting poverty from
daytime satellite imagery. Recognizing the differences be-
tween ImageNet data and satellite imagery, we use the inter-mediate problem P
2to learn about the bird’s-eye viewpoint
of satellite imagery and extract features relevant to socioe-
conomic development.
ImageNet to Nighttime Lights
ImageNet is an object classiﬁcation image dataset of over14 million images with 1000 class labels that, along withCNN models, have fueled major breakthroughs in many vi-sion tasks (Russakovsky et al. 2014). CNN models trainedon the ImageNet dataset are recognized as good generic fea-ture extractors, with low-level and mid-level features such
as edges and corners that are able to generalize to many newtasks (Donahue et al. 2013; Oquab et al. 2014). Our goal is
to transfer knowledge from the ImageNet object recognitionchallenge (P
1) to the target problem of predicting nighttime
light intensity from daytime satellite imagery (P 2).
InP1, we have an object classiﬁcation problem with
source domain data D1={(x1i,y1i)}from ImageNet that
consists of natural images x1i∈X1and object class labels.
InP2, we have a nighttime light intensity prediction prob-
lem with target domain data D2={(x2i,y2i})that consists
of daytime satellite images x2i∈X2and nighttime light
intensity labels. Although satellite data is still in the spaceof image data, satellite imagery presents information from abird’s-eye view and at a much different scale than the object-centric ImageNet dataset (P (X
1)̸=P(X2)). Previous work
in domains with images fundamentally different from nor-mal “human-eye view” images typically resort to curating anew, speciﬁc dataset such as Places205 (Zhou et al. 2014).In contrast, our transfer learning approach does not requirehuman annotation and is much more scalable. Additionally,unsupervised approaches such as autoencoders may wasterepresentational capacity on irrelevant features, while thenighttime light labels guide learning towards features rele-vant to wealth and economic development.
The National Oceanic and Atmospheric Administration
(NOAA) provides annual nighttime images of the worldwith 30 arc-second resolution, or about 1 square kilometer(NOAA National Geophysical Data Center 2014). The lightintensity values are averaged and denoised for each year toensure that ephemeral light sources do not affect the data.
The nighttime light dataset D
2is constructed as follows:
The Demographic Health Survey (DHS) Program conductsnationally representative surveys in Africa that focus mainlyon health outcomes (ICF International 2015). Predicting
Figure 1: Locations (in white) of 330,000 sampled daytimeimages near DHS survey locations for the nighttime lightintensity prediction problem.
health outcomes is beyond the scope of this paper; how-
ever, the DHS surveys offer the most comprehensive dataavailable for Africa. Thus, we use DHS survey locations asguidelines for sampling training images (see Figure 1). Im-ages in D
2are daytime satellite images randomly sampled
near DHS survey locations in Africa. Satellite images are
downloaded using the Google Static Maps API, each with400×400 pixels at zoom level 16, resulting in images sim-
ilar in size to pixels in the NOAA nighttime lights data. The
aggregate dataset D
2consists of over 330,000 images, each
labeled with an integer nighttime light intensity value rang-ing from 0 to 63
1. We further subsample and bin the data
using a Gaussian mixture model, as detailed in the compan-ion technical report (Xie et al. 2015).
Nighttime Lights to Poverty Estimation
The ﬁnal and most important learning task P3is that of pre-
dicting poverty from satellite imagery, for which we havevery limited training data. Our goal is to transfer knowledgefromP
2, a data-rich problem, to P3.
The target domain data D3={(x3i,y3i)}consists of
satellite images x3i∈X3from the feature space of satellite
images of Uganda and a limited number of poverty labels
y3i∈Y3, detailed below. The source data is D2, the night-
time lights data. Here, the input feature space of images issimilar in both the source and target domains, drawn from
a similar distribution of images (satellite images) from re-
lated areas (Africa and Uganda), implying that X
2=X3,
P(X2)≈P(X3). The source (lights) and target (poverty)
tasks both have economic elements, but are quite different.
The poverty training data D3relies on the Living Stan-
dards Measurement Study (LSMS) survey conducted inUganda by the Uganda Bureau of Statistics between 2011and 2012 (Uganda Bureau of Statistics 2012). The LSMS
1Nighttime light intensities are from 2013, while the daytime
satellite images are from 2015. We assume that the areas under
study have not changed signiﬁcantly in this two-year period, but
this temporal mismatch is a potential source of error.
3931
survey consists of data from 2,716 households in Uganda,
which are grouped into 643 unique location groups. The
average latitude and longitude location of the households
within each group is given, with added noise of up to 5kmin each direction. Individual household locations are with-held to preserve anonymity. In addition, each household hasa binary poverty label based on expenditure data from thesurvey. We use the majority poverty classiﬁcation of house-holds in each group as the overall location group povertylabel. For a given group, we sample approximately 1001km× 1km images tiling a 10km ×10km area centered at
the average household location as input. This deﬁnes the
probability distribution P(X
3)of the input images for the
poverty classiﬁcation problem P3.
Predicting Nighttime Light Intensity
Our ﬁrst goal is to transfer knowledge from the ImageNet
object recognition task to the nighttime light intensity pre-diction problem. We start with a CNN model with parame-ters trained on ImageNet, then modify the network to adapt itto the new task (i.e., change the classiﬁer on the last layer toreﬂect the new nighttime light prediction task). We train onthe new task using SGD with momentum, using ImageNetparameters as initialization to achieve knowledge transfer.
We choose the VGG F model trained on ImageNet as the
starting CNN model (Chatﬁeld et al. 2014). The VGG F
model has 8 convolutional and fully connected layers. Like
many other ImageNet models, the VGG F model accepts a
ﬁxed input image size of 224×224 pixels. Input images
inD
2, however, are 400×400 pixels, corresponding to the
resolution of the nighttime lights data.
We consider two ways of adapting the original VGG F
network. The ﬁrst approach is to keep the structure of the
network (except for the ﬁnal classiﬁer) and crop the inputto224×224 pixels (random cropping). This is a reason-
able approach, as the original model was trained by cropping224×224 images from a larger 256×256 image (Chatﬁeld
et al. 2014). Ideally, we would evaluate the network at multi-ple crops of the 400×400 input and average the predictions
to leverage the context of the entire input image. However,doing this explicitly with one forward pass for each cropwould be too costly. Alternatively, if we allow the multiplecrops of the image to overlap, we can use a convolution tocompute scores for each crop simultaneously, gaining speedby reusing ﬁlter outputs at all layers. We therefore proposea fully convolutional architecture (fully convolutional).
Fully Convolutional Model
Fully convolutional models have been used successfully forspatial analysis of arbitrary size inputs (Wolf and Platt 1994;Long, Shelhamer, and Darrell 2014). We construct the fullyconvolutional model by converting the fully connected lay-ers of the VGG F network to convolutional layers. This al-
lows the network to efﬁciently “slide” across a larger input
image and make multiple evaluations of different parts of the
image, incorporating all available contextual information.
Given an unrolled h×w×d-dimensional input x∈R
hwd,fully connected layers perform a matrix-vector product
ˆx=f(Wx+b)
where W∈Rk×hwdis a weight matrix, bis a bias term,
fis a nonlinearity function, and ˆx∈Rkis the output. In
the fully connected layer, we take kinner products with the
unrolled xvector. Thus, given a differently sized input, it is
unclear how to evaluate the dot products.
We replace a fully connected layer by a convolutional
layer with kconvolutional ﬁlters of size h×w, the same
size as the input. The ﬁlter weights are shared across allchannels, which means that the convolutional layer actually
uses fewer parameters than the fully connected layer. Since
the ﬁlter size is matched with the input size, we can takean element-wise product and add, which is equivalent to aninner product. This results in a scalar output for each ﬁl-ter, creating an output ˆx∈R
1×1×k. Further fully connected
layers are converted to convolutional layers with ﬁlter size
1×1, matching the new input ˆx∈R1×1×k. Fully connected
layers are usually the last layers of the network, while all
previous layers are typically convolutional. After convertingfully connected layers to convolutional layers, the entire net-work becomes convolutional, allowing the outputs of eachlayer to be reused as the convolution slides the network overa larger input. Instead of a scalar output, the new output is a2-dimensional map of ﬁlter activations.
In our fully convolutional model, the 400×400 input pro-
duces an output of size 2×2×4096, which represents the
scores of four (overlapping) quadrants of the image for 4096features. The regional scores are then averaged to obtain a4096-dimensional feature vector that becomes the ﬁnal in-put to the classiﬁer predicting nighttime light intensity.
Training and Performance Evaluation
Both CNN models are trained using minibatched gradientdescent with momentum. Random mirroring is used for dataaugmentation, along with 50% dropout on convolutionallayers replacing fully connected layers. The learning rate be-gins at 1e-6, a hundredth of the ending learning rate of theVGG model. All other hyperparameters are the same as in the
VGG model as described in (Chatﬁeld et al. 2014). The VGG
model parameters are obtained from the Caffe Model Zoo,and all networks are trained with Caffe (Jia et al. 2014). Thefully convolutional model is ﬁne-tuned from the pre-trainedparameters of the VGG F model, but it randomly initializes
the convolutional layers that replace fully connected layers.
In the process of cropping, the random cropping model
throws away over 68% of the input image when predict-ing the class scores, losing much of the spatial context. Therandom cropping model achieved a validation accuracy of70.04% after 400,200 SGD iterations. In comparison, thefully convolutional model achieved 71.58% validation accu-racy after only 223,500 iterations. Both models were trainedin roughly three days. Despite reinitializing the ﬁnal convo-lutional layers from scratch, the fully convolutional modelexhibits faster learning and better performance. The ﬁnalfully convolutional model achieves a validation accuracy of71.71%, trained over 345,000 iterations.
3932
Figure 2: Left: Each row shows ﬁve maximally activating images for a different ﬁlter in the ﬁfth convolutional layer of the CNN
trained on the nighttime light intensity prediction problem. The ﬁrst ﬁlter (ﬁrst row) activates for urban areas. The second ﬁlter
activates for farmland and grid-like patterns. The third ﬁlter activates for roads. The fourth ﬁlter activates for water, plains, andforests, terrains contributing similarly to nighttime light intensity. The only supervision used is nighttime light intensity, i.e., nolabeled examples of roads or farmlands are provided. Right: Filter activations for the corresponding images on the left. Filters
mostly activate on the relevant portions of the image. For example, in the third row, the strongest activations coincide with theroad segments. Best seen in color. See the companion technical report for more visualizations (Xie et al. 2015). Images fromGoogle Static Maps.
Visualizing the Extracted Features
Nighttime lights are used as a data-rich proxy, so absoluteperformance on this task is not directly relevant for povertymapping. The goal is to learn high-level features that areindicative of economic development and can be used forpoverty mapping in the spirit of transfer learning.
We visualize the ﬁlters learned by the fully convolutional
network by inspecting the 25 maximally activating imagesfor each ﬁlter (Figure 2, left and the companion technicalreport for more visualizations (Xie et al. 2015)). Activationlevels for ﬁlters in the middle of the network are obtained bypassing the images forward through the ﬁlter, applying theReLU nonlinearity, and then averaging the map of activationvalues. We ﬁnd that many ﬁlters learn to identify semanti-cally meaningful features such as urban areas, water, roads,barren land, forests, and farmland. Amazingly, these fea-
tures are learned without direct supervision, in contrastto previous efforts to extract features from aerial imagery,which have relied heavily on large amounts of expert-labeleddata, e.g., labeled examples of roads (Mnih and Hinton2010; 2012). To conﬁrm the semantics of the ﬁlters, we vi-sualize their activations for the same set of images (Figure 2,right). These maps conﬁrm our interpretation by identifyingthe image parts that are most responsible for activating theﬁlter. For example, the ﬁlter in the third row mostly acti-vates on road segments. These features are extremely usefulsocioeconomic indicators and suggest that transfer learningto the poverty task is possible.
Poverty Estimation and Mapping
The ﬁrst target task we consider is to predict whether the ma-jority of households are above or below the poverty thresh-old for 643 groups of households in Uganda.Given the limited amount of training data, we do not at-
tempt to learn new feature representations for the target task.Instead, we directly use the feature representation learned bythe CNN on the nighttime lights task (P
2). Speciﬁcally, we
evaluate the CNN model on new input images and feed thefeature vector produced in the last layer as input to a logis-tic regression classiﬁer, which is trained on the poverty task(transfer model). Approximately 100 images in a 10km ×
10km area around the average household location of eachgroup are used as input. We compare against the perfor-mance of a classiﬁer with features from the VGG F model
trained on ImageNet only (ImageNet model), i.e., withouttransfer learning from nighttime lights. In both the ImageNet
model and the transfer model, the feature vectors are aver-aged over the input images for each group.
The Uganda LSMS survey also includes household-
speciﬁc data. We extract the features that could feasibly be
detected with remote sensing techniques, including roof ma-
terial, number of rooms, house type, distances to various in-frastructure points, urban or rural classiﬁcation, annual tem-
perature, and annual precipitation. These survey features are
then averaged over each household group. The performance
of the classiﬁer trained with survey features (survey model)
represents the gold standard for remote sensing techniques.
We also compare with a classiﬁer trained using the nighttime
light intensities themselves as features (lights model). Thenighttime light features consist of the average light intensity,summary statistics, and histogram-based features for eacharea. Finally, we compare with a classiﬁer trained using aconcatenation of ImageNet features and nighttime light fea-tures (ImageNet + lights model), an explicit way of com-
bining information from both source problems.
All models are trained using a logistic regression classiﬁer
with L1 regularization using a nested 10-fold cross valida-
3933
Figure 3: Left: Predicted poverty probabilities at a ﬁne-grained 10km ×10km block level. Middle: Predicted poverty probabil-
ities aggregated at the district-level. Right: 2005 survey results for comparison (World Resources Institute 2009).
Survey ImgNet LightsImgNetTransfer+Lights
Accuracy 0.754 0.686 0.526 0.683 0.716
F1 Score 0.552 0.398 0.448 0.400 0.489
Precision 0.450 0.340 0.298 0.338 0.394
Recall 0.722 0.492 0.914 0.506 0.658
AUC 0.776 0.690 0.719 0.700 0.761
Table 1: Cross validation test performance for predicting
aggregate-level poverty measures. Survey is trained on sur-vey data collected in the ﬁeld. All other models are basedon satellite imagery. Our transfer learning approach outper-forms all non-survey classiﬁers signiﬁcantly in every mea-
sure except recall, and approaches the survey model.
tion (CV) scheme, where the inner CV is used to tune a new
regularization parameter for each outer CV iteration. Theregularization parameter is found by a two-stage approach:a coarse linearly spaced search is followed by a ﬁner lin-early spaced search around the best value found in the coarsesearch. The tuned regularization parameter is then validatedon the test set of the outer CV loop, which remained unseenas the parameter was tuned. All performance metrics are av-eraged over the outer 10 folds and reported in Table 1.
Our transfer model signiﬁcantly outperforms every model
except the survey model in every measure except recall. No-tably, the transfer model outperforms all combinations offeatures from the source problems, implying that transferlearning was successful in learning novel and useful fea-tures. Remarkably, our transfer model based on remotelysensed data approaches the performance of the survey modelbased on data expensively collected in the ﬁeld. As a sanitycheck, we ﬁnd that using simple traditional computer visionfeatures such as HOG and color histograms only achievesslightly better performance than random guessing. This fur-ther afﬁrms that the transfer learning features are nontrivialand contain information more complex than just edges andcolors.To understand the high recall of the lights model, we
analyze the conditional probability of predicting “poverty”given that the average light intensity is zero: The lightsmodel predicts “poverty” almost 100% of the time, thoughonly 51% of groups with zero average intensity are actu-ally below the poverty line. Furthermore, only 6% of groupswith nonzero average light intensity are below the povertyline, explaining the high recall of the lights model. In con-trast, the transfer model predicts “poverty” in 52% of groupswhere the average nighttime light intensity is 0, more accu-rately reﬂecting the actual probability. The transfer modelfeatures (visualized in Figure 2) clearly contain additional,meaningful information beyond what nighttime lights canprovide. The fact that the transfer model outperforms thelights model indicates that transfer learning has succeeded.
Mapping Poverty Distribution
Using our transfer model, we can scalably and inexpen-sively construct ﬁne-grained poverty maps at the countryor even continent level. We evaluate this capability by es-timating a country-level poverty map for Uganda. We down-load over 370,000 satellite images covering Uganda and esti-mate poverty probabilities at 1km ×1km resolution with the
transfer model. Areas where the model assigns a low proba-bility of being impoverished are colored green, while areasassigned a high risk of poverty are colored red. A 10km ×
10km resolution map is shown in Figure 3 (left), smoothed ata 0.5 degree radius for easy identiﬁcation of dominant spa-tial patterns. Notably, poverty reduction in northern Ugandais lagging (Ministry of Finance 2014). Figure 3 (middle)shows poverty estimates aggregated at the district level. As avalidity check, we qualitatively compare this map against themost recent map of poverty rates available (Figure 3, right),which is based on 2005 survey data (World Resources In-stitute 2009). This data is now a decade old, but it looselycorroborates the major patterns in our predicted distribution.Whereas current maps are coarse and outdated, our methodoffers much ﬁner temporal and spatial resolution and an in-expensive way to evaluate poverty at a global scale.
3934
Conclusion
We introduce a new transfer learning approach for analyz-
ing satellite imagery that leverages recent deep learning ad-vances and multiple data-rich proxy tasks to learn high-levelfeature representations of satellite images. This knowledge
is then transferred to data-poor tasks of interest in the spiritof transfer learning. We demonstrate an application of thisidea in the context of poverty mapping and introduce a fully
convolutional CNN model that, without explicit supervision,learns to identify complex features such as roads, urban ar-eas, and various terrains. Using these features, we are ableto approach the performance of data collected in the ﬁeld forpoverty estimation. Remarkably, our approach outperformsmodels based directly on the data-rich proxies used in ourtransfer learning pipeline. Our approach can easily be gen-
eralized to other remote sensing tasks and has great potentialto help solve global sustainability challenges.
Acknowledgements
We acknowledge the support of the Department of De-fense through the National Defense Science and EngineeringGraduate Fellowship Program. We would also like to thankNVIDIA Corporation for their contribution to this projectthrough an NVIDIA Academic Hardware Grant.
References
Abelson, B.; Varshney, K.; and Sun, J. 2014. Targeting directcash transfers to the extremely poor. In Proceedings of the
20th ACM SIGKDD international conference on Knowledge
discovery and data mining, 1563–1572. ACM.
Bouvrie, J. 2006. Notes on convolutional neural networks.Chatﬁeld, K.; Simonyan, K.; Vedaldi, A.; and Zisserman, A.
2014. Return of the devil in the details: Delving deep intoconvolutional nets. arXiv preprint arXiv:1405.3531.
Donahue, J.; Jia, Y .; Vinyals, O.; Hoffman, J.; Zhang, N.;Tzeng, E.; and Darrell, T. 2013. DeCAF: A deep con-volutional activation feature for generic visual recognition.CoRR abs/1310.1531.
ICF International. 2015. Demographic and health surveys(various) [datasets].
Independent Expert Advisory Group Secretariat. 2014. A
world that counts: Mobilising the data revolution for sus-tainable development. Technical report.
Jia, Y .; Shelhamer, E.; Donahue, J.; Karayev, S.; Long, J.;
Girshick, R. B.; Guadarrama, S.; and Darrell, T. 2014.Caffe: Convolutional architecture for fast feature embed-ding. CoRR abs/1408.5093.
Le, Q. V .; Ranzato, M.; Monga, R.; Devin, M.; Chen, K.;Corrado, G. S.; Dean, J.; and Ng, A. Y . 2012. Buildinghigh-level features using large scale unsupervised learning.InInternational Conference on Machine Learning.
Long, J.; Shelhamer, E.; and Darrell, T. 2014. Fullyconvolutional networks for semantic segmentation. CoRR
abs/1411.4038.
Ministry of Finance. 2014. Poverty status report 2014:
Structural change and poverty reduction in Uganda.Mnih, V ., and Hinton, G. E. 2010. Learning to detect roadsin high-resolution aerial images. In Computer Vision–ECCV
2010. Springer. 210–223.
Mnih, V ., and Hinton, G. E. 2012. Learning to label aerial
images from noisy data. In Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12), 567–574.
Murthy, K.; Shearn, M.; Smiley, B. D.; Chau, A. H.; Levine,
J.; and Robinson, D. 2014. Skysat-1: very high-resolutionimagery from a small satellite. In SPIE Remote Sensing,
92411E–92411E. International Society for Optics and Pho-tonics.
NOAA National Geophysical Data Center. 2014. F18 2013
nighttime lights composite.
Oquab, M.; Bottou, L.; Laptev, I.; and Sivic, J. 2014. Learn-
ing and transferring mid-level image representations usingconvolutional neural networks. In Proceedings of the 2014
IEEE Conference on Computer Vision and Pattern Recogni-tion, CVPR ’14, 1717–1724. Washington, DC, USA: IEEEComputer Society.
Pan, S. J., and Yang, Q. 2010. A survey on transfer learn-
ing. Knowledge and Data Engineering, IEEE Transactions
on22(10):1345–1359.
Razavian, A. S.; Azizpour, H.; Sullivan, J.; and Carlsson, S.2014. CNN features off-the-shelf: an astounding baselinefor recognition. CoRR abs/1403.6382.
Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;Berg, A. C.; and Fei-Fei, L. 2014. ImageNet large scalevisual recognition challenge. International Journal of Com-
puter Vision 1–42.
Uganda Bureau of Statistics. 2012. Uganda national panelsurvey 2011/2012.
United Nations. 2015. The millennium development goals
report 2015.
Varshney, K. R.; Chen, G. H.; Abelson, B.; Nowocin, K.;
Sakhrani, V .; Xu, L.; and Spatocco, B. L. 2015. Targetingvillages for rural development using satellite image analysis.
Big Data 3(1):41–53.
Wolf, R., and Platt, J. C. 1994. Postal address block loca-tion using a convolutional locator network. In Advances in
Neural Information Processing Systems, 745–752. MorganKaufmann Publishers.
World Resources Institute. 2009. Mapping a better fu-
ture: How spatial analysis can beneﬁt wetlands and reduce
poverty in Uganda.
Xie, M.; Jean, N.; Burke, M.; Lobell, D.; and Ermon, S.
2015. Transfer learning from deep features for remote sens-
ing and poverty mapping. CoRR abs/1510.00098.
Zeiler, M. D., and Fergus, R. 2013. Visualizing and under-
standing convolutional networks. CoRR abs/1311.2901.
Zhou, B.; Lapedriza, A.; Xiao, J.; Torralba, A.; and Oliva,
A. 2014. Learning deep features for scene recognition usingPlaces database. In Advances in Neural Information Pro-
cessing Systems, 487–495.
3935
Panoptic Segmentation of Satellite Image Time Series
with Convolutional Temporal Attention Networks
Vivien Sainte Fare Garnot Loic Landrieu
LASTIG, Univ. Gustave Eiffel, ENSG, IGN, F-94160 Saint-Mande, France
{vivien.sainte-fare-garnot, loic.landrieu }@ign.fr
Abstract
Unprecedented access to multi-temporal satellite im-
agery has opened new perspectives for a variety of Earth
observation tasks. Among them, pixel-precise panoptic seg-
mentation of agricultural parcels has major economic and
environmental implications. While researchers have ex-
plored this problem for single images, we argue that the
complex temporal patterns of crop phenology are better ad-
dressed with temporal sequences of images. In this pa-
per, we present the first end-to-end, single-stage method
for panoptic segmentation of Satellite Image Time Series
(SITS). This module can be combined with our novel image
sequence encoding network which relies on temporal self-
attention to extract rich and adaptive multi-scale spatio-
temporal features. We also introduce PASTIS, the first open-
access SITS dataset with panoptic annotations. We demon-
strate the superiority of our encoder for semantic segmen-
tation against multiple competing architectures, and set up
the first state-of-the-art of panoptic segmentation of SITS.
Our implementation and PASTIS are publicly available.
1. Introduction
The precision and availability of Earth observations have
continuously improved thanks to sustained advances in
space-based remote sensing, such as the launch of the Planet
[5] and the open-access Sentinel constellations [8]. In par-
ticular, satellites with high revisit frequency contribute to a
better understanding of phenomena with complex temporal
dynamics. Crop mapping—the driving application of this
paper—relies on exploiting such temporal patterns [37] and
entails major financial and environmental stakes. Indeed,
remote monitoring of the surface and nature of agricultural
parcels is necessary for a fair allocation of agricultural sub-
sidies ( 50and22billion euros per year in Europe and in
the US, respectively) and for ensuring that best crop rota-
tion practices are respected. More generally, the automated
analysis of SITS represents a significant interest for a wide
Figure 1: Overview. We propose an end-to-end, single-
stage model for panoptic segmentation of agricultural
parcels from time series of satellite images. Note the diffi-
culty of resolving the parcels’ borders from a single image,
highlighting the need for modeling temporal dynamics.
range of applications, such as surveying urban development
and deforestation.
The task of monitoring both the content and extent of
agricultural parcels can be framed as the panoptic segmen-
tation of an image sequence. Panoptic segmentation con-
sists of assigning to each pixel a class and a unique instance
label, and has become a standard visual perception task in
computer vision [18, 25]. However, panoptic segmentation
is a fundamentally different task for SITS versus sequences
of natural images or videos. Indeed, understanding videos
requires tracking objects through time and space [43]. In
yearly SITS, the targets are static in a geo-referenced frame,
which removes the need for spatial tracking. Additionally,
SITS share a common temporal frame of reference, which
means that the time of acquisition itself contains informa-
tion useful for modeling the underlying temporal dynamics.
In contrast, the frame number in videos is often arbitrary.
Finally, while objects on the Earth surface generally do not
occlude one another, as is commonly the case for objects in
natural images, varying cloud cover can make the analysis
of SITS arduous. For the specific problem addressed in this
4872

paper, individualizing agricultural parcels requires learning
complex and specific temporal, spatial, and spectral patterns
not commonly encountered in video processing, such as dif-
ferences in plant phenological profiles, subpixel border in-
formation, and swift human interventions such as harvests.
While deep networks have proven efficient for learning
such complex patterns for pixel classification [16, 12, 1],
there is no dedicated approach for detecting individual ob-
jects in SITS. Existing work on instance segmentation has
been restricted to analysing a single satellite image [32]. In
summary, specialized remote sensing methods are limited
to semantic segmentation or single-image instance segmen-
tation, while computer vision’s panoptic-ready networks re-
quire significant adaptation to be applied to SITS.
In this paper, we introduce U-TAE (U-net with Temporal
Attention Encoder), a novel spatio-temporal encoder com-
bining multi-scale spatial convolutions [33] and a tempo-
ral self-attention mechanism [37] which learns to focus on
the most salient acquisitions. While convolutional-recurrent
methods are limited to extracting temporal features at the
highest [34] or lowest [36] spatial resolutions, our proposed
method can use the predicted temporal masks to extract spe-
cialized and adaptive spatio-temporal features at different
resolutions simultaneously. We also propose Parcels-as-
Points (PaPs), the first end-to-end deep learning method for
panoptic segmentation of SITS. Our approach is built upon
the efficient CenterMask network [48], which we modify
to fit our problem. Lastly, we present Panoptic Agricul-
tural Satellite TIme-Series (PASTIS), the first open-access
dataset for training and evaluating panoptic segmentation
models on SITS, with over 2billion annotated pixels cover-
ing over 4000 km2. Evaluated on this dataset, our approach
outperforms all reimplemented competing methods for se-
mantic segmentation, and defines the first state-of-the-art of
SITS panoptic segmentation.
2. Related Work
To the best of our knowledge, no instance or panoptic
segmentation method operating on SITS has been proposed
to date. However, there is a large body of work on both the
encoding of satellite sequences, and the panoptic segmenta-
tion of videos and single satellite images.
Encoding Satellite Image Sequences. While the first au-
tomated tools for SITS analysis relied on traditional ma-
chine learning [13, 45], deep convolutional networks allow
for the extraction of richer spatial descriptors [19, 12, 1, 16].
The temporal dimension was initially dealt via handcrafted
temporal descriptors [2, 42, 51] or probabilistic models
[3], which have been advantageously replaced by recurrent
[34, 37, 27], convolutional [29, 36, 15], or differential [24]
architectures. Recently, attention-based approaches havebeen adapted to encode sequences of remote sensing im-
ages and have led to significant progress for pixel-wise and
parcel-wise classification [38, 35, 53]. In parallel, hybrid
architectures [41, 36, 28] relying on U-Net-type architec-
tures [33] for encoding the spatial dimension and recurrent
networks for the temporal dimension have shown to be well
suited for the semantic segmentation of SITS. In this pa-
per, we propose to combine this hybrid architecture with
the promising temporal attention mechanism.
Instance Segmentation of Satellite Images. The first
step of panoptic segmentation is to delineate all individual
instances, i.e. instance segmentation. Most remote sens-
ing instanciation approaches operate on a single acquisition.
For example, several methods have been proposed to de-
tect individual instances of trees [31, 54], buildings [46], or
fields [32]. Several algorithms start with a delineation step
(border detection) [9, 23, 47], and require postprocessing to
obtain individual instances. Other methods use segmenta-
tion as a preprocessing step and compute cluster-based fea-
tures [6, 7], but do not produce explicit cluster-to-object
mappings. Petitjean et al . [30] propose a segmentation-
aided classification method operating on image time series.
However, their approach partitions each image separately
and does not attempt to retrieve individual objects consis-
tently across the entire sequence. In this paper, we propose
the first end-to-end framework for directly performing joint
semantic and instance segmentation on SITS.
Panoptic Segmentation of Videos. Among the vast lit-
erature on instance segmentation, Mask-RCNN [11] is the
leading method for natural images. Recently, Wang et al.
proposed CenterMask [48], a lighter and more efficient
single-stage method which we use as a starting point in
this paper. Several approaches propose extending instance
or panoptic segmentation methods from image to video
[50, 43, 17]. However, as explained in the introduction,
SITS differs from natural video in several key ways which
require specific algorithmic and architectural adaptations.
3. Method
We consider an image time sequence X, organized into
a four-dimensional tensor of shape T×C×H×W, with T
the length of the sequence, Cthe number of channels, and
H×Wthe spatial extent.
3.1. Spatio-Temporal Encoding
Our model, dubbed U-TAE (U-Net with Temporal At-
tention Encoder), encodes a sequence Xin three steps: (a)
each image in the sequence is embedded simultaneously
and independently by a shared multi-level spatial convolu-
tional encoder, (b)a temporal attention encoder collapses
4873

Conv BlockConv BlockConv BlockConv Block
Temporal Attention Conv 1x1Conv BlockConv Block
Conv 1x1Conv BlockConv 1x1Conv 1x1
(×2)(×4)Blockwise temporal  weighted sumUp-convolutionDown-convolutionFeature map sequenceFeature mapAttention masks(×2)UpsamplingT×C1×H×WC1×H×W
D4×H8×W8D3×H4×W4D2×H2×W2D1×H×W
Attention masksd4
d3
d2
d1T
e3e2e1(×8)
Figure 2: Spatio-temporal Encoding. A sequence of images is processed in parallel by a shared convolutional encoder. At
the lowest resolution, an attention-based temporal encoder produces a set of temporal attention masks for each pixel, which
are then spatially interpolated at all resolutions. These masks are used to collapse the temporal dimension of the feature
map sequences into a single map per resolution. A convolutional decoder then computes features at all resolution levels. All
convolutions operate purely on the spatial and channel dimensions, and we use strided convolutions for both spatial up and
down-sampling. The feature maps are projected in RGB space to help visual interpretation.
the temporal dimension of the resulting sequence of feature
maps into a single map for each level, (c)a spatial convolu-
tional decoder produces a single feature map with the same
resolution as the input images, see Figure 2.
a) Spatial Encoding. We consider a convolutional en-
coderEwithLlevels 1,···, L. Each level is composed of
a sequence of convolutions, Rectified Linear Unit (ReLu)
activations, and normalizations. Except for the first level,
each block starts with a strided convolution, dividing the
resolution of the feature maps by a factor 2.
For each time stamp tsimultaneously, the encoder Elat
level ltakes as input the feature map of the previous level
el−1
t, and outputs a feature map el
tof size Cl×Hl×Wl
withHl=H/2l−1andWl=W/2l−1. The resulting fea-
ture maps are then temporally stacked into a feature map
sequence elof size T×Cl×Hl×Wl:
el= [El(el−1
t)]T
t=0forl∈[1, L], (1)
withe0=Xand[·]the concatenation operator along the
temporal dimension. When constituting batches, we flatten
the temporal and batch dimensions. Since each sequence
comprises images acquired at different times, the batches’
samples are not identically distributed. To address this is-
sue, we use Group Normalization [49] with 4 groups instead
of Batch Normalization [14] in the encoder.b) Temporal Encoding. In order to obtain a single rep-
resentation per sequence, we need to collapse the tempo-
ral dimension of each feature map sequence elbefore using
them as skip connections . Convolutional-recurrent U-Net
networks [41, 36, 28] only process the temporal dimension
of the lowest resolution feature map with a temporal en-
coder. The rest of the skip connections are collapsed with
a simple temporal average. This prevents the extraction of
spatially adaptive and parcel-specific temporal patterns at
higher resolutions. Conversely, processing the highest res-
olution would result in small spatial receptive fields for the
temporal encoder, and an increased memory requirement.
Instead, we propose an attention-based scheme which only
processes the temporal dimension at the lowest feature map
resolution, but is able to utilize the predicted temporal at-
tention masks at all resolutions simultaneously.
Based on its performance and computational efficiency,
we choose the Lightweight-Temporal Attention Encoder
(L-TAE) [10] to handle the temporal dimension. The L-
TAE is a simplified multi-head self-attention network [44]
in which the attention masks are directly applied to the in-
put sequence of vectors instead of predicted values . Addi-
tionally, the L-TAE implements a channel grouping strategy
similar to Group Normalization [49].
We apply a shared L-TAE with Gheads independently
at each pixel of eL, the feature map sequence at the low-
est level resolution L. This generates Gtemporal attention
masks for each pixel, which can be arranged into Gtensors
4874

aL,gwith values in [0,1]and of shape T×HL×WL:
aL,1,···, aL,G=LTAE (eL), applied pixelwise. (2)
In order to use these attention masks at all scale levels lof
the encoder, we compute spatially-interpolated masks al,g
of shape T×Hl×Wlfor all lin[1, L−1]andgin[1, G]
with bilinear interpolation:
al,g=resize aL,gtoHl×Wl. (3)
The interpolated masks al,gat level lof the encoder are
then used as if they were generated by a temporal atten-
tion module operating at this resolution. We apply the
L-TAE channel-grouping strategy at all resolution levels:
the channels of each feature map sequence elare split
intoGcontiguous groups el,1,···, el,Gof identical shape
T×Cl/G×Wl×Hl. For each group g, the feature map se-
quence el,gis averaged on the spatial dimension using al,g
as weights. The resulting maps are concatenated along the
channel dimension, and processed by a shared 1×1con-
volution layer Convl
1×1of width Cl. We denote by flthe
resulting map of size Cl×Wl×Hlby :
fl=Convl
1×1
"TX
t=1al,g
t⊙el,g
t#G
g=1
, (4)
with[·]the concatenation along the channel dimension and
⊙the term-wise multiplication with channel broadcasting.
c) Spatial Decoding. We combine the feature maps fl
learned at the previous step with a convolutional decoder
to obtain spatio-temporal features at all resolutions. The
decoder is composed of L−1blocks Dlfor1≤l < L ,
with convolutions, ReLu activations, and BatchNorms [14].
Each decoder block uses a strided transposed convolution
Dup
lto up-sample the previous feature map. The decoder at
levellproduces a feature map dlof size Dl×Hl×Wl. In a
U-Net fashion, the encoder’s map at level lis concatenated
with the output of the decoder block at level l−1:
dl=Dl([Dup
l(dl+1), fl])forl∈[1, L−1], (5)
withdL=fLand[·]is the channelwise concatenation.
3.2. Panoptic Segmentation
Our goal is to use the multi-scale feature maps {dl}L
l=1
learnt by the spatio-temporal encoder to perform panop-
tic segmentation of a sequence of satellite images over an
area of interest. The first stage of panoptic segmentation
is to produce instance proposals, which are then combined
into a single panoptic instance map. Since an entire se-
quence of images (often over 50) must be encoded to com-
pute{dl}L
l=1, we favor a simple approach for our panop-
tic segmentation module. Furthermore, given the relative
(a) Instance masks
 (b) Target heatmap
(c) Observation from sequence.
 (d) Predicted centerpoints
Figure 3: Centerpoint Detection. The ground truth in-
stance masks (a) is used to construct a target heatmap (b).
Our parcel detection module maps the raw sequence of ob-
servation (c) to a predicted heatmap (d). The predicted cen-
terpoints (red crosses) are the local maxima of the predicted
heatmap (d). The black dots are the true parcels centers.
simplicity of parcels’ borders, we avoid complex region
proposal networks such as Mask-RCNN. Instead, we adapt
the single-stage CenterMask instance segmentation network
[48], and detail our modifications in the following para-
graphs. We name our approach Parcels-as-Points (PaPs) to
highlight our inspiration from CenterNet/Mask [55, 48].
We denote by Pthe set of ground truth parcels in the
image sequence X. Note that the position of these parcels
is time-invariant and hence only defined by their spatial ex-
tent. Each parcel pis associated with (i) a centerpoint ˆ ıp,ˆ ȷp
with integer coordinates, (ii) a bounding box of size ˆhp,ˆwp,
(iii) a binary instance mask ˆsp∈ {0,1}H×W, (iv) a class
ˆkp∈[1, K]withKthe total number of classes.
Centerpoint Detection. Following CenterMask, we per-
form parcel detection by predicting centerness heatmaps
supervized by the ground truth parcels’ bounding boxes. In
the original approach [55], each class has its own heatmap:
detection doubles as classification. This is a sensible choice
for natural images, since the tasks of detecting an object’s
nature, location, and shape are intrinsically related. In our
setting however, the parcels’ shapes and border characteris-
tics are mostly independent of the cultivated crop. For this
4875

ConvConvlocal  maxima(D1+D2+D3+D4)×H×WMLPCenterness  heatmapSaliency
Detected centersShape patchMSelectReshapeMulti-scale descriptorsResized shapeCropped  saliencyPredicted binary maskSizewchcThreshold
Classkc
Conv
S×Shc×wcM×Figure 4: Panoptic Segmentation. The local maxima of the predicted centerness heatmap defines Mtentative parcels. For
each one, the pixel features at all levels are concatenated and used to predict a bounding box size, a semantic class, and
anS×Sshape patch. The latter is combined with a global saliency map for predicting pixel-precise masks. The instance
predictions are combined into a panoptic segmentation using the centerness as quality.
reason, we use a single centerness heatmap and postpone
class identification to a subsequent specialized module. See
Figure 3 for an illustration of our parcel detection method.
We associate each parcel pwith a Gaussian kernel of
deviations σver
pandσhor
ptaken respectively as 1/20of the
height and width of the parcels’ bounding box. Unlike Law
and Deng [20], we use heteroschedastic kernels to reflect
the potential narrowness of parcels. We then define the tar-
get centerness heatmap ˆm∈[0,1]H×Was the maximum
value of all parcel kernels at each pixel (i, j)inH×W:
ˆmi,j= max
p∈Pexp
−(i−ˆ ıp)2
2(σverp)2+(j−ˆ ȷp)2
2(σhorp)2
(6)
A convolutional layer takes the highest-resolution feature
mapd1as input and predicts a centerness heatmap m∈
[0,1]H×W. The predicted heatmap is supervized using the
loss defined in Equation 7 with β= 4:
Lcenter=−1
|P|X
i=1···H
j=1···W(
log(mi,j)ifˆmi,j= 1
(1−ˆmi,j)βlog(1−mi,j)else.(7)
We define the predicted centerpoints as the local maxima
ofm,i.e. pixels with larger values than their 8adjacent
neighbors. This set can be efficiently computed with a sin-
gle max-pooling operation. Replacing the max operator by
argmax in Equation 6 defines a mapping H×W7→P
between pixels and parcels. During training, we associate
each true parcel pwith the predicted centerpoint c(p)with
highest predicted centerness mamong the set of center-
points which coordinates are mapped to p. If this set isempty, then c(p)is undefined: the parcel pis not detected.
We denote by P′the subset of detected parcels, i.e. for
which c(p)is well defined.
Size and Class Prediction. We associate to a predicted
centerpoint cof coordinate (ic, jc)the multi-scale feature
vector ˜dcof size D1+···+DLby concatenating chan-
nelwise the pixel features at location (ic, jc)in all maps dl:
˜dc=
dl 
ic/2l−1
,
jc/2l−1L
l=1, (8)
with [·]the channelwise concatenation. This vector ˜dc
is then processed by four different multilayer perceptrons
(MLP) to obtain three vectors of sizes 2,K, and S2repre-
senting respectively: (i) a bounding box size hc, wc, (ii) a
vector of class probabilities kcof size K, and (iii) a shape
patch scof fixed size S×S. The latter is described in the
next paragraph.
The class prediction kc(p)associated to the true parcel p
is supervized with the cross-entropy loss, and the size pre-
diction with a normalized L1 loss. For all pinP′, we have:
Lp
class=−log(kc(p)[ˆkp]) (9)
Lp
size=|hc(p)−ˆhp|
ˆhp+|wc(p)−ˆwp|
ˆwp. (10)
Shape Prediction. The idea of this step is to combine for
a predicted centerpoint ca rough shape patch scwith a full-
resolution global saliency map zto obtain a pixel-precise
4876

instance mask, see Figure 4. For a centerpoint cof coordi-
nates (ic, jc), the predicted shape patch scof size S×Sis
resized to the predicted size ⌈hc⌉×⌈wc⌉with bilinear inter-
polation. A convolutional layer maps the outermost feature
mapd1to a saliency map zof size H×W, which is shared
by all predicted parcels. This saliency map is then cropped
along the predicted bounding box (ic, jc,⌈hc⌉,⌈wc⌉). The
resized shape and the cropped saliency are added (11) to ob-
tain a first local shape ˜lc, which is then further refined with
a residual convolutional network CNN (12). We denote the
resulting predicted shape by lc:
˜lc=resize c(sc) +cropc(z) (11)
lc=sigmoid (˜lc+CNN (˜lc)), (12)
with resize cand cropcdefined by the coordinates (ic, jc)
and predicted bounding box size (⌈hc⌉,⌈wc⌉). The shape
and saliency predictions are supervized for each parcel pin
P′by computing the pixelwise binary cross-entropy (BCE)
between the predicted shape lc(p)and the corresponding
true binary instance mask ˆspcropped along the predicted
bounding box (ic(p), jc(p),⌈hc(p)⌉,⌈wc(p)⌉):
Lp
shape=BCE(lc(p),cropc(p)(ˆsp)). (13)
For inference, we associate a binary mask with a predicted
centerpoint cby thresholding lcwith the value 0.4.
Loss Function : These four losses are combined into a
single loss with no weight and optimized end-to-end:
L=Lcenter+1
|P′|X
p∈P′
Lp
class+Lp
size+Lp
shape
.(14)
Differences with CenterMask. Our approach differs
from CenterMask in several key ways: (i) We compute a
single saliency map and heatmap instead of Kdifferent
ones. This represents the absence of parcel occlusion and
the similarity of their shapes. (ii) Accounting for the lower
resolution of satellite images, centerpoints are computed at
full resolution to detect potentially small parcels, thus dis-
pensing us from predicting offsets. (iii) The class prediction
is handled centerpoint-wise instead of pixel-wise for effi-
ciency. (iv) Only the selected centerpoints predict shape,
class, and size vectors, saving computation and memory.
(v) We use simple feature concatenation to compute multi-
scale descriptors instead of deep layer aggregation [52] or
stacked Hourglass-Networks [26]. (vi) A convolutional net-
work learns to combine the saliency and the mask instead
of a simple term-wise product.
Converting to Panoptic Segmentation Panoptic seg-
mentation consists of associating to each pixel a semanticlabel and, for non-background pixels (our only stuff class),
an instance label [18]. Our predicted binary instance masks
can have overlaps, which we resolve by associating to each
predicted parcel a quality measure equal to the predicted
centerness mat its associated centerpoint. Masks with
higher quality overtake the pixels of overlapping masks with
lesser predicted quality. If a mask loses more than 50% of
its pixels through this process, it is removed altogether from
the predicted instances. Predicted parcels with a quality un-
der a given threshold are dropped. This threshold can be
tuned on a validation set to maximize the parcel detection
F-score. All pixels not associated with a parcel mask are
labelled as background.
Implementation Details. Our implementation of U-TAE
allows for batch training on sequences of variable length
thanks to a simple padding strategy. The complete con-
figuration and training details can be found in the Ap-
pendix. A Pytorch implementation is available at https:
//github.com/VSainteuf/utae-paps .
4. Experiments
4.1. The PASTIS Dataset
We present PASTIS (Panoptic Agricultural Satellite
TIme Series), the first large-scale, publicly available SITS
dataset with both semantic and panoptic annotations. This
dataset, as well as more information about its composi-
tion, are publicly available at https://github.com/
VSainteuf/pastis-benchmark .
Description. PASTIS is comprised of 2 433 sequences of
multi-spectral images of shape 10×128×128. Each se-
quence contains between 38and61observations taken be-
tween September 2018 and November 2019 , for a total of
over 2 billion pixels. The time between acquisitions is
uneven with a median of 5days. This lack of regularity
is due to the automatic filtering of acquisitions with ex-
tensive cloud cover by the satellite data provider THEIA.
The10channels correspond to the non-atmospheric spectral
bands of the Sentinel-2 satellite, after atmospheric correc-
tion and re-sampling at a spatial resolution of 10meters per
pixel. The dataset spans over 4000 km2, with images taken
from four different regions of France with diverse climates
and crop distributions, covering almost 1%of the French
Metropolitan territory. We estimate that close to 28% of
images have at least partial cloud cover.
Annotation. Each pixel of PASTIS is associated with a
semantic label taken from a nomenclature of 18crop types
plus a background class. As is common in remote sensing
applications, the dataset is highly unbalanced, with a ra-
tio of over 50between the most and least common classes.
4877

(a) Image from the sequence.
 (b) Panoptic annotation.
 (c) Panoptic segmentation.
 (d) Semantic segmentation.
Figure 5: Qualitative results. We consider an image sequence (a) with panoptic annotations (b). We represent the results of
our method in terms of panoptic segmentation (c) and semantic segmentation (d). The parcels’ and pixels’ color corresponds
to the crop type, according to a legend given in the appendix. The predominantly correct class predictions highlight the fact
that the difficulty of panoptic segmentation lies in the precise delineation of each individual parcel. We observe cases where
the temporal structure of the SITS was successfully leveraged to resolve boundary ambiguities that could not be seen from a
single image (cyan circle ). Conversely, some visually fragmented parcels are annotated as a single instance (red circle ).
Each non-background pixel also has a unique instance la-
bel corresponding to its parcel index. In total, 124 422
parcels are individualized, each with their bounding box,
pixel-precise mask, and crop type. All annotations are taken
from the publicly available French Land Parcel Identifica-
tion System. The French Payment Agency estimates the ac-
curacy of the crop annotations via in situ control over 98%
and the relative error in terms of surfaces under 0.3%. To
allow for cross-validation, the dataset is split into 5folds,
chosen with a 1km buffer between images to avoid cross-
fold contamination.
4.2. Semantic Segmentation
Our U-TAE has L= 4 resolution levels and a LTAE
withG= 16 heads, see appendix for an exact configuration.
For the semantic segmentation task, the feature map d1with
highest resolution is set to have Kchannels, with Kthe
number of classes. We can then interpret d1as pixel-wise
predictions to be supervized with the cross-entropy loss. In
this setting, we do not use the PaPs module.
Competing Methods. We reimplemented six of the top-
performing SITS encoders proposed in the literature:
•ConvLSTM [34, 39] and ConvGRU [4]. These ap-
proaches are recurrent neural networks in which all lin-
ear layers are replaced by spatial convolutions.
•U-ConvLSTM [36] and U-BiConvLSTM [22]. To repro-
duce these UNet-Based architectures, we replaced the
L-TAE in our architecture by either a convLSTM [40] or
a bidirectional convLSTM. Skip connections are tempo-
rally averaged. In contrast to the original methods, we
replaced the batch normalization in the encoders withTable 1: Semantic Segmentation. We report for our
method and six competing methods the model size in train-
able parameters, Overall Accuracy (OA), mean Intersection
over Union (mIoU), and Inference Time for one fold of
∼490sequences (IT). The second part of the table report
results from our ablation study.
Model# paramOA mIoU IT (s)×1000
U-TAE (ours) 1 087 83.2 63.1 25.7
3D-Unet [36] 1 554 81.3 58.4 29.5
U-ConvLSTM [36] 1 508 82.1 57.8 28.3
FPN-ConvLSTM [22] 1 261 81.6 57.1 103.6
U-BiConvLSTM [22] 1 434 81.8 55.9 32.7
ConvGRU [4] 1 040 79.8 54.2 49.0
ConvLSTM [34, 39] 1 010 77.9 49.1 49.1
Mean Attention 1 087 82.8 60.1 24.8
Skip Mean + Conv 1 087 82.4 58.9 24.5
Skip Mean 1 074 82.0 58.3 24.5
BatchNorm 1 087 71.9 36.0 22.3
Single Date (August) 1 004 65.6 28.3 1.3
Single Date (May) 1 004 58.1 20.6 1.3
group normalization which significantly improved the
results across-the-board.
•3D-Unet [36]. A U-Net in which the convolutions of the
encoding branch are three-dimensional to handle simul-
taneously the spatial and temporal dimensions.
•FPN-ConvLSTM [22]. This model combines a feature
pyramid network [21] to extract spatial features and a
bidirectional ConvLSTM for the temporal dimension.
4878

Analysis. In Table 1, we detail the performance obtained
with 5-fold cross validation of our approach and the six
reimplemented baselines. We report the Overall Accuracy
(OA) as the ratio between correct and total predictions,
and (mIoU) the class-averaged classification IoU. We ob-
serve that the convolutional-recurrent methods ConvGRU
andConvLSTM perform worse. Recurrent networks em-
bedded in an U-Net or a FPN share similar performance,
with a much longer inference time for FPN. Our approach
significantly outperforms all other methods in terms of pre-
cision. In Figure 5, we present a qualitative illustration of
the semantic segmentation results.
Ablation Study. We first study the impact of using spa-
tially interpolated attention masks to collapse the tempo-
ral dimension of the spatio-temporal feature maps at dif-
ferent levels of the encoder simultaneously. Simply com-
puting the temporal average of skip connections for levels
without temporal encoding as proposed by [41, 36], we ob-
serve a drop of 4.8mIoU points (Skip Mean). This puts our
method performance on par with its competing approaches.
Adding a 1×1convolutional layer after the temporal av-
erage reduces this drop to 4.2points (Skip Mean + Conv).
Lastly, using interpolated masks but foregoing the channel
grouping strategy by averaging the masks group-wise into a
single attention mask per level results in a drop of 3.1points
(Mean Attention). This implies that our network is able to
use the grouping scheme at different resolutions simultane-
ously. In conclusion, the main advantage of our proposed
attention scheme is that the temporal collapse is controlled
at all resolutions, in contrast to recurrent methods.
Using batch normalization in the encoder leads to a se-
vere degradation of the performance of 27.1points (Batch-
Norm). We conclude that the temporal diversity of the
acquisitions requires special considerations. This was ob-
served for all U-Net models alike. We also train our model
on a single acquisition date (with a classic U-Net and no
temporal encoding) for two different cloudless dates in Au-
gust and May (Single Date). We observe a drop of 24.8and
42.5points respectively, highlighting the crucial importance
of the temporal dimension for crop classification. We also
observed that images with at least partial cloud cover re-
ceived on average 58% less attention than their cloud-free
counterparts. This suggests that our model is able to use the
attention module to automatically filter out corrupted data.
4.3. Panoptic Segmentation
We use the same U-TAE configuration for panoptic seg-
mentation, and select a PaPs module with 190k parameters
and a shape patch size of 16×16. In Table 2, we report
the class-averaged Segmentation Quality (SQ), Recognition
Quality (RQ), and Panoptic Quality (PQ) [18]. We observe
that while the network is able to correctly detect and clas-Table 2: Panoptic Segmentation Experiment. We report
class-averaged panoptic metrics: SQ, RQ, PQ.
SQ RQ PQ
U-TAE + PaPs 81.3 49.2 40.4
U-ConvLSTM + Paps 80.9 40.8 33.4
S= 24 81.3 48.5 39.9
S= 8 81.0 48.6 39.8
Multiplicative Saliency 74.5 47.2 35.5
Single-image 72.3 16.9 12.4
sify most parcels, the task remains difficult. In particular,
the combination of ambiguous borders and hard-to-classify
parcel content makes for a challenging panoptic segmenta-
tion problem. We illustrate these difficulties in Figure 5,
along with qualitative results.
Replacing the temporal encoder by a U-BiConvLSTM as
described in Section 4.2 (U-BiConvLSTM+PaPs), we ob-
serve a noticeable performance drop of 8.4RQ, which is
consistent with the results of Table 1. As expected, our
model’s performance is not sensitive to changes in the size
Sof the shape patch. Indeed, the shape patches only de-
termine the rough outline of parcels while the pixel-precise
instance masks are derived from the saliency map. Perform-
ing shape prediction with a simple element-wise multiplica-
tion as in [48] (Multiplicative Saliency) instead of our resid-
ual CNN results in a drop of over −6.8SQ. Using a sin-
gle image (August) leads to a low panoptic quality. Indeed,
identifying crop types and parcel borders from a single im-
age at the resolution of Sentinel-2 is particularly difficult.
Inference on 490sequences takes 129s:26s to generate
U-TAE embeddings, 1s for the heatmap and saliency, 90s
for instance proposals, and 12s to merge them into a panop-
tic segmentation. Note that the training time is also doubled
compared to simple semantic segmentation.
5. Conclusion
We introduced U-TAE, a novel spatio-temporal encoder
using a combination of spatial convolution and temporal at-
tention. This model can be easily combined with PaPs , the
first panoptic segmentation framework operating on SITS.
Lastly, we presented PASTIS, the first large-scale panoptic-
ready SITS dataset. Evaluated on this dataset, our approach
significantly outperformed all other approaches for seman-
tic segmentation, and set up the first state-of-the-art for
panoptic segmentation of satellite image sequences.
We hope that the combination of our open-access dataset
and promising results will encourage both remote sensing
and computer vision communities to consider the challeng-
ing problem of panoptic SITS segmentation, whose eco-
nomic and environmental stakes can not be understated.
4879

References
[1] Nicolas Audebert, Bertrand Le Saux, and S ´ebastien Lef `evre.
Semantic segmentation of earth observation data using mul-
timodal and multi-scale deep networks. In ACCV , 2016.
[2] Adeline Bailly, Simon Malinowski, Romain Tavenard, Laeti-
tia Chapel, and Thomas Guyet. Dense bag-of-temporal-sift-
words for time series classification. In International Work-
shop on Advanced Analysis and Learning on Temporal Data .
Springer, 2015.
[3] Simon Bailly, Sebastien Giordano, Loic Landrieu, and Nes-
rine Chehata. Crop-rotation structured classification using
multi-source Sentinel images and LPIS for crop type map-
ping. In IGARSS , 2018.
[4] Nicolas Ballas, Li Yao, Chris Pal, and Aaron Courville.
Delving deeper into convolutional networks for learning
video representations. ICLR , 2016.
[5] Christopher Boshuizen, James Mason, Pete Klupar, and
Shannon Spanhake. Results from the planet labs flock con-
stellation. AIAA/USU Conference on Small Satellites , 2014.
[6] Alessandro Michele Censi, Dino Ienco, Yawogan Jean Eu-
des Gbodjo, Ruggero Gaetano Pensa, Roberto Interdonato,
and Raffaele Gaetano. Spatial-temporal GraphCNN for land
cover mapping. IEEE Access , 2021.
[7] Dawa Derksen, Jordi Inglada, and Julien Michel. Spatially
precise contextual features based on superpixel neighbor-
hoods for land cover mapping with high resolution satellite
image time series. In IGARSS , 2018.
[8] Matthias Drusch, Umberto Del Bello, S ´ebastien Carlier,
Olivier Colin, Veronica Fernandez, Ferran Gascon, Bianca
Hoersch, Claudia Isola, Paolo Laberinti, Philippe Martimort,
et al. Sentinel-2: Esa’s optical high-resolution mission for
gmes operational services. Remote sensing of Environment ,
2012.
[9] Angel Garcia-Pedrero, Consuelo Gonzalo-Martin, and M
Lillo-Saavedra. A machine learning approach for agricul-
tural parcel delineation through agglomerative segmentation.
International journal of remote sensing , 2017.
[10] Vivien Sainte Fare Garnot and Loic Landrieu. Lightweight
temporal self-attention for classifying satellite images time
series. In International Workshop on Advanced Analytics
and Learning on Temporal Data . Springer, 2020.
[11] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask R-CNN. In ICCV , 2017.
[12] Dino Ienco, Raffaele Gaetano, Claire Dupaquier, and Pierre
Maurel. Land cover classification via multitemporal spatial
data by deep recurrent neural networks. Geoscience and Re-
mote Sensing Letters , 2017.
[13] Jordi Inglada, Marcela Arias, Benjamin Tardy, Olivier
Hagolle, Silvia Valero, David Morin, G ´erard Dedieu,
Guadalupe Sepulcre, Sophie Bontemps, Pierre Defourny,
et al. Assessment of an operational system for crop type
map production using high temporal and spatial resolution
satellite optical imagery. Remote Sensing , 2015.
[14] Sergey Ioffe and Christian Szegedy. Batch normalization:
Accelerating deep network training by reducing internal co-
variate shift. In ICML , 2015.[15] Shunping Ji, Chi Zhang, Anjian Xu, Yun Shi, and Yulin
Duan. 3d convolutional neural networks for crop classifi-
cation with multi-temporal remote sensing images. Remote
Sensing , 2018.
[16] Andreas Kamilaris and Francesc X Prenafeta-Bold ´u. Deep
learning in agriculture: A survey. Computers and electronics
in agriculture , 2018.
[17] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So
Kweon. Video panoptic segmentation. In CVPR , 2020.
[18] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr
Doll´ar. Panoptic feature pyramid networks. In CVPR , 2019.
[19] Nataliia Kussul, Mykola Lavreniuk, Sergii Skakun, and An-
drii Shelestov. Deep learning classification of land cover and
crop types using remote sensing data. Geoscience and Re-
mote Sensing Letters , 2017.
[20] Hei Law and Jia Deng. Cornernet: Detecting objects as
paired keypoints. In ECCV , 2018.
[21] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In CVPR , 2017.
[22] Jorge Andres Chamorro Martinez, Laura Elena Cu ´e
La Rosa, Raul Queiroz Feitosa, Ieda Del’Arco Sanches, and
Patrick Nigri Happ. Fully convolutional recurrent networks
for multidate crop recognition from multitemporal image se-
quences. ISPRS , 2021.
[23] Khairiya Mudrik Masoud, Claudio Persello, and Valentyn A
Tolpekin. Delineation of agricultural field boundaries from
Sentinel-2 images using a novel super-resolution contour de-
tector based on fully convolutional networks. Remote sens-
ing, 2020.
[24] Nando Metzger, Mehmet Ozgur Turkoglu, Stefano
D’Aronco, Jan Dirk Wegner, and Konrad Schindler. Crop
classification under varying cloud cover with neural ordinary
differential equations. arXiv preprint arXiv:2012.02542 ,
2020.
[25] Rohit Mohan and Abhinav Valada. Efficientps: Efficient
panoptic segmentation. International Journal of Computer
Vision , 2021.
[26] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-
glass networks for human pose estimation. In ECCV , 2016.
[27] Mehmet Ozgur Turkoglu, Stefano D’Aronco, Gregor Perich,
Frank Liebisch, Constantin Streit, Konrad Schindler, and
Jan Dirk Wegner. Crop mapping from image time se-
ries: deep learning with multi-scale label hierarchies. arXiv
preprint arXiv:2102.08820 , 2021.
[28] Maria Papadomanolaki, Maria Vakalopoulou, and Konstanti-
nos Karantzalos. A deep multi-task learning framework cou-
pling semantic segmentation and fully convolutional LSTM
networks for urban change detection. Transactions on Geo-
science and Remote Sensing , 2021.
[29] Charlotte Pelletier, Geoffrey I Webb, and Franc ¸ois Petitjean.
Temporal convolutional neural network for the classification
of satellite image time series. Remote Sensing , 2019.
[30] Franc ¸ois Petitjean, Camille Kurtz, Nicolas Passat, and Pierre
Ganc ¸arski. Spatio-temporal reasoning for the classification
of satellite image time series. Pattern Recognition Letters ,
2012.
4880

[31] Yuchu Qin, Antonio Ferraz, Cl ´ement Mallet, and Corina Io-
van. Individual tree segmentation over large areas using air-
borne lidar point cloud and very high resolution optical im-
agery. In IGARSS , 2014.
[32] Christoph Rieke. Deep learning for instance segmentation of
agricultural fields. https://github.com/chrieke/
InstanceSegmentation_Sentinel2 , 2017.
[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
InMICCAI , 2015.
[34] Marc Rußwurm and Marco K ¨orner. Convolutional LSTMs
for cloud-robust segmentation of remote sensing imagery.
NeurIPS Workshops , 2018.
[35] Marc Rußwurm and Marco K ¨orner. Self-attention for raw
optical satellite time series classification. ISPRS , 2020.
[36] Rose Rustowicz, Robin Cheong, Lijing Wang, Stefano Er-
mon, Marshall Burke, and David Lobell. Semantic segmen-
tation of crop type in africa: A novel dataset and analysis of
deep learning methods. In CVPR Workshops , 2019.
[37] Vivien Sainte Fare Garnot, Loic Landrieu, Sebastien Gior-
dano, and Nesrine Chehata. Time-space tradeoff in deep
learning models for crop classification on satellite multi-
spectral image time series. In IGARSS , 2019.
[38] Vivien Sainte Fare Garnot, Loic Landrieu, Sebastien Gior-
dano, and Nesrine Chehata. Satellite image time series clas-
sification with pixel-set encoders and temporal self-attention.
InCVPR , 2020.
[39] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM
network: A machine learning approach for precipitation
nowcasting. In NeurIPS , 2015.
[40] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung,
Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM
network: A machine learning approach for precipitation
nowcasting. arXiv preprint arXiv:1506.04214 , 2015.
[41] Andrei Stoian, Vincent Poulain, Jordi Inglada, Victor
Poughon, and Dawa Derksen. Land cover maps production
with high resolution satellite image time series and convo-
lutional neural networks: Adaptations and limits for opera-
tional systems. Remote Sensing , 2019.
[42] Romain Tavenard, Simon Malinowski, Laetitia Chapel, Ade-
line Bailly, Heider Sanchez, and Benjamin Bustos. Efficient
temporal kernels between feature sets for time series classi-
fication. In ECML-KDD . Springer, 2017.[43] Pavel Tokmakov, Cordelia Schmid, and Karteek Alahari.
Learning to segment moving objects. International Journal
of Computer Vision , 2019.
[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. NeurIPS , 2017.
[45] Francesco Vuolo, Martin Neuwirth, Markus Immitzer,
Clement Atzberger, and Wai-Tim Ng. How much does
multi-temporal Sentinel-2 data improve crop type classifica-
tion? International journal of applied earth observation and
geoinformation , 2018.
[46] Fabien H Wagner, Ricardo Dalagnol, Yuliya Tarabalka, Tas-
siana YF Segantine, Rog ´erio Thom ´e, and Mayumi Hirye. U-
net-id, an instance segmentation model for building extrac-
tion from satellite images—case study in the joan ´opolis city,
brazil. Remote Sensing , 2020.
[47] Franc ¸ois Waldner and Foivos I Diakogiannis. Deep learning
on edge: extracting field boundaries from satellite images
with a convolutional neural network. Remote Sensing of En-
vironment , 2020.
[48] Yuqing Wang, Zhaoliang Xu, Hao Shen, Baoshan Cheng,
and Lirong Yang. Centermask: single shot instance segmen-
tation with point representation. In CVPR , 2020.
[49] Yuxin Wu and Kaiming He. Group normalization. In ECCV ,
2018.
[50] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance seg-
mentation. In CVPR , 2019.
[51] Lexiang Ye and Eamonn Keogh. Time series shapelets: a
new primitive for data mining. In ACM SIGKDD , 2009.
[52] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Dar-
rell. Deep layer aggregation. In CVPR , 2018.
[53] Yuan Yuan and Lei Lin. Self-supervised pre-training of
transformers for satellite image time series classification.
Journal of Selected Topics in Applied Earth Observations
and Remote Sensing , 2020.
[54] Tiebiao Zhao, Haoyu Niu, Erick de la Rosa, David Doll,
Dong Wang, and YangQuan Chen. Tree canopy differentia-
tion using instance-aware semantic segmentation. In ASABE
Annual International Meeting , 2018.
[55] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb ¨uhl. Ob-
jects as points. arXiv preprint arXiv:1904.07850 , 2019.
4881

Model Evaluation for Geospatial Problems∗
Jing Wang1, Tyler A. Hallman2, Laurel M. Hopkins3, John B. Kilbride4, W. Douglas Robinson5, and
Rebecca A. Hutchinson6
1,3,4,5,6Oregon State University, Corvallis, OR, USA
2Bangor University, Bangor, Gwynedd, UK
Email ids:1,3,5,6{wangji9, hopkilau, douglas.robinson, rah}@oregonstate.edu,
2t.hallman@bangor.ac.uk,4john.b.kilbride@gmail.com
Abstract
Geospatial problems often involve spatial autocorrelation and covariate shift, which
violate the independent, identically distributed assumption underlying standard
cross-validation. In this work, we establish a theoretical criterion for unbiased cross-
validation, introduce a preliminary categorization framework to guide practitioners
in choosing suitable cross-validation strategies for geospatial problems, reconcile
conflicting recommendations on best practices, and develop a novel, straightforward
method with both theoretical guarantees and empirical success.
1 Introduction
Cross-validation (CV) estimates how reliable and accurate a model’s predictions are on new, unseen
data. Standard CV provides unbiased model error estimates for independent, identically distributed
(iid) data [ 2]. However, it can substantially underestimate model errors for geospatial problems due to
the inherent spatial autocorrelation and the frequent presence of covariate shift. For example, natural
resource managers may use environmental features (e.g., soil characteristics, canopy cover, rainfall)
from a region that a threatened species currently occupies to learn a species distribution model (SDM)
describing the species-habitat relationship. Conservation actions like translocating individuals of the
species to a new area may require the SDM to be applied outside the region where it was fit, raising a
key question: How good are the model predictions in this new area? If spatial autocorrelation within
the training region permits information leakage between training and validation folds, CV estimates
computed within the training region may be optimistically biased. Furthermore, the distribution of
the features in the occupied region may differ from those in the new area, a phenomenon referred
to as covariate shift. Standard CV fails to account for the challenges of spatial autocorrelation and
covariate shift. This paper addresses the question of how to evaluate models for geospatial problems
from both theoretical and empirical perspectives.
2 Background
Geospatial Problems Consider a dataset Twconsisting of a training set Ttrand a test set Tte:
Tw={Ttr, Tte}={{Xtr,ytr},{Xte,yte}}={{xi, yi}ntr
i=1,{xj, yj}nte
j=1}, where X,xare fea-
tures (all or some are spatial variables); y, yare the response variables; i and j subscripts respectively
denote training and test samples. A spatial random variable is a stochastic process Z:D×Ω→R,
whereD⊂Rdis a region (typically, d= 2or 3), and Ωis a sample space. When we observe a spatial
variable’s value, it creates a realization of the process by fixing ω∈Ω.Spatial autocorrelation
(SAC) refers to the degree of spatial dependence (SD) between feature values measured at locations.
∗This submission is an extended abstract of Wang et al. (2023) [1].
Computational Sustainability Workshop at NeurIPS 2023.
TtrandTteare collected from a training region Dtrand a test region Dte, respectively. Covariate
shift (CS) is defined by PXtr̸=PXtewhile Pytr|Xtr=Pyte|Xte, where Pdenotes distribution [ 3].
It is likely to happen when DtrandDtediffer. In this paper, we focus on geospatial problems with
the common traits: 1) Models are learned and applied at geolocated data points. 2) Geolocation
information is available for each data point but not explicitly used in the model, i.e., XteandXtrdo
not include geocoordinates. 3) TtrandXteare known or estimable, while yteis unknown.
Model Error Test error and risk are two common quantities for model error [ 4].Test error (ErrT)
is the expected loss over test samples, given a fixed training set T. (Tis short for Ttrwhen different
predictive goals are not emphasized.) Risk (R) is the expected test error over training sets from the
same population. The key distinction is that risk considers expectations over training sets while test
error conditions on a single training set. CV is occasionally mistaken for estimating test error, but it
truly estimates the risk under the iid assumption [5].
Cross-validation Methods Standard CV approaches split data points into folds uniformly at
random. As the most widely used variant, K-Fold Cross-Validation (KFCV) divides a training set
randomly into K non-overlapping folds, iteratively holds out one fold as the validation fold , and
trains a model on the remaining training folds (e.g., Fig. 1(a)). Leave-One-Out Cross-Validation
(LOOCV) is an extreme case, where it uses n-fold CV with a single data point as the validation fold,
while the training folds contain the rest of the data. For iid data, these standard CV estimators are
unbiased; the random resampling mimics how a new sample would be drawn from the population.
Additional CV techniques have been developed for non-iid data. Spatial CV strategies handle data
dependencies by spatially segregating training and validation folds, and they can be broadly classified
into two categories. First, BLock Cross-Validation (BLCV) groups geographically close points
into blocks [ 6,7]. BLCV reduces much spatial dependence across folds as most nearby points end
up in the same fold (e.g., Fig. 1(b)). Second, BuFfered Cross-Validation (BFCV) inserts a buffer
between training and validation folds and excludes points within it [ 8,9]. This procedure removes
spatial dependence between folds at the cost of losing training samples located within the buffer
region (e.g., Fig. 1(c)). One concern with spatial CV methods is the possibility of introducing
covariate shift between folds. Spatially separated folds may have differing feature spaces, possibly
resulting in pessimistically biased error estimates. Importance-Weighted Cross-Validation (IWCV)
does not assume identical distributions between training and test sets but was developed in non-spatial
contexts [ 10,11]. It provides an asymptotically unbiased estimator by adjusting the loss function
with the ratio of test and training probability densities. Fig. 1 illustrates how these CV methods assign
training samples to folds.
0 20 40 60
Longitude0204060Latitude
(a) KFCV , IWCV
0 12 24 36 48 60
Longitude01224364860Latitude1 2 3 4 5
6 7 8 9 1
2 3 4 5 6
7 8 9 1 2
3 4 5 6 7 (b) BLCV (bl = 12)
0 20 40 60
Longitude0204060Latitude1 2 3
4 5 6
7 8 9 (c) BFCV (bl = 20, bf = 12)
Figure 1: Visualization of various CV methods dividing 1800 training data points on a 60×60
landscape into 9 folds. (a) Different colors represent different folds for KFCV or IWCV . (b) An
illustration of BLCV with a block size (bl) of 12 grid cells, assigning each block to one of the 9 folds.
(c) An example of BFCV using a block size (bl) of 20 grid cells and a buffer size (bf) of 12 grid cells.
It highlights fold 5 as the validation fold, with training samples in its adjacent buffer region excluded.
2
3 Methodology
A Criterion for Unbiased CV Rabinowicz and Rosset (2020) prove that when training and test
features are iid and a latent variable induces correlation structure in the response variable, the CV
risk estimate (under squared error loss) is unbiased when the joint distribution of the test and training
sets matches that of the validation and training folds, for all folds. To extend this result, we consider
autocorrelation as the mechanism creating dependence, with a general loss function. We establish a
criterion for unbiasedness in Thm. 1, where Kis the number of CV folds, and the subscripts k,−k
denote the validation fold and the training folds, respectively. (See proof in Appx.)
Theorem 1. IfPXte|Xtr=PXk|X−k,∀k∈1, . . . , K , then cross-validation is an asymptotically
unbiased estimator of the risk R(n).
Framework for Selecting a CV Method For the purpose of exploring the relationships between
problem characteristics and CV strategies, we introduce a categorization framework determined by
two dimensions (Tab. 1). We assess spatial (in)dependence by the maximal semivariogram range of
all features, which refers to the distance at which semivariance reaches its maximum value, indicating
that observations at this distance or farther are spatially independent. We characterize covariate shift
between training and test features by the Cramér-von Mises two-sample test (Cramér test) [ 12], a
multivariate, distribution-free test with null hypothesis that the samples are identically distributed.
Table 1: Geospatial scenarios determined by semivariogram range and the Cramér test. We consider
spatial (in)dependence between training and testing sets by comparing the nearest distance ( d)
between training and test samples with the semivariogram range ( r) of the training features; and
covariate shift by comparing the p-value ( p) of Cramér test and the user-defined significance level α.
Spatial Dependence Spatial Independence
No Covariate ShiftScenario SD:
d < r andp≥αScenario SI:
d≥randp≥α
Covariate ShiftScenario SD + CS:
d < r andp < αScenario SI + CS:
d≥randp < α
A New CV Method For Scenario SI + CS (Tab. 1) which requires extrapolation to a new, spatially
independent test region, none of the existing CV methods are quite appropriate. KFCV , BLCV , and
BFCV are ineffectively in dealing with covariate shift, while IWCV is not tailored for autocorrelated
data. Therefore, we propose Importance-weighted Buffered Cross-Validation (IBCV), combining the
strengths of BFCV and IWCV . IBCV separates training and validation folds with a buffer region (as
Fig. 1c) and employs density ratio weighting to correct covariate shift. The K-fold IBCV estimator is
ˆR(n)
KIBCV ≡1
KPK
k=11
nkP
i∈kthpte(xi)
ptr(xi)L(yi,ˆyi(xi;T−k−bf)),
where bfis the buffer region, T−k−bfis the training fold, the subscript (n)denotes the size of
training set,pte(xi)
ptr(xi)is the density ratio of a validation sample Ti, andLcan be any loss function. We
claim that IBCV is asymptotically unbiased. (See proof and Algo. 1 in Appx.)
Proposition 2. IBCV is asymptotically unbiased: ET[ˆR(n)
IBCV ] =R(n)when n→ ∞ .
Prospective users of IBCV should weigh a few caveats. First, IBCV may not be well-suited for small
datasets. Eliminating buffer points would have a stronger impact on smaller datasets, potentially
resulting in a pessimistic bias and high variance. Second, IBCV should be expected to struggle with
severe covariate shift just as other importance-weighed methods do. Such methods perform poorly
when the support of PXtehas little overlap with the support of PXtr.
4 Experiments
Simulation Experiments We generated data with the model y=x(1)+x(2)+x(1)·x(2)+ϵ,
where x(1)andx(2)are two features with varying degrees of spatial autocorrelation and ϵis an iid
normal error term. We generated 100 simulations for each scenario, with each simulation comprising
3
Table 2: Simulations: average biases of 9-fold CV estimates of RMSE with respect to risk, across
varying spatial autocorrelation ranges (r). Bias is calculated as the mean CV estimate minus the risk
except for Scenario SD + CS, where bias is calculated as the average of absolute differences between
CV estimates and test errors. The smallest biases in each row of the same scenario block are in bold,
and the best CV methods are summarized alongside the scenario names.
KFCV BLCV BFCV IWCV IBCV KFCV BLCV BFCV IWCV IBCV
r Scenario SD: KFCV Scenario SI: BLCV
4 0.0103 0.0174 0.0280 -0.0517 -0.0354 0.0023 0.0094 0.0200 -0.0611 -0.0449
8-0.0024 0.0030 0.0691 -0.0605 0.0062 -0.0299 0.0046 0.0388 -0.1106 -0.0482
12 -0.0010 0.0778 0.1602 -0.0550 0.0956 -0.0438 0.0347 0.1123 -0.1778 -0.0477
Scenario SD + CS: BFCV Scenario SI + CS: IBCV
4 0.1043 0.1068 0.0973 0.1651 0.1453 0.4883 0.4954 0.5060 -0.0018 0.0059
8 0.1938 0.1861 0.1853 0.2524 0.1872 0.4603 0.4955 0.5320 -0.0652 -0.0347
12 0.2472 0.2492 0.2583 0.2472 0.1870 0.4429 0.5217 0.6041 -0.0646 0.0070
1800 training points and 500 test points. We fitted linear models without the interaction term to mimic
the common case of model misspecification.
We measured bias in the CV estimates of RMSE across all scenarios and varying degrees of spatial
autocorrelation (Tab. 2). In Scenario SD, KFCV is the least biased. Its internal random partitioning
mechanism intersperses training points among validation points, aligning with the spatial structure
between training and test sets in this case. Instead, BLCV and BFCV always produce pessimistic
bias via introduced covariate shift, especially for strongly autocorrelated data (when r= 12 ). In
Scenario SI, BLCV is advantageous. BLCV retains some spatial dependence across fold boundaries,
potentially causing error underestimation. However, it can also introduce covariate shift across folds,
which could lead to error overestimation. When the two offset each other, as may be controlled with
block size, BLCV can achieve an accurate error estimate. In Scenario SD + CS, BFCV seems the
best choice though it displays the highest variability among all methods (Appx. Fig. 3). In Scenario
SI + CS, IBCV and IWCV , explicitly addressing covariate shift, outperform others significantly. The
gap between IBCV and IWCV widens gradually with SAC range. IBCV is 46.78% less biased than
IWCV when r= 8, and 89.16% less biased when r= 12 .
Example Application We predicted whether a Hermit Warbler (HEWA), one of the most prevalent
species in the Oregon 2020 dataset [ 13], was observed or not at a certain survey site based on
the surrounding habitat, as represented by four vegetation indexes computed from remote sensing
data [ 14,15]. 1000 training points and 500 test points were randomly sampled from different regions
of Oregon to create datasets for each of the four scenarios of Tab. 1 (Fig. 2). For these real datasets,
we could not directly estimate risk due to having only one landscape per analysis. Instead, we used
test error as a proxy and assessed the five machine learning models - Ridge classifier (Ridge), Linear
SVM (LSVM), K-Nearest Neighbors (KNN), Random Forest (RF), and Naive Bayes (NB) - based on
classification error rate, i.e., the proportion of misclassified samples in the test set.
Figure 2: HEWA dataset: sampling strategies of training (blue) and test (orange) points in four
scenarios. Subplots (a), (c) and (d) share the same training set. We fitted Matérn variogram functions
with the lag class estimated by Scott’s rule [ 16], and obtained the maximal range of the features
wasr= 0.28degree for (a), (c), (d), while for (b) it was r= 0.33degree. The nearest distances
between training and test samples were d= 0.00,0.36,0.00,0.71degrees for (a), (b), (c), and (d),
respectively. The p-values from the Cramér test were p= 0.71,0.04,0.00,0.00for (a), (b), (c) and
(d), respectively. We set α= 0.01. Therefore, the classification of the datasets align with Tab. 1.
4
From Tab. 3, the recommended CV methods for the four scenarios are almost consistent with those in
the simulations. The only difference is that in Scenario SD, IWCV outperforms KFCV , though their
estimates are quite close. Except for Scenario SD, KFCV usually severely underestimates model
errors because when nearby points end up in different CV folds, spatial autocorrelation in features
can transmit information across fold boundaries, leading to optimistic model error estimates. In
Tab. 3, we set hyperparameters (block size and buffer size) to the maximum semivariogram range of
all features. We also fine-tuned the hyperparameters and did a peak-to-peak comparison; in this case,
the best CV method for each scenario remained the same (Appx. Tab. 4). That said, hyperparameters
significantly influence CV estimates, and finding the best ones in practice can be challenging.
Table 3: HEWA: model classification error rates and 9-fold CV estimates thereof. The best estimates
of test error (target) in each column of the same scenario block are in bold. The best CV methods
which produce the closest estimates for most models are summarized alongside the scenario names.
Classifier Ridge LSVM KNN RF NB Ridge LSVM KNN RF NB
Scenario SD: IWCV Scenario SI: BLCV
Test error 0.1700 0.1720 0.1740 0.1740 0.1700 0.2320 0.2280 0.2440 0.2520 0.2440
KFCV 0.1709 0.1709 0.1779 0.1910 0.1729 0.1890 0.1900 0.2180 0.2120 0.1950
IWCV 0.1706 0.1706 0.1777 0.1907 0.1727 0.1888 0.1898 0.2178 0.2118 0.1948
BLCV 0.1664 0.1678 0.1905 0.1964 0.1663 0.2007 0.2047 0.2292 0.2607 0.2043
BFCV 0.1783 0.1775 0.1909 0.1989 0.1678 0.2173 0.2876 0.2628 0.2692 0.2110
IBCV 0.1780 0.1773 0.1906 0.1986 0.1676 0.2170 0.2872 0.2625 0.2689 0.2107
Scenario SD + CS: BFCV Scenario SI + CS: IBCV
Test error 0.2140 0.2040 0.2080 0.1840 0.2160 0.2420 0.2540 0.2440 0.2540 0.2640
KFCV 0.1709 0.1709 0.1779 0.1910 0.1729 0.1709 0.1709 0.1779 0.1910 0.1729
IWCV 0.2400 0.2430 0.2533 0.2706 0.2469 0.2239 0.2245 0.2370 0.2507 0.2289
BLCV 0.1644 0.1678 0.1905 0.1964 0.1663 0.1644 0.1678 0.1905 0.1964 0.1663
BFCV 0.1783 0.1775 0.1909 0.1989 0.1678 0.1783 0.1775 0.1909 0.1989 0.1678
IBCV 0.2526 0.2489 0.2649 0.2789 0.2393 0.2370 0.2334 0.2484 0.2572 0.2222
5 Conclusion
Recent studies have come to mixed conclusions on the best practice in evaluating models for
geospatial problems, and our analysis yields points of both agreement and disagreement with the
ongoing discussion. For example, Roberts et al. (2017) and Valavi et al. (2019) argue that spatial CV
is less biased than non-spatial CV [ 6,7]. Ploton et al. (2020) advocate for the adoption of spatial CV
as the norm for spatially autocorrelated data [17]. Our results do provide evidence for spatial CV in
some scenarios. However, Hoffimann et al. (2021) and Wadoux et al. (2021) show that spatial CV
can yield notably pessimistic estimates [ 18,19]. Our study corroborates this perspective, particularly
in Scenario SD. Our agreements with these conflicting studies simply highlights our main message:
the best evaluation strategy for a geospatial problem depends on how the training set relates to the
intended test set; specifically, the spatial and distributional relationships between features across CV
folds should match those between the training and testing features.
We see several directions for future work. The framework outlined in Tab. 1 served this study
adequately, but to offer more precise recommendations to practitioners, we need more nuanced tools
for characterizing geospatial problems, going beyond the current four discrete quadrants. While
IBCV shows promise, further developments to aid hyperparameter selection, and improvements to
the method itself, may be fruitful. Finally, we hope this paper not only serves practitioners interested
in assessing models with geospatial data but also triggers the exploration of thoughtful ways for
evaluating performance on other non-iid datasets.
6 Acknowledgements
We thank Tom Dietterich for comments on an early version of the manuscript, and four anonymous
reviewers for comments that improved the paper. This research was supported in part by the National
Science Foundation (NSF) under Grant No. III-2046678 (JW, RAH), the United States Department
of Agriculture National Institute of Food and Agriculture (USDA-NIFA) award No. 2021-67021-
35344 (AgAID AI Institute; JW, RAH), the National Aeronautics and Space Administration (NASA)
5
under Future Investigators in NASA Earth and Space Science and Technology (FINESST) Grant No.
80NSSC20K1664 (LMH), and the Bob and Phyllis Mace professorship (WDR).
References
[1]Jing Wang, Laurel Hopkins, Tyler Hallman, W. Douglas Robinson, and Rebecca Hutchinson.
Cross-validation for geospatial data: Estimating generalization performance in geostatistical
problems. Transactions on Machine Learning Research , 2023.
[2]Sylvain Arlot and Alain Celisse. A survey of cross-validation procedures for model selection.
Statistics surveys , 4:40–79, 2010.
[3]Jose Garcia Moreno-Torres, Troy Raeder, Rocío Alaíz-Rodríguez, N. Chawla, and Francisco
Herrera. A unifying view on dataset shift in classification. Pattern Recognit. , 45:521–530, 2012.
[4]Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning:
Data mining, inference, and prediction . Springer, 2001.
[5]Stephen Bates, Trevor Hastie, and Robert Tibshirani. Cross-validation: what does it estimate
and how well does it do it? Journal of the American Statistical Association , pages 1–22, 2023.
[6]David R Roberts, V olker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith, Gurutzeta Guillera-
Arroita, Severin Hauenstein, José J. Lahoz-Monfort, Boris Schröder, Wilfried Thuiller, David I.
Warton, Brendan A. Wintle, Florian Hartig, and Carsten F. Dormann. Cross-validation strategies
for data with temporal, spatial, hierarchical, or phylogenetic structure. Ecography , 40(8):913–
929, 2017.
[7]Roozbeh Valavi, Jane Elith, José J. Lahoz-Monfort, and Gurutzeta Guillera-Arroita. BLOCKCV:
An R package for generating spatially or environmentally separated folds for k-fold cross-
validation of species distribution models. Methods in Ecology and Evolution , 10(2):225–232,
2018.
[8]Kévin Le Rest, David Pinaud, Pascal Monestiez, Joël Chadoeuf, and Vincent Bretagnolle. Spatial
leave-one-out cross-validation for variable selection in the presence of spatial autocorrelation.
Global ecology and biogeography , 23(7):811–820, 2014.
[9]Jonne Pohjankukka, Tapio Pahikkala, Paavo Nevalainen, and Jukka Heikkonen. Estimating
the prediction performance of spatial models via spatial k-fold cross validation. International
Journal of Geographical Information Science , 31(10):2001–2019, 2017.
[10] Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Müller. Covariate shift adaptation by
importance weighted cross validation. Journal of Machine Learning Research , 8:985–1005,
May 2007.
[11] Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Masashi Sugiyama.
Relative density-ratio estimation for robust distribution comparison. Neural Computation ,
25:1324–1370, 2011.
[12] Theodore W Anderson. On the distribution of the two-sample cramer-von mises criterion. The
Annals of Mathematical Statistics , 33(3):1148–1159, 1962.
[13] William D. Robinson, Tyler A. Hallman, and Jenna R. Curtis. Benchmarking the avian diversity
of oregon in an era of rapid change. Northwestern Naturalist , 101:180 – 193, 2020.
[14] Eric P. Crist and Richard C. Cicone. A physically-based transformation of thematic mapper data—
the tm tasseled cap. IEEE Transactions on Geoscience and Remote Sensing , GE-22(3):256–263,
1984.
[15] Laurel M Hopkins, Tyler A Hallman, John Kilbride, W Douglas Robinson, and Rebecca A
Hutchinson. A comparison of remotely sensed environmental predictors for avian distributions.
Landscape Ecology , 37(4):997–1016, 2022.
[16] Mirko Mälicke. Scikit-gstat 1.0: A scipy flavoured geostatistical variogram estimation toolbox
written in python. Geoscientific Model Development Discussions , pages 1–43, 2021.
6
[17] Pierre Ploton, Frédéric Mortier, Maxime Réjou-Méchain, Nicolas Barbier, Nicolas Picard,
Vivien Rossi, Carsten Dormann, Guillaume Cornu, Gaëlle Viennois, Nicolas Bayol, Alexei Lya-
pustin, Sylvie Gourlet-Fleury, and Pélissier Raphaël. Spatial validation reveals poor predictive
performance of large-scale ecological mapping models. Nature communications , 11, 2020.
[18] Júlio Hoffimann, Maciel Zortea, Breno De Carvalho, and Bianca Zadrozny. Geostatistical
learning: Challenges and opportunities. Frontiers in Applied Mathematics and Statistics , 7,
2021.
[19] Alexandre M.J.-C. Wadoux, Gerard B. M. Heuvelink, Sytze de Bruin, and Dick J. Brus.
Spatial cross-validation is not the right way to evaluate map accuracy. Ecological Modelling ,
457:109692, 2021.
7
Appendix
Proof of Theorem 1 With the geospatial settings defined in Section 2, we further assume the
training and test features have the same domain: xi,xj∈X⊂Rm, and the function f:x→y
is unchanged between training and test sets (i.e., no concept shift). Therefore, the domains of the
response variables are also the same: yi, yj∈Y⊂R. The subscripts k,−kdenote the validation
and training folds, respectively.
Proof. Since p(Xte|Xtr) =p(Xk|X−k), we have
p(Xte, Xtr)/p(Xtr) =p(Xk, X−k)/p(X−k). (1)
We multiply the LHS by p(yte|Xte)and the RHS by p(yk|Xk). Since fis assumed constant, these
quantities are equal, i.e., p(yte|Xte) =p(ytr|Xtr) =p(yk|Xk).
We focus first on the LHS. Since yis conditionally independent of all other variables given its
corresponding X, we can condition on additional variables, we have p(yte|Xte) =p(yte|Xte, Xtr)),
andp(ytr|Xtr) =p(ytr|Xtr, Xte, yte). Therefore, the condensed process is:
p(Xte, Xtr)·p(yte|Xte)
p(Xtr)
=p(Xte, Xtr)·p(yte|Xte, Xtr)·p(ytr|Xtr, Xte,yte)
p(Xtr)·p(ytr|Xtr)
=p(yk, Xk,y−k, X−k)
p(y−k, X−k)
=p(Tte|Ttr).
Similarly, the result of multiplying RHS of Eqn. 1 with p(yk|Xk)isp(Tk|T−k). Note that Tis short
forTtrwhen not emphasizing different predictive goals. Combining the manipulated LHS and RHS,
we can conclude that
p(Tte|Ttr) =p(Tk|T−k). (2)
Now we are ready to show unbiasedness for the leave-one-out (LOO) setting. For this setting,
Eqn. 2 is written as p(Tj|T) =p(Ti|T−i), recalling that Tjis a single intended test instance outside
of the full training set T(containing ntraining samples), Tiis a single-instance validation fold,
andT−iis the training fold (i.e., excluding Tifrom T). As is typical, we assume that T−iis
distributed as Tand of size n, ignoring the bias from the different sizes of T−iandT; this gives
p(Tj|T) =p(Tj|T−i) =p(Ti|T−i), which is needed for step (1) below. We use shorthand Lifor
L(yi,ˆyi(xi;T−i))andLjforL(yj,ˆyj(xj;T−i)). The error estimate of standard LOOCV is
ˆR(n)
LOOCV ≡1
nPn
i=1L(yi,ˆyi(xi;T−i)).
So we have
ET[ˆR(n)
LOOCV ] =1
nPn
i=1ET−i,Ti[Li]
=1
nPn
i=1ET−iZ
YZ
Xp(xi, yi|T−i)Lidxidyi
(1)=1
nPn
i=1ET−iZ
YZ
Xp(xj, yj|T−i)Ljdxjdyj
=1
nPn
i=1ET−i,Tj[Lj]
=1
nPn
i=1R(n−1)n→∞−→ ≈ R(n)
All of these claims also hold for KFCV , with more bookkeeping required to account for varying fold
sizes.
8
Proof of Proposition 2 With the same settings and assumptions of Theorem 1, we show that
IBCV is asymptotically unbiased.
Proof. We demonstrate the claim for LOOIBCV; it also valid for KIBCV with more bookkeeping for
the folds. The leave-one-out IBCV estimator is
ˆR(n)
LOOIBCV ≡1
nPn
i=1pte(xi)
ptr(xi)L(yi,ˆyi(xi;T−i−bf)),
where bfis the buffer region, T−i−bfis the training fold, the supscript (n)denotes the size of training
set, andpte(xi)
ptr(xi)is the density ratio of a validation sample Ti. Step (1) below holds because T−i−bf
andTiare independent. Step (2) holds because T−i−bfandTjare independent. We use shorthand
LiforL(yi,ˆyi(xi;T−i−bf))andLjforL(yj,ˆyj(xj;T−i−bf)).
ET[ˆR(n)
LOOIBCV ]
=1
nPn
i=1ET−i−bf,Tipte(xi)
ptr(xi)Li
(1)=1
nPn
i=1ET−i−bfZ
YZ
Xpte(xi)
ptr(xi)ptr(xi, yi)Lidxidyi
=1
nPn
i=1ET−i−bfZ
YZ
Xpte(xj)pte(yj|xj)Ljdxjdyj
(2)=1
nPn
i=1ET−i−bf,Tj
Lj
=1
nPn
i=1R(n−1−nbf)n→∞−→ ≈ R(n).
Algorithm 1 LOOIBCV
Input : training set associated with the geocoordinates (lat, long) and density ratio (w) of each
training point: {xi, yi, lati, long i, wi}n
i=1
Parameters : buffer size bf
Output : estimated error Err
1:fori = 1 to n do
2: Compute the distances from the validation point Tito other training samples T−i;
3: Remove training samples with distance smaller than bf;
4: Fit a model ˆfon the remaining training fold T−i−bf;
5: Compute density ratio weighted loss on the validation fold Ti:
Erri=wi· L(yi,ˆyi(xi;T−i−bf)).
6:end for
7:Return the estimated error: Err=1
nPn
i=1Erri.
9
4 8 12
SAC Range1.5
1.0
0.5
0.00.51.01.5Bias
(a) Scenario SD4 8 12
SAC Range1.5
1.0
0.5
0.00.51.01.5Bias
(b) Scenario SI
4 8 12
SAC Range1.5
1.0
0.5
0.00.51.01.5Absolute Bias
(c) Scenario SD + CS4 8 12
SAC Range1.5
1.0
0.5
0.00.51.01.5Bias
(d) Scenario SI + CS
CV algorithm: KFCV BLCV BFCV IWCV IBCVFigure 3: Biases of CV estimates in scenarios with various characteristics: spatially dependent
(SD), spatially independent (SI), spatially dependent with covariate shift (SD + CS), and spatially
independent with covariate shift (SI + CS). Circles inside the boxes display the mean values of biases.
The black dash lines illustrate no bias. (a), (b) and (d) show bias of the average CV estimate to the
risk. Since the feature distributions change across simulations in Scenario SD + CS, (c) plots the
absolute bias, i.e., the absolute value of CV estimate minus test error in each simulation.
10
Table 4: HEWA1000: model classification test error rates (targets) and 9-fold CV estimates thereof
(best estimates in each column in bold). BLCV-range, BFCV-range, and IBCV-range set the tuning
parameters a priori based on the maximum semivariogram range of all features. BLCV-best, BFCV-
best and IBCV-best estimates are selected from the best ones, for a peak-to-peak comparison. A dash
means that setting the hyperparameters based on the range gives the best value.
Model Test error KFCV IWCV BLCV BFCV IBCV BLCV BFCV IBCV
(target) -range -range -range -best -best -best
SD
Ridge 0.1700 0.1709 0.1706 0.1664 0.1783 0.1780 - - -
LSVM 0.1720 0.1709 0.1706 0.1678 0.1775 0.1773 - - -
KNN 0.1740 0.1779 0.1777 0.1905 0.1909 0.1906 - - -
RF 0.1740 0.1910 0.1907 0.1964 0.1989 0.1986 - - -
NB 0.1700 0.1729 0.1727 0.1663 0.1678 0.1676 - - -
SI
Ridge 0.2320 0.1890 0.1888 0.2007 0.2173 0.2170 0.2357 0.2411 0.2408
LSVM 0.2280 0.1900 0.1898 0.2047 0.2876 0.2872 0.2406 0.2396 0.2394
KNN 0.2440 0.2180 0.2178 0.2292 0.2628 0.2625 0.2402 0.2383 0.2381
RF 0.2520 0.2120 0.2118 0.2607 0.2692 0.2689 - 0.2365 0.2363
NB 0.2440 0.1950 0.1948 0.2043 0.2110 0.2107 0.2424 0.2441 0.2438
SD + CS
Ridge 0.2140 0.1709 0.2400 0.1644 0.1783 0.2526 0.1976 0.1997 -
LSVM 0.2040 0.1709 0.2430 0.1678 0.1775 0.2489 0.2055 0.2035 -
KNN 0.2080 0.1779 0.2533 0.1905 0.1909 0.2649 0.2186 - -
RF 0.1840 0.1910 0.2706 0.1964 0.1989 0.2789 - - -
NB 0.2160 0.1729 0.2469 0.1663 0.1678 0.2393 0.1984 0.2037 -
SI + CS
Ridge 0.2420 0.1709 0.2239 0.1644 0.1783 0.2370 0.1976 0.1997 0.2384
LSVM 0.2540 0.1709 0.2245 0.1678 0.1775 0.2334 0.2055 0.2035 0.2544
KNN 0.2440 0.1779 0.2370 0.1905 0.1909 0.2484 0.2425 0.2406 -
RF 0.2540 0.1910 0.2507 0.1964 0.1989 0.2572 0.2322 0.2453 -
NB 0.2640 0.1729 0.2289 0.1663 0.1678 0.2222 0.1984 0.2037 0.2546
11
KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP 1
Learning Tri-modal Embeddings for
Zero-Shot Soundscape Mapping
Subash Khanal
k.subash@wustl.edu
Srikumar Sastry
s.sastry@wustl.edu
Aayush Dhakal
a.dhakal@wustl.edu
Nathan Jacobs
jacobsn@wustl.eduComputer Science & Engineering
Washington University in St. Louis
St. Louis, MO, USA
Abstract
We focus on the task of soundscape mapping, which involves predicting the most
probable sounds that could be perceived at a particular geographic location. We utilise
recent state-of-the-art models to encode geotagged audio, a textual description of the au-
dio, and an overhead image of its capture location using contrastive pre-training. The
end result is a shared embedding space for the three modalities, which enables the con-
struction of soundscape maps for any geographic region from textual or audio queries.
Using the SoundingEarth dataset, we find that our approach significantly outperforms
the existing SOTA, with an improvement of image-to-audio Recall@100 from 0 .256 to
0.450. Our code is available at https://github.com/mvrl/geoclap .
1 Introduction
Sound is one of the fundamental senses that helps us reason about our environment. There
exists an intricate relationship between the visual appearance and sound of a location [15,
16]. Learning about the type of sound at a geographic location allows one to understand
many high-level concepts of the area. For example, just by hearing the sound of traffic, we
can imagine the location to be an urban setting with a rush of cars and people, whereas the
sound of sea waves might elicit the beautiful scenery of a beach.
There have been several studies conducted on different cities around the world attempt-
ing to understand human perception of various types of environmental sound [1, 3, 15, 22,
28, 30]. Moreover, it has been established that there is a strong correlation between the phys-
iological and psychological health of a person and the environmental sound condition they
live in [8, 21, 33]. Therefore, understanding the soundscape for a given geographic area can
be of great importance to policymakers focused on urban planning and environmental noise
management. Soundscapes also serve value to the general public for whom environmental
sound plays a vital role in decisions such as buying a house or setting up a business.
© 2023. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.
2 KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP
Most of the existing works on creating soundscape focus on crowd-sourcing human per-
ception of sound in their surroundings [1, 3, 22, 30, 40]. While serving as an important
tool for understanding the sound distribution of a region, such approaches have two major
limitations. First, the abstraction of sound into a fixed set of indicators and psycho-acoustic
descriptors limits our ability to have a complete picture of underlying physical factors as-
sociated with sound. Second, such soundscapes are usually created for only highly visited
places in the world, creating massive sparsity of soundscapes on a global scale. In order
to solve both of these limitations, we propose to leverage the intrinsic relationship between
sound and visual cues of the location and learn to directly predict the most probable sound
that could be heard at any given location. Specifically, we train a multi-modal deep learning
framework that learns a shared embedding space where the sound that is most likely to come
from a given location, is pulled closer while pushing other unlikely sounds farther apart. We
represent the location (latitude, longitude) by an overhead image of size H×Wcentered
around it. Once trained, our multi-modal embedding space and free availability of overhead
imagery makes it possible for us to create soundscape maps for any area in the world.
One of the successful approaches to learning shared embedding space between different
modalities is contrastive learning. In recent years, contrastive learning between image and
text [32]; image, text, and audio [17]; text, audio [9, 12, 38]; overhead image and audio [19]
has been an effective self-supervised training objective to learn a multi-modal embedding
space. Such a space has an understanding of the correspondence between the modalities that
can be transferred to various downstream tasks, where impressive results have been observed.
Motivated by these works, we also adopt contrastive learning as our pre-training strategy to
learn a multi-modal embedding space. However, unlike the prior works, we are interested
in incorporating geographic knowledge into the embedding space learned by audio-language
pre-training. We achieve this by adding an overhead image, capturing the geographic context
of a scene, as an additional modality in our contrastive learning framework. With the shared
embedding space that has knowledge of correspondence between audio and its corresponding
overhead image, we can then formulate the task of soundscape mapping as a cross-modal
retrieval problem, where the objective is to predict the most likely sound from a gallery of N
sounds given an overhead image.
Our work builds upon a prior work [19] that introduced the SoundingEarth dataset con-
taining over 50 kgeotagged audios paired with their corresponding overhead image. The
objective of work by Heidler et al. [19] was to learn a good audio-visual embedding space
useful to be transferred for different downstream tasks in remote sensing. However, in the
interest of learning an embedding space to create accurate soundscapes, our work is focused
on improving the task of cross-modal retrieval. In this regard, we utilise weights of publicly
available modality-specific SOTA models. Moreover, unlike Heidler et al., who build an
embedding space capturing two modalities (overhead-image and audio), we propose to also
incorporate textual description of audio into the embedding space. This essentially creates a
tri-modal embedding space with richer understanding of three modalities: overhead-image,
audio, and text. We call our framework GeoCLAP: Geography-Aware Contrastive Language
Audio Pre-training. As demonstrated by our results adding the textual modality improves
the representational capability of both overhead-image and audio encoders. Moreover, with
an understanding of three modalities, we are now able to create soundscapes either from a
textual or audio query for any geographic region. The main contributions of our work are as
follows:
• We significantly improve the prior baseline on the task of cross-modal retrieval of
overhead image to sound and vice-versa.
KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP 3
• We build a tri-modal embedding space that has an understanding of overhead image,
audio, and textual description of audio at a given location.
• We demonstrate a simple and scalable way of creating soundscape for any geographic
area using either a textual or audio query.
2 Related Work
2.1 Soundscape Mapping
The soundscape of a geographic region can be defined as the acoustic environment perceived
by individuals within its context [14]. There exists a large body of work focusing on the prob-
lem of soundscape mapping [1, 3, 13, 16, 22, 26, 28, 30, 40, 41]. In these works, soundscape
mapping is formulated as a framework containing three components: indicators, descriptors,
and a predictive model that maps indicators to descriptors. Indicators are psycho-acoustic
measures (for example, sound pressure level, loudness, spectral slope, etc.) which determine
the perceived value of descriptors (for example, pleasant, unpleasant, eventful, etc.). In this
paper, we refer to this line of work as perceptual soundscape mapping.
One of the common findings from the literature of perceptual soundscape mapping is
that there exists a strong correlation between the human perception of sound and the envi-
ronmental variables of the scene such as building, road category, etc. [15]. Utilising this
correlation between sound and visual cues, there have been a few works that use deep learn-
ing to learn a shared embedding space between sound and either ground level image [29]
or overhead image [20] of the scene. This multimodal learning approach leads to improved
performance on visual tasks such as aerial scene recognition [20], image classification [29],
and object detection [29]. Closer to our work, a few prior works [6, 25, 27, 39] focus on the
task of cross-modal image-to-voice retrieval. Such tasks require a dataset containing over-
head imagery paired with spoken audio captions, which is very limited. Moreover, instead
of learning from speech, we are interested in learning from free-form audio such as field
recordings, natural sounds, etc. which capture diverse concepts of the location. Another
closer work by Salem et al. [35], proposed learning a shared embedding space between au-
dio, overhead image, and ground level image, enabling them to predict a distribution over
sound clusters from an overhead image. The problem formulation of soundscape mapping
in our work is similar to [35]. However, the striking difference as well as the strength of
our work is that leveraging the power of contrastive language audio pre-training (CLAP), we
are able to create soundscape conditioned on any textual or audio query. In doing so, we
still retain the ability to create soundscape with desired set of sound categories in a zero-shot
manner.
2.2 Contrastive Learning
Radford et al., in their seminal work, CLIP [32], trained large image-text dataset using con-
trastive loss and demonstrated it’s impressive zero-shot performance on many computer vi-
sion tasks. AudioCLIP [17], extends CLIP to three modalities: image, text, and audio. Such
tri-modal embedding space enables one to perform query between three pairs of modalities.
Wav2clip [37], distilled the knowledge of CLIP embedding space by freezing the image en-
coder of CLIP and contrastively training an audio encoder to learn a new embedding space
shared by audio and a corresponding image. With similar training objective as CLIP, an-
4 KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP
other work CLAP [12] performs contrastive learning between audio and natural language.
CLAP training has proven to be an effective strategy with impressive audio retrieval perfor-
mance [9]. Inspired by this, Wu et al. [38] further improved the CLAP’s performance by
training on large-scale data with effective audio feature fusion and text augmentation strate-
gies. We refer the work by Wu et al. [38] as L-CLAP in our paper and use the pre-trained
encoders from L-CLAP to embed audio and text for GeoCLAP pre-training.
Our work takes motivation from the proven performance of contrastive learning as an
effective pre-training strategy. The focus of our work is soundscape mapping. The embed-
ding space for such tasks should have an understanding of geography of a location where
the sound is coming from [4]. Therefore, we propose to learn an embedding space trained
contrastively on three modalities: overhead image, text, and audio.
2.3 Pretrained Models
Availability of modality specific pre-trained models trained with various self-supervision
objectives have proven to be crucial in bringing performance improvement in various tasks
in remote sensing [36]. In the recent years, masked auto-encoders (MAE) [18] based models
trained on satellite imagery have demonstrated to be a good starting checkpoints to be fine-
tuned for various downstream tasks [7, 34]. In our work, we start with the pre-trained weights
of Vision Transformer (ViT) [11] encoder of SATMAE [7] as the overhead-image encoder
for GeoCLAP. SATMAE [7] was pre-trained on large-scale (over 700K) satellite imagery
of the world. To learn representations for audio and text, we use L-CLAP’s pre-trained
encoders. It uses HTSAT [5] as the audio encoder and RoBERTa [23] as the text encoder.
HTSAT is a swin-transformer [24] based model with SOTA performance on various audio
classification tasks. RoBERTa is a powerful transformer-based language model trained with
improved design choices than BERT [10]. L-CLAP [38] was contrastively pre-trained on
over 630K audio-text paired dataset.
3 Approach
We present a detailed description of our approach, including the high-level problem for-
mulation, a description of our primary evaluation dataset, and a detailed description of the
network architecture and training procedure for our method, GeoCLAP.
3.1 Problem formulation
The objective of our work is to learn a shared embedding space that allows us to predict
the most probable sounds that can be heard at a given geographic location. This can be
represented as s∗=max sP(s|l)where P(s|l)represents the conditional distribution of sounds
for a given location lands∗is the most likely sound. Unfortunately, direct conditioning on
location does not generalize to regions without a large number of training samples, which
means truly global mapping wouldn’t be possible. On the other hand, overhead imagery has
a strong correlation to the type of sound at a given location and is freely available across
the globe. Therefore, in our work, we represent the location indirectly, using an overhead
image I(l)of the location. We learn a conditional distribution P(s|I(l)), which is able to
make high-resolution predictions even for regions without training samples.
KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP 5
3.2 Dataset
We use the SoundingEarth dataset to train and evaluate our method. The dataset contains
more than 50 kgeotagged audio recordings from 136 countries and overhead image pairs.
The overhead images have size of 1024 ×1024 collected from Google Earth with an ap-
proximate ground-sample distance (GSD) of 0 .2 meters (m). Audio data in the dataset was
collected from the project Radio Aporee:::Maps [2], which hosts an online platform dedi-
cated to creating a global soundmap. It contains diverse audio recordings from urban, rural
and natural environments, published under the creative commons license. For our project,
we remove the audio files with a sampling frequency less than 16 k. This yields a dataset size
of 50792 samples.
The high-resolution Google Earth imagery is not available to be used freely. Therefore,
in order to have the ability to globally scale soundscape mapping, we augment the exist-
ingSoundingEarth dataset by including freely available lower-resolution images. Specifi-
cally, we use the RGB bands of the Sentinel-2 cloudless imagery with 10 m GSD . For each
location, we download a 256 ×256 image tile with the coverage radius of 512 mcentered at
that location.
3.3 GeoCLAP
Figure 1 represents the overall framework of GeoCLAP. Given a geotagged audio Xa
k, textual
description of the audio Xt
k, and an overhead image at a given location Xi
k, where ( Xa
k,Xt
k,Xi
k) is
one audio-text-image triplet. We obtain embeddings for each modalities by passing through
modality-specific encoder and linear projection layer, yielding embeddings with the same
dimension for audio, text, and overhead image, respectively.
Figure 1: GeoCLAP: A tri-modal contrastive learning framework to learn shared embedding
space between overhead image, sound, and textual description of the corresponding sound.
Ea
k=gaudio (faudio (Xa
k)) (1)
Et
k=gtext(ftext(Xt
k)) (2)
Ei
k=gimage (fimage (Xi
k)) (3)
where (faudio ,gaudio ),(ftext,gtext),(fimage ,gimage )are (encoder, linear projection layer) pairs
producing l2-normalized ddimensional embeddings: Ea
k,Et
k, and Ei
k, for audio, text, and
overhead image respectively.
GeoCLAP is trained on embedding triplets using contrastive learning objective similar
to CLIP [32] for all three pairs of embeddings:
6 KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP
Lat=1
2NN
∑
k=1(
logexp(Ea
k.Et
k/τat)
∑N
j=1exp(Ea
k.Et
j/τat)+logexp(Et
k.Ea
k/τat)
∑N
j=1exp(Et
k.Ea
j/τat))
(4)
Lai=1
2NN
∑
k=1(
logexp(Ea
k.Ei
k/τai)
∑N
j=1exp(Ea
k.Ei
j/τai)+logexp(Ei
k.Ea
k/τai)
∑N
j=1exp(Ei
k.Ea
j/τai))
(5)
Lti=1
2NN
∑
k=1(
logexp(Et
k.Ei
k/τti)
∑N
j=1exp(Et
k.Ei
j/τti)+logexp(Ei
k.Et
k/τti)
∑N
j=1exp(Ei
k.Et
j/τti))
(6)
where, Nis the training batch size and τat,τai, and τtiare learnable temperature parameters
used to scale logits in loss computation for each pairs of embeddings.
Combining equations 4, 5, and 6, the final loss for which GeoCLAP is trained is as
follows:
L=Lat+Lai+Lti (7)
4 Experimental Details
4.1 Data Preprocessing
For audio preprocessing, we convert each audio sample into mel-spectrogram using the de-
fault settings: {feature_size=64, sampling_rate=48000, hop_length=480,
max_length_s=10, fft_window_size=1024} provided in the HuggingFace -
wrapper: ClapProcessor for the pre-trained L-CLAP model clap-htsat-fused .
In the SoundingEarth dataset, most of the audio recordings (except 6333 samples) are
also accompanied by a brief description and a title uploaded by the contributor. In order
to have textual description for all audio recordings as well as to further encode geographic
information in text, we use a python client, geopy to obtain the address of the location
and append an additional sentence, “The location of the sound is:{address}. ” to the textual
description of each sample. For example, for the geolocation (52.509663 ,13.376481 ), the
added sentence would be “The location of the sound is: Potsdamer Platz, Tiergarten, Mitte,
Berlin, 10785, Germany” . Following L-CLAP, we use RobertaTokenizer with the
parameter max_length set to 77.
For overhead imagery, we adopt the same data augmentation as SATMAE [7]. We per-
form RandomResizedCrop with parameters: {input_size=224, scale=(0.2,1.0)} ,
followed by a RandomHorizontalFlip , during training. During inference, we extract a 224 ×
224 center crop of the image.
4.2 Implementation and metrics
We implement our code in Pytorch and utilise HuggingFace for loading L-CLAP en-
coders and their respective data pre-processing wrappers. We split the dataset with ratio
70:10:20 yielding a total of 35554, 5079, and 10159 samples into training, validation, and
test split, respectively. For experiments regarding the baseline, we ran the publicly available
code for [19] using the data splits of our study. We used the experimental setting for their
best reported results on cross-modal retrieval task, which is as follows: {batch_size=256
encoders=ResNet18, latent_dim=128, loss=SymmetricCL, tau=0.2} .
The baseline was trained for 300 epochs withAdam optimizer and learning rate of 1 e−3.
KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP 7
4.2.1 Encoders
We use the pre-trained model clap-htsat-fused [38] to encode audio and text. The
audio encoder used in our study, HTSAT, has 4 swin-transformer blocks with hidden feature
dimension of 768. The text encoder RoBERTa from [38] used in our study, has 12 trans-
former blocks with hidden feature dimension of 768. For both audio and text encoders, we
take the output of their respective L-CLAP’s projection layer producing 512-dimensional
embeddings. For encoding overhead image, we use the pre-trained vit_base_patch16
encoder of SATMAE [7]. It processes input as a sequence of 16 ×16 image patches passing
through 12 layers of transformer blocks. In order to match dimension with audio and text
embeddings, we pass the output from SATMAE encoder to a ReLU activation followed by
a 512-dimension linear layer. Starting from weights of these pre-trained encoders, we con-
duct two set of experiments. First, we allow only the overhead-image encoder to train while
freezing L-CLAP. Second, we allow fine-tuning of all encoders in our framework.
4.2.2 Training
We train GeoCLAP using the contrastive loss objective presented in Equation 7. We ini-
tialize all three learnable temperature parameters to 0 .07. We also run experiments with
and without using textin our framework. While using text, we further experiment the im-
pact of adding an additional sentence describing detailed address of the location to the text.
For experiments where we use overhead image and audio only, we train our model with
image-audio contrastive loss represented by Equation 5. Moreover, for experiments using
overhead image, audio, and text, while keeping the L-CLAP encoders frozen, we train with
Loss =Lai+Lti. We use a training batch size of 256 for the baseline, and our experiments
with frozen L-CLAP, while using batch size of 128 for experiments allowing fine tuning
of L-CLAP. We use the Adam optimizer and set the initial learning rate to 5 e−5. We use
weight_decay=0.2 andbetas=(0.9,0.98) . We use cosine annealing learning rate
scheduler with number of warm up iterations set to 2000. We set max_epochs to 100 for
experiments with frozen L-CLAP and 30 for experiments allowing fine tuning of L-CLAP.
4.2.3 Metrics
Following Heidler et al . [19], we use Recall@100 and Median Rank (Median-R) of the
ground-truth as the evaluation metrics of our approach. We use the test set containing 10159
samples as the gallery for both image-to-sound and sound-to-image retrieval evaluation.
5 Evaluation
5.1 Experiments with SoundingEarth data
Table 1 shows the results of our experiments with the SoundingEarth dataset while using the
original overhead imagery of 0 .2mresolution. One of the interesting results from this table is
that by just using frozen pre-trained audio encoder from L-CLAP [38], while allowing only
overhead-image encoder to be fine-tuned, we already get about 10 points improvement in
cross-modal retrieval. This highlights the advantage of leveraging rich representation space
of pre-trained models like L-CLAP. However, when we introduce text modality into train-
ing, while still keeping both text and audio encoders frozen, the image-to-sound Recall@100
8 KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP
Method Image2Sound Sound2Image
Experiment Image Encoder Text-Audio Encoder Text Address R@100 Median-R R@100 Median-R
Baseline [19] ResNet18 ResNet18 ✗ ✗ 0.256 814 0.250 816
ours SATMAE L-CLAP-frozen ✗ ✗ 0.352 360 0.348 369
ours SATMAE L-CLAP-frozen ✓ ✗ 0.328 428 0.325 428
ours SATMAE L-CLAP-frozen ✗ ✓ 0.298 546 0.295 544
ours SATMAE L-CLAP-frozen ✓ ✓ 0.317 439 0.311 443
ours SATMAE L-CLAP ✗ ✗ 0.384 230 0.385 237
ours SATMAE L-CLAP ✓ ✗ 0.423 172 0.419 175
ours SATMAE L-CLAP ✗ ✓ 0.432 166 0.431 167
ours SATMAE L-CLAP ✓ ✓ 0.434 159 0.434 167
Table 1: Cross-modal retrieval performance for models using 0.2m GSD overhead imagery.
drops to 0 .32. L-CLAP was trained on large corpus of text-audio pairs where textual descrip-
tion of audio have relatively high quality. However, the primary focus of the SoundingEarth
dataset has been to collect geotagged audio from all around the world and associate them
with high-resolution overhead imagery. We observed that the textual descriptions of audio in
theSoundingEarth dataset are noisy and do not reflect the type of textual prompts L-CLAP
models were trained on. In our experiments, we use three different types of texts: textual
description of audio, only address of the audio, and text containing both description and
address of the audio. We observed that for any type of text, learning with frozen representa-
tion lowers the performance when compared to learning with frozen representation of audio
alone. With this observation, we decided to allow fine-tuning of L-CLAP encoders. Accord-
ingly, the performance of our approach noticeably improves to image-to-sound Recall@100
of 0.384 while learning with overhead image and audio. The performance further improves
to Recall@100 of 0 .423 with Median Rank of 172 when we learn with overhead image, au-
dio, and text. This performance is further improved to Recall@100 of 0 .434 with Median
Rank of 159, when we add address of the audio location in the text. This is an absolute
improvement of the baseline performance by 0 .178 points in image-to-sound Recall@100
and 655 in Median Rank. We see similar trends on sound-to-image retrieval task.
5.2 Experiments with Sentinel data
Table 2 shows the results of our experiments with Sentinel-2 cloudless imagery with 10 m GSD .
We found that performance in all of our experiments noticeably improved while using lower-
resolution overhead imagery. This choice brought in 12 .89% relative improvement in the
baseline Recall@100 performance as well. We believe the reason for this improvement is
the larger coverage of geographic area in a single overhead image with 10m GSD . Moreover,
the lower-resolution sentinel imagery is inherently blurry offering some regularization effect
during training, leading to improved generalizability of our models. Following similar trends
as in Table 1, an absolute Recall@100 improvement of about 10 points is observed, when
using a pre-trained frozen audio encoder from L-CLAP. Similarly, the retrieval performance
improves to 0 .396 when the audio encoder is allowed to be fine-tuned. We also observe gain
in performance of fine-tuned GeoCLAP models trained with text containing address. The
best performance for GeoCLAP trained with all three modalities, yields (Recall@100, Me-
dian Rank) of (0 .450,143) and (0 .447,144) for image-to-sound and sound-to-image retrieval,
respectively. Compared to the baseline, this is a relative gain of 55 .71% and 57 .95% for
Recall@100 on tasks: image-to-sound and sound-to-image retrieval, respectively.
KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP 9
Method Image2Sound Sound2Image
Experiment Image Encoder Text-Audio Encoder Text Address R@100 Median-R R@100 Median-R
Baseline [19] ResNet18 ResNet18 ✗ ✗ 0.289 620 0.283 635
ours SATMAE L-CLAP-frozen ✗ ✗ 0.384 274 0.381 271
ours SATMAE L-CLAP-frozen ✓ ✗ 0.340 369 0.338 367
ours SATMAE L-CLAP-frozen ✗ ✓ 0.311 453 0.304 461
ours SATMAE L-CLAP-frozen ✓ ✓ 0.337 378 0.331 370
ours SATMAE L-CLAP ✗ ✗ 0.396 199 0.396 205
ours SATMAE L-CLAP ✓ ✗ 0.441 152 0.441 155
ours SATMAE L-CLAP ✗ ✓ 0.441 153 0.440 156
ours SATMAE L-CLAP ✓ ✓ 0.450 143 0.447 144
Table 2: Cross-modal retrieval performance for models using 10m GSD overhead imagery.
5.3 Zero-Shot Soundscape Mapping
Utilising the rich representation space of our best-performing GeoCLAP model, we demon-
strate zero-shot soundscape mapping using both text and audio queries. Soundscape maps,
in our work, are the similarity-score heatmaps for a given query. Specifically, we use the
appropriate encoder from GeoCLAP to produce an embedding of the query and embeddings
for a dense set of overhead images in the region of interest. Then, the cosine similarity
score between the query embedding and all overhead image embeddings is overlaid on the
corresponding region to yield a soundscape map (Figure 2). In Figure 3, we demonstrate
a country-scale soundscape map for the Netherlands. For this, we compute soundscape for
three prompts: { This is a sound of car horn; This is a sound of chirping birds; This is a sound
of animal farm } and overlay them together to create a composite pseudo-color map. We
compare this soundscape with ESRI’s Sentinel-2 land cover classes. We observe a strikingly
high correlation between the related land-cover classes with the category of sound likely to
be heard at the location. More such soundscape maps can be found in the supplemental
material of this paper.
(b)(a)
Figure 2: Soundscape maps along with reference overhead image for two regions. Sound-
scape created for queries: (a) A textual prompt: This is a sound of sea waves ; (b) randomly
selected sound from the class chirping_birds from ESC50 database [31] (green: more
probable, white: less probable).
10 KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP
(a)(b)
Figure 3: Comparison of (a) Soundscape map of the Netherlands with (b) Sentinel-2 land
cover classes . The soundscape map was created by querying GeoCLAP with textual prompts
for three sound categories: car horn ,chirping birds , and animal farm .
6 Conclusion
We proposed GeoCLAP, a contrastive-learning framework capable of embedding the modal-
ities of overhead imagery, audio, and text into a common space. Our approach significantly
improves the state of the art for cross-modal retrieval between overhead imagery and audio.
We utilise the learned, multi-modal representation space for soundscape mapping, demon-
strating a simple and scalable way to create soundscape maps for any geographic area using
only satellite imagery and audio or textual queries. With this approach, we can construct
global, high-resolution soundmaps with minimal effort.
References
[1] Luca Maria Aiello, Rossano Schifanella, Daniele Quercia, and Francesco Aletta.
Chatty maps: constructing sound maps of urban areas from social media data. Royal
Society open science , 3(3):150690, 2016.
[2] Radio Aporee. https://aporee.org/maps.
[3] Eiji Aramaki and Shoko Wakamiya. Image and sound of the city. In The Social
City: Space as Collaborative Media to Enhance the Value of the City , pages 205–214.
Springer, 2023.
[4] Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David
Lobell, and Stefano Ermon. Geography-aware self-supervised learning. In Proceedings
of the IEEE/CVF International Conference on Computer Vision , 2021.
[5] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-Kirkpatrick, and Shlomo
Dubnov. Hts-at: A hierarchical token-semantic audio transformer for sound classifica-
KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP 11
tion and detection. In Proceedings of the IEEE International Conference on Acoustics,
Speech and Signal Processing , 2022.
[6] Yaxiong Chen, Xiaoqiang Lu, and Shuai Wang. Deep cross-modal image–voice re-
trieval in remote sensing. IEEE Transactions on Geoscience and Remote Sensing , 58
(10):7049–7061, 2020.
[7] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Mar-
shall Burke, David Lobell, and Stefano Ermon. Satmae: Pre-training transformers for
temporal and multi-spectral satellite imagery. Advances in Neural Information Pro-
cessing Systems , 35:197–211, 2022.
[8] Peng Cui, Tingting Li, Zhengwei Xia, and Chunyu Dai. Research on the effects of
soundscapes on human psychological health in an old community of a cold region. In-
ternational Journal of Environmental Research and Public Health , 19(12):7212, 2022.
[9] Soham Deshmukh, Benjamin Elizalde, and Huaming Wang. Audio retrieval with wav-
text5k and clap training. arXiv preprint arXiv:2209.14275 , 2022.
[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-
training of deep bidirectional transformers for language understanding. In Proceedings
of the Conference of the North American Chapter of the Association for Computational
Linguistics , 2018.
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua
Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Trans-
formers for image recognition at scale. Proceedings of the International Conference on
Learning Representations , 2021.
[12] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap
learning audio concepts from natural language supervision. In Proceedings of the IEEE
International Conference on Acoustics, Speech and Signal Processing , 2023.
[13] Margret Sibylle Engel, André Fiebig, Carmella Pfaffenbach, and Janina Fels. A re-
view of the use of psychoacoustic indicators on soundscape studies. Current Pollution
Reports , pages 1–20, 2021.
[14] International Organization for Standardization. Iso 12913-1: 2014 acous-
tics—soundscape—part 1: definition and conceptual framework. ISO, Geneva , 2014.
[15] Luis Garzón, Luis Bravo-Moncayo, Julián Arellana, and Juan de Dios Ortúzar. On the
relationships between auditory and visual factors in a residential environment context:
A sem approach. Frontiers in Psychology , 14, 2023.
[16] David Montes González, Juan Miguel Barrigón Morillas, and Guillermo Rey-Gozalo.
Effects of noise on pedestrians in urban environments where road traffic is the main
source of sound. Science of the total environment , 857:159406, 2023.
[17] Andrey Guzhov, Federico Raue, Jörn Hees, and Andreas Dengel. Audioclip: Extending
clip to image, text and audio. In Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing , 2022.
12 KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP
[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.
Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2022.
[19] Konrad Heidler, Lichao Mou, Di Hu, Pu Jin, Guangyao Li, Chuang Gan, Ji-Rong Wen,
and Xiao Xiang Zhu. Self-supervised audiovisual representation learning for remote
sensing data. International Journal of Applied Earth Observation and Geoinformation ,
116:103130, 2023.
[20] Di Hu, Xuhong Li, Lichao Mou, Pu Jin, Dong Chen, Liping Jing, Xiaoxiang Zhu, and
Dejing Dou. Cross-task transfer for geotagged audiovisual aerial scene recognition. In
Proceedings of the European Conference on Computer Vision . Springer, 2020.
[21] Peter Lercher and Angel M Dzhambov. Soundscape and health. In Soundscapes:
Humans and Their Acoustic Environment , pages 243–276. Springer, 2023.
[22] Matteo Lionello, Francesco Aletta, and Jian Kang. A systematic review of prediction
models for the experience of urban soundscapes. Applied Acoustics , 170:107479, 2020.
[23] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.
[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo. Swin transformer: Hierarchical vision transformer using shifted win-
dows. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
2021.
[25] Guo Mao, Yuan Yuan, and Lu Xiaoqiang. Deep cross-modal retrieval for remote sens-
ing image and audio. In 10th IAPR workshop on pattern recognition in remote sensing ,
2018.
[26] Efstathios Margaritis and Jian Kang. Soundscape mapping in environmental noise man-
agement and urban planning: case studies in two uk cities. Noise mapping , 4(1):87–
103, 2017.
[27] Hailong Ning, Bin Zhao, and Yuan Yuan. Semantics-consistent representation learn-
ing for remote sensing image–voice retrieval. IEEE Transactions on Geoscience and
Remote Sensing , 60:1–14, 2021.
[28] Kenneth Ooi, Zhen-Ting Ong, Karn N Watcharasupat, Bhan Lam, Joo Young Hong,
and Woon-Seng Gan. Araus: A large-scale dataset and baseline models of affective re-
sponses to augmented urban soundscapes. IEEE Transactions on Affective Computing ,
2023.
[29] Andrew Owens, Jiajun Wu, Josh H McDermott, William T Freeman, and Antonio Tor-
ralba. Ambient sound provides supervision for visual learning. In Proceedings of the
European Conference on Computer Vision , 2016.
[30] Judicaël Picaut, Nicolas Fortin, Erwan Bocher, Gwendall Petit, Pierre Aumond, and
Gwenaël Guillaume. An open-science crowdsourcing approach for producing commu-
nity noise maps using smartphones. Building and Environment , 148:20–33, 2019.
KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP 13
[31] Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings
of the Association for Computing Machinery Conference on Multimedia , 2015.
[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervision. In International Confer-
ence on Machine Learning . PMLR, 2021.
[33] Antonella Radicchi, Pınar Cevikayak Yelmi, Andy Chung, Pamela Jordan, Sharon
Stewart, Aggelos Tsaligopoulos, Lindsay McCunn, and Marcus Grant. Sound and the
healthy city. Cities & Health , 5(1-2):1–13, 2021.
[34] Colorado J Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian
Clipp, Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scale-mae: A scale-
aware masked autoencoder for multiscale geospatial representation learning. arXiv
preprint arXiv:2212.14532 , 2022.
[35] Tawfiq Salem, Menghua Zhai, Scott Workman, and Nathan Jacobs. A multimodal ap-
proach to mapping soundscapes. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition Workshops , 2018.
[36] Yi Wang, Conrad Albrecht, Nassim Ait Ali Braham, Lichao Mou, and Xiaoxiang Zhu.
Self-supervised learning in remote sensing: A review. IEEE Geoscience and Remote
Sensing Magazine , 2022.
[37] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip:
Learning robust audio representations from clip. In Proceedings of the IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing , 2022.
[38] Yusong Wu*, Ke Chen*, Tianyu Zhang*, Yuchen Hui*, Taylor Berg-Kirkpatrick, and
Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fu-
sion and keyword-to-caption augmentation. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Processing , 2023.
[39] Rui Yang, Shuang Wang, Yingzhi Sun, Huan Zhang, Yu Liao, Yu Gu, Biao Hou, and
Licheng Jiao. Multimodal fusion remote sensing image–audio retrieval. IEEE Journal
of Selected Topics in Applied Earth Observations and Remote Sensing , 15:6220–6235,
2022.
[40] Ran Yue, Qi Meng, Da Yang, Yue Wu, Fangfang Liu, and Wei Yan. A visualized sound-
scape prediction model for design processes in urban parks. In Building Simulation ,
volume 16, pages 337–356. Springer, 2023.
[41] Tianhong Zhao, Xiucheng Liang, Wei Tu, Zhengdong Huang, and Filip Biljecki. Sens-
ing urban soundscapes from street view imagery. Computers, Environment and Urban
Systems , 99:101915, 2023.
KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP 1
Supplementary Material: Learning Tri-modal
Embeddings for Zero-Shot Soundscape
Mapping
Subash Khanal
k.subash@wustl.edu
Srikumar Sastry
s.sastry@wustl.edu
Aayush Dhakal
a.dhakal@wustl.edu
Nathan Jacobs
jacobsn@wustl.eduComputer Science & Engineering
Washington University in St. Louis
St. Louis, MO, USA
In this supplemental material, we present a demonstration of the zero-shot soundscape
mapping capability offered by our proposed framework, GeoCLAP. Specifically, we show-
case the soundscape maps created by querying our best performing model with diverse
sound-related textual prompts. Furthermore, in a video demonstration accompanying this
material, we highlight the satellite image to audio retrieval capability of GeoCLAP.
1 Zero-Shot Soundscape Mapping
Following the same methodology from Section 5 .3 in the main paper, we constructed a
soundscape map of England. We selected three prompts: This is a sound of car horn; This
is a sound of chirping birds; This is a sound of animal farm . We downloaded Sentinel-2
cloudless images for England, each with dimension 256 ×256. Then, using cosine similarity
scores between image and text embeddings, we created a dense soundscape map for the
region. All visualizations were created using Q-GIS.
As observed in Figure 1, there is a strong correlation between sound categories and
relevant land-cover classes. As expected, the soundscape map reveals that urban areas in
England, such as the region around London, are highly associated with the sound category
car horn indicated by the colour blue in Figure 1 (a). On the other hand, less populated areas
with crops exhibit a notable association with the sound category animal farm . An intriguing
observation is that around built-up areas in England, a combination of both car horn and
chirping birds sound is observed, as indicated by purple-coloured regions in soundscape.
This suggests that despite human activities in these regions, birds still inhabit them.
Soundscapes can be viewed as composite pseudo-colour maps representing a desired set
of sound categories, as shown in Figure 1. However, if one is specifically interested in a sin-
gle sound category, the GeoCLAP model can be queried with a textual prompt correspond-
ing to that particular sound category, as demonstrated in Figure 4. Furthermore, visualizing
© 2023. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.
2 KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP
soundscapes for smaller geographic regions, as showcased in Figure 2 and 3, can provide a
better understanding of sound-related concepts learned by the model.
The results shown in Figure 2 indicate high similarity between the prompt: This is a
sound of a manufacturing factory and a sub-region that likely contains structures resem-
bling manufacturing factories. Similarly, in Figure 3, areas associated with water bodies
exhibit a high similarity with the prompt: This is a sound of a flowing river. These findings
demonstrate that the embedding space of GeoCLAP possesses an understanding of high-level
sound-related concepts within geographic regions.
(a)(b)
Figure 1: Comparison of (a) Soundscape map of England with (b) ESRI’s Sentinel-2 land
cover classes . The soundscape map was created by querying GeoCLAP with textual prompts
for three sound categories: car horn ,chirping birds , and animal farm . Best viewed in colour.
(a)(b)
Figure 2: Soundscape map of a small region (a) along with the reference overhead image (b).
Soundscape created for the textual prompt: This is a sound of manufacturing factory . (green:
more probable, white: less probable).
KHANAL, SASTRY, DHAKAL, JACOBS: GEOCLAP 3
(a)(b)
Figure 3: Soundscape map of a small region (a) along with the reference overhead image
(b). Soundscape created for the textual prompt: This is a sound of flowing river . (green:
more probable, white: less probable).
Figure 4: Soundscape map of England created by querying GeoCLAP with a textual prompt:
This is a sound of church bells .
1
EarthNets: Empowering AI in Earth Observation
Zhitong Xiong, Member, IEEE , Fahong Zhang†, Yi Wang†, Yilei Shi, Member, IEEE ,
and Xiao Xiang Zhu*, Fellow, IEEE
Abstract —Earth observation, aiming at monitoring the state of planet Earth using remote sensing data, is critical for improving our
daily lives and living environment. With a growing number of satellites in orbit, an increasing number of datasets with diverse sensors
and research domains are being published to facilitate the research of the remote sensing community. In this paper, we present a
comprehensive review of more than 400 publicly published datasets, including applications like land use/cover, change/disaster
monitoring, scene understanding, agriculture, climate change, and weather forecasting. We systematically analyze these Earth
observation datasets with respect to ﬁve aspects volume, bibliometric analysis, resolution distributions, research domains, and the
correlation between datasets. Based on the dataset attributes, we propose to measure, rank, and select datasets to build a new
benchmark for model evaluation. Furthermore, a new platform for Earth observation, termed EarthNets, is released as a means of
achieving a fair and consistent evaluation of deep learning methods on remote sensing data. EarthNets supports standard dataset
libraries and cutting-edge deep learning models to bridge the gap between the remote sensing and machine learning communities.
Based on this platform, extensive deep learning methods are evaluated on the new benchmark. The insightful results are beneﬁcial to
future research. The platform and dataset collections are publicly available at https://earthnets.github.io/.
Index Terms —Benchmarking, dataset review, deep learning, Earth observation, remote sensing
!
1 I NTRODUCTION
EARTH Observation (EO) aims to monitor and assess
the status of the Earth’s surface using various remote
sensing (RS) technologies [1], [2]. EO can make a signiﬁcant
contribution to our ability to better understand and analyze
the planet Earth using RS data. The research in EO has
been successfully applied to urban planning [3], natural
resources management [4], agriculture [5], food security
[6] and disaster monitoring [7], [8]. All these applications
are important for the sustainable development of human
society.
With the development of Earth observation technology,
more and more satellites with diverse imaging sensors have
been launched for different missions. A huge amount of RS
data with global coverage and high resolution is received
every day for automatic processing and analysis. To deal
with large-scale data, deep learning techniques [9] have
been proven effective for many different research areas.
In this context, recent RS datasets are constructed with
larger and larger volumes of data. In Fig. 1, we show a
chronological overview of the volumes of more than 400
existing benchmark datasets. As seen from this ﬁgure, more
numerous and larger datasets have been constructed and
published during the past decade. Although considerable
progress has been made with the overwhelming success of
•Z. Xiong, F. Zhang and X. X. Zhu are with the Chair of Data Science
in Earth Observation, Technical University of Munich (TUM), 80333
Munich, Germany.
•Y. Wang is with the Chair of Data Science in Earth Observation, Tech-
nical University of Munich (TUM), Germany, and the Remote Sensing
Technology Institute, German Aerospace Center (DLR), Germany.
•Y. Shi is with the Chair of Remote Sensing Technology, Technical Univer-
sity of Munich (TUM), 80333 Munich, Germany.
• † indicates equal contribution, and * indicates the corresponding author
(e-mail: xiaoxiang.zhu@tum.de).
100,000
60,000
30,000
10,000
6000
3000
1000
600
300
100
60
30
10
6
3
1
0.5
0.2
0.1
0.06
0.03
0.01
0.01
0
0
0Volume (GB)
Poverty in AfricaSatStereo
WHU MVS /Stereo
WHU Multi -View
WHU Stereo
ISPRS 2021WHU -TCL SatMVS
URB 3DCD
Brazilian CoffeeSalinasAgricultural Crop Cover
GF2 3DFGCCrop Type
CV4A  Kenya
TimeSen 2CropAgriculture -Vision
CaneSatUA V Multispectral /ThermalSen4AgriNet
PASTISZueriCrop
EuroCropsCropHarvestSouth Africa CropDENETHOR
The Canadian Cropland
Space 2Ground
Paddy Rice South KoreaCropLand (CLCD )
 PlanesNetMTAR SI (Aircraft )RarePlanes
CGI Planes
Airbus Aircraft DetectionSAR AircraftMilitary AircraftLEarning ,VIsion & Remote Bridges DatasetSpaceNet -1 (Building )SpaceNet -2 (Building )
INRIA  AerialAIST Building Change
  SpaceNet -4 (Multi -View )
xView 2 (xBD)
 WHU Building
CrowdAI Mapping WHU Building change
 Built-structure count
   Microsoft Building (Australia )
Microsoft Building (Uganda /Tanzania )Microsoft Building (USA)
Microsoft Building (Canada )Roof SegmentationSpaceNet -6 (Multi -Sensor )
SpaceNet -7 (Multi -Temporal )
LEVIR -CD
Synthinel -1
Kaggle Water Bodies
Kaggle aerial segmentation
 GTA-V SIDUS Building Footprints
    Urban Building ClassificationBONAI
RIDBiome : L8 Cloud Cover
SPARCS38-Cloud
STGAN Cloud RemovalKaggle Cloud Detection
Sentinel -2 Cloud Detection
HRC _WHUCloudCast
95-CloudSentinel -2 Cloud Mask
WHU Cloud DatasetSEN 12MS-CR
WHUS 2-CD+
   Sentinel -2 Cloud Cover
DroneCrowdE R A (Event Recognition )
AFO-Floating objects
Kaggle Hurricane DamageSEN -12-FLOODSen1Floods 11
OMBRIA
AISNWPU -VHR 10RSODTGRS HRRSD
AIDxViewfMoW
D O TA v 1.0
DLRSDGETNET datasetD O TA v 1.5
DIOR
ORSSDD O TA v 2.0
iSAID
OVERHEAD MNISTVAL I D
DSIFNUA VidSeCo
MultiSceneMillion AID
U AVO D 10
AIR-PolSAR -SegSSL4EO-S12
Semantic 3D.netGT-CrossView
CVUSAUCF Cross View DatasetCV ACT
MLRSNetUniversity -1652VIGOR
BIRDSAI
UCM captionSydney captionRSICD
  WHU -SEN -City
Aerial to Map (Pixel 2Pixel )SemCity Toulouse
UC Merced
Pavia Center
Pavia UniversityISPRS 2D - Potsdam ISPRS 2D - V aihingen
WHU -RS19Washington DC MALL
RSSCN 7S AT-4
S AT-6
Indian Pines
DFC 2015 ZeebrugesBotswana
Kennedy Space CenterZurich Summer
Brazilian Cerrado -SavannaNWPU -RESISC 45RSC 11SIRI-WHUDSTL (3Band )
DSTL (16Band )RSD 46-WHU
RSI-CB256
RSI-CB128Kaggle Planet Forest
AIS Dataset
Austin Zoning
    Dstl Satellite ImageryAIS Online Maps
EuroSA TDeepGlobe (LandCover )
Onera Satellite ChangeSynthetic & Real
Proba -V Super ResolutionDFC 18
SynthAer
AeroscapesGaofen Image Dataset (GID)
Urban Drone Dataset (UDD )
HyRANKRIT-18AICD
WHDLDHyperspectral Change
MSLCCBigEarthNetSEN 12MS
Slovenia Land Cover
MtS-WHUrban Semantic 3D
Semantic Drone SemSeg
XiongAn
AeroRITHistAerial
OPTIMAL -31Chesapeake Land Cover
AiRound
CV-BrCT (Construction Type )
SECONDDFC 20
Kaggle building segmentation
AISD
EORSSDGoogle CDOpen Cities
LandCoverAICLRSSenseEarth classify
Google Data SetGID15DFC 21-MSD
SYSU -CDLoveDA
Satellite ClassificationFloodNet
TG1HRSSCAIR-CD
 MiniFrance -DFC 22
GeoNRWSEASONETMultiSenGE
The WorldStratFive-Billion -PixelsOpenSentinelMap
WHU -OHS
TimeSpec 4LULC
Bijie LandslideMidAir
GTAH
  RSUSSIARPA Multi -View Stereo
S2UC
RSOC (Object Counting )Oil and Gas Tank (OGST )
WiDS Datathon 2019
Airbus Oil Storage DetectionOil Storage TanksQXS-SAROPTDraper Satellite ChronologyStanford Drone Dataset
SateHaze 1k
MUSIC 4GC ( Golf Course )Okutama -Action
Iceberg DetectionPatternNet
Cactus Aerial PhotosSo2Sat LCZ 42
TenGeoP -SARwvBrazilDAM
DFC 21-DSEHephaestus
MUSIC 4H A (Hot Area )Kaggle Find a Car ParkAPKLOTSemantic Drone -ODSatellite Pose EstimationTTPLA
Thermal /Visible aerial imagesMassachusetts Roads
ERM PAIW
HD-MapsSpaceNet -3 (Road )
AerialLanes 18DeepGlobe (Road )
RoadNetSpaceNet -5 (Road )
Microsoft Road
RSVQA -LRRSVQA -HRRSVQAxBEN RSIVQANOAA  Sea Lion Count
MAS AT I (Maritime )
Aerial Maritime DroneArctic Sea IceSeaDronesSee
  SEN 1-2
OpenSARShipHRSC 2016 (Ship )
Ships in SatelliteKaggle Airbus ShipAirbus Ship Detection
Ships in Google Earth
SAR Ship Detection
AIR-SARShip -1.0AIR-SARShip -2.0 (GF-3)LS-SSDD (Large Scale )
HRSID (Ship )FGSCR -42SWIM -ShipxView 3-SAR
SynthWakeSARAI-TOD
USTC _SmokeRSHyperview ChallengeMUSIC 4P3 (Photovoltaic Power Plants )GeoLifeCLEF 2021
AL S AT -2BSEN 2VENm uS
BH-Pools + BH-WaterTanksAU-AIR
   Forest Cover Type
  Mexican Forest Canopy Height
Aerial Cactus IdentificationUrban Tree DetectionNEON Tree Crowns
Forest DamagesReforesTreeThe Auto Arborist
TreeSatAISemanticKITTI
Toronto 3DSensatUrbanHeisenheim 3D
DLR HySUOakland 3-D PointCloudIQmulus & TerraMobilitaColumbia LiDAR 2015
Paris -Lille-3DColumbia LiDAR 2018
DublinCity
Zhang et al . CD
SZTAKI AirChange
     CDD (season -varying )HRSCD
California floodOSCD
  Sentinel -2 Multitemporal CitiesRelative Radiometric NormalizationHTCDS2LookingS2MTCP
MSBC
MSOSCDDynamic World SUM -Helsinki
Things And Stuff (TAS)OIRDSUCAS _AODVEDAI (V ehicle )PKLot (Parking Lot )COWC
Car Parking Lot (CARPK )
DLR 3k/DLR -MVDAWAMI DIRSIGITCVD (V ehicle )
 VisDrone 2019 -DETVisDrone 2019 -VIDVisDrone 2019 -SOT
VisDrone 2019 -MOT
SIMD (Multi -vehicles )VisDrone
 DroneV ehicle
VISO -Detection
Swimming pool /carGTA  Birds Eye View (V ehicle )
Kaggle Massachusetts Buildings
Global inland watersHistorical Hourly Weather DataDeepWeather
EarthNet 2021
Next Day Wildfire Spread
Airbus Wind Turbine PatchesHurricane Wind Speed
Fig. 1. Chronological overview of the volumes of existing 400 datasets.
As can be seen, more and more numerous and larger datasets have
been constructed and published during the past decade. (Best viewed
by zooming in.)
deep learning techniques [10], [11], there are still several
problems that need more research efforts to handle.
There is a lack of comprehensive dataset review for
EO tasks. Owing to the efforts made by EO researchers,
there are numerous datasets in the RS community with
different modalities, resolutions, and application domains.
Some of these RS data modalities include optical (RGB), hy-
perspectral, synthetic aperture radar (SAR), multi-spectral
(MS), and point cloud. Regarding the application domains,
the published datasets may be designed for land use and
land cover (LULC) [12], change monitoring [13], disaster
monitoring [14], scene recognition, semantic segmentation,
ground object detection, object tracking, agriculture, climate
change, and weather forecasting. A comprehensive reviewarXiv:2210.04936v2  [cs.CV]  7 Dec 2022
2
of the RS datasets can provide researchers with a holistic
view of the status of the research community. Bakula et al.
[15] review benchmarking in photogrammetry and remote
sensing relating to geodata. They provide dataset collections
and a bibliographic analysis, which are both useful for
future research. Schmitt et al. [16] provide a historic review
of RS datasets. They discuss dataset features based on a
few examples and present some important criteria for the
establishment of a standard database. Although a few works
[17], [18], [19] attempt to review existing RS datasets, they
are not exhaustive or sufﬁciently comprehensive to cover a
large range of research domains.
There is no systematic summary and analysis of RS
datasets. The ever-growing quantity of RS datasets makes
it difﬁcult to ﬁnd the proper one for a speciﬁc application
in the jungle of remote sensing datasets. For example, there
are more than 20 datasets related to building extraction [20].
Finding, assessing, and selecting the one most suitable for
a given application can be laborious and time-consuming.
Thus, it is vitally important to summarize and categorize
RS datasets to provide valuable guidance and reference for
researchers. Both the EOD1[21] and the AiTLAS Seman-
tic Data Catalog2have been designed as search engines
for RS datasets, and are valuable and helpful for the RS
community. However, at present, the information about RS
datasets collected in these existing databases is still limited.
Beyond summarization and categorization, insightful anal-
ysis of existing datasets can help researchers understand the
current research state and trends of the whole community.
Systematic analysis of RS datasets is thus crucial to the
future development of different research ﬁelds.
There is no uniﬁed benchmark for fair comparisons
of remote sensing methods. In the computer vision (CV)
community, a large-scale dataset like ImageNet [22] is
usually used for the evaluation of newly developed deep
learning models. Compared with small-scale datasets, large-
scale datasets with rich semantic annotations align better
with complex real-world scenarios [17]. Thus they can be
more reliable for performance validation and comparison of
deep learning algorithms. Although several large-volume
RS datasets have been published [23], [24], [25], many of
the currently developed methods are still evaluated on
small-scale datasets. However, datasets with a small scale
or limited geographic coverage may bias to a speciﬁc data
distribution that is not representative of real-world scenar-
ios. Moreover, many RS datasets are published with no
standard train/validation/test splits. This increases uncer-
tainty during the evaluation of algorithms. Thus, building
new RS benchmarks to enable a fair comparison of different
methods is urgently needed.
At present, there is a lack of an open platform for
different EO tasks. For deep learning methods, backbone
networks, hyper-parameters, and training tricks are inﬂuen-
tial factors that should be considered for fair performance
comparison. However, existing works usually evaluate the
performance with different dataset splits, which makes it
difﬁcult to fairly and reliably compare different algorithms.
Due to the large variance in data collection sensors and
1. https://eod-grss-ieee.com/dataset-search
2. http://eodata.bvlabs.ai/#/pre-processing pipelines, it is non-trivial to adapt modern
deep learning models to RS datasets [26]. As a result, many
cutting-edge and off-the-shelf deep learning methods from
the machine learning community are not evaluated and
compared on RS data. To address the aforementioned prob-
lems, in this study, we ﬁrst make an exhaustive and compre-
hensive review of the publicly accessible RS datasets. Next, a
systematic analysis is undertaken based on the information
about these datasets. Based on the attribute information, we
ﬁlter, rank, and select ﬁve large-scale datasets designed for
general purposes in order to build a new benchmark for
model evaluation. To further enable a fair and reproducible
comparison of different algorithms, we construct a new
deep learning platform, termed EarthNets, as a foundation
for future work. Our main contributions are summarized
below.
1) We review more than 400 datasets published in
the RS community. These datasets are summarized
and categorized into different research tasks and
research domains. Detailed attributes including ten
different aspects are provided for each dataset.
2) Systematic analyses are made with respect to ﬁve
aspects of these datasets to provide insights and
ideas for future research within the RS community.
Speciﬁcally, the volume, bibliometric analysis, reso-
lution distributions, research domains, and dataset
relationship are considered for a comprehensive
dataset analysis.
3) To our knowledge, we are the ﬁrst to measure and
rank existing RS datasets using the dataset attributes
provided in this study. Based on this ranking and se-
lection, a new benchmark is built for the evaluation
of RS methods.
4) We analyze the relationships between different
datasets. The dataset correlation matrix provides a
new perspective to the RS community for the explo-
ration of new algorithms across different datasets.
5) We release an open platform, termed EarthNets, for
EO tasks. EarthNets aims to enable fair compar-
isons, efﬁcient development of methods, and the
greater availability of the RS data to a larger research
community.
The rest of this paper is organized as follows. Section 2
reviews existing RS datasets for different tasks. Section
3 presents the analyses of the reviewed datasets from
ﬁve different perspectives. Section 4 introduces the pro-
posed dataset ranking and selection method, as well as
the benchmark building process. Section 5 describes the
newly released EarthNets platform. Section 6 presents the
benchmarking results and analysis on the ﬁve selected RS
datasets. Section 7 concludes the paper.
2 R EMOTE SENSING DATASET REVIEW
In this section, we review and organize more than 400 exist-
ing public RS datasets into ﬁve parts related to their tasks.
Four of these tasks are common to many RS datasets: image
classiﬁcation, object detection, semantic segmentation, and
change detection. For the purpose of an in-depth analysis of
these datasets, we collect as much detailed information as
3
Publication year100,000
70,000
50,000
30,000
20,000
10,000
7000
5000
3000
2000
1000
700
500
300
200
100
70
50
30
20
10
7
5
3
2
1# Citations
Year @2014
Year @2020
Cite @100Things And Stuff
OIRDSOakland 3DUC Merced
    SZT AKI AirChangeWHU -RS19
Aerial Image Seg
 Massachusetts Roads
  NWPU -V HR10
  UCA S _AOD
EuroSDR Image Matching
Par is -rue-MadameBrazilian Coffee Scene
RSSCN 7
SAT-4 SAT-6
 DFC2015 Zeebruges
  Vehicle Detection in Aerial Imagery (VE D AI )
 Zurich Summer DatasetParking Lot Database (PKLot )
ERM PA IW IQmulus & T erraMobilita Contest
      Brazilian Cerrado -Savanna ScenesNWPU -RESISC 45
RSC11SIRI-WHU (Google + USGS )
SpaceNet -1 (Building Detection v 1)
DSTL Feature Detection (3Band ) DSTL Feature Detection (16Band )COWC
Pasadena Urban TreesBiome : L8 Cloud Cover Validation data
   UCM captionSydney captionCar Parking Lot Dataset (CA RPK )
LEarning , VIsion and Remote (LEV IR )SPA RCS
HD-MapsStanford Drone Dataset
GT-CrossViewA ID
SSDD (RadarSat -2, T erraSAR -X, S-1)
HRSC 2016 (Ship Detection )Remote Sensing Object Detection (RSOD ) RSD46-WHU
RSI-CB256 RSI-CB128SpaceNet -2 (Building Detection v 2) SpaceNet -3 (Road Netw ork Detection )INRIA Aerial Image Labeling
AIST Building Change Detection
  DFC17 (Local Climate Zones Classification )
GT A Birds Eye View  (Surrounding Vehicle Aw areness )WA MI DIRSIG
NOAA Fisheries Steller Sea Lion Population Count
MUSIC 4P3 (Photovoltaic Pow er Plants )MUSIC 4GC ( Golf Course )
    Aerial Image Segmentation Dataset
EvLab -SS DatasetOpenSARShip
Urban 3D Challenge
             RSICDAerial to Map (Pix el2Pix el)
SateHaze 1kThe IARP A Multi -View  Stereo 3DTGRS HRRSD
   Learning Aerial Image Segmentation From Online MapsSemantic 3D.net
Par is -Lille-3D
Okutama -ActionCV USA UCF Cross View  DatasetGETNET dataset
EuroSATPatternNet
SpaceNet -4 (Multi-View  Overhead Imagery )xView
xView 2 (xBD)f MoWDeepGlobe (Road Detection ) DeepGlobe (Building Detection ) DeepGlobe (LandCover Classification )
Onera Satellite Change DetectionDOTA v 1.0
   WHU Building Dataset
Synthetic & Real Dataset
Proba -V Super ResolutionSEN1-2DFC18
38-Cloud
     Aeroscapes ITCV D (Vehicle Detection )
       Gaofen Image Dataset (GID)
  Urban Drone Dataset (UDD)
HyRA NKWHU Building change detection Dataset
    RoadNet
Cactus Aerial PhotosMap Challenge
   MAritime SAT ellite Imagery dataset (MASATI )RIT-18
Aerial Change Detection in Video Games (A ICD )
AerialLanes 18CDD (season -varying )
    WHDLD
DLRSD
Hyperspectral Change Detection Dataset
MSLCCHighD
          BigEarthNet
Bridges DatasetDataset for Ship Classification (DSCR )SAR Ship Detection (GF-2, S-1)
HRSCDSEN12MS
          DOTA v 1.5
DLR-SkyScapes
DLR-A CDSTGAN Cloud RemovalSo2Sat LCZ 42
               USTC _SmokeRSWHU -SEN-City
California flood dataset
  Aerial Imagery for Roof Segmentation (A IRS )
built-structure -count datasetOSCD
       RoadTracer
GF2 Dataset for 3DFGCORSSD
             VisDrone 2019 -DET VisDrone 2019 -V ID VisDrone 2019 -SOT VisDrone 2019 -MOT
SatStereoSatellite Pose Estimation
Kaggle Satellite Images of Hurricane DamageMidAir
    AeroRITSentinel -2 Cloud Detection (A LCD )High-Resolution Cloud Detection (HRC_WHU )
HistAerialOPTIMA L -31
  DIOR
Dataset of thermal and visible aerial images
  T enGeoP -SA Rw vChesapeake Land CoverInteraction Dataset
DublinCitySemanticKITTI
CVA CT
CV4A KenyaTimeSen 2Crop BrazilDAM
BH-Pools + BH-WaterT anksAiRound CV-BrCT (Cross -View  Brazilian Construction T ype )A IR-SARShip -2.0 (GF-3)
LS-SSDD (Large Scale )HRSID (Ship Detection , S-1, T erraSAR -X)
Bijie Landslide
SemCity T oulouseSpaceNet -6 (Multi-Sensor All Weather Mapping )
SpaceNet -7 (Multi-T emporal Urban Development )Agriculture -Vision
DOTA v 2.0iSA ID
RarePlanes
CloudCast
SECONDLEV IR -CD
DFC20
SEN-12-FLOODWHU Cloud DatasetWHU MV S /Stereo Dataset WHU Multi -View  Dataset WHU Stereo Dataset
Aerial Maritime Drone Dataset
ERA (Event Recognition in Aerial videos )AU-A IR
BIRDSA I
95-Cloud
SIMD (Satellite Imagery Multi -vehicles Dataset )OV ERHEA D MNIST
RSOC (Remote Sensing Object Counting )
   Synthinel -1
VA LID
         Kaggle Massachusetts Buildings Dataset
     A ISD
    Zhang et al . CD dataset
DeepWeatherEORSSDDSIFN
Google dataset
    University -1652 : Drone -based Geolocalization (Image Retrieval )
MLRSNetVisDrone
DroneCrow d
A PKLOToriEnted object detection using Aerial imaGery in real -w orLd scEnarios (EA GLE )UAV id
   LandCoverAI
Sentinel -2 Multitemporal Cities PairsHi-UCD
GTA-V  SIDtransmission tow ers and pow er lines (TTPLA )
WHU -Hi-LongKou WHU -Hi-HanChuan WHU -Hi-HongHuMOR -UAV
CLRS
       BreizhCrops
    Sen1Floods 11SEN12MS-CR
Google Data SetLEV IR
AFO - Aerial dataset of floating objects
NEON Tree Crow ns DatasetIntersection Drone Dataset
Roundabouts Drone DatasetGID15
  RSV QA -LR RSV QA -HRT oronto 3D
Sw iss 3DCitiesDA LES
LA SDU
Campus 3DSensatUrbanUniversity -1652
Sen4AgriNetPA S T ISZueriCrop
EuroCropsCropHarvestMillion AID
MultiSceneFGSCR -42Hurricane Wind Speed
benchmark _ISPRS 2021
    NaSC -TG2Aw esome Remote Sensing Relative Radiometric NormalizationSeCo
    GeoLifeCLEF 2021
HTCDS2LookingSYSU -CD
     LoveDA
         WHU TCL SatMV S
  QXS-SA ROPT
DroneVehicle
    Sentinel -2 Cloud Detection (WHUS 2-CD+)FloodNet
TG1HRSSC
S2UCV ISO -DetectionA IR-CD
Search And Rescue
    DENETHOR
        S2MTCP
URB3DCD
       DLR HySU
     AI-TOD - tiny object detection
GTA HRSVQAxBENRSIV QA
AHN Height EstimationHeisenheim 3D
SUM-HelsinkiV IGORMiniFrance -DFC22
  Next Day Wildfire Spread
SAR-A CD MUSIC 4HA (Hot Area )
   ReforesTree
The Auto Arborist Dataset
   GeoNRW
            The WorldStrat
 OMBRIA
    SAR Aircraft Detection Dataset ?SA DD ?
BONA I
   SeaDronesSeeDynamic World
Fig. 2. Chronological overview of the number of citations of existing RS
datasets. UC Merd [27], AID [28] and DOTA [29] are the most cited
works. (Best viewed by zooming in)
Publication year02000
1000
800
600
400
2001800
1600
1400
1200# CitationsClass
Publication yearOD
Publication yearSemSeg
Things And Stuff
OIRDSUC Merced
    WHU -RS19
Aerial Image Seg
 Massachusetts Roads
  NWPU -VHR10
UCAS _AODBrazilian Coffee Scene
RSSCN 7
S AT-4S AT-6
 DFC2015 Zeebruges
  V ehicle Detection in Aerial Imagery (VEDAI )
Zurich Summer DatasetParking Lot Database (PKLot )
ERM PAIW
Brazilian Cerrado -Savanna ScenesNWPU -RESISC 45
RSC 11SIRI-WHU (Google + USGS )
SpaceNet -1 (Building Detection v 1)
DSTL Feature Detection (3Band ) DSTL Feature Detection (16Band )COWC
Pasadena Urban TreesBiome : L8 Cloud Cover V alidation data
Car Parking Lot Dataset (CARPK )
LEarning , VIsion and Remote (LEVIR ) HD-MapsStanford Drone DatasetAID
SSDD (RadarSat -2, TerraSAR -X, S-1)
HRSC 2016 (Ship Detection )Remote Sensing Object Detection (RSOD ) RSD 46-WHU
RSI-CB256 RSI-CB128SpaceNet -2 (Building Detection v 2) SpaceNet -3 (Road Network Detection )INRIA  Aerial Image Labeling
  DFC17 (Local Climate Zones Classification ) WAMI DIRSIGMUSIC 4P3 (Photovoltaic Power Plants ) MUSIC 4GC ( Golf Course )Aerial Image Segmentation Dataset
EvLab -SS DatasetOpenSARShip
   TGRS HRRSD
   Learning Aerial Image Segmentation From Online Maps
EuroSA TPatternNet
SpaceNet -4 (Multi-View Overhead Imagery )xViewDeepGlobe (Road Detection ) DeepGlobe (LandCover Classification )D O TA v 1.0
   DFC18
38-Cloud
     Aeroscapes ITCVD (V ehicle Detection )Gaofen Image Dataset (GID)
Urban Drone Dataset (UDD ) HyRANK
    RoadNet MAritime SA Tellite Imagery dataset (MAS AT I )RIT-18
AerialLanes 18
    WHDLDDLRSDMSLCC
  BigEarthNet
Bridges Dataset Dataset for Ship Classification (DSCR )SAR Ship Detection (GF-2, S-1)
SEN12MS
          D O TA v 1.5
DLR-SkyScapesSo2Sat LCZ 42
       Kaggle Cloud Detection
      USTC _SmokeRS
  Aerial Imagery for Roof Segmentation (AIRS )
built-structure -count dataset
       RoadTracer
GF2 Dataset for 3DFGCORSSD
   VisDrone 2019 -DET VisDrone 2019 -VID Kaggle Satellite Images of Hurricane Damage
    AeroRITSentinel -2 Cloud Detection (ALCD )High -Resolution Cloud Detection (HRC _WHU )
HistAerialOPTIMAL -31
  DIOR
AIR-SARShip -1.0
  Chesapeake Land CoverCV4A  Kenya TimeSen 2Crop BrazilDAM BH-Pools + BH-WaterTanks AiRound CV-BrCT (Cross -View Brazilian Construction Type )AIR-SARShip -2.0 (GF-3)LS-SSDD (Large Scale )HRSID (Ship Detection , S-1, TerraSAR -X)Bijie Landslide
SemCity ToulouseSpaceNet -6 (Multi-Sensor All Weather Mapping )SpaceNet -7 (Multi-Temporal Urban Development )Agriculture -VisionD O TA v 2.0RarePlanesCloudCastDFC20
SEN-12-FLOOD WHU Cloud Dataset Aerial Maritime Drone Dataset E R A (Event Recognition in Aerial videos )AU-AIR95-Cloud SIMD (Satellite Imagery Multi -vehicles Dataset )OVERHEAD MNIST Synthinel -1
         Kaggle Massachusetts Buildings Dataset
     AISD
    EORSSD
   MLRSNetAPKLOT oriEnted object detection using Aerial imaGery in real -worLd scEnarios (EAGLE )UA VidLandCoverAIGTA-V SID transmission towers and power lines (TTPLA ) WHU -Hi-LongKou WHU -Hi-HanChuan WHU -Hi-HongHuMOR -U AV CLRS
 BreizhCrops
    AFO - Aerial dataset of floating objectsGID15
ZueriCropEuroCrops CropHarvestMillion AIDMultiSceneFGSCR -42 NaSC -TG2 GeoLifeCLEF 2021SWIM -Ship
     LoveDA
           DroneV ehicle
    Sentinel -2 Cloud Detection (WHUS 2-CD+) TG1HRSSCS2UCAIR-CD
Search And Rescue
    DENETHOR
        AI-TOD - tiny object detection MiniFrance -DFC22 The Canadian Cropland xView 3-SARNext Day Wildfire Spread
SAR-ACD SynthWakeSAR SODA -A MUSIC 4H A (Hot Area ) Military Aircraft Recognition (MAR20) The Auto Arborist Dataset TreeSatAI RID GeoNRWSEASONETMultiSenGE TimeSpec 4LULC U AVO D 10 Space 2Ground
  Five-Billion -Pixels OpenSentinelMap
    SAR Aircraft Detection Dataset ?SADD ? RoadDetections dataset by Microsoft RSUSSWHU -OHS
Fig. 3. Visualization of the citation information of three different tasks.
This ﬁgure shows that scene recognition and object detection datasets
have more citations than segmentation datasets. However, object de-
tection and semantic segmentation datasets attract more research at-
tention after the year of 2019.
possible for each dataset. Compared with existing review
articles, our work provides richer information on the at-
tributes of the datasets. Speciﬁcally, the following 10 aspects
are considered.
1) Research domain . In order to provide a clear presenta-
tion of these reviewed datasets, we organize them according
to the speciﬁc domains in which they are created. Some
typical research domains include agriculture, building, road,
cloud, land use land cover (LULC), general scenes or objects,
and so on.
2) Publication year . We provide the year of publication
of the dataset, which is useful for the chronological analysis
of these datasets.
3) Number of samples : To measure the dataset scale,
we list the number of samples for each dataset. Note that it
could be the number of images (for image-based datasets),
the number of video clips (for video-based datasets), the
number of points (for point cloud datasets), or the number
of image pairs (for change detection datasets).
4) Size of sample : The size of each sample in the dataset
is also an important factor for measuring the dataset scale.
For images, the size is the height and width. For datasetsdesigned for 3D understanding, the sample size could be
the covered area.
5) Volume : The volume of each RS dataset is also fac-
tored in as a measurement of the dataset scale.
6) Number of Classes : For image classiﬁcation, object
detection, and semantic segmentation tasks, we provide the
number of classes annotated in each dataset.
7) Data modality : For different EO tasks, a wide range of
imaging sensors can be used to build datasets. For example,
some of the data sources of RS datasets may include optical
images, multi-spectral images, hyperspectral images [30],
SAR [31], point cloud [32], and DSM (digital surface model)
[33], [34].
8) Resolution range : The spatial resolutions of RS im-
ages have a high correlation with the image content. The
resolution range is also highly relevant to speciﬁc EO tasks
to which it can be applied.
9) Number of citations : We provide the number of
citations for each dataset to measure its popularity in the
RS community.
10) Dataset link : To facilitate the research, we provide
the download link of each dataset. More detailed informa-
tion can be found on https://earthnets.github.io/.
2.1 RS Image Classiﬁcation Datasets
Image classiﬁcation is a fundamental task in both the CV
and RS communities. With image- or patch-level annota-
tions, RS image classiﬁcation has been employed in many
different real-world applications. In Table 1, 91 RS image
classiﬁcation datasets are reviewed and presented. In order
to facilitate researchers to search and index, we organize
them into different research domains. To be speciﬁc, Table 1
contains 13 agriculture-related datasets [35], [36], [37], [38].
For agriculture-related applications, the images or patches
in the datasets are labeled with binary labels (crop/non-
crop) or crop type labels (up to 348 granular labels). For
general scene classiﬁcation, 16 datasets are presented in the
table. Million-AID the largest of these, contains a million
instances for training and evaluation of scene classiﬁcation
methods. MLRSNet [39], RSD46-WHU [40], and NWPU-
RESISC45 [41] are annotated with more than 40 class labels.
There are 19 datasets for LULC applications in the table.
Among these datasets, multiple types of data sources are
used, including hyperspectral [30], multi-spectral [23], [42],
[43], [44], SAR [45] and RGB data [46], [47].
There are 5 ship-related [48], [49], [50], [51] and 5 ﬂood-
related [52], [53], [54] datasets for RS image classiﬁcation
task. For the cloud-related research domain, 4 datasets are
reviewed in this table. Some speciﬁc domains with fewer
datasets are also presented, like smoke [55], sea lion [56],
solar power plants [57] and wind datasets [58]. It is worth
noting that species classiﬁcation datasets are annotated with
the most semantic labels.
Compared with object detection (with object-level an-
notation) and pixel-level segmentation tasks, agriculture-
related applications are mainly modeled as image classiﬁca-
tion tasks. In contrast, aircraft [31] and ship-related datasets
are built primarily for object detection tasks. For agriculture-
related datasets, data from the Sentinel satellites is mostly
used with a lower spatial resolution to cover larger crop
4
areas. General scene classiﬁcation and LULC are the two
dominant domains in RS image classiﬁcation datasets. From
the bibliometric view, general scene classiﬁcation and LULC
datasets are cited much more often than other research
domains. This indicates that scene classiﬁcation is a heavily
studied research direction in the RS community.
2.2 RS Object Detection Datasets
Object detection has a close relationship with real-world
applications like autonomous driving, video surveillance,
and many other high-level scene understanding tasks. Thus,
a number of widely-read works have been published in the
CV community, like Faster RCNN [111], SSD [112], YOLO
[113], and Transformer-based detectors [114]. For the RS
object detection task, more and larger datasets are also being
published for different EO applications, including aircraft
detection [115], [116], [117], building detection [118], [119],
[120], [121], ship detection [122], [123], [124], [125], vehicle
detection [126], [127], [128], [129], [130], general ground
object detection [29], [131], [132], [133], [134], [135] and other
research domains [136], [137], [138].
In Table 2, 91 RS object detection datasets are reviewed
and organized into 18 different research domains. The most
popular domains for RS object detection tasks are general
object detection with 12 datasets, building detection with 12
datasets, aircraft detection with 8 datasets, ship detection
with 14 datasets, and vehicle-related object detection with
20 datasets. Similar to RS image classiﬁcation tasks, datasets
for general object detection have higher citation numbers.
Since object detection is a task with object-level annotations,
the spatial resolutions of these datasets are usually higher
than those of image classiﬁcation datasets. However, for
objects with large sizes, like ships, the data from Sentinel-1
and Sentinel-2 satellites are also used [139], [140]. For the
detection of trafﬁc objects [141] or other small objects [142],
images captured from Unmanned Aerial Vehicles (UAV) are
usually used.
2.3 RS Semantic Segmentation Datasets
Pixel-level semantic segmentation aims to assign semantic
labels to each pixel of the input image. Compared with
image-level and object-level tasks, interpreting the image
data with semantic maps can provide a more complete un-
derstanding of the scene. In Table 3, we review and present
101 datasets for RS semantic segmentation tasks. Among
them, 25 datasets are built for LULC or general scene seg-
mentation tasks [25], [33], [34], [143], [144], [145], [146], [147],
[148]. There are 18 additional datasets constructed for the
segmentation of general scenes. There are 13 datasets [119],
[157], [158], [159], [160], [161], [162] that are designed for
building extraction with pixel-level annotations. Note that
some of them are constructed for building instance segmen-
tation tasks. In those cases, the buildings are annotated with
both the object-level and pixel-level labels. For road extrac-
tion, 9 datasets [19], [204], [205], [206], [207], [208], [209] are
constructed. Datasets designed for LULC, general scenes,
buildings, and road segmentation dominate the RS semantic
segmentation tasks. For cloud-related applications, there 10
datasets built with lower spatial resolution RS images than
other domains [210], [211], [212], [213], [214]. Furthermore,
SemSegSemSegClass
Class
ODOD
CDCD
OtherOther
InstanceSegInstanceSegO
D
/TrackingOD/TrackingPointCloudSegPointCloudSeg
3D3DCarbonStock
CarbonStockSuper Resolution
Super Resolution
3DOD
3DOD
GeoLocalization
GeoLocalizationVQA
VQA
Class/OD
Class/OD
3DOD/PointCloudSeg
3DOD/PointCloudSegCounting
Counting
Class/SemSeg
Class/SemSegSuperResolution
SuperResolutionSSL
SSL
MonocularHeightEsti…
MonocularHeightEsti…3DOD/PointCloudSeg/…
3DOD/PointCloudSeg/…OD/SemSeg
OD/SemSegSemseg
SemsegPanopticSeg
PanopticSegCloudRemoval
CloudRemovalRegression
RegressionSRSRUnmixing
UnmixingClass/OD/SemSeg
Class/OD/SemSegSementicCDSementicCDPopulationPopulation
Plant/Tree1
Plant/Tree1Super Resolution1
Super Resolution1ADADLand Cover20 Land Cover20General Scenes18General Scenes18Building13Building13Cloud10Cloud10Road9Road9Agriculture8Agriculture8General Objects3General Objects3Roof3Roof3Building,Road2Building,Road2Land Use,Land Cover2Land Use,Land Cover2 Salient Objects2Salient Objects2Traﬃc Scenes1Traﬃc Scenes1Swimming pool,Water Tank1Swimming pool,Water Tank1
Water Body1Water Body1 Shadow1Shadow1Parking1Parking1Transmission Towers,Power Lines1Transmission Towers,Power Lines1
human settlements1human settlements1
Arctic1Arctic1W
ildﬁre1Wildﬁre1Land U
se1Land Use1General S
cenes14General Scenes14
Agriculture13Agriculture13Land C
over12Land Cover12
Plant/Tree6Plant/Tree6Land U
se5Land Use5
Ship5Ship5Flood4Flood4Cloud3Cloud3Vegetation2Vegetation2
Vehicles2Vehicles2Land U
se,Land C
over2Land Use,Land Cover2Land U
se,Construction T
ype2Land Use,Construction Type2
M
ilitary1Military1Forest1Forest1
1Solar Power Plants
1Solar Power Plants1Golf Course
1Golf Course1Iceberg
1Iceberg1Vehicles Attributes
1Vehicles Attributes1Smoke
1Smoke1Tailings Dam
1Tailings Dam1Landslide
1Landslide1Event
1Event1General Objects
1General Objects1General Scenes,Multi-label
1General Scenes,Multi-label1Species
1Species1Wind
1Wind1Urban Village
1Urban Village1Aircraft
1Aircraft1Hot Area
1Hot Area13Vehicles
13Vehicles
12Ship
12Ship
10General Objects
10General Objects
8Aircraft
8Aircraft
4Traﬃc Objects
4Traﬃc Objects
2Building
2Building
2Person
2Person
2Maritime Objects
2Maritime Objects
1Aircraft,Ships,Oilpots
1Aircraft,Ships,Oilpots
1Road
1Road
1Oil Storage Tanks
1Oil Storage Tanks
1Bridge
1Bridge
1Vehicles/Swiming Pool
1Vehicles/Swiming Pool
1Oil Tank,Gas Tank
1Oil Tank,Gas Tank
1Plant/Tree
1Plant/Tree
1Oil Tank
1Oil Tank34Land Change
34Land Change
2Building2Building
2Building Change
2Building Change
1Building Damage1Building Damage
1Flood Change 1Flood Change
13D13D
1CropLand Change1CropLand Change
4Image Captioning4Image Captioning
3Weather3Weather
2Image Translation2Image Translation
2Geo-localization2Geo-localization
23D23D
2Agriculture2Agriculture
1Image Matching1Image Matching
1Chronology1Chronology
1View Translation1View Translation
1Dehazing1Dehazing
1Action Event1Action Event
1Data Fusion1Data Fusion
1Forest1Forest
1Plant1Plant
1Cloud1Cloud
1Pose1Pose
1Image R
egistration1Image Registration
1Geophysical1Geophysical1Poverty1Poverty1Im
age F
usion1Image Fusion1W
ater B
ody1Water Body1Air Q
uality1Air Quality1Soil P
aram
eter1Soil Parameter
9Building9Building2Ship2Ship1General O
bjects1General Objects1Agriculture1Agriculture
8Traﬃ
c O
bjects8Traﬃc Objects1Vehicles,Aircraft1Vehicles,Aircraft1Vehicles1Vehicles1M
aritim
e O
bjects1Maritime Objects
10U
rban 3
D
 P
oint C
loud10Urban 3D Point Cloud5M
ultiview
 3
D5Multiview 3D
Building1
Building1
Urban 3D Point Cloud4
Urban 3D Point Cloud4Vehicles1
Vehicles1
Geo-localization5
Geo-localization5RSVQA4
RSVQA4
VQA,Change Detection1
VQA,Change Detection1Sea lion1
Sea lion1
General Objects1
General Objects1Object Counting1
Object Counting1Plant/Tree1
Plant/Tree1
Urban 3D Point Cloud3
Urban 3D Point Cloud3Crowd Counting2
Crowd Counting2Plant/Tree1
Plant/Tree1Cloud1
Cloud1
Agriculture1
Agriculture1Flood1
Flood1
Land Cover1
Land Cover1
Super Resolution1
Super Resolution1General Scenes1
General Scenes1Land Cover1
Land Cover1Monocular 3D2
Monocular 3D2
Urban 3D Point Cloud1
Urban 3D Point Cloud1Building1
Building1Agriculture1
Agriculture1 General Objects1
General Objects1Cloud1
Cloud1Wind1
Wind1
Super Resolution1
Super Resolution1Unmixing1
Unmixing1Geophysical1
Geophysical1Land Change1
Land Change1Population1Population1Anomaly Objects1 Anomaly Objects1Fig. 4. Research Domains (outer perimeter) Organized by EO Tasks
(inner labels). It can be seen that there are strong correlations between
research domain and EO tasks.
9 agriculture datasets are annotated with pixel-level labels
[149], [215], [216], [217], [218]. From the bibliometric view,
building and road extraction are highly cited domains. This
makes sense because building and road segmentation tasks
are widely used in real-world applications.
2.4 RS Change Detection and Other Tasks
RS change detection aims to quantitatively analyze surface
changes based on remotely-sensed data. It is a critical tool
for real-world applications like damage monitoring and
urban planning. In Table 4, we present 35 datasets that are
built for RS change detection tasks. Most of them focus on
the change of land cover or land use. Several other datasets
are constructed for 3D change detection [219], building-
speciﬁc change detection [220], and ﬂood-related change
detection [221]. Many different data modalities are used for
constructing these datasets, including optical (RGB), point
cloud, hyperspectral, multi-spectral, and SAR. Note that
some datasets like SECOND [222] and Dynamic World [223]
are annotated with pixel-wise semantic labels, not only the
change/non-change binary label.
Apart from RS image classiﬁcation, object detection, se-
mantic segmentation and change detection tasks, we also list
83 datasets constructed for other tasks in the supplementary
materials. For example, image captioning datasets [224],
[225] and visual question answering datasets [226], [227]
combine RS data with natural languages. Multi-view stereo
datasets [228], [229], [230] are used for 3D reconstruction.
There are also datasets used for more sporadic tasks like
geo-localization [231], [232], weather forecasting [233], [234],
oil parameter estimation [235], and wind speed estimation
[58], [236].
5
TABLE 1
Detailed Information about 91 RS Image Classiﬁcation Datasets. These datasets are grouped into 27 different research domains in alphabetical
order. Note that / denotes the missing information; the number of citations are retrieved as of Sep. 2022. The download links for all these datasets
can be found at https://earthnets.github.io.
Domain Name Year #Samples Sample Size #Classes Modailty Resolution Vol.(GB) #Cita.
AgricultureBrazilian Coffee Scene [35] 2015 2876 64 2 RGB 20m 0.004 703
Indian Pines [59] 2015 1 145 16 Hyperspectral 20m 0.0059 /
Salinas [30] 2015 1 365 16 Hyperspectral 3.7m 0.026 /
Crop Type Mapping Ghana [60] 2019 / / 18 Sentinel-1,Sentinel-2,Planet 3 ∼10m 312.54 /
CV4A Kenya [61] 2020 4688 2016x3035 7 Sentinel-2 10m 3.5 9
BreizhCrops [62] 2020 610000 / 9 Sentinel-2,MT 10 ∼60m / 34
CaneSat [63] 2020 1627 10 2 Sentinel-2,MT 10m 0.006 /
CropHarvest [64] 2021 90,480 12 ts 343 Sentinel-1,Sentinel-2,ERA5,DEM 10 ∼60m 20 5
South Africa Crop Type [65] 2021 122736 / 9 Sentinel-1,Sentinel-2 / 82.77 /
DENETHOR [36] 2021 / / 9 Sentinel-1,Sentinel-2 3m 254.5 9
The Canadian Cropland [38] 2022 78536 64 10 Sentinel-2 10m 26 0
Space2Ground [66] 2022 10102 260 2 Sentinel-1,Sentinel-2,RGB 10 ∼60m 0.501 0
Sen4AgriNet [37] 2021 225000 366 158 Sentinel-2 10 ∼60m 10240 1
Aircraft SAR-ACD [31] 2022 4322 64 20 SAR / / 1
CloudSPARCS [67] 2016 80 1000 7 MS,Landsat 30m 1.43 168
Kaggle Cloud Detection [68] 2019 9244 1750 4 RGB / 5.86 0
CloudCast [69] 2020 70080 1229 10 NWP 3km 320.31 6
Sentinel-2 Cloud Mask Catalogue [70] 2020 513 1022 3 Sentinel-2 20m 15.38 /
Event ERA [71] 2020 343680 640 25 RGB Video / 6.3 12
FloodHurricane Damage [52] 2019 16000 128 2 RGB 1m 0.064 33
SEN-12-FLOOD [53] 2020 336 512 2 RGB,SAR,MS 10m 12.2 14
Sen1Floods11 [54] 2020 4831 512 1 Sentinel-1 10m 14.3 40
OMBRIA [72] 2022 3376 256 2 Sentinel-1,Sentinel-2 10m ∼20m 0.19 2
FloodNet [14] 2021 2343 4000 9 RGB 0.015m 2.1 31
Forest Kaggle Planet Forest [73] 2017 150000 256 17 RGB-NIR 5m 32.23 /
General ScenesOVERHEAD MNIST [74] 2020 1000 28 9 Grayscale / 0.017 35
fMoW [24] 2018 523846 / 63 RGB,MS 0.3m 3500 146
UC Merced [27] 2010 2100 256 21 RGB 0.3m 0.3 1808
WHU-RS19 [75] 2012 1013 600 19 RGB 0.5m 0.1 212
RSSCN7 [76] 2015 2800 400 7 RGB / 0.086 441
NWPU-RESISC45 [41] 2016 31500 256 45 RGB 0.2 ∼30m 0.404 176
RSC11 [77] 2016 1232 512 11 RGB / 0.63 75
AID [28] 2017 10000 600 30 RGB 3m 2.4 1028
RSD46-WHU [40] 2017 117000 256 46 RGB 0.5 ∼2m 11 460
PatternNet [78] 2018 30,400 256 38 RGB 0.062 ∼4.693m 1.3 260
OPTIMAL-31 [79] 2019 1860 256 31 RGB / 0.024 373
MLRSNet [39] 2020 109,161 256 46 RGB 0.1 ∼10m 1.254 25
CLRS [80] 2020 15000 256 25 RGB 0.26 ∼8.85m 1.735 16
Million AID [17] 2021 1,000,000 150 ∼550 28 RGB 0.5 ∼153m 133.5 29
NaSC-TG2 [81] 2021 20000 256 10 RGB-NIR 100m / 8
Satellite Image Classiﬁcation [82] 2021 5631 256 4 RGB / 0.023 /
Multi-label Scenes MultiScene [83] 2021 100,000 512 36 RGB 0.3 ∼0.6m 0.85 1
Geophysical Hephaestus [84] 2022 216106 224 6 InSAR / 93.71 0
Golf Course MUSIC4GC (Golf Course) [85] 2017 83431 16 2 MS,Landsat 30m 0.37 12
Hot Area MUSIC4HA (Hot Area) [85] 2022 2511 16 6 Sentinel-2 10m 0.01 1
Iceberg Iceberg Detection [86] 2018 10028 75 2 SAR / 0.295 /
Land CoverSAT-4 [87] 2015 500000 28 4 RGB-NIR 1 ∼6m 7.25 43
SAT-6 [87] 2015 405000 28 6 RGB-NIR 1m 5.65 43
Botswana [30] 2015 1 875 14 Hyperspectral 30m 0.077 /
TiSeLaC [88] 2017 23 2866x2633 9 RGB-NIR,MT 30m / /
Gaofen Image Dataset (GID) [89] 2018 150 7200 15 RGB-NIR 4m 71.1 274
MSLCC [45] 2018 2 5596×6031,8149×5957 4 SAR,MS 10m 0.5 26
BigEarthNet [42] 2019 590326 120 43 Sentinel-1,Sentinel-2 10m, 20m, 60m 121 203
Slovenia Land Cover [90] 2019 940 500 10 Sentinel-2 10m 11.55 /
So2Sat LCZ42 [23] 2019 400673 32 17 Sentinel-1,Sentinel-2 10m 50.59 75
TG1HRSSC [91] 2021 204 512 9 Hyperspectral 5m, 10m, 20m 0.277 4
Land UseSIRI-WHU (Google+USGS) [46] 2016 2400 200 12 RGB 2m 0.7 288
RSI-CB256 [47] 2017 24000 256 35 RGB 0.3 ∼3m 2.2 37
RSI-CB128 [47] 2017 36000 128 45 RGB 0.3 ∼3m 0.88 37
Austin Zoning [92] 2017 3,666 773x961 5 RGB / 0.596 /
HistAerial [93] 2019 42000 25,50,100 7 Grayscale / 7.6 29
AiRound [43] 2020 11753 300 11 RGB,Sentinel-2,Ground,Aerial / 33 5
CV-BrCT [43] 2020 24000 500 9 RGB / 9.2 5
EuroSAT [44] 2018 27000 64 10 Sentinel-2 10m 1.92 32
SenseEarth classify [94] 2020 70000 100 ∼12655 51 RGB 0.2 ∼153m 10.8 /
Landslide Bijie Landslide [95] 2020 2773 200 2 RGB 0.68m 0.51 84
Military MSTAR-8class [96] 1996 9466 368 8 SAR 0.3m 0.444 /
Plant/TreeForest Cover Type [97] 2013 581012 12 7 Tree Attributes / 0.07 /
Pasadena Urban Trees 2016 100,000 / 18 RGB / / 161
Aerial Cactus Identiﬁcation [98] 2019 17000 32 2 RGB / 0.025 /
WiDS Datathon 2019 [99] 2019 11000 256 2 RGB 3m 0.46 /
The Auto Arborist Dataset [100] 2022 2,637,208 1,024 344 RGB,MS / 24 1
TreeSatAI [101] 2022 50381 304x304,6x6 47 Sentinel-1,Sentinel-2,RGB 10m,0.2m 16.3 0
Forest Damages Larch Casebearer [102] 2021 1543 1500 5 RGB UAV 3.3 /
Sea lion NOAA Sea Lion Population Count [56] 2017 950 4900 4 RGB / 96 13
ShipShips in Satellite Imagery [48] 2017 4000 80 2 RGB 3m 0.343 /
MASATI [49] 2018 7389 512 7 RGB 0.08 ∼2m 2.3 93
DSCR [50] 2019 20,675 150 ∼800 7 RGB / / 7
FGSCR-42 [50] 2021 9320 140 ∼800 42 RGB / 4.76 7
SynthWakeSAR [51] 2022 46080 96000 10 SAR 3.3m 4.3 0
Smoke USTC SmokeRS [55] 2019 6225 256 6 RGB 1000m 0.79 49
Solar Power Plants MUSIC4P3 [57] 2017 1280000 16 2 MS,Landsat 30m 4.6 7
Species GeoLifeCLEF 2021 [103] 2021 19,000,000 256 31,435 RGB-IR,MS,LC,DEM 1m,0.3m,0.1m 840 19
Tailings Dam BrazilDAM [104] 2020 769 384 2 RGB 10 ∼60m 57 11
Urban Village S2UC [105] 2021 1714 224 2 RGB 2m 1.8 1
VegetationKennedy Space Center [106] 2015 1 550 13 Hyperspectral 0.18m 0.055 /
Brazilian Cerrado-Savanna [107] 2016 1311 64 4 MS 5m 0.011 17
VehiclesWAMI DIRSIG [108] 2017 55226 64 2 Hyperspectral 0.3m 0.33 48
Kaggle Find a Car Park [109] 2019 3262 1296 2 RGB / 2.75 /
MAFAT-Fine-Grained [110] 2021 4216 / 37 RGB 0.05 ∼0.15m / /
Wind Airbus Wind Turbine Patches [58] 2021 155,000 128 2 RGB,MS 1.5m 1 /
6
TABLE 2
Detailed Information about 91 RS Object Detection Datasets. These datasets are grouped into 18 different research domains in alphabetical order.
Note that / denotes the missing information; the number of citations are retrieved as of Sep. 2022. The download links for all these datasets can be
found at https://earthnets.github.io.
Domain Name Year # samples Size # classes Modality Resolution Volume (GB) # Citations
Agriculture PASTIS [149] 2021 2433 128 18 Sentinel-2 10m 29 15
AircraftPlanesNet [115] 2017 32000 20 2 RGB / 0.4 /
MTARSI (Aircraft) [150] 2019 9385 256 2 RGB / 0.48 /
RarePlanes [151] 2020 713348 512 110 MS,WorldView3 0.3 ∼1.5m 310.55 32
CGI Planes [152] 2021 500 / 2 RGB / 0.7 /
CASIA-aircraft [117] 2021 58,121 399 2 RGB / / /
Airbus Aircraft Detection [116] 2021 109 2560 2 RGB 0.5m 0.092 /
SAR Aircraft [153] 2022 2966 224 2 SAR 0.5m ∼3m 0.18 2
Military Aircraft [154] 2022 3842 800 20 RGB / 1.1 0
Bridge Bridges Dataset [155] 2019 500 4800x2843 2 RGB 0.5m 1.45 6
BuildingSpaceNet-4 (Multi-View) [121] 2018 60000 900 1 MS,WorldView2 0.3m 186 45
DeepGlobe (Building) [118] 2018 24586 650 2 Panchromatic,RGB,MS 0.5m / 470
WHU Building [156] 2018 25577 512 2 RGB 0.3m 24.41 414
CrowdAI Mapping [157] 2018 401,755 300 1 RGB / 5.3 /
Map Challenge [157] 2018 341,058 300 2 RGB / / 23
TBF [158] 2018 13 40,000 2 RGB / / /
Microsoft Building (Australia) [159] 2019 11,334,866 / 2 / / 6.4 /
Microsoft Building (Uganda/Tanzania) [160] 2019 17,942,345 / 2 / / 3.5 /
Microsoft Building (USA) [119] 2019 129,591,852 / 2 / / 34.4 /
Microsoft Building (Canada) [161] 2019 11,842,186 / 2 / / 2.5 /
Urban Building Classiﬁcation [162] 2022 800 600 61 RGB 0.5 ∼0.8m 0.675 /
BONAI [163] 2022 3,300 1,024 1 RGB 0.3m ∼0.6m 4.86 1
GeneralNWPU-VHR10 [131] 2014 800 1000 10 RGB,IRRG 0.08 ∼2m 0.07 1264
RSOD [132] 2017 976 1000 4 RGB 0.3 ∼3m 0.077 460
TGRS HRRSD [133] 2017 21761 10569 13 RGB 0.15 ∼1.2 m 8.6 126
xView [134] 2018 1,413 3,000 60 RGB,MS 0.3m 20 188
fMoW [135] 2018 523846 / 63 RGB,MS 0.3m 3500 146
DOTA v1.0 [29] 2018 2806 4000 15 RGB / 18 1193
DOTA v1.5 [164] 2019 2806 4000 16 RGB / 18 1198
DIOR [18] 2019 23463 800 20 RGB 0.5 ∼30 m 6.93 481
DOTA v2.0 [165] 2020 11268 4000 18 RGB 0.1 ∼0.81 34.3 69
iSAID [166] 2020 2806 4000 15 RGB / 18 110
VALID [167] 2020 6690 1024 30 RGBD / 15.7 15
UAVOD10 [136] 2022 844 1000 ∼4800 10 RGB 0.15m 0.9 0
Human/Animals BIRDSAI [168] 2020 162000 640 10 Thermal UAV@60-120m 3.7 25
Land Covers Dstl Satellite Imagery [137] 2017 57 3,348 10 RGB,MS 0.3m ∼7.5m 21.7 /
Object Counting RSOC (Object Counting) [169] 2020 3057 2500 4 RGB / 0.082 14
Oil Storage TanksOil and Gas Tank (OGST) [170] 2020 10000 512 2 RGB 0.3m 1.87 /
Airbus Oil Storage Detection [138] 2021 103 2560 2 MS 1.2m 0.102 /
Oil Storage Tanks [171] 2019 10000 512 2 RGB 0.5m 3 /
Volcanoes Hephaestus [172] 2022 216106 224 6 InSAR / 93.71 0
PersonSemantic Drone-OD [173] 2019 400 5000 2 RGB / 3.91 /
Stanford Drone [174] 2016 100 1400x1904 6 RGB Video 0.025m 69 616
SeaNOAA Sea Lion Count [56] 2017 950 4,900 4 RGB / 96 13
Aerial Maritime Drone [175] 2020 508 800x600 5 RGB / 0.038 18
SeaDronesSee [176] 2022 5630 3,840 ∼5,456 6 RGB / 60.3 9
AFO-Floating objects [177] 2020 3647 720 ∼3840 6 RGB / 4.7 9
Search/Rescue Search And Rescue [178] 2021 2552 1000 1 RGB 0.5m / 3
ShipOpenSARShip [122] 2017 11346 900 1 Sentinnel-1 10m 1.7 176
SSDD [123] 2017 1160 500 2 SAR 1 ∼15m / 298
HRSC2016 (Ship) [179] 2017 1061 300x300 ∼1500x900 26 RGB 0.4 ∼2m 3.74 203
Kaggle Airbus Ship [124] 2018 192556 768 2 / 1.5m 31.41 /
Airbus Ship Detection [124] 2018 40,000 768 2 RGB / 31.4 /
Ships in Google Earth [125] 2018 794 2000 2 RGB / 2 /
SAR Ship Detection [180] 2019 43819 256 2 SAR 3m, 5m, 8m,10m 0.4 204
AIR-SARShip-1.0 [181] 2019 31 3000 2 SAR 1 ∼3m 0.24 0
AIR-SARShip-2.0 (GF-3) [148] 2020 300 1,000 2 SAR 1 ∼3m 0.22 108
LS-SSDD (Large Scale) [139] 2020 15 20,000 2 Sentinel-1,SAR 0.5,1,3m 7.8 69
HRSID (Ship) [140] 2020 5,604 800 2 Sentinel-1,SAR 0.5 ∼3m 0.58 128
SWIM-Ship [182] 2021 14610 768 2 RGB 0.5 ∼2.5m 12.5 0
CASIA-Ship [183] 2021 1,118 1,680 2 RGB / / /
xView3-SAR [184] 2022 1000 ∼29400x24400 2 Sentinel-1 10m 1500 0
Small ObjectsAI-TOD [141] 2021 28036 800 8 RGB 0.3m ∼30m 42 22
SODA-A [142] 2022 2510 4761×2777 9 RGB / / 0
Trafﬁc ObjectsAU-AIR [185] 2020 32823 1920 8 RGB UAV@30m 2.2 57
HighD [186] 2018 110000 4096x2160 2 RGB / / 519
Interaction Dataset [187] 2019 10,933 / 1 RGB / / 196
Intersection Drone [188] 2020 11,500 4096x2160 5 RGB / / 120
Roundabouts Drone [189] 2020 13,746 4096x2160 8 RGB / / 52
TreeNEON Tree Crowns [190] 2020 11,000 100 million trees 2 RGB / 27.4 3
Forest Damages [191] 2021 1543 1500 5 RGB UAV 3.3 /
VehiclesThings And Stuff (TAS) [126] 2008 30 792 2 RGB 0.5m 0.01 550
OIRDS [192] 2009 900 256 ∼640 5 RGB 0.15m 0.153 30
UCAS AOD [193] 2014 976 1,000 2 RGB / 3.24 232
VEDAI(Vehicle) [127] 2015 1,250 1,024 9 IRGB 0.125m 3.9 350
PKLot(Parking Lot) [194] 2015 12417 1280 2 RGB UAV 4.6 236
COWC [128] 2016 388435 256 2 RGB 0.15m 62.5 265
Car Parking Lot (CARPK) [195] 2016 1448 1280 2 RGB UAV 2 235
DLR3k/DLR-MVDA [129] 2016 20 3744 7 RGB 0.13m 0.162 /
ITCVD (Vehicle) [130] 2018 173 5616 2 RGB 0.1m 12 47
VisDrone2019-DET [196] 2019 10209 2000x1500 10 RGB UAV 2 48
VisDrone2019-VID [196] 2019 40000 3840x2160 5 RGB UAV 14 48
VisDrone2019-SOT [196] 2019 139300 3840x2160 3 RGB Video UAV 68 48
VisDrone2019-MOT [196] 2019 40000 3840x2160 5 RGB Video UAV 14 48
SIMD (Multi-vehicles) [197] 2020 5000 1024 15 RGB UAV@150m 1 6
VisDrone [198] 2020 275,437 1,400 11 RGB UAV 16 46
EAGLE [199] 2020 8820 936 2 RGB 0.05 ∼0.45m / 16
MOR-UAV [200] 2020 10948 1080 1 RGB UAV / 21
DroneVehicle [201] 2021 56,878 840 5 RGB-Infrared UAV@100m 13.09 4
Swimming pool/car [202] 2019 3750 224 2 RGB / 0.12 /
ArtiﬁVe-Potsdam [203] 2021 4800 600 1 MS 0.05m 15.6 2
7
SemSegSemSegClass
Class
ODOD
CDCD
OtherOther
InstanceSegInstanceSegO
D/TrackingOD/TrackingPointCloudSegPointCloudSeg3D3D
3DOD
3DOD
GeoLocalization
GeoLocalizationVQA
VQA
Class/OD
Class/OD
3DOD/PointCloudSeg
3DOD/PointCloudSegCounting
Counting
Class/SemSeg
Class/SemSeg SuperResolution
SuperResolutionSSL
SSL
MonocularHeightEst…
MonocularHeightEst…3DOD/PointCloudSe…
3DOD/PointCloudSe…OD/SemSeg
OD/SemSegSemseg
SemsegPanopticSeg
PanopticSegCloudRemoval
CloudRemovalRegression
RegressionSRSRUnmixing
UnmixingCarbonStock
CarbonStockSuper Resolution
Super ResolutionClass/OD/SemSeg
Class/OD/SemSegSementicCDSementicCDPopulationPopulationADADRGB51 RGB51Hyperspectral6Hyperspectral6Sentinel-26Sentinel-26RGB,MS5RGB,MS5MS5MS5RGB,nDSM4RGB,nDSM4RGB-NIR3RGB-NIR3Panchromatic,RGB,MS2Panchromatic,RGB,MS2
MS,MT2MS,MT2Sentinel-1,Sentinel-22Sentinel-1,Sentinel-22 RGB,DSM,LiDAR1RGB,DSM,LiDAR1MS,Hyperspectral,RGB1MS,Hyperspectral,RGB1
MS,SAR1MS,SAR1MS,LiDAR1MS,LiDAR1RGBD,Odometry1RGBD,Odometry1RGB,Hyperspectral1RGB,Hyperspectral1MS,Sentinel-21MS,Sentinel-21
SAR,RGB1SAR,RGB1SAR,MS,Hyperspectral1SAR,MS,Hyperspectral1
RG-NIR1RG-NIR1Elevation,Tem
p,W
ind,Drought,Hum
idity,P…
1Elevation,Temp,Wind,Drought,Humidity,P…1
SAR1SAR1RGB,Sentinel-21RGB,Sentinel-21Hyperspectral(32 b
ands)1Hyperspectral(32 bands)1
RGB37RGB37Hyperspectral7Hyperspectral7
RGB-N
IR5RGB-NIR5Sentinel-25Sentinel-25Sentinel-1,Sentinel-25Sentinel-1,Sentinel-25
SAR4SAR4M
S,Landsat3MS,Landsat3
G
rayscale2Grayscale2Sentinel-2,MT2 Sentinel-2,MT2
2RGB,MS
2RGB,MS2Sentinel-1,Sentinel-2,RGB
2Sentinel-1,Sentinel-2,RGB1Tree Attributes
1Tree Attributes1MS
1MS1RGB-NIR,MT
1RGB-NIR,MT1SAR,MS
1SAR,MS1Sentinel-1,Sentinel-2,Planet
1Sentinel-1,Sentinel-2,Planet1RGB,Sentinel-2,Ground,Aerial
1RGB,Sentinel-2,Ground,Aerial1NWP
1NWP1RGB,SAR,MS
1RGB,SAR,MS1RGB Video
1RGB Video1MS,Thermal,RGB
1MS,Thermal,RGB1Sentinel-1
1Sentinel-1
1Sentinel-1,Sentinel-2,ERA5,DEM
1Sentinel-1,Sentinel-2,ERA5,DEM
1RGB-IR,MS,LC,DEM
1RGB-IR,MS,LC,DEM44RGB
44RGB
5SAR
5SAR
2Sentinel-1,SAR
2Sentinel-1,SAR
1RGB,IRRG
1RGB,IRRG
1IRGB
1IRGB
1RGB Video
1RGB Video
1Sentinnel-1
1Sentinnel-1
1Panchromatic,MS
1Panchromatic,MS
1RGB,MS
1RGB,MS
1MS,WorldView3
1MS,WorldView3
1RGB-Infrared
1RGB-Infrared
1MS1MS
1Sentinel-1
1Sentinel-122RGB22RGB
6MS6MS
3Hyperspectral3Hyperspectral
3Sentinel-2 3Sentinel-2
2RGB,SAR,MS 2RGB,SAR,MS
1RGB-NIR1RGB-NIR
1RGB,MS1RGB,MS
1MS,Hyperspectral,RGB1MS,Hyperspectral,RGB
1RGB,NIR1RGB,NIR
1MS,MT1MS,MT
1PointCloud1PointCloud
15RGB15RGB2Weather Attributes2Weather Attributes
2Sentinel-1,Sentinel-22Sentinel-1,Sentinel-2
2Sentinel-22Sentinel-2
1LiDAR1LiDAR
1Sentinel-2,RGBIR1Sentinel-2,RGBIR
1Grayscale1Grayscale
1RGB,Thermal1RGB,Thermal
1SAR1SAR
1Panchromatic1Panchromatic
1SAR,RGB1SAR,RGB
1MS1MS1/1/1Sentinel-11Sentinel-11Hyperspectral1Hyperspectral
12RGB12RGB1Sentinel-21Sentinel-2
8RGB8RGB2RGB V
ideo2RGB Video1Therm
al1Thermal
8PointCloud8PointCloud1Panchrom
atic,M
S,PointCloud1Panchromatic,MS,PointCloud1M
esh1Mesh4RG
B4RGB1M
S,LiD
AR1MS,LiDARPanchromatic1 Panchromatic1PointCloud4
PointCloud4
RGB,PointCloud1
RGB,PointCloud1RGB5
RGB5RGB3
RGB3
Sentinel-22
Sentinel-22RGB3
RGB3
RGB,MS1
RGB,MS1
PointCloud3
PointCloud3RGB2
RGB2
RGB-NIR1
RGB-NIR1Sentinel-22
Sentinel-22RGB1
RGB1RNIR1
RNIR1
Sentinel-1,Sentinel-21
Sentinel-1,Sentinel-21Sentinel-21
Sentinel-21
Sentinel-1,Sentinel-21
Sentinel-1,Sentinel-21SyntheticRGB1
SyntheticRGB1RGB,nDSM1
RGB,nDSM1PointCloud1
PointCloud1
Panchromatic,RGB,MS1
Panchromatic,RGB,MS1RGB-NIR1
RGB-NIR1RGBD1
RGBD1
Sentinel-1,Sentinel-21
Sentinel-1,Sentinel-21LWIR1
LWIR1RGB-NIR1
RGB-NIR1Hyperspectral1
Hyperspectral1RGB1RGB1MS,Sentinel-2,RGB1
MS,Sentinel-2,RGB1InSAR1InSAR1Sentinel-21Sentinel-21DEM,Sentinel-21DEM,Sentinel-21RGB1RGB1
Fig. 5. Data Modalities (outer perimeter) Organized by EO tasks (inner
labels). Although there is a wide range of data sources used for EO,
optical data (RGB) is still the most used modality for the majority of RS
tasks.
Resolution400
200
100
70
50
30
20
10
8
5
3
2
1# classesClass
ResolutionOD
ResolutionSemSeg
Fig. 6. Visualization of The Relationships Between Data Resolution and
The Number of Annotated Classes. An interesting ﬁnding is that most
datasets have the resolution that is either smaller than 1m or larger than
10m. Datasets with resolution range of between 1 to 10m are obviously
scarce.
3 R EMOTE SENSING DATASET ANALYSIS
In this section, we analyze the reviewed more than 400
RS datasets and provide statistics related to ﬁve different
aspects of these datasets.
3.1 The Volume Trend
Thanks to their powerful representation learning capa-
bilities, deep learning networks trained with large-scale
datasets have shown performance that is superior to that
of classical machine learning methods. In the deep learning
era, large-scale datasets play an important role in training
deep models that yield better performance as well as better
generalizability. Another advantage of large-scale datasets
is that they align better with real-world scenarios. In Fig. 1,
we visualize a chronological overview of the volume of 401
0%100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
5.56%8.33%23.96%62.5%
51.22%
46.15%60%
30.99%72.22%
25%32.29%18.75%
25.82%22.22% 66.67% 43.75% 18.75% 43.9% 53.85% 40% 43.19%
Fig. 7. Visualization of the task distributions with regard to different
spatial resolutions. Datasets with UAV images are mostly constructed
for object detection. Datasets with the resolution range of 1m to 30m
are mostly designed for image classiﬁcation and semantic segmentation
tasks.
RS datasets. Note that the volume (in GBs) shown in this
ﬁgure is transformed into the logarithmic scale, and larger
circles indicate larger volumes. It can be clearly seen that
datasets before the year 2015 are usually smaller in volume.
Similar to the CV community, after deep learning became
the mainstream technique, both the number and the volume
of RS datasets signiﬁcantly increase. For example, the vol-
umes of fMoW [135] and Sen4AgriNet [37] are greater than
4000 GB. Well-annotated large scale datasets can greatly
help the RS community in developing and evaluating more
powerful deep learning models with better performance
and generalizability.
3.2 Bibliometric Analysis
Citation information from 2008 to 2022 is shown in a chrono-
logical order in Fig. 2. The number of citations was collected
as of Sep. 2022. From the ﬁgure it can be seen that the
dataset with the highest number of citations is Aerial2map
[237]. Aerial2map is a dataset for image translation, and the
pixel2pixel algorithm is the reason for its high number of
citations. In general, Fig. 2 shows that datasets published
during the year range of 2014 to 2020 have a higher number
of citations than those before or after.
UC Merced [27], AID [28] for scene classiﬁcation, NWPU
VHR10 [41], and DOTA [29], [164] for object detection are
datasets with a particularly high number of citations. After
the year of 2020, we can see that the VQA dataset [226]
and SeCo dataset [238] for self-supervised learning have
attracted increasing research attention in the RS community.
In Fig. 3, we display the citation information of three
different tasks: the RS image classiﬁcation, object detection,
and semantic segmentation. From this ﬁgure we can clearly
see that scene recognition and object detection datasets
have more citations than segmentation datasets. However,
there are more object detection and semantic segmentation
datasets published after the year of 2019. This indicates that
object-level and pixel-level understanding of RS images is
becoming increasingly popular in the RS community.
8
Fig. 8. Visualization of the correlation between different datasets. Lighter
color means higher correlation. For the ﬁrst time, we propose to analyze
the correlations between different RS datasets based on their attribute
information collected in this study (Best viewed in color).
3.3 Analysis of Data Modalities
To analyze the image sources used in the RS community, we
summarize and visualize the data modalities used for dif-
ferent RS tasks. The relationships between data modalities
and RS tasks are shown in Fig. 5. Although there is a wide
range of image sources, optical data (RGB) is still the most
frequently used modality for the majority of RS tasks. In Fig
4, we display the relationships between tasks and research
domains.
3.4 Analysis of Spatial Resolutions
For RS images, spatial resolution has a high correlation to
image content. In Fig. 6, we show the relationships between
data resolution and the number of annotated classes. In
general, this ﬁgure clearly shows that the number of se-
mantic classes for RS image classiﬁcation tasks is higher
than for RS object detection and segmentation tasks. The
reason is that object-level and pixel-level datasets require
much more annotation efforts when the number of semantic
classes increases.
Another interesting ﬁnding is that most datasets have
a resolution of smaller than 1m or larger than 10m. The
datasets with resolution in the range between 1 to 10m are
obviously scarce. The reason for this phenomenon is that
many EO applications require either high-resolution ( <1m)
imagery or global coverage (Sentinel 1&2, >10m). However,
the EO data with resolution 1 ∼10m also has great potential
in a range of applications. More research attention should
be devoted to ﬁlling in this gap.
The task proportion distribution with regard to different
spatial resolutions is displayed in Fig. 7. As shown in this
ﬁgure, datasets with UAV images are mostly constructed
for object detection. More than 60% of the datasets withvery high-resolution ( <0.1m) are designed for RS semantic
segmentation task. For datasets with a resolution range of
1m to 30m, RS image classiﬁcation and semantic segmen-
tation have the largest proportion of tasks. Less than 20
percent of the datasets are built for RS object detection in
this resolution range.
3.5 The correlation between different datasets
To provide a more global view of the 401 RS datasets, for the
ﬁrst time, we propose to analyze the correlation between dif-
ferent datasets based on the attribute information provided
in this study.
We treat the attribute information of each dataset as a
data sample, and measure the similarity between different
datasets. Formally, let nrepresent the number of samples,
andsrepresent the size of each sample in dataset D. We
denotevas the volume of D. Then the scale of Dcan be
quantitatively measured using n,s, andv. Furthermore, we
quantify the annotation level of Dand represent it using m.
Speciﬁcally, we assign 1 to mfor image-level annotation, 2
for object-level, 3 for pixel-level, 4 for instance-level, 5 for
panoptic-level, and 0 for no-label. Similarly, we also quan-
tify the task of Dtot. According to the task type, tcan be
1 for RS image classiﬁcation, 2 for object-detection, 3 for se-
mantic segmentation, 4 for change detection and 0 for other
tasks. Then we use cto represent the number of annotated
classes in dataset D, andrto denote the max resolution
of samples in D. With these deﬁnitions, n,s,v,m,c,r,t are
numerical values representing the attributes of D.
Since the research domain of a dataset is provided by
a word or phrase, it is non-trivial to measure the distance
between them. For example, the research domain “Ship”
should be closer to “Sea” than “Tree” or “Aircraft.” The
research domain “Tree” should be more similar to “Forest”
not “Building.” To this end, we propose to use the pre-
trained word embedding [326] models to compute the real-
valued vector feature dfor each research domain. Here we
denotedas the textual features of the domain.
Following these pre-processing pipelines, we are able to
quantify the attributes of dataset Dinto two feature vectors:
F= [n,s,v,m,c,r,t ]and the word embeddings for the
research domain d. For two datasets D1andD2, we useF1,
F2andd1,d2to represent the features of these two datasets.
Then, we can compute the similarity using the following
formula:
cos(θ1) =F1·F2
∥F1∥2∥F2∥2,
cos(θ2) =d1·d2
∥d1∥2∥d2∥2,
sim(D1,D2) = cos(θ1) + cos(θ2).(1)
In Fig. 8, the correlation matrix between 401 datasets is
visualized. The lighter the color, the higher the similarity.
To our knowledge, this is the ﬁrst work that analyzes the
correlation between all existing RS datasets. The correlation
matrix reﬂects the distances between different pairs of the
RS dataset. The distance information could be valuable
for the RS community. Some possible ways to leverage
the correlation between RS datasets for future research are
outlined below.
9
TABLE 3
Detailed Information of 101 RS Semantic Segmentation Datasets. These datasets are grouped into 19 different research domains in alphabetical
order. Note that denotes the missing information; the number of citations are retrieved as of Sep. 2022. The download links for all these datasets
can be found at https://earthnets.github.io.
Domain Name Year #Samples Sample Size #Classes Modailty Resolution Vol.(GB) #Cita.
AgricultureAgricultural Crop Cover [215] 2018 40 / 2 MS 30m 4.4 /
GF2 Dataset for 3DFGC [216] 2019 11 2,652 5 RGB-NIR 4m 0.056 22
TimeSen2Crop [217] 2020 1000000 10980 16 Sentinel-2 10m 1.1 11
Agriculture-Vision [218] 2020 94986 512 9 RGB-NIR 0.1 ∼0.2m 4.4 81
WHU-Hi-LongKou [239] 2020 1 550x400 9 Hyperspectral 0.463m / 2
WHU-Hi-HanChuan [239] 2020 1 1217x303 16 Hyperspectral 0.109m / 2
WHU-Hi-HongHu [239] 2020 1 940x475 22 Hyperspectral 0.043m / 2
ZueriCrop [240] 2021 28000 24 48 Sentinel-2 10m 39 24
EuroCrops [241] 2021 805,401 0.5 ha 43 Sentinel-2 10m 8.6 2
Arctic Arctic Sea Ice Image Masking [242] 2021 3392 357x306 8 RG-NIR 10m 0.092 /
BuildingSpaceNet-1 (Building) [243] 2016 9735 650 2 RGB,MS 0.5 ∼1m 31 231
SpaceNet-2 (Building) [243] 2017 24586 650 2 RGB,MS 0.3m 104 231
INRIA Aerial Image Labeling [205] 2017 360 1500 1 RGB 0.3m 19.5 494
built-structure-count dataset [244] 2019 5364 512 1 RGB 0.3m 2.23 12
SpaceNet-6 (Multi-Sensor All Weather) [245] 2020 3401 900 2 SAR,RGB 0.5m 55.9 51
SpaceNet-7 (Multi-Temporal Urban) [246] 2020 1525 1024 2 MS,MT 4m 20.1 18
Synthinel-1 [247] 2020 2108 572 2 RGB 0.3m 0.977 39
Kaggle buildings segmentation [120] 2020 6038 256 2 RGB / 0.899 /
Kaggle Massachusetts Buildings [248] 2020 151 1500 2 RGB 1m 2.93 580
Open Cities AI Challenge [249] 2020 11,000 1,024 2 RGB 0.03 ∼0.2m 81.5 /
Mini Inria Aerial Image Labeling Dataset [205] 2021 32,500 512 2 RGB 0.3m / /
High-speed Rail Line Building Dataset [250] 2021 336 2000 2 RGB 0.5m / /
AIS From Online Maps [251] 2017 1671 3000 2 RGB 0.5m 23.8 243
CloudBiome: L8 Cloud Cover [210] 2016 96 / 4 RGB 30m 96 599
38-Cloud [211] 2018 17601 384 2 RGB 30m 13 73
Sentinel-2 Cloud Detection (ALCD) [252] 2019 38 1830 2 MS,Sentinel-2 10 ∼60m 0.234 141
HRC WHU [213] 2019 150 1280x720 2 RGB 0.5 ∼15m 0.17 165
WHU Cloud Dataset [253] 2020 859 512 2 RGB 30m 3.56 25
95-Cloud [212] 2020 34701 384 2 RGB 30m 18 12
WHUS2-CD+ [214] 2021 36 10980 2 Sentinel-2 10m 27.8 13
AIR-CD [254] 2021 34 7300 2 RGB-NIR 4m 13 93
The Azavea Cloud Dataset [255] 2021 32 / 2 Sentinel-2 10m ∼60m / /
Sentinel-2 Cloud Cover [256] 2022 22728 / 2 MS 10m ∼60m 51.2 /
General ObjectsDLRSD [257] 2018 2100 256 17 RGB 0.3m 0.004 70
Kaggle aerial segmentation [147] 2020 72 800 6 RGB / 0.033 /
AIR-PolSAR-Seg [148] 2022 2000 512 6 SAR 8m 0.609 /
General ScenesISPRS 2D - Potsdam [33] 2011 38 6000 6 RGB,nDSM 0.05m 15.625 /
ISPRS 2D - Vaihingen [34] 2011 33 2200 6 RGB,nDSM 0.09m 16.6 /
Aerial Image Segmentation [258] 2013 80 512 2 RGB 0.3 ∼1m 0.007 33
DFC2015 Zeebruges [259] 2015 7 100,000 8 RGB,DSM,LiDAR 0.05m 0.0024 70
Zurich Summer Dataset [260] 2015 20 1000 8 RGB-NIR 0.61m 0.38 7
DSTL Feature Detection (3Band) [261] 2016 450 3391 10 RGB 0.31m 13.82 174
DSTL Feature Detection (16Band) [261] 2016 1350 3391 10 MS 1.24m,7.5m 7.84 174
EvLab-SS Dataset [262] 2017 60 4500 11 RGB 0.1m,0.25m / 42
SynthAer [263] 2018 765 1280 8 RGB / 0.977 /
Aeroscapes [264] 2018 3269 1280 11 RGB UAV@5-50m 0.73 48
Urban Drone Dataset (UDD) [265] 2018 301 4,096 6 RGB UAV 1.1 18
RIT-18 [144] 2018 3 9393x5642,8833x6918,12446x7654 18 MS 0.047m 1.5 297
Semantic Drone Dataset-SemSeg [173] 2019 400 5000 20 RGB / 3.91 /
DroneDeploy [266] 2019 55 6,000 7 RGB 0.1m / /
MidAir [267] 2019 420000 1024 12 RGBD,Odometry / 1000 38
AeroRIT [268] 2019 1 3975x1973 6 RGB,Hyperspectral 0.4m 1.8 18
SemCity Toulouse [269] 2020 16 3500 8 MS 0.5 ∼2m 8.8 10
UAVid [270] 2020 420 4000 8 RGB UAV 5.88 75
Settlements DFC21-DSE [271] 2021 98 800 2 SAR,MS,Hyperspectral 10 ∼750m 18 /
Land CoverWashington DC MALL [143] 2013 1 1,280 7 Hyperspectral / 0.14 /
Pavia Center [30] 2011 1 1096 9 Hyperspectral 1.3m 0.121 /
Pavia University [30] 2011 1 610 9 Hyperspectral 1.3m 0.032 /
DeepGlobe (LandCover) [272] 2018 1146 2448 7 RGB 0.5m 2.96 470
HyRANK [273] 2018 5 1,000 14 Hyperspectral 30m 0.4 5
WHDLD [257] 2018 4940 256 6 RGB 2m 0.102 80
SEN12MS [146] 2019 541,986 256 17 MS,SAR 10m 510 119
Urban Semantic 3D (DFC19) [274] 2019 2783 1024 6 MS,LiDAR 0.3 ∼1.3m 285 /
XiongAn [275] 2019 1 3,750 19 Hyperspectral 0.5m 3 /
Chesapeake Land Cover [276] 2019 100000 224 6 RGB,MS 1m 404.95 52
DFC20 [277] 2020 180662 256 10 Sentinel-1,Sentinel-2 10m 9.6 121
BDCI2020 [278] 2020 145,981 256 7 RGB 2m 1.3 /
LandCoverAI [145] 2020 41 9000 3 RGB 0.25m,0.5m 1.4 44
GID15 [89] 2020 150 6800x7200 15 RGB,MS 4m 18 270
LoveDA [279] 2021 5,987 1,024 7 RGB 0.3m 9.6 21
MiniFrance-DFC22 [280] 2022 2322 2000 15 RGB 0.5m 93 15
GeoNRW [281] 2022 7783 1000 10 RGB,nDSM 1m 32 8
SEASONET [25] 2022 1759830 120 33 Sentinel-2 10m 229 0
TimeSpec4LULC [282] 2022 / 262 months 29 MS,MT 500m 60 0
Five-Billion-Pixels [283] 2022 150 7200×6800 24 RGB,MS 4m 104 0
WHU-OHS [284] 2022 7795 512 24 Hyperspectral 10m 94.9 0
Land UseOpenSentinelMap [285] 2022 137045 192, 96 15 RGB,Sentinel-2 10m ∼60m 455 0
DFC18 [286] 2018 10,798 2,001 20 MS,Hyperspectral,RGB 0.05 ∼1m 10.1 136
MultiSenGE [287] 2022 8157 256 14 Sentinel-1,Sentinel-2 10m 530 0
Parking APKLOT [288] 2020 501 / 2 RGB / 3 6
RoadMassachusetts Roads [248] 2013 1171 1500 1 RGB 1m 10.56 580
ERM PAIW [207] 2015 41 4000 2 RGB 0.3m 0.635 117
HD-Maps [206] 2016 20 4000 5 RGB 0.3m 0.146 133
SpaceNet-3 (Road ) [243] 2017 3711 1300 2 Panchromatic,RGB,MS 0.3 ∼1.24m 106 231
RoadNet [204] 2018 20 / 2 RGB 0.21m 0.905 89
AerialLanes18 [289] 2018 20 5616 1 RGB 0.125m 0.0014 1
SpaceNet-5 (Road Network) [243] 2019 2369 1300 2 Panchromatic,RGB,MS 0.3m 84 /
SpaceNet-8 (Flooded Road) [290] 2022 / 1300 4 Panchromatic,RGB 0.3 ∼0.8m / /
RoadTracer [208] 2019 3,000 4,096 1 RGB 0.6m / 192
Microsoft RoadDetections [209] 2022 20000 1088 1 RGB 1m 9.25 0
RoofAIRS [291] 2019 1047 10000 1 RGB 0.075m 17.6 73
Open AI Challenge: Caribbean [292] 2019 7 52,318 5 RGB 0.04m / /
RID [293] 2022 2000 / 16 RGB 0.1m 1.5 0
Salient ObjectsORSSD [294] 2019 800 500 8 RGB / 0.026 104
EORSSD [294] 2020 2,000 500 2 RGB / 0.06 74
Shadow AISD [295] 2020 514 512 2 RGB / 0.29 25
Water Tank BH-Pools+WaterTanks [296] 2020 350 3000 2 RGB / 1.9 4
Trafﬁc Scenes DLR-SkyScapes [297] 2019 16 4680 31 RGB 0.13m / 52
Power TTPL [298] 2020 1100 3840 3 RGB UAV 4.2 19
Water Body Kaggle Water Bodies [299] 2020 2841 1000 2 RGB / 0.28 /
Wildﬁre Next Day Wildﬁre Spread [300] 2022 18,445 64 2 Multi-source 1000m 4 57
10
TABLE 4
Detailed Information of 35 RS Change Detection Datasets. These datasets are grouped into 5 different research domains in alphabetical order.
Note that / denotes the missing information and the number of citations are retrieved as of Sep. 2022. The download links for all these datasets
can be found at https://earthnets.github.io.
Domain Name Year #Samples Sample Size #Classes Modailty Resolution Vol.(GB) #Cita.
3D URB3DCD [219] 2021 50 / 2 PointCloud 0.5 pm 1.5 4
BuildingAIST Building Change Detection [301] 2017 16950 160 2 RGB 0.4m 17.77 87
WHU Building change detection [156] 2018 2 15354×32507 2 RGB 0.075m 5 409
LEVIR-CD [302] 2020 637 1024 2 RGB 0.5m 2.64 224
xView2 (xBD) [303] 2018 22068 1024 4 RGB 0.5m 51 211
CropLand CropLand Change Dection (CLCD) [304] 2022 600 512 2 RGB 0.5 ∼2 m 0.5 0
Flood California ﬂood dataset [221] 2019 1 1534×808 2 RGB,MS 5m,30m 0.33 50
Land ChangeSZTAKI AirChange [305] 2008 13 800 2 RGB 1.5m 0.04 193
Taizhou Data [306] 2014 1 400 4 MS 30m / /
Kunshan Data [306] 2014 1 800 3 MS 30m / /
Cross-sensor Bastrop [307] 2015 4 444x300,1534x808 2 MS 30m,120m / /
GETNET dataset [308] 2018 1 463x241 2 Hyperspectral 30m 0.05 297
Onera Satellite CD [309] 2018 24 600 2 Sentinel-2 10m 0.48 186
AICD [310] 2018 1000 800 2 RGB / 1.7 84
CDD (season-varying) [311] 2018 16000 256 2 RGB 0.03 ∼0.1m 2.7 149
Hyperspectral CD [312] 2018 3 984x740,600x500,390x200 5 Hyperspectral 30m 1.7 33
HRSCD [309] 2019 291 10000 5 RGB 0.5m 5 86
MtS-WH [313] 2019 190 150 2 RGB-NIR 1m 0.43 /
SECOND [222] 2020 4662 512 6 RGB 0.5 ∼3m 2.2 4
Zhang et al. CD dataset [314] 2020 4 1431×1431,458×559,1154×740 2 RGB,NIR 2m,2.4m,5.8m 0.1 74
DSIFN [315] 2020 3,988 512 2 RGB 10m 0.46 152
Hermiston City Oregon [316] 2018 1 390x200 5 Hyperspectral 30m / /
Hi-UCD [317] 2020 1293 1024 9 RGB 0.1m / 19
Google Data Set [318] 2020 19 1000 ∼5000 2 RGB 0.55m 0.6 57
DFC21-MSD [271] 2021 2250 4000 15 MS,MT 1 ∼30m 325 /
Relative Radiometric Normalization [319] 2021 7 300 ∼5000 2 MS 0.31m,0.4m,10m,20m,30m,60m 1 10
HTCD [320] 2021 2 11 K×15 K,1.38 M×1.04 M 2 RGB 0.5971m, 0.074m 1.74 4
S2Looking [220] 2021 5000 1,024 2 RGB 0.5 ∼0.8m 10.21 12
SYSU-CD [321] 2021 20,000 256 2 RGB 0.5m 5.17 89
WH-MAVS [322] 2021 47,134 200 15 RGB 1.2m / /
S2MTCP [323] 2021 1520 600 / MS 10m 10.6 18
Dynamic EarthNet Challenge [324] 2021 22500 1024 7 RGB 3m / 0
MSBC [325] 2022 3,769 256 2 RGB,SAR,MS 2m 3.9 0
Dynamic World [223] 2022 / / 9 Sentinel-2 10m / 14
MSOSCD [325] 2022 5,107 256 2 RGB,SAR,MS 10 ∼60m 2.7 0
Detection
3DCounting
1 2
3 4Change 
DetectionBuilding
Road
DetectionMulti -view
Scene
3DForestChange 
Detection
Fig. 9. With the correlation matrix, we can visualize the RS datasets
using network graphs. The node represents the RS dataset, and the
link between nodes denotes the similarity between nodes. Datasets
gradually cluster together when the connecting threshold decreases.
1)Dataset recommendation . Based on the relation-
ships between datasets and given one dataset, we
are able to recommend similar and related datasets.
This will help researchers ﬁnd desired datasets for
their research tasks.
2)Domain adaptation . Domain adaption aims to im-
prove the performance of a model on a target do-
main using the knowledge learned in the source
domain. With the correlation map, researchers can
easily ﬁnd the proper source and target datasets for
developing novel domain adaptation algorithms.
3)Dataset assembling . The distance between datasets
can also be used to assemble multiple small but
similar datasets into a larger one for training large-
scale deep networks.4)Multi-task model training . Similarly, using the
distance between datasets, we can also combine
datasets with similar spatial resolution, data modal-
ities or research domains, but different tasks into a
uniﬁed dataset for training multi-task deep models.
Furthermore, with the correlation matrix, we can visual-
ize the RS datasets using an interactive network graph. The
node represents the RS dataset, and the link between nodes
denotes the similarity between them. In Fig. 9, we can see
that different datasets gradually cluster together when the
connecting threshold decreases.
4 D ATASET RANKING AND BENCHMARK BUILDING
Researchers in the RS community have been publishing
more and more datasets to beneﬁt the development of
new methods. However, algorithms can easily saturate their
performance on these datasets [327]. Deep learning models
can achieve almost perfect performance on small-scale or
domain-speciﬁc datasets. However, small-scale datasets are
more likely to have bias and cannot reﬂect the performance
of methods in real-world complex scenarios [17]. Methods
developed on small datasets or speciﬁc domains are dif-
ﬁcult to generalize to other scenarios. Considering these
disadvantages, it is urgent to employ new benchmarks with
large-scale, general research domain, and datasets with high
quality annotation for a fair and consistent evaluation of
RS methods. Although the attributes of a large number of
datasets are provided, it is still not intuitive to compare
the quality of different datasets. Thus, for the ﬁrst time, in
this study, we propose to rank these datasets based on their
attributes.
11
4.1 Dataset Ranking Metrics
Regarding the desirable properties of benchmark datasets,
Long et al. [17] propose the DiRS formula, so named for its
focus on the diversity, richness, and scalability of datasets.
These properties are good references for designing metrics
to measure and rank the RS dataset. However, it is non-
trivial to quantitatively measure the diversity and richness
of existing datasets. In order to approximate the DiRS met-
ric, we consider both data diversity and annotation diversity
in this study.
To measure the data diversity, we ﬁrst examine the
research domain of one dataset. Some datasets constructed
with speciﬁc domains will have limitations on the diversity
of data sources. Hence, we ﬁrst ﬁlter them and only keep
datasets designed for general purposes, like LULC or gen-
eral scene understanding. Next, we choose to measure the
dataset scale using the number, size of samples, and the
volume of the dataset, i.e., attribute variables n,s,v . Fur-
thermore, we take the modality diversity, that is, the number
of data modalities k, into consideration. Since images with
higher spatial resolution can provide richer visual content,
we also factor in the resolution ras a part of the metric.
Considering the annotation richness, we use the number
of annotation classes cand the quantiﬁed annotation level m
to measure the richness of the labels. To start with, given 401
RS datasets, we ﬁrst ﬁlter out datasets designed for speciﬁc
domains. After this step, 114 datasets remain as candidates.
Then, we use the aforementioned attributes to quantitatively
measure the diversity and richness of each dataset. Since
there exist signiﬁcantly high values in the n,s,v of different
datasets, we use log normalization to standardize them into
the range 0 to 1. Next, we normalize each of the dataset
attributes in r,m,c,k into the range 0 to 1. Finally, we add
them together to form the ﬁnal score for the given dataset.
Based on the measurement deﬁned above, we can com-
pute the scores and rank these RS datasets. Based on
the rankings, we select datasets for three different tasks.
Speciﬁcally, for the RS image classiﬁcation task, the top ﬁve
ranked datasets are: 1). fMoW [135], 2) BigEarthNet [42],
3) Million AID [17], 4) So2Sat LCZ42 [23], and 5) RSD46-
WHU [40]. For the RS object detection task, the top ﬁve
datasets are 1) fMoW [135], 2) DIOR [18], 3) xView [134],
4) DOTA v2.0 [165], and 5) TGRS HRRSD [133]. Finally,
for the RS semantic segmentation task, the top ﬁve ranked
datasets are 1) SEASONET [25], 2) OpenSentinelMap [285],
3) SEN12MS [146], 4) GeoNRW [281], and 5) Five-Billion-
Pixels [283]. A complete list of the charts is displayed on
https://earthnets.github.io, where the radar charts are used
to compare the attributes of some top ranked datasets.
4.2 Dataset Selection for Benchmarking
We aim to select several datasets designed with general
purpose, large diversity and high richness for developing
and evaluating deep learning methods. Although there are
many large-scale RS datasets that meet these standards, it is
unacceptable and not environment-friendly to benchmark
all of them for the evaluation of RS algorithms. Thus, in this
study, we choose to select two datasets for each task, includ-
ing one with high-resolution and one with low resolution for
larger geographical coverage.
PyTorchTorchDataDatasets4EORS-Segmentation
RS-Classification
RS-Detection
ML/DL/ CV Methodologies EarthNets Platform More than 400EODatasets
Optical Multispectral SAR Point cloud
Hyperspectral Panchromatic Time series
Land cover/use Building Road Water body
Ship Forest Traffic Sea Cloud Agriculture
Flood Landslide Wind Change Poverty
Weather Climate Carbon &EcosystemsRS-ChangeDetectionVGG , ResNet , ResNeXt , HRNet , 
ResNeSt , SE-ResNet , Vision 
Transformer, MLP -Mixer, Swin
Transformer , Twins, EfficientNet , 
ConvNeXt , …, MAE
ASPP, FPN, UNet , PSPNet , YOLO, 
MaskRCNN, Mask2Former, 
DeepLab, DPT, … , SegFormerBackbones
Necks & Heads
Loss FunctionsFocal loss, Dice loss,
Lovasz loss , Tversky loss Fig. 10. The architecture design of the proposed EarthNets platform.
EarthNets is based on PyTorch [328] and TorchData. It contains the
Dataset4EO for a standard and easy-to-use dataset loading library, and
some high-level libraries for different EO tasks.
Following this constraint, the following datasets are
selected. 1) fMoW with high resolution ( ∼1m) data and
BigEarthNet with low resolution ( >10m) imagery are se-
lected for image classiﬁcation. 2) DIOR with high-resolution
(∼1m) data and fMoW with large objects are selected for the
RS object detection task. 3) GeoNRW with high-resolution
(∼1m) images and SEASONET with low-resolution ( >10m)
images are selected for RS semantic segmentation. In total,
there are ﬁve datasets selected to build a uniﬁed benchmark
for three different tasks.
5 T HEEARTH NETSOPENPLATFORM
Large-scale, high-quality datasets are important for a faith-
ful evaluation of RS algorithms, while other factors like
training tricks, hyper-parameters, optimizers, and initial-
ization methods are also critical for a fair and reliable
comparison of different methods. Thus, an open platform is
crucial for the fair evaluation, reproducibility, and efﬁcient
development of novel methods. However, there is still no
uniﬁed deep learning platform for different RS tasks. Torch-
geo [26] mainly focuses on the data loading part. AiTLAS
[327] mainly contains codebase for the RS classiﬁcation task.
In contrast, we aim to build a new uniﬁed platform for the
RS community that not only deals with dataset loading, but
also includes libraries for different RS tasks.
Fig. 10 illustrates the overall architecture design of the
proposed EarthNets platform. The platform is based on
PyTorch [328] and TorchData. The library Dataset4EO is de-
signed as a standard and easy-to-use dataset loading library.
Note that Dataset4EO can be used alone or together with
our high-level libraries, like RS-Classiﬁcation, RS-Detection,
and so on.
For the design of the EarthNets platform, we consider
two main factors. The ﬁrst is the decoupling between dataset
loading and high-level EO tasks. As shown in this study,
there are more than 400 RS datasets with different ﬁle
formats, data modalities, research domains, and download
links. Building a standard and scalable dataset loading
library can largely accelerate research for the whole RS com-
munity. Furthermore, researchers from other machine learn-
ing community can also beneﬁt from the standard dataset
loading library. The second factor considered is pushing the
12
TABLE 5
Image Classiﬁcation Results on the fMoW [135] Dataset. Top-1
accuracy, precision, recall, and F1 score are reported. The best results
are in bold.
Image ClassiﬁcationMethodsPre-trained Top-1 P R F1
ViT-Small Random 54.1 53.45 51.8 52.03
MLP-Mixer Random 43.11 40.33 41.09 40.18
ResNet-50 ImageNet 58.25 58.73 57 57.28
EfﬁcientNet-b4 ImageNet 58.8 58.7 57.01 57.33
ConvNext-Small ImageNet 62.05 63.81 60.34 61.19
Swin-Tiny ImageNet 66.42 66.33 65.29 65.5
TABLE 6
Multi-label Image Classiﬁcation Results on the BigEarthNet [42]
Dataset. mAP , micro precision, micro recall and F1 score are reported.
The best results are in bold.
Image ClassiﬁcationMethodsPre-trained mAP P R F1
ResNet-18* [238] Random 79.80 - - -
ResNet-18* [238] ImageNet 85.90 - - -
ResNet-18* [238] MoCo-v2 85.23 - - -
ResNet-50* [238] ImageNet 86.74 - - -
ResNet-50 ImageNet 85.74 76.87 75.89 76.38
EfﬁcientNet-b4 ImageNet 84.48 73.84 77.19 75.48
ConvNext-Small ImageNet 85.59 73.64 79.91 76.65
Swin-Tiny ImageNet 87.19 78.01 80.22 79.1
MLP Mixer ImageNet 82.76 73 74.36 73.67
RS data to a larger machine learning community. There are a
number of novel deep learning models published in the CV
and machine learning community, including different back-
bones, models, and loss functions. The EarthNets platform is
designed to easily apply these models to RS datasets, and in
order to ﬁll in the gap between the RS and CV communities.
6 E XPERIMENTS
In this section, we benchmark state-of-the-art deep learn-
ing models from the CV community on ﬁve selected RS
datasets. We also compare them with the methods specif-
ically designed for the RS datasets. The implementation
details can be found in the supplementary materials.
Metrics : For multi-label image classiﬁcation datasets, we
report the following metrics: precision (P), recall (R), F1
score, and mean average precision (mAP). For precision, re-
call, and F1, we set the threshold value to 0.5 for all models.
For object detection, mAP is used as the measurement for
performance evaluation. Three metrics are used to evaluate
the semantic segmentation task: overall (micro-averaged)
Accuracy (aAcc), mean (macro-averaged) Accuracy(mAcc),
and mean Intersection over Union (mIoU).
6.1 Benchmarking Results and Comparisons
In this section, we benchmark the ﬁve selected datasets
using the proposed EarthNets platform. In order to avoid
excessive computation costs, we choose to evaluate some
representative state-of-the-art (SOTA) methods from the CV
community on the large-scale RS datasets.
Comparisons on the fMoW Dataset. fMoW [135] is
a large-scale dataset built for recognizing the functional
purpose of buildings and land use. It contains 1 millionTABLE 7
Object Detection Results on the DIOR [18] Dataset. The mAP
performance is reported. The best results are in bold.
MethodObject Detection
Backbone Optimizer Epochs mAP
RetinaNet* [18] ResNet-50 - - 65.7
RetinaNet* [18] ResNet-101 - - 66.1
PANet* [18] ResNet-50 - - 63.8
PANet* [18] ResNet-101 - - 66.1
Mask-RCNN* [18] ResNet-50 - - 63.5
Mask-RCNN* [18] ResNet-101 - - 65.2
YoloV3* [18] DarkNet53 - - 57.1
YoloV3 DarkNet53 SGD 120 64.0
YoloV3 Swin-Tiny AdamW 120 64.6
YoloV3 ConvNext-Small AdamW 120 67.6
Mask-RCNN ResNet-50 SGD 120 68.5
Mask-RCNN Swin-Tiny AdamW 120 70.5
Mask-RCNN ConvNext-Small AdamW 120 72.4
TABLE 8
Semantic Segmentation Results on the SEASONET [25] Dataset. We
report the aAcc, mAcc and mIoU metrics. The best results are in bold.
MethodSemantic Segmentation
Backbone Optimizer Iter. aAcc mAcc mIoU
DeeplabV3* [25] DenseNet121 – – – – 47.53
DeeplabV3,PT* [25] DenseNet121 – – – – 48.69
DeeplabV3 ResNet-50 SGD 80k 82.87 58.49 47.5
DeeplabV3 ResNet-50 SGD 160k 83.52 62.65 50.79
DeeplabV3 ConvNext-Small AdamW 120k 81.36 56.31 46.39
DeeplabV3 Swin-Tiny AdamW 120k 82.75 61.5 50.81
Upernet ResNet-50 SGD 120k 83.2 60.36 49.59
SegFormer MiT AdamW 120k 83.75 64.25 53.87
images from over 200 countries, annotated with 63 differ-
ent classes. In this study, we use the fMoW-rgb version
of the dataset for model evaluation. Table 5 reports the
benchmarking results. In general, we can see that using
the ImageNet pre-trained weights can greatly improve the
performance. When we compare the CNN-based methods
with the Transformer-based method, we ﬁnd that Swin-
Tiny [330] clearly outperforms other CNN-based methods
in all the four metrics. Among the CNN-based methods,
ConvNext [331] is the best performing one.
Comparisons on the BigEarthNet Dataset. BigEarthNet
is a large-scale multi-label Sentinel-2 benchmark dataset
annotated with the CORINE Land Cover classes. There
are two versions of the labels, one with 43 categories and
another with 19 categories. In this study, we adopt the
new class nomenclature (19 categories) introduced in [332].
TABLE 9
Semantic Segmentation Results on the GeoNRW [281] Dataset. We
report the aAcc, mAcc and mIoU metrics. The best results are in bold.
Semantic SegmentationMethodBackbone Optimizer #Epochs aAcc mAcc mIoU
MultiTask* [329] Transfomer AdamW 100k 76.75 71.89 57.3
Lu et al.* [329] Transfomer AdamW 100k 76.53 70.12 56.2
FCN UNet SGD 40k 78.8 66.86 55.6
PSPNet ResNet-50 SGD 40k 81.56 74.92 62.73
Deeplabv3+ ResNet-50 SGD 40k 81.91 75.26 63.01
Deeplabv3+ ConvNext-Tiny AdamW 40k 80.89 73.61 61.63
Deeplabv3+ Swin-Tiny SGD 40k 78.09 70.01 56.78
Deeplabv3+ Swin-Tiny AdamW 40k 81.11 74.28 62.18
Upernet ResNet-50 SGD 40k 81.87 75.7 63.1
Upernet ConvNext-Tiny AdamW 40k 82.08 74.9 63.48
Upernet Vit-Small AdamW 40k 78.65 71.13 59.43
Upernet Swin-Tiny AdamW 40k 82.31 75.68 64.48
SegFormer MiT AdamW 40k 82.55 75.63 64.38
13
Regarding the methods, we evaluate four CNN-based archi-
tectures (ResNet-18, ResNet-50, EfﬁcientNet-b4, ConvNext).
For Transformer-based method, we evaluate the Swin-Tiny,
which is usually overlooked in existing benchmarking re-
sults. Furthermore, an MLP-based method, the MLP-Mixer
[333] is also compared. Additionally, we also compare the
results reported by existing work [238] on the BigEarthNet
dataset.
Table 6 reports the benchmarking results. In general,
the results indicate that Swin-Tiny performs best on this
multi-label classiﬁcation dataset. However, there is no sig-
niﬁcant advantage compared with other CNN-based meth-
ods. Another conclusion we can make is that ResNet-50
is a strong baseline method. From the results, it can be
seen that ResNet-50, pre-trained on ImageNet or using self-
supervised MoCo-V2 [334], can perform better than MLP-
Mixer, EfﬁcientNet-b4 on this dataset. The performance of
ConvNext is competitive to ResNet-50, but lower than the
transformer-based method Swin-Tiny. Note that∗indicates
that the results of the method are reported in existing
work. Generally speaking, the results benchmarked using
the EarthNets platform are higher than or comparable to
existing reported results.
Comparisons on the DIOR Dataset. Table 7 presents
the benchmarking results on the DIOR dataset built for
the object detection task. We choose two representative and
widely-used object detection methods designed by the CV
community. To be speciﬁc, YoloV3 [113] and Mask-RCNN
[335] are evaluated on the DIOR dataset. YoloV3 is de-
signed for light-weight and real-time object detection. Mask-
RCNN is an extension of Faster-RCNN [111] with ROI align
and a third segmentation branch. The experimental results
reveal that Mask-RCNN performs better than YoloV3 on
this dataset. With regard to different backbones, the results
clearly show that Swin-Tiny and ConvNext can outperform
other compared methods. The Mask-RCNN method with
ConvNext backbone achieves an mAP of 72.4%, which is 6.3
percentage points higher than the best results reported in
[18]. Notably, we observe that our benchmarked results can
greatly outperform the same method reported in existing
work. This comparison reveals that the choice of the opti-
mizer, hyper-parameters, or other training tricks can greatly
affect the ﬁnal results, even when the same method is used.
Comparisons on the SEASONET Dataset. SEASONET
is a large-scale multi-label LULC scene understanding
dataset. It includes 1,759,830 images from Sentinel-2 tiles,
and can be used for scene classiﬁcation, segmentation, and
retrieval tasks. In this study, we evaluate segmentation
performance on this dataset. On this dataset, we evaluate
the widely-used semantic segmentation method DeeplabV3
[336] with three different backbones: ResNet-50, ConvNext,
and Swin-Tiny. Upernet and SegFormer [337] with mixed-
Transformer encoders (MiT) are also compared. Table 8
reports the benchmarking results. It can be seen that ResNet-
50 and Swin-Tiny obtain comparable results and clearly
surpass other backbones. SegFormer with MiT encoder
clearly outperforms other models. We also ﬁnd that the
results obtained using EarthNets signiﬁcantly outperform
performance reported in existing work [25].
Comparisons on the GeoNRW Dataset. The benchmark-
ing results on the GeoNRW dataset are displayed in Table 9.Five segmentation methods, FCN [338], DeeplabV3+ [339],
PSPNet [340], Upernet [341] and SegFormer [337] with
mixed-Transformer encoders (MiT), are evaluated on this
dataset. We observe that Transformer-based models like
SegFormer and Swin Transformer perform better than other
methods. However, the performance of ViT-Small is worse
than ResNet-50. In comparison to the reported results in
existing work, we can ﬁnd that using the EarthNets platform
can obtain clearly better performance.
7 C ONCLUSION
In this study, we present a comprehensive review and build
a taxonomy for more than 400 publicly published datasets
in the remote sensing community. Based on the attribute
information of these datasets, we systemically analyze them
with respect to ﬁve aspects: volumes, bibliometric analysis,
resolution distributions, research domains, and the correla-
tion between datasets. Next, a new benchmark including
ﬁve selected large-scale datasets is built for model evalua-
tion. A deep learning platform termed EarthNets is released
with the intention to support a consistent evaluation of deep
learning methods on remote sensing data. We further use
the EarthNets platform to benchmark state-of-the-art meth-
ods on the new benchmark. The performance comparisons
are insightful for future research.
REFERENCES
[1] C. Toth and G. J ´o´zk´ow, “Remote sensing platforms and sensors:
A survey,” ISPRS J. Photogramm. Remote Sens. , vol. 115, pp. 22–36,
2016.
[2] X. X. Zhu, D. Tuia, L. Mou, G.-S. Xia, L. Zhang, F. Xu, and
F. Fraundorfer, “Deep learning in remote sensing: A comprehen-
sive review and list of resources,” IEEE GRSM , vol. 5, no. 4, pp.
8–36, 2017.
[3] A. Shaker, W. Y. Yan, and P . E. LaRocque, “Automatic land-
water classiﬁcation using multispectral airborne lidar data for
near-shore and river environments,” ISPRS J. Photogramm. Remote
Sens. , vol. 152, pp. 94–108, 2019.
[4] M. E. Bauer, “Remote sensing of environment: history, phi-
losophy, approach and contributions, 1969–2019,” Remote Sens.
Environ. , vol. 237, p. 111522, 2020.
[5] M. W ´ojtowicz, A. W ´ojtowicz, J. Piekarczyk et al. , “Application
of remote sensing methods in agriculture,” Communications in
Biometry and Crop Science , vol. 11, no. 1, pp. 31–50, 2016.
[6] L. Karthikeyan, I. Chawla, and A. K. Mishra, “A review of remote
sensing applications in agriculture for food security: Crop growth
and yield, irrigation, and crop losses,” Journal of Hydrology , vol.
586, p. 124905, 2020.
[7] K. E. Joyce, K. C. Wright, S. V . Samsonov, and V . G. Ambrosia,
“Remote sensing and the disaster management cycle,” Advances
in geoscience and remote sensing , vol. 48, p. 7, 2009.
[8] C. Van Westen, “Remote sensing for natural disaster manage-
ment,” ISPRS Archives , vol. 33, no. B7/4; PART 7, pp. 1609–1617,
2000.
[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classi-
ﬁcation with deep convolutional neural networks,” Communica-
tions of the ACM , vol. 60, no. 6, pp. 84–90, 2017.
[10] L. Zhang, L. Zhang, and B. Du, “Deep learning for remote sensing
data: A technical tutorial on the state of the art,” IEEE GRSM ,
vol. 4, no. 2, pp. 22–40, 2016.
[11] Z. Xiong, Y. Yuan, and Q. Wang, “AI-NET: Attention incep-
tion neural networks for hyperspectral image classiﬁcation,” in
IGARSS . IEEE, 2018, pp. 2647–2650.
[12] G. Marchisio, P . Helber, B. Bischke, T. Davis, C. Senaras,
D. Zanaga, R. Van De Kerchove, and A. Wania, “Rapidai4eo: A
corpus for higher spatial and temporal reasoning,” in 2021 IEEE
International Geoscience and Remote Sensing Symposium IGARSS .
IEEE, 2021, pp. 1161–1164.
14
[13] A. Hecheltjen, F. Thonfeld, and G. Menz, “Recent advances in
remote sensing change detection–a review,” Land use and land
cover mapping in Europe , pp. 145–178, 2014.
[14] M. Rahnemoonfar, T. Chowdhury, A. Sarkar, D. Varshney,
M. Yari, and R. R. Murphy, “Floodnet: A high resolution aerial
imagery dataset for post ﬂood scene understanding,” IEEE Ac-
cess, vol. 9, pp. 89 644–89 654, 2021.
[15] K. Bakula, J. Mills, and F. Remondino, “A review of benchmark-
ing in photogrammetry and remote sensing,” ISPRS Archives ,
2019.
[16] M. Schmitt, S. A. Ahmadi, and R. H ¨ansch, “There is no data like
more data-current status of machine learning datasets in remote
sensing,” in IGARSS . IEEE, 2021, pp. 1206–1209.
[17] Y. Long, G.-S. Xia, S. Li, W. Yang, M. Y. Yang, X. X. Zhu, L. Zhang,
and D. Li, “On creating benchmark dataset for aerial image
interpretation: Reviews, guidances, and million-aid,” IEEE J. Sel.
Top. Appl. Earth Obs. Remote Sens. , vol. 14, pp. 4205–4230, 2021.
[18] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, “Object detection in
optical remote sensing images: A survey and a new benchmark,”
ISPRS J. Photogramm. Remote Sens. , vol. 159, pp. 296–307, 2020.
[19] A. Abdollahi, B. Pradhan, N. Shukla, S. Chakraborty, and
A. Alamri, “Deep learning approaches applied to remote sensing
datasets for road extraction: A state-of-the-art review,” Remote
Sensing , vol. 12, no. 9, p. 1444, 2020.
[20] I. Tomljenovic, B. H ¨oﬂe, D. Tiede, and T. Blaschke, “Building
extraction from airborne laser scanning data: An analysis of the
state of the art,” Remote Sensing , vol. 7, no. 4, pp. 3826–3862, 2015.
[21] M. Schmitt, P . Ghamisi, N. Yokoya, and R. H ¨ansch, “Eod: The
ieee grss earth observation database,” in IGARSS . IEEE, 2022,
pp. 5365–5368.
[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,
“Imagenet: A large-scale hierarchical image database,” in CVPR .
Ieee, 2009, pp. 248–255.
[23] X. X. Zhu, J. Hu, C. Qiu, Y. Shi, J. Kang, L. Mou, H. Bagheri,
M. H ¨aberle, Y. Hua, R. Huang et al. , “So2sat lcz42: A bench-
mark dataset for global local climate zones classiﬁcation,” arXiv
preprint arXiv:1912.12171 , 2019.
[24] G. Christie, N. Fendley, J. Wilson, and R. Mukherjee, “Functional
map of the world,” in CVPR , 2018, pp. 6172–6180.
[25] D. Koßmann, V . Brack, and T. Wilhelm, “Seasonet: A seasonal
scene classiﬁcation, segmentation and retrieval dataset for satel-
lite imagery over germany,” in IGARSS . IEEE, 2022, pp. 243–246.
[26] A. J. Stewart, C. Robinson, I. A. Corley, A. Ortiz, J. M. L. Ferres,
and A. Banerjee, “Torchgeo: deep learning with geospatial data,”
arXiv preprint arXiv:2111.08872 , 2021.
[27] Y. Yang and S. Newsam, “Bag-of-visual-words and spatial exten-
sions for land-use classiﬁcation,” in ACM SIGSP ATIAL , 2010, pp.
270–279.
[28] G.-S. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y. Zhong, L. Zhang, and
X. Lu, “Aid: A benchmark data set for performance evaluation of
aerial scene classiﬁcation,” IEEE TGRS , vol. 55, no. 7, pp. 3965–
3981, 2017.
[29] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu,
M. Pelillo, and L. Zhang, “Dota: A large-scale dataset for object
detection in aerial images,” in CVPR , June 2018.
[30] M. V . M Grana and B. Ayerdi. (2022) Hyperspectral datasets.
[Online]. Available: http://www.ehu.eus/ccwintco/index.php/
Hyperspectral Remote Sensing Scenes
[31] X. Sun, Y. Lv, Z. Wang, and K. Fu, “Scan: Scattering characteris-
tics analysis network for few-shot aircraft classiﬁcation in high-
resolution sar images,” IEEE TGRS , vol. 60, pp. 1–17, 2022.
[32] (2015) District of columbia – classiﬁed point cloud lidar. [Online].
Available: https://github.com/awslabs/open-data-docs/tree/
main/docs/dc-lidar-2018
[33] ISPRS-Contest. (2022) Isprs 2d semantic labeling
contest,” accessed on oct. 9, 2022. [Online].
Available: https://www.isprs.org/education/benchmarks/
UrbanSemLab/2d-sem-label-potsdam.aspx
[34] ——. (2022) Isprs 2d semantic labeling contest,” accessed on oct.
9, 2022. [Online]. Available: https://www.isprs.org/education/
benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx
[35] O. A. Penatti, K. Nogueira, and J. A. Dos Santos, “Do deep
features generalize from everyday objects to remote sensing and
aerial scenes domains?” in CVPRW , 2015, pp. 44–51.
[36] L. Kondmann, A. Toker, M. Rußwurm, A. Camero Unzueta,
D. Peressuti, G. Milcinski, N. Long ´ep´e, P .-P . Mathieu, T. Davis,
G. Marchisio et al. , “DENETHOR: The DynamicEarthNET datasetfor Harmonized, inter-Operable, analysis-Ready, daily crop mon-
itoring from space,” in NeurIPS Datasets and Benchmarks Track ,
2021.
[37] D. Sykas, I. Papoutsis, and D. Zografakis, “Sen4agrinet: A har-
monized multi-country, multi-temporal benchmark dataset for
agricultural earth observation machine learning applications,” in
IGARSS . IEEE, 2021, pp. 5830–5833.
[38] E. L. Amanda A. Boatswain Jacques, Abdoulaye Banir ´e Diallo,
“Towards the creation of a canadian land-use dataset for agri-
cultural land classiﬁcation,” in Canadian Symposium on Remote
Sensing , 2021.
[39] X. Qi, P . Zhu, Y. Wang, L. Zhang, J. Peng, M. Wu, J. Chen,
X. Zhao, N. Zang, and P . T. Mathiopoulos, “Mlrsnet: A multi-
label high spatial resolution remote sensing dataset for semantic
scene understanding,” ISPRS J. Photogramm. Remote Sens. , vol.
169, pp. 337–350, 2020.
[40] Z. Xiao, Y. Long, D. Li, C. Wei, G. Tang, and J. Liu, “High-
resolution remote sensing image retrieval based on cnns from
a dimensional perspective,” Remote Sensing , vol. 9, no. 7, p. 725,
2017.
[41] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene
classiﬁcation: Benchmark and state of the art,” Proceedings of the
IEEE , vol. 105, no. 10, pp. 1865–1883, 2017.
[42] G. Sumbul, M. Charfuelan, B. Demir, and V . Markl, “Bigearth-
net: A large-scale benchmark archive for remote sensing image
understanding,” in IGARSS . IEEE, 2019, pp. 5901–5904.
[43] G. Machado, E. Ferreira, K. Nogueira, H. Oliveira, M. Brito,
P . H. T. Gama, and J. A. dos Santos, “Airound and cv-brct: Novel
multiview datasets for scene classiﬁcation,” IEEE J. Sel. Top. Appl.
Earth Obs. Remote Sens. , vol. 14, pp. 488–503, 2020.
[44] P . Helber, B. Bischke, A. Dengel, and D. Borth, “Eurosat: A novel
dataset and deep learning benchmark for land use and land cover
classiﬁcation,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. ,
vol. 12, no. 7, pp. 2217–2226, 2019.
[45] R. Bahmanyar, D. Espinoza-Molina, and M. Datcu, “Multisensor
earth observation image classiﬁcation based on a multimodal
latent dirichlet allocation model,” IEEE Geosci. Remote Sens. Lett. ,
vol. 15, no. 3, pp. 459–463, 2018.
[46] Y. Zhong, Q. Zhu, and L. Zhang, “Scene classiﬁcation based on
the multifeature fusion probabilistic topic model for high spatial
resolution remote sensing imagery,” IEEE TGRS , vol. 53, no. 11,
pp. 6207–6222, 2015.
[47] H. Li, X. Dou, C. Tao, Z. Wu, J. Chen, J. Peng, M. Deng, and
L. Zhao, “Rsi-cb: A large-scale remote sensing image classiﬁca-
tion benchmark using crowdsourced data,” Sensors , vol. 20, no. 6,
p. 1594, 2020.
[48] (2018) Ships in satellite imagery. [On-
line]. Available: https://www.kaggle.com/datasets/rhammell/
ships-in-satellite-imagery
[49] A. P . Antonio-Javier Gallego and P . Gil, “Automatic ship clas-
siﬁcation from optical aerial images with convolutional neural
networks,” Remote Sensing , vol. 10, no. 4, 2018.
[50] Y. Di, Z. Jiang, H. Zhang, and G. Meng, “A public dataset for
ship classiﬁcation in remote sensing images,” in SPIE , vol. 11155,
2019, pp. 515–521.
[51] I. G. Rizaev and A. Achim, “Synthwakesar: A synthetic sar
dataset for deep learning classiﬁcation of ships at sea,” Remote
Sensing , vol. 14, no. 16, p. 3999, 2022.
[52] Q. D. Cao and Y. Choe, “Building damage annotation on post-
hurricane satellite imagery based on convolutional neural net-
works,” Natural Hazards , vol. 103, no. 3, pp. 3357–3376, 2020.
[53] C. Rambour, N. Audebert, E. Koeniguer, B. Le Saux, M. Crucianu,
and M. Datcu, “Flood detection in time series of optical and sar
images,” ISPRS Archives , vol. 43, pp. 1343–1346, 2020.
[54] D. Bonaﬁlia, B. Tellman, T. Anderson, and E. Issenberg,
“Sen1ﬂoods11: A georeferenced dataset to train and test deep
learning ﬂood algorithms for sentinel-1,” in CVPRW , 2020, pp.
210–211.
[55] R. Ba, C. Chen, J. Yuan, W. Song, and S. Lo, “Smokenet: Satel-
lite smoke scene detection using convolutional neural network
with spatial and channel-wise attention,” Remote Sensing , vol. 11,
no. 14, p. 1702, 2019.
[56] (2017) Noaa ﬁsheries steller sea lion population
count. [Online]. Available: https://www.kaggle.com/c/
noaa-ﬁsheries-steller-sea-lion-population-count/data
[57] N. Imamoglu, M. Kimura, H. Miyamoto, A. Fujita, and R. Naka-
mura, “Solar power plant detection on multi-spectral satellite
15
imagery using weakly-supervised cnn with feedback features
and m-pcnn fusion,” arXiv preprint arXiv:1704.06410 , 2017.
[58] (2018) Airbus wind turbines patches. [On-
line]. Available: https://www.kaggle.com/datasets/airbusgeo/
airbus-wind-turbines-patches
[59] M. F. Baumgardner, L. L. Biehl, and D. A. Landgrebe, “220 band
aviris hyperspectral image data set: June 12, 1992 indian pine test
site 3,” Purdue University Research Repository , vol. 10, no. 7, p. 991,
2015.
[60] R. M Rustowicz, R. Cheong, L. Wang, S. Ermon, M. Burke, and
D. Lobell, “Semantic segmentation of crop type in africa: A novel
dataset and analysis of deep learning methods,” in CVPRW , 2019,
pp. 75–82.
[61] H. Kerner, C. Nakalembe, and I. Becker-Reshef, “Field-level crop
type classiﬁcation with k nearest neighbors: a baseline for a new
kenya smallholder dataset,” arXiv preprint arXiv:2004.03023 , 2020.
[62] M. Rußwurm, C. Pelletier, M. Zollner, S. Lef `evre, and M. K ¨orner,
“Breizhcrops: A time series dataset for crop type mapping,”
ISPRS (2020) , 2020.
[63] S. Virnodkar, V . Pachghare, V . Patil, and S. K. Jha,
“Canesat,” 2020. [Online]. Available: https://dx.doi.org/10.
21227/vzbn-qj64
[64] G. Tseng, I. Zvonkov, C. L. Nakalembe, and H. Kerner, “Crophar-
vest: A global dataset for crop-type classiﬁcation,” in NeurIPS
Datasets and Benchmarks Track , 2021.
[65] R. E. F. Western Cape Department of Agricultures. (2021)
Crop type classiﬁcation dataset for western cape, south africa.
[Online]. Available: https://doi.org/10.34911/rdnt.j0co8q
[66] G. Choumos, A. Koukos, V . Sitokonstantinou, and C. Kontoes,
“Towards space-to-ground data availability for agriculture mon-
itoring,” in IVMSP , 2022, pp. 1–5.
[67] M. J. Hughes and D. J. Hayes, “Automated detection of cloud
and cloud shadow in single-date landsat imagery using neural
networks and spatial post-processing,” Remote Sensing , vol. 6,
no. 6, pp. 4907–4926, 2014.
[68] M. P . I. for Meteorology. (2019) Understanding clouds from
satellite images. [Online]. Available: https://www.kaggle.com/
c/understanding cloud organization/data
[69] A. H. Nielsen, A. Iosiﬁdis, and H. Karstoft, “Cloudcast: A
satellite-based dataset and baseline for forecasting clouds,” IEEE
J. Sel. Top. Appl. Earth Obs. Remote Sens. , vol. 14, pp. 3485–3494,
2021.
[70] (2020) Sentinel-2 cloud mask catalogue. [Online]. Available:
https://zenodo.org/record/4172871#.Y0LEOHbP1D-
[71] L. Mou, Y. Hua, P . Jin, and X. X. Zhu, “ERA: A dataset and deep
learning benchmark for event recognition in aerial videos,” IEEE
GRSM , in press.
[72] G. I. Drakonakis, G. Tsagkatakis, K. Fotiadou, and P . Tsakalides,
“Ombrianet—supervised ﬂood mapping via convolutional neu-
ral networks using multitemporal sentinel-1 and sentinel-2 data
fusion,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. , vol. 15, pp.
2341–2356, 2022.
[73] S. Chandak, V . Chitters, and S. Honnungar, “Understanding the
amazon rainforest from space using cnns,” 2017.
[74] D. Noever and S. E. M. Noever, “Overhead mnist: A benchmark
satellite dataset,” arXiv preprint arXiv:2102.04266 , 2021.
[75] G.-S. Xia, W. Yang, J. Delon, Y. Gousseau, H. Sun, and H. Ma ˆItre,
“Structural high-resolution satellite image indexing,” Vienna,
Austria, 2010.
[76] Q. Zou, L. Ni, T. Zhang, and Q. Wang, “Deep learning based
feature selection for remote sensing scene classiﬁcation,” IEEE
Geosci. Remote Sens. Lett. , vol. 12, no. 11, pp. 2321–2325, 2015.
[77] L. Zhao, P . Tang, and L. Huo, “Feature signiﬁcance-based
multibag-of-visual-words model for remote sensing image scene
classiﬁcation,” Journal of Applied Remote Sensing , vol. 10, no. 3, p.
035004, 2016.
[78] W. Zhou, S. Newsam, C. Li, and Z. Shao, “Patternnet: A bench-
mark dataset for performance evaluation of remote sensing im-
age retrieval,” ISPRS J. Photogramm. Remote Sens. , vol. 145, pp.
197–209, 2018.
[79] Q. Wang, S. Liu, J. Chanussot, and X. Li, “Scene classiﬁcation with
recurrent attention of vhr remote sensing images,” IEEE TGRS ,
vol. 57, no. 2, pp. 1155–1167, 2018.
[80] H. Li, H. Jiang, X. Gu, J. Peng, W. Li, L. Hong, and C. Tao, “Clrs:
Continual learning benchmark for remote sensing image scene
classiﬁcation,” Sensors , vol. 20, no. 4, p. 1226, 2020.[81] Z. Zhou, S. Li, W. Wu, W. Guo, X. Li, G. Xia, and Z. Zhao, “Nasc-
tg2: Natural scene classiﬁcation with tiangong-2 remotely sensed
imagery,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. , vol. 14,
pp. 3228–3242, 2021.
[82] M. Reda. (2021) Satellite image classiﬁcation. [Online]. Avail-
able: https://www.kaggle.com/datasets/mahmoudreda55/
satellite-image-classiﬁcation
[83] Y. Hua, L. Mou, P . Jin, and X. X. Zhu, “Multiscene: A large-scale
dataset and benchmark for multiscene recognition in single aerial
images,” IEEE TGRS , vol. 60, pp. 1–13, 2021.
[84] N. I. Bountos, I. Papoutsis, D. Michail, A. Karavias, P . Elias,
and I. Parcharidis, “Hephaestus: A large scale multitask dataset
towards insar understanding,” in CVPR , 2022, pp. 1453–1462.
[85] K. Uehara, H. Sakanashi, H. Nosato, M. Murakawa,
H. Miyamoto, and R. Nakamura, “Object detection of satellite
images using multi-channel higher-order local autocorrelation,”
inIEEE SMC . IEEE, 2017, pp. 1339–1344.
[86] Statoil. (2017) Statoil/c-core iceberg classiﬁer challenge.
[Online]. Available: https://www.kaggle.com/competitions/
statoil-iceberg-classiﬁer-challenge
[87] S. Basu, S. Ganguly, S. Mukhopadhyay, R. DiBiano, M. Karki,
and R. Nemani, “Deepsat: a learning framework for satellite
imagery,” in ACM SIGSP ATIAL , 2015, pp. 1–10.
[88] (2017) Tiselac: Time series land cover classiﬁcation challeng.
[Online]. Available: https://sites.google.com/site/dinoienco/
tiselc
[89] X.-Y. Tong, G.-S. Xia, Q. Lu, H. Shen, S. Li, S. You, and L. Zhang,
“Land-cover classiﬁcation with high-resolution remote sensing
images using transferable deep models,” Remote Sens. Environ. ,
vol. 237, p. 111322, 2020.
[90] (2019) Eopatches for slovenia 2019. [Online]. Available:
http://eo-learn.sentinel-hub.com/
[91] (2016) Tianggong1 high-resolution satellite scene classiﬁcation.
[Online]. Available: http://www.msadc.cn/main/setsubDetail?
id=1369487569196158978
[92] (2017) Austin zoning satellite images. [Online].
Available: https://www.kaggle.com/datasets/franchenstein/
austin-zoning-satellite-images
[93] R. Ratajczak, C. F. Crispim-Junior, ´E. Faure, B. Fervers, and
L. Tougne, “Automatic Land Cover Reconstruction From His-
torical Aerial Images: An Evaluation of Features Extraction and
Classiﬁcation Algorithms,” IEEE Trans. Image Process. , Jan. 2019.
[94] (2020) Sense earth satellite scene classiﬁcation. [Online].
Available: https://aistudio.baidu.com/aistudio/datasetdetail/
52728
[95] S. Ji, D. Yu, C. Shen, W. Li, and Q. Xu, “Landslide detection from
an open satellite imagery and digital elevation model dataset us-
ing attention boosted convolutional neural networks,” Landslides ,
vol. 17, no. 6, pp. 1337–1352, 2020.
[96] U. S. Ranjan and A. Narayana, “Classiﬁcation of objects in sar
images using scaling features.” in ICVGIP , 2002.
[97] (2018) Forest cover type (kernels only). [Online]. Available: https:
//www.kaggle.com/c/forest-cover-type-kernels-only/data
[98] (2019) Aerial cactus identiﬁcation. [Online]. Available: https:
//www.kaggle.com/c/aerial-cactus-identiﬁcation
[99] (2019) Wids datathon 2019. [Online]. Available: https://www.
kaggle.com/c/widsdatathon2019
[100] S. Beery, G. Wu, T. Edwards, F. Pavetic, B. Majewski, S. Mukher-
jee, S. Chan, J. Morgan, V . Rathod, and J. Huang, “The auto
arborist dataset: A large-scale benchmark for multiview urban
forest monitoring under domain shift,” in CVPR , 2022, pp.
21 294–21 307.
[101] S. Ahlswede, C. Schulz, C. Gava, P . Helber, B. Bischke, M. F ¨orster,
F. Arias, J. Hees, B. Demir, and B. Kleinschmit, “Treesatai bench-
mark archive: A multi-sensor, multi-label dataset for tree species
classiﬁcation in remote sensing,” Earth System Science Data Dis-
cussions , pp. 1–22, 2022.
[102] S. F. Agency. (2021) Forest damages – larch casebearer
1.0. national forest data lab. dataset. [Online]. Available:
https://lila.science/datasets/forest-damages-larch-casebearer
[103] E. Cole, B. Deneu, T. Lorieul, M. Servajean, C. Botella, D. Morris,
N. Jojic, P . Bonnet, and A. Joly, “The geolifeclef 2020 dataset,”
arXiv preprint arXiv:2004.04192 , 2020.
[104] E. Ferreira, M. Brito, R. Balaniuk, M. S. Alvim, and J. A. dos
Santos, “Brazildam: A benchmark dataset for tailings dam detec-
tion,” in LAGIRS . IEEE, 2020, pp. 339–344.
16
[105] B. Chen, Q. Feng, B. Niu, F. Yan, B. Gao, J. Yang, J. Gong, and
J. Liu, “Multi-modal fusion of satellite and street-view images for
urban village classiﬁcation based on a dual-branch deep neural
network,” Int. J. Appl. Earth Obs. Geoinf. , vol. 109, p. 102794, 2022.
[106] (2022) Kennedy space center. [Online]. Available: https:
//www.csr.utexas.edu/projects/rs/hrs/classify.html
[107] K. Nogueira, J. A. Dos Santos, T. Fornazari, T. S. F. Silva, L. P .
Morellato, and R. d. S. Torres, “Towards vegetation species dis-
crimination by using data-driven descriptors,” in PRRS . Ieee,
2016, pp. 1–6.
[108] B. Uzkent, A. Rangnekar, and M. J. Hoffman, “Tracking in aerial
hyperspectral videos using deep kernelized correlation ﬁlters,”
arXiv preprint arXiv:1711.07235 , 2017.
[109] (2019) Find a car park. [Online]. Available: https://www.kaggle.
com/datasets/daggysheep/ﬁnd-a-car-park
[110] E. Dahan, T. Diskin, A. Amram, A. Moryossef, and O. Koren,
“Cofga: A dataset for ﬁne grained classiﬁcation of objects from
aerial imagery,” arXiv preprint arXiv:2105.12786 , 2021.
[111] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-
time object detection with region proposal networks,” NeurIPS ,
vol. 28, 2015.
[112] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu,
and A. C. Berg, “SSD: Single shot multibox detector,” in ECCV .
Springer, 2016, pp. 21–37.
[113] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only
look once: Uniﬁed, real-time object detection,” in CVPR , 2016,
pp. 779–788.
[114] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and
S. Zagoruyko, “End-to-end object detection with transformers,”
inECCV . Springer, 2020, pp. 213–229.
[115] RHAMMELL. (2017) Planes in satellite imagery. [Online]. Avail-
able: https://www.kaggle.com/datasets/rhammell/planesnet
[116] B. A. Jeff Faudi. (2021) Airbus aircraft detection. [On-
line]. Available: https://www.kaggle.com/datasets/airbusgeo/
airbus-aircrafts-sample-dataset
[117] S. Rawat. (2021) Casia-aircraft. [Online]. Available: https:
//www.rsaicp.com/portal/dataDetail?id=16
[118] I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu,
F. Hughes, D. Tuia, and R. Raskar, “Deepglobe 2018: A challenge
to parse the earth through satellite images,” in CVPRW , June
2018.
[119] Microsoft. (2019) Us building footprints. [Online]. Available:
https://github.com/microsoft/USBuildingFootprints
[120] (2018) Segmenting buildings in satellite images.
[Online]. Available: https://www.kaggle.com/code/kmader/
segmenting-buildings-in-satellite-images/data
[121] N. Weir, D. Lindenbaum, A. Bastidas, A. V . Etten, S. McPherson,
J. Shermeyer, V . Kumar, and H. Tang, “Spacenet mvoi: A multi-
view overhead imagery dataset,” in ICCV , 2019, pp. 992–1001.
[122] L. Huang, B. Liu, B. Li, W. Guo, W. Yu, Z. Zhang, and W. Yu,
“Opensarship: A dataset dedicated to sentinel-1 ship interpreta-
tion,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. , vol. 11, no. 1,
pp. 195–208, 2017.
[123] J. Li, C. Qu, and J. Shao, “Ship detection in sar images based on
an improved faster r-cnn,” in BIGSARDATA . IEEE, 2017, pp.
1–6.
[124] Airbus. (2018) Airbus ship detection challenge. [Online].
Available: https://www.kaggle.com/c/airbus-ship-detection
[125] p. m. adrian and A. Umam. (2018) Ships in google
earth. [Online]. Available: https://www.kaggle.com/datasets/
tomluther/ships-in-google-earth
[126] G. Heitz and D. Koller, “Learning spatial context: Using stuff to
ﬁnd things,” in ECCV . Springer, 2008, pp. 30–43.
[127] S. Razakarivony and F. Jurie, “Vehicle detection in aerial imagery:
A small target detection benchmark,” Journal of Visual Communi-
cation and Image Representation , vol. 34, pp. 187–203, 2016.
[128] T. N. Mundhenk, G. Konjevod, W. A. Sakla, and K. Boakye, “A
large contextual dataset for classiﬁcation, detection and counting
of cars with deep learning,” in ECCV . Springer, 2016, pp. 785–
800.
[129] DLR. (2016) Dlr multi-class vehicle detection and orientation
in aerial imagery. [Online]. Available: https://www.dlr.de/eoc/
en/desktopdefault.aspx/tabid-12760/22294 read-52777
[130] M. Y. Yang, W. Liao, X. Li, and B. Rosenhahn, “Deep learning for
vehicle detection in aerial images,” in ICIP , 2018.[131] G. Cheng and J. Han, “A survey on object detection in optical
remote sensing images,” ISPRS J. Photogramm. Remote Sens. , vol.
117, pp. 11–28, 2016.
[132] Y. Long, Y. Gong, Z. Xiao, and Q. Liu, “Accurate object local-
ization in remote sensing images based on convolutional neural
networks,” IEEE TGRS , vol. 55, no. 5, pp. 2486–2498, 2017.
[133] Y. Zhang, Y. Yuan, Y. Feng, and X. Lu, “Hierarchical and robust
convolutional neural network for very high-resolution remote
sensing object detection,” IEEE TGRS , vol. 57, no. 8, pp. 5535–
5548, 2019.
[134] xView. (2019) xview 2018 object detection challenge. [Online].
Available: https://challenge.xviewdataset.org/welcome
[135] G. Christie, N. Fendley, J. Wilson, and R. Mukherjee, “Functional
map of the world,” in CVPR , 2018.
[136] W. Han, J. Li, S. Wang, Y. Wang, J. Yan, R. Fan, X. Zhang, and
L. Wang, “A context-scale-aware detector and a new benchmark
for remote sensing small weak object detection in unmanned
aerial vehicle images,” Int. J. Appl. Earth Obs. Geoinformation , vol.
112, p. 102966, 2022.
[137] (2017) Dstl satellite imagery feature detection.
[Online]. Available: https://www.kaggle.com/c/
dstl-satellite-imagery-feature-detection/data?select=three
band.zip
[138] Airbus. (2021) Airbus oil storage detection. [On-
line]. Available: https://www.kaggle.com/datasets/airbusgeo/
airbus-oil-storage-detection-dataset
[139] T. Zhang. (2020) Large-scale sar ship detection dataset-v1.0.
[Online]. Available: https://github.com/TianwenZhang0825/
LS-SSDD-v1.0-OPEN
[140] S. Wei, X. Zeng, Q. Qu, M. Wang, H. Su, and J. Shi, “Hrsid: A
high-resolution sar images dataset for ship detection and instance
segmentation,” Ieee Access , vol. 8, pp. 120 234–120 254, 2020.
[141] J. Wang, W. Yang, H. Guo, R. Zhang, and G.-S. Xia, “Tiny object
detection in aerial images,” 2021, pp. 3791–3798.
[142] G. Cheng, X. Yuan, X. Yao, K. Yan, Q. Zeng, and J. Han, “Towards
large-scale small object detection: Survey and benchmarks,”
arXiv preprint arXiv:2207.14096 , 2022.
[143] (2022) Hydice image of washington dc mall. [Online].
Available: https://engineering.purdue.edu/ ∼biehl/MultiSpec/
hyperspectral.html
[144] R. Kemker, C. Salvaggio, and C. Kanan, “Algorithms
for semantic segmentation of multispectral remote sensing
imagery using deep learning,” ISPRS J. Photogramm. Remote
Sens. , 2018. [Online]. Available: http://www.sciencedirect.com/
science/article/pii/S0924271618301229
[145] A. Boguszewski, D. Batorski, N. Ziemba-Jankowska, T. Dziedzic,
and A. Zambrzycka, “Landcover. ai: Dataset for automatic map-
ping of buildings, woodlands, water and roads from aerial im-
agery,” in CVPR , 2021, pp. 1102–1110.
[146] M. Schmitt, L. H. Hughes, C. Qiu, and X. X. Zhu, “Sen12ms–
a curated dataset of georeferenced multi-spectral sentinel-1/2
imagery for deep learning and data fusion,” arXiv preprint
arXiv:1906.07789 , 2019.
[147] (2019) Semantic segmentation of aerial imagery. [Online]. Avail-
able: https://www.kaggle.com/datasets/humansintheloop/
semantic-segmentation-of-aerial-imagery
[148] Z. Wang, X. Zeng, Z. Yan, J. Kang, and X. Sun, “Air-polsar-seg:
A large-scale data set for terrain segmentation in complex-scene
polsar images,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. ,
vol. 15, pp. 3830–3841, 2022.
[149] V . Sainte Fare Garnot and L. Landrieu, “Panoptic segmentation of
satellite image time series with convolutional temporal attention
networks,” ICCV , 2021.
[150] Z. Wu, “Muti-type aircraft of remote sensing images:
Mtarsi,” 2019. [Online]. Available: https://github.com/azavea/
cloud-model
[151] (2020) Rareplanes. [Online]. Available: https://www.
cosmiqworks.org/rareplanes-public-user-guide/
[152] S. Rawat. (2021) Cgi planes in satellite imagery w/
bboxes. [Online]. Available: https://www.kaggle.com/datasets/
aceofspades914/cgi-planes-in-satellite-imagery-w-bboxes
[153] P . Zhang, H. Xu, T. Tian, P . Gao, L. Li, T. Zhao, N. Zhang,
and J. Tian, “Sefepnet: Scale expansion and feature enhancement
pyramid network for sar aircraft detection with small sample
dataset,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. , vol. 15,
pp. 3365–3375, 2022.
17
[154] W. Yu, G. Cheng, M. Wang, Y. Yao, X. Xie, X. Yao, and J. Han,
“Mar20: A benchmark for military aircraft recognition in remote
sensing images,” National Remote Sensing Bulletin .
[155] K. Nogueira, C. da Silva, P . Gama, G. Machado, and J. A.
Dos Santos, “A tool for bridge detection in major infrastructure
works using satellite images.” WVC , 2019.
[156] S. Ji, S. Wei, and M. Lu, “Fully convolutional networks for
multisource building extraction from an open aerial and satellite
imagery data set,” IEEE TGRS , vol. 57, no. 1, pp. 574–586, 2018.
[157] S. P . Mohanty, J. Czakon, K. A. Kaczmarek, A. Pyskir,
P . Tarasiewicz, S. Kunwar, J. Rohrbach, D. Luo, M. Prasad, S. Fleer
et al. , “Deep learning for understanding satellite imagery: An
experimental survey,” Frontiers in Artiﬁcial Intelligence , vol. 3,
2020.
[158] (2018) 2018 open ai tanzania building footprint segmentation
challenge. [Online]. Available: https://competitions.codalab.
org/competitions/20100
[159] Microsoft. (2019) Australia building foot-
prints. [Online]. Available: https://github.com/microsoft/
AustraliaBuildingFootprints
[160] ——. (2019) Uganda tanzania building footprints
public. [Online]. Available: https://github.com/microsoft/
Uganda-Tanzania-Building-Footprints
[161] ——. (2019) Canadian building footprints. [Online]. Available:
https://github.com/Microsoft/CanadianBuildingFootprints
[162] AICyberTeam. (2022) Urban building classiﬁcation
dataset. [Online]. Available: https://github.com/AICyberTeam/
UBC-dataset
[163] J. Wang, L. Meng, W. Li, W. Yang, L. Yu, and G.-S. Xia, “Learning
to extract building footprints from off-nadir aerial images,” IEEE
Trans. Pattern Anal. Mach. Intell. , pp. 1–1, 2022.
[164] Y. L. G.-S. X. Q. L. Jian Ding, Nan Xue, “Learning roi transformer
for detecting oriented objects in aerial images,” in CVPR , June
2019.
[165] J. Ding, N. Xue, G.-S. Xia, X. Bai, W. Yang, M. Yang, S. Belongie,
J. Luo, M. Datcu, M. Pelillo, and L. Zhang, “Object detection in
aerial images: A large-scale benchmark and challenges,” IEEE
Trans. Pattern Anal. Mach. Intell. , pp. 1–1, 2021.
[166] S. Waqas Zamir, A. Arora, A. Gupta, S. Khan, G. Sun, F. Shah-
baz Khan, F. Zhu, L. Shao, G.-S. Xia, and X. Bai, “isaid: A
large-scale dataset for instance segmentation in aerial images,”
inCVPRW , 2019, pp. 28–37.
[167] VALID. (2020) Virtual aerial image dataset. [Online]. Available:
https://sites.google.com/view/valid-dataset
[168] E. Bondi, R. Jain, P . Aggrawal, S. Anand, R. Hannaford,
A. Kapoor, J. Piavis, S. Shah, L. Joppa, B. Dilkina et al. , “Birdsai:
A dataset for detection and tracking in aerial thermal infrared
videos,” in WACV , 2020, pp. 1747–1756.
[169] G. Gao, Q. Liu, and Y. Wang, “Counting from sky: A large-scale
data set for remote sensing object counting and a benchmark
method,” IEEE TGRS , vol. 59, no. 5, pp. 3642–3655, 2020.
[170] S. C. D. Rabbi, Jakaria; Chowdhury, “Oil and gas tank dataset,”
2020. [Online]. Available: https://data.mendeley.com/datasets/
bkxj8z84m9/3
[171] H. Karl and M. Md. (2019) Oil storage tanks. [Online].
Available: https://www.kaggle.com/datasets/towardsentropy/
oil-storage-tanks
[172] N. I. Bountos, I. Papoutsis, D. Michail, A. Karavias, P . Elias,
and I. Parcharidis, “Hephaestus: A large scale multitask dataset
towards insar understanding,” in CVPRW , June 2022, pp. 1453–
1462.
[173] (2019) Semantic drone dataset. [Online]. Available: https:
//www.tugraz.at/index.php?id=22387
[174] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learning
social etiquette: Human trajectory prediction in crowded scenes,”
inECCV , vol. 2, 2020.
[175] J. Solawetz. (2020) Aerial maritime drone dataset. [On-
line]. Available: https://public.roboﬂow.com/object-detection/
aerial-maritime
[176] L. A. Varga, B. Kiefer, M. Messmer, and A. Zell, “Seadronessee:
A maritime benchmark for detecting humans in open water,” in
WACV , 2022, pp. 2260–2270.
[177] J. Gasienica-Jozkowy, M. Knapik, and B. Cyganek, “An ensemble
deep learning method with optimized weights for drone-based
water rescue and surveillance,” Integrated Computer-Aided Engi-
neering , pp. 1–15, 01 2021.[178] M. Thoreau and F. Wilson, “Sarnet: A dataset for deep learning
assisted search and rescue with satellite imagery,” 2021.
[179] Z. Liu, L. Yuan, L. Weng, and Y. Yang, “A high resolution
optical satellite image dataset for ship recognition and some new
baselines,” in ICPRAM , vol. 2. SciTePress, 2017, pp. 324–331.
[180] Y. Wang, C. Wang, H. Zhang, Y. Dong, and S. Wei, “A sar
dataset of ship detection for deep learning under complex
backgrounds,” Remote Sensing , vol. 11, no. 7, 2019. [Online].
Available: http://www.mdpi.com/2072-4292/11/7/765
[181] S. Xian, W. Zhirui, S. Yuanrui, D. Wenhui, Z. Yue, and F. Kun,
“Air-sarship-1.0: High-resolution sar ship detection dataset,”
Journal of Radars , vol. 8, no. 6, pp. 852–863, 2019.
[182] F. Xue, W. Jin, S. Qiu, and J. Yang, “Rethinking automatic ship
wake detection: State-of-the-art cnn-based wake detection via
optical images,” IEEE TGRS , vol. 60, pp. 1–22, 2021.
[183] (2020) Casia-ship. [Online]. Available: https://www.rsaicp.com/
portal/dataDetail?id=14
[184] xview. (2022) xview3 competition. [Online]. Available: https:
//iuu.xview.us/
[185] I. Bozcan and E. Kayacan, “Au-air: A multi-modal unmanned
aerial vehicle dataset for low altitude trafﬁc surveillance,” in
ICRA . IEEE, 2020, pp. 8504–8510.
[186] R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein, “The highd
dataset: A drone dataset of naturalistic vehicle trajectories on
german highways for validation of highly automated driving
systems,” in ITSC , 2018, pp. 2118–2125.
[187] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann,
J. K ¨ummerle, H. K ¨onigshof, C. Stiller, A. de La Fortelle, and
M. Tomizuka, “INTERACTION Dataset: An INTERnational, Ad-
versarial and Cooperative moTION Dataset in Interactive Driv-
ing Scenarios with Semantic Maps,” arXiv:1910.03088 [cs, eess] ,
2019.
[188] J. Bock, R. Krajewski, T. Moers, S. Runde, L. Vater, and L. Eck-
stein, “The ind dataset: A drone dataset of naturalistic road user
trajectories at german intersections,” in IV, 2020, pp. 1929–1934.
[189] R. Krajewski, T. Moers, J. Bock, L. Vater, and L. Eckstein, “The
round dataset: A drone dataset of road user trajectories at round-
abouts in germany,” in ITSC , 2020, pp. 1–6.
[190] B. Weinstein, S. Marconi, A. Zare, S. Bohlman, S. Graves,
A. Singh, and E. White, “Neon tree crowns dataset,” Apr.
2020, gordon and Betty Moore Foundation: GBMF4563. [Online].
Available: https://doi.org/10.5281/zenodo.3765872
[191] S. F. Agency. (2021) Forest damages – larch case-
bearer 1.0. [Online]. Available: https://lila.science/datasets/
forest-damages-larch-casebearer/
[192] (2009) Overhead imagery research data set. [Online]. Available:
https://sourceforge.net/projects/oirds/
[193] H. Zhu, X. Chen, W. Dai, K. Fu, Q. Ye, and J. Jiao, “Orientation
robust object detection in aerial images using deep convolutional
neural network,” in ICIP . IEEE, 2015, pp. 3735–3739.
[194] P . R. De Almeida, L. S. Oliveira, A. S. Britto Jr, E. J. Silva Jr,
and A. L. Koerich, “Pklot–a robust dataset for parking lot clas-
siﬁcation,” Expert Systems with Applications , vol. 42, no. 11, pp.
4937–4949, 2015.
[195] M.-R. Hsieh, Y.-L. Lin, and W. H. Hsu, “Drone-based object
counting by spatially regularized regional proposal network,” in
ICCV , 2017, pp. 4145–4153.
[196] P . Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, and H. Ling,
“Detection and tracking meet drones challenge,” IEEE Trans.
Pattern Anal. Mach. Intell. , pp. 1–1, 2021.
[197] M. Haroon, M. Shahzad, and M. M. Fraz, “Multi-sized object
detection using spaceborne optical imagery,” IEEE J. Sel. Top.
Appl. Earth Obs. Remote Sens. , vol. 13, pp. 3032–3046, 2020.
[198] P . Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, and H. Ling,
“Detection and tracking meet drones challenge,” IEEE Trans.
Pattern Anal. Mach. Intell. , 2021.
[199] S. M. Azimi, R. Bahmanyar, C. Henry, and F. Kurz, “Eagle: Large-
scale vehicle detection dataset in real-world scenarios using aerial
imagery,” in ICPR , 2021, pp. 6920–6927.
[200] M. Mandal, L. K. Kumar, and S. K. Vipparthi, “Mor-uav: A
benchmark dataset and baselines for moving object recognition
in uav videos,” in ACM Multimedia , 2020, pp. 2626–2635.
[201] Y. Sun, B. Cao, P . Zhu, and Q. Hu, “Drone-based rgb-infrared
cross-modality vehicle detection via uncertainty-aware learning,”
IEEE TCSVT , pp. 1–1, 2022.
18
[202] Ari, M. Enxhi, and K. Elyes. (2019) Swimming pool and
car detection. [Online]. Available: https://www.kaggle.com/
datasets/kbhartiya83/swimming-pool-and-car-detection
[203] I. Weber, J. Bongartz, and R. Roscher, “Artiﬁve-potsdam: A
benchmark for learning with artiﬁcial objects for improved aerial
vehicle detection,” in IGARSS . IEEE, 2021, pp. 1214–1217.
[204] Y. Liu, J. Yao, X. Lu, M. Xia, X. Wang, and Y. Liu, “Roadnet:
Learning to comprehensively analyze road networks in complex
urban scenes from high-resolution remotely sensed images,”
IEEE TGRS , vol. 57, no. 4, pp. 2043–2056, 2019.
[205] E. Maggiori, Y. Tarabalka, G. Charpiat, and P . Alliez, “Can seman-
tic labeling methods generalize to any city? the inria aerial image
labeling benchmark,” in IGARSS . IEEE, 2017, pp. 3226–3229.
[206] G. M ´attyus, S. Wang, S. Fidler, and R. Urtasun, “Hd maps:
Fine-grained road segmentation by parsing ground and aerial
images,” in CVPR , 2016, pp. 3611–3619.
[207] G. Mattyus, S. Wang, S. Fidler, and R. Urtasun, “Enhancing road
maps by parsing aerial images around the world,” in ICCV , 2015,
pp. 1689–1697.
[208] F. Bastani, S. He, S. Abbar, M. Alizadeh, H. Balakrishnan,
S. Chawla, S. Madden, and D. DeWitt, “Roadtracer: Automatic
extraction of road networks from aerial images,” in CVPR , 2018,
pp. 4720–4728.
[209] (2020) Microsoft roaddetections. [Online]. Available: https:
//github.com/microsoft/RoadDetections
[210] S. Foga, P . L. Scaramuzza, S. Guo, Z. Zhu, R. D. Dilley Jr,
T. Beckmann, G. L. Schmidt, J. L. Dwyer, M. J. Hughes, and
B. Laue, “Cloud detection algorithm comparison and validation
for operational landsat data products,” Remote Sens. Environ. , vol.
194, pp. 379–390, 2017.
[211] S. Mohajerani and P . Saeedi, “Cloud-net: An end-to-end cloud
detection algorithm for landsat 8 imagery,” in IGARSS , July 2019,
pp. 1029–1032.
[212] S. Mohajerani and P . Saeedi, “Cloud-Net+: A Cloud Segmenta-
tion CNN for Landsat 8 Remote Sensing Imagery Optimized with
Filtered Jaccard Loss Function,” vol. 2001.08768, 2020.
[213] Z. Li, H. Shen, Q. Cheng, Y. Liu, S. You, and Z. He, “Deep
learning based cloud detection for remote sensing images by
the fusion of multi-scale convolutional features,” arXiv preprint
arXiv:1810.05801 , 2018.
[214] J. Li, Z. Wu, Z. Hu, C. Jian, S. Luo, L. Mou, X. X. Zhu, and
M. Molinier, “A lightweight deep learning-based cloud detection
method for sentinel-2a imagery fusing multiscale spectral and
spatial features,” IEEE TGRS , vol. 60, pp. 1–19, 2021.
[215] J. A. Barsi, K. Lee, G. Kvaran, B. L. Markham, and J. A. Pedelty,
“The spectral response of the landsat-8 operational land imager,”
Remote sensing , vol. 6, no. 10, pp. 10 232–10 251, 2014.
[216] S. Ji, Z. Zhang, C. Zhang, S. Wei, M. Lu, and Y. Duan, “Learning
discriminative spatiotemporal features for precise crop classiﬁ-
cation from multi-temporal satellite images,” Int. J. Remote Sens. ,
vol. 41, no. 8, pp. 3162–3174, 2020.
[217] G. Weikmann, C. Paris, and L. Bruzzone, “Timesen2crop: A
million labeled samples dataset of sentinel 2 image time series for
crop-type classiﬁcation,” IEEE J. Sel. Top. Appl. Earth Obs. Remote
Sens. , vol. 14, pp. 4699–4708, 2021.
[218] M. T. Chiu, X. Xu, Y. Wei, Z. Huang, A. G. Schwing, R. Brun-
ner, H. Khachatrian, H. Karapetyan, I. Dozier, G. Rose et al. ,
“Agriculture-vision: A large aerial image database for agricul-
tural pattern analysis,” in CVPR , 2020, pp. 2828–2838.
[219] I. de G ´elis, S. Lef `evre, and T. Corpetti, “Change detection in urban
point clouds: An experimental comparison with simulated 3d
datasets,” Remote Sensing , vol. 13, no. 13, p. 2629, 2021.
[220] L. Shen, Y. Lu, H. Chen, H. Wei, D. Xie, J. Yue, R. Chen, S. Lv, and
B. Jiang, “S2looking: A satellite side-looking dataset for building
change detection,” Remote Sensing , vol. 13, no. 24, p. 5094, 2021.
[221] L. T. Luppino, F. M. Bianchi, G. Moser, and S. N. Anﬁnsen,
“Unsupervised image regression for heterogeneous change de-
tection,” arXiv preprint arXiv:1909.05948 , 2019.
[222] K. Yang, G.-S. Xia, Z. Liu, B. Du, W. Yang, M. Pelillo, and
L. Zhang, “Asymmetric siamese networks for semantic change
detection in aerial images,” IEEE TGRS , vol. 60, pp. 1–18, 2021.
[223] C. F. Brown, S. P . Brumby, B. Guzder-Williams, T. Birch, S. B.
Hyde, J. Mazzariello, W. Czerwinski, V . J. Pasquarella, R. Haertel,
S. Ilyushchenko et al. , “Dynamic world, near real-time global 10
m land use land cover mapping,” Scientiﬁc Data , vol. 9, no. 1, pp.
1–17, 2022.[224] X. Lu, B. Wang, X. Zheng, and X. Li, “Exploring models and
data for remote sensing image caption generation,” IEEE TGRS ,
vol. 56, no. 4, pp. 2183–2195.
[225] Chen-Yang-Liu. (2022) Remote sensing image change captioning
(rsicc). [Online]. Available: https://github.com/Chen-Yang-Liu/
RSICC
[226] S. Lobry, D. Marcos, J. Murray, and D. Tuia, “Rsvqa: Visual
question answering for remote sensing data,” IEEE TGRS , vol. 58,
no. 12, pp. 8555–8566, 2020.
[227] Z. Yuan, L. Mou, Z. Xiong, and X. X. Zhu, “Change detection
meets visual question answering,” IEEE TGRS , 2022.
[228] T. P . Sonali Patil, Bharath Comandur and A. C. Kak, “A
New Stereo Benchmarking Dataset for Satellite Images,”
arXiv:1907.04404 , 2019. [Online]. Available: http://arxiv.org/
abs/1907.04404
[229] J. Liu and S. Ji, “A novel recurrent encoder-decoder structure for
large-scale multi-view stereo reconstruction from an open aerial
dataset,” in CVPR , 2020, pp. 6050–6059.
[230] M. K ¨olle, D. Laupheimer, S. Schmohl, N. Haala, F. Rottensteiner,
J. D. Wegner, and H. Ledoux, “The hessigheim 3d (h3d) bench-
mark on semantic segmentation of high-resolution 3d point
clouds and textured meshes from uav lidar and multi-view-
stereo,” ISPRS Open Journal of Photogrammetry and Remote Sensing ,
vol. 1, p. 100001, 2021.
[231] S. Workman, R. Souvenir, and N. Jacobs, “Wide-area image
geolocalization with aerial reference imagery,” in ICCV , 2015, pp.
3961–3969.
[232] Z. Zheng, Y. Wei, and Y. Yang, “University-1652: A multi-view
multi-source benchmark for drone-based geo-localization,” ACM
Multimedia , 2020.
[233] K. Kashinath, M. Mudigonda, S. Kim, L. Kapp-Schwoerer,
A. Graubner, E. Karaismailoglu, L. Von Kleist, T. Kurth,
A. Greiner, A. Mahesh et al. , “Climatenet: an expert-labeled
open dataset and deep learning architecture for enabling high-
precision analyses of extreme weather,” Geoscientiﬁc Model Devel-
opment , vol. 14, no. 1, pp. 107–124, 2021.
[234] P . Gr ¨onquist, C. Yao, T. Ben-Nun, N. Dryden, P . Dueben, S. Li, and
T. Hoeﬂer, “Deep learning for post-processing ensemble weather
forecasts,” 2020.
[235] (2022) Hyperview challenge. [Online]. Available: https://
platform.ai4eo.eu/seeing-beyond-the-visible
[236] R. E. Foundation. (2022) Tropical cyclone wind estimation
competition dataset, version 1.0, radiant mlhub. [Online].
Available: https://doi.org/10.34911/rdnt.xs53up
[237] P . Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image
translation with conditional adversarial networks,” CVPR , 2017.
[238] O. Ma ˜nas, A. Lacoste, X. Giro-i Nieto, D. Vazquez, and P . Ro-
driguez, “Seasonal contrast: Unsupervised pre-training from un-
curated remote sensing data,” arXiv preprint arXiv:2103.16607 ,
2021.
[239] Y. Zhong, X. Hu, C. Luo, X. Wang, J. Zhao, and L. Zhang, “Whu-
hi: Uav-borne hyperspectral with high spatial resolution (h2)
benchmark datasets and classiﬁer for precise crop identiﬁcation
based on deep convolutional neural network with crf,” Remote
Sens. Environ. , vol. 250, p. 112012, 2020.
[240] M. O. Turkoglu, S. D’Aronco, G. Perich, F. Liebisch, C. Streit,
K. Schindler, and J. D. Wegner, “Crop mapping from image time
series: Deep learning with multi-scale label hierarchies,” Remote
Sens. Environ. , vol. 264, p. 112603, 2021.
[241] M. Schneider, A. Broszeit, and M. K ¨orner, “EuroCrops: A pan-
european dataset for time series crop type classiﬁcation,” in BiDS ,
P . Soille, S. Loekken, and S. Albani, Eds.
[242] Y.-R. Wang and X.-M. Li, “Arctic sea ice cover data from space-
borne synthetic aperture radar by deep learning,” Earth System
Science Data , vol. 13, no. 6, pp. 2723–2742, 2021.
[243] A. Van Etten, D. Lindenbaum, and T. M. Bacastow, “Spacenet:
A remote sensing dataset and challenge series,” arXiv preprint
arXiv:1807.01232 , 2018.
[244] A. Shakeel, W. Sultani, and M. Ali, “Deep built-structure counting
in satellite imagery using attention based re-weighting,” ISPRS J.
Photogramm. Remote Sens. , vol. 151, pp. 313–321, 2019.
[245] J. Shermeyer, D. Hogan, J. Brown, A. Van Etten, N. Weir,
F. Paciﬁci, R. Hansch, A. Bastidas, S. Soenen, T. Bacastow et al. ,
“Spacenet 6: Multi-sensor all weather mapping dataset,” in
CVPRW , 2020, pp. 196–197.
19
[246] A. Van Etten, D. Hogan, J. M. Manso, J. Shermeyer, N. Weir,
and R. Lewis, “The multi-temporal urban development spacenet
dataset,” in CVPR , 2021, pp. 6398–6407.
[247] F. Kong, B. Huang, K. Bradbury, and J. Malof, “The synthinel-
1 dataset: A collection of high resolution synthetic overhead
imagery for building segmentation,” in WACV , 2020, pp. 1814–
1823.
[248] V . Mnih, “Machine learning for aerial image labeling,” Ph.D.
dissertation, University of Toronto, 2013.
[249] G. Labs. (2018) Open cities ai challenge dataset,version 1.0,
radiant mlhub. [Online]. Available: https://doi.org/10.34911/
rdnt.f94cxb
[250] G. repository. (2018) Segmenting buildings in satellite images.
[Online]. Available: https://github.com/QiaoWenfan/Building
Dataset
[251] P . Kaiser, J. D. Wegner, A. Lucchi, M. Jaggi, T. Hofmann, and
K. Schindler, “Learning aerial image segmentation from online
maps,” IEEE TGRS , vol. 55, no. 11, pp. 6054–6068, 2017.
[252] L. Baetens, C. Desjardins, and O. Hagolle, “Validation of coper-
nicus sentinel-2 cloud masks obtained from maja, sen2cor, and
fmask processors using reference cloud masks generated with a
supervised active learning procedure,” Remote Sensing , vol. 11,
no. 4, p. 433, 2019.
[253] S. Ji, P . Dai, M. Lu, and Y. Zhang, “Simultaneous cloud detection
and removal from bitemporal remote sensing images using cas-
cade convolutional neural networks,” IEEE TGRS , vol. 59, no. 1,
pp. 732–748, 2020.
[254] Q. He, X. Sun, Z. Yan, and K. Fu, “Dabnet: Deformable contextual
and boundary-weighted network for cloud detection in remote
sensing images,” IEEE TGRS , pp. 1–16, 2021.
[255] G. repository. (2018) The azavea cloud dataset. [Online].
Available: https://github.com/azavea/cloud-model
[256] R. E. Foundation. (2022) Sentinel-2 cloud cover segmentation
dataset (version 1). radiant mlhub. [Online]. Available:
https://doi.org/10.34911/rdnt.hfq6m7
[257] Z. Shao, K. Yang, and W. Zhou, “Performance evaluation of
single-label and multi-label remote sensing image retrieval using
a dense labeling dataset,” Remote Sensing , vol. 10, no. 6, p. 964,
2018.
[258] J. Yuan, S. S. Gleason, and A. M. Cheriyadat, “Systematic bench-
marking of aerial image segmentation,” IEEE Geosci. Remote Sens.
Lett., vol. 10, no. 6, pp. 1527–1531, 2013.
[259] G. Moser, D. Tuia, and M. Shimoni, “2015 ieee grss data fusion
contest: Extremely high resolution lidar and optical data [techni-
cal committees],” IEEE GRSM , vol. 3, no. 1, pp. 40–41, 2015.
[260] M. Volpi and V . Ferrari, “Semantic segmentation of urban scenes
by learning local class interactions,” in CVPRW , 2015, pp. 1–9.
[261] (2016) Dstl satellite imagery feature detection. [On-
line]. Available: https://www.kaggle.com/competitions/
dstl-satellite-imagery-feature-detection/data
[262] M. Zhang, X. Hu, L. Zhao, Y. Lv, M. Luo, and S. Pang, “Learning
dual multi-scale manifold ranking for semantic segmentation of
high-resolution images,” Remote Sensing , vol. 9, no. 5, p. 500, 2017.
[263] M. Scanlon, “Semantic annotation of aerial images using deep
learning, transfer learning, and synthetic training data,” Ph.D.
dissertation, 09 2018.
[264] I. Nigam, C. Huang, and D. Ramanan, “Ensemble knowledge
transfer for semantic segmentation,” in WACV . IEEE, 2018, pp.
1499–1508.
[265] Y. Chen, Y. Wang, P . Lu, Y. Chen, and G. Wang, “Large-scale struc-
ture from motion with semantic constraints of aerial images,” in
PRCV . Springer, 2018, pp. 347–359.
[266] (2019) Drone deploy medium dataset. [Online]. Available: https:
//github.com/dronedeploy/dd-ml-segmentation-benchmark
[267] M. Fonder and M. Van Droogenbroeck, “Mid-air: A multi-modal
dataset for extremely low altitude drone ﬂights,” in CVPRW ,
2019, pp. 0–0.
[268] A. Rangnekar, N. Mokashi, E. J. Ientilucci, C. Kanan, and M. J.
Hoffman, “Aerorit: A new scene for hyperspectral image analy-
sis,” IEEE TGRS , vol. 58, no. 11, pp. 8116–8124, 2020.
[269] R. Roscher, M. Volpi, C. Mallet, L. Drees, and J. D. Wegner, “Sem-
city toulouse: A benchmark for building instance segmentation
in satellite images,” in ISPRS Annals , vol. 5, 2020, pp. 109–116.
[270] Y. Lyu, G. Vosselman, G.-S. Xia, A. Yilmaz, and M. Y. Yang,
“Uavid: A semantic segmentation dataset for uav imagery,”
ISPRS J. Photogramm. Remote Sens. , vol. 165, pp. 108 –119, 2020. [Online]. Available: http://www.sciencedirect.com/
science/article/pii/S0924271620301295
[271] (2021) 2021 ieee grss data fusion contest track
dse. [Online]. Available: www.grss-ieee.org/community/
technical-committees/data-fusion
[272] I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu,
F. Hughes, D. Tuia, and R. Raskar, “Deepglobe 2018: A challenge
to parse the earth through satellite images,” in CVPRW , 2018, pp.
172–181.
[273] C. K. Z. A.-G. Karantzalos, K.; Karakizi, “Hyrank hyperspectral
satellite dataset i (version v001),” in I.W.III/4; ISPRS: Hannover,
Germany , 2018.
[274] B. Le Saux, N. Yokoya, R. H ¨ansch, and M. Brown, “2019 ieee grss
data fusion contest: large-scale semantic 3d reconstruction,” IEEE
GRSM , vol. 7, no. 4, pp. 33–36, 2019.
[275] (2019) Aerial hyperspectral remote sensing image dataset
of horseshoe bay village in xiongan. [Online]. Avail-
able: http://www.hrs-cas.com/a/share/shujuchanpin/2019/
0501/1049.html
[276] C. Robinson, L. Hou, K. Malkin, B. Dilkina, and N. Jojic, “Large
scale high-resolution land cover mapping with multi-resolution
data,” in CVPR , 2019, pp. 12 726–12 735.
[277] N. Yokoya, P . Ghamisi, R. H ¨ansch, and M. Schmitt, “2020 IEEE
GRSS data fusion contest: Global land cover mapping with weak
supervision [technical committees],” IEEE GRSM , vol. 8, no. 1,
pp. 154–157, 2020.
[278] (2019) Ccf big data & computing intelligence contest
2020. [Online]. Available: https://www.datafountain.cn/
competitions/466
[279] J. Wang, Z. Zheng, A. Ma, X. Lu, and Y. Zhong, “Loveda: A
remote sensing land-cover dataset for domain adaptive semantic
segmentation,” arXiv preprint arXiv:2110.08733 , 2021.
[280] J. Castillo-Navarro, B. Le Saux, A. Boulch, N. Audebert, and
S. Lef `evre, “Semi-supervised semantic segmentation in earth
observation: The minifrance suite, dataset analysis and multi-task
network study,” Machine Learning , pp. 1–36, 2021.
[281] G. Baier, A. Deschemps, M. Schmitt, and N. Yokoya, “Synthesiz-
ing optical and sar imagery from land cover maps and auxiliary
raster data,” IEEE TGRS , vol. 60, pp. 1–12, 2021.
[282] R. Khaldi, D. Alcaraz-Segura, E. Guirado, Y. Benhammou,
A. El Aﬁa, F. Herrera, and S. Tabik, “Timespec4lulc: a global
multispectral time series database for training lulc mapping
models with machine learning,” Earth System Science Data , vol. 14,
no. 3, pp. 1377–1411, 2022.
[283] X.-Y. Tong, G.-S. Xia, and X. X. Zhu, “Enabling country-scale land
cover mapping with meter-resolution satellite imagery,” arXiv
preprint arXiv:2209.00727 , 2022.
[284] J. Li, X. Huang, and L. Tu, “Whu-ohs: A benchmark dataset for
large-scale hersepctral image classiﬁcation,” Int. J. Appl. Earth
Obs. Geoinf. , vol. 113, p. 103022, 2022.
[285] N. Johnson, W. Treible, and D. Crispell, “Opensentinelmap: A
large-scale land use dataset using openstreetmap and sentinel-2
imagery,” in CVPR , 2022, pp. 1333–1341.
[286] B. Le Saux, N. Yokoya, R. H ¨ansch, and S. Prasad, “2018 ieee grss
data fusion contest: Multimodal land use classiﬁcation [technical
committees],” IEEE GRSM , vol. 6, no. 1, pp. 52–54, 2018.
[287] R. Wenger, A. Puissant, J. Weber, L. Idoumghar, and G. Forestier,
“Multisenge: a multimodal and multitemporal benchmark
dataset for land use/land cover remote sensing applications,”
ISPRS Annals , pp. 635–640, 2022.
[288] N. Hurst-Tarrab, L. Chang, M. Gonzalez-Mendoza, and
N. Hernandez-Gress, “Robust parking block segmentation from a
surveillance camera perspective,” Applied Sciences , vol. 10, no. 15,
p. 5364, 2020.
[289] S. He and H. Balakrishnan, “Lane-level street map extraction
from aerial imagery,” in WACV , 2022, pp. 2080–2089.
[290] R. H ¨ansch, J. Arndt, D. Lunga, M. Gibb, T. Pedelose, A. Boedi-
hardjo, D. Petrie, and T. M. Bacastow, “Spacenet 8-the detection
of ﬂooded roads and buildings,” in CVPR , 2022, pp. 1472–1480.
[291] Q. Chen, L. Wang, Y. Wu, G. Wu, Z. Guo, and S. L. Waslander,
“Aerial imagery for roof segmentation: A large-scale dataset
towards automatic mapping of buildings,” ISPRS J. Photogramm.
Remote Sens. , vol. 147, pp. 42–55, 2019.
[292] Drivendata. (2019) Open AI caribbean challenge:mapping disas-
ter risk from aerial imagery. [Online]. Available: https://www.
drivendata.org/competitions/58/disaster-response-roof-type/
20
[293] S. Krapf, L. Bogenrieder, F. Netzler, G. Balke, and M. Lienkamp,
“Rid—roof information dataset for computer vision-based pho-
tovoltaic potential assessment,” Remote Sensing , vol. 14, no. 10, p.
2299, 2022.
[294] Q. Zhang, R. Cong, C. Li, M.-M. Cheng, Y. Fang, X. Cao, Y. Zhao,
and S. Kwong, “Dense attention ﬂuid network for salient object
detection in optical remote sensing images,” IEEE Trans. Image
Process. , vol. 30, pp. 1305–1317, 2020.
[295] S. Luo, H. Li, and H. Shen, “Deeply supervised convolutional
neural network for shadow detection based on a novel aerial
shadow imagery dataset,” ISPRS J. Photogramm. Remote Sens. , vol.
167, pp. 443–457, 2020.
[296] (2020) Bh-pools/watertanks datasets. [Online]. Available: http://
patreo.dcc.ufmg.br/2020/07/29/bh-pools-watertanks-datasets/
[297] S. M. Azimi, C. Henry, L. Sommer, A. Schumann, and
E. Vig, “Skyscapes ﬁne-grained semantic understanding of aerial
scenes,” in CVPR , 2019, pp. 7393–7403.
[298] R. Abdelfattah, X. Wang, and S. Wang, “Ttpla: An aerial-image
dataset for detection and segmentation of transmission towers
and power lines,” in ACCV , 2020.
[299] (2020) Satellite images of water bodies. [Online]. Avail-
able: https://www.kaggle.com/datasets/franciscoescobar/
satellite-images-of-water-bodies
[300] F. Huot, R. L. Hu, N. Goyal, T. Sankar, M. Ihme, and Y.-F.
Chen, “Next day wildﬁre spread: A machine learning dataset
to predict wildﬁre spreading from remote-sensing data,” IEEE
TGRS , vol. 60, pp. 1–13, 2022.
[301] A. Fujita, K. Sakurada, T. Imaizumi, R. Ito, S. Hikosaka, and
R. Nakamura, “Damage detection from aerial images via con-
volutional neural networks,” in IAPR MV A . IEEE, 2017, pp. 5–8.
[302] H. Chen and Z. Shi, “A spatial-temporal attention-based method
and a new dataset for remote sensing image change detection,”
Remote Sensing , vol. 12, no. 10, 2020. [Online]. Available:
https://www.mdpi.com/2072-4292/12/10/1662
[303] R. Gupta, R. Hosfelt, S. Sajeev, N. Patel, B. Goodman, J. Doshi,
E. Heim, H. Choset, and M. Gaston, “xBD: A dataset for as-
sessing building damage from satellite imagery,” arXiv preprint
arXiv:1911.09296 , 2019.
[304] M. Liu, Z. Chai, H. Deng, and R. Liu, “A cnn-transformer
network with multi-scale context aggregation for ﬁne-grained
cropland change detection,” IEEE J. Sel. Top. Appl. Earth Obs.
Remote Sens. , 2022.
[305] C. Benedek and T. Szir ´anyi, “Change detection in optical aerial
images by a multilayer conditional mixed markov model,” IEEE
TGRS , vol. 47, no. 10, pp. 3416–3430, 2009.
[306] C. Wu, B. Du, and L. Zhang, “Slow feature analysis for change
detection in multispectral imagery,” IEEE TGRS , vol. 52, no. 5,
pp. 2858–2874, 2013.
[307] M. Volpi, G. Camps-Valls, and D. Tuia, “Spectral alignment
of multi-temporal cross-sensor images with automated kernel
canonical correlation analysis,” ISPRS J. Photogramm. Remote
Sens. , vol. 107, pp. 50–63, 2015.
[308] Q. Wang, Z. Yuan, Q. Du, and X. Li, “GETNET: A general
end-to-end 2-d cnn framework for hyperspectral image change
detection,” IEEE TGRS , vol. 57, no. 1, pp. 3–13, 2018.
[309] R. Caye Daudt, B. Le Saux, A. Boulch, and Y. Gousseau, “Ur-
ban change detection for multispectral earth observation using
convolutional neural networks,” in IGARSS , July 2018.
[310] (2018) Aerial change detection in video games. [On-
line]. Available: https://www.kaggle.com/datasets/kmader/
aerial-change-detection-in-video-games
[311] M. Lebedev, Y. V . Vizilter, O. Vygolov, V . Knyaz, and A. Y. Rubis,
“Change detection in remote sensing images using conditional
adversarial networks.” ISPRS Archives , vol. 42, no. 2, 2018.
[312] (2016) Multitemporal hyperspectral change detection.
[Online]. Available: https://gitlab.citius.usc.es/hiperespectral/
ChangeDetectionDataset
[313] C. Wu, L. Zhang, and L. Zhang, “A scene change detection frame-
work for multi-temporal very high resolution remote sensing
images,” Signal Processing , vol. 124, pp. 184–197, 2016.
[314] M. Zhang and W. Shi, “A feature difference convolutional
neural network-based change detection method,” IEEE
TGRS , 2020. [Online]. Available: https://ieeexplore.ieee.org/
document/9052762
[315] C. Zhang, P . Yue, D. Tapete, L. Jiang, B. Shangguan, L. Huang,
and G. Liu, “A deeply supervised image fusion network for
change detection in high resolution bi-temporal remote sensingimages,” ISPRS J. Photogramm. Remote Sens. , vol. 166, pp. 183–200,
2020.
[316] J. L ´opez-Fandi ˜no, A. S. Garea, D. B. Heras, and F. Arg ¨uello,
“Stacked autoencoders for multiclass change detection in hyper-
spectral images,” in IGARSS . IEEE, 2018, pp. 1906–1909.
[317] S. Tian, A. Ma, Z. Zheng, and Y. Zhong, “Hi-ucd: A large-scale
dataset for urban semantic change detection in remote sensing
imagery,” arXiv preprint arXiv:2011.03247 , 2020.
[318] D. Peng, L. Bruzzone, Y. Zhang, H. Guan, H. Ding, and X. Huang,
“Semicdnet: A semisupervised convolutional neural network for
change detection in high resolution remote-sensing images,”
IEEE TGRS , vol. 59, no. 7, pp. 5891–5906, 2020.
[319] A. Moghimi, A. Mohammadzadeh, T. Celik, and M. Amani,
“A novel radiometric control set sample selection strategy for
relative radiometric normalization of multitemporal satellite im-
ages,” IEEE TGRS , vol. 59, no. 3, pp. 2503–2519, 2020.
[320] R. Shao, C. Du, H. Chen, and J. Li, “Sunet: Change detection
for heterogeneous remote sensing images from satellite and uav
using a dual-channel fully convolution network,” Remote Sensing ,
vol. 13, no. 18, p. 3750, 2021.
[321] Q. Shi, M. Liu, S. Li, X. Liu, F. Wang, and L. Zhang, “A deeply
supervised attention metric-based network and an open aerial
image dataset for remote sensing change detection,” IEEE TGRS ,
vol. 60, pp. 1–16, 2021.
[322] J. Yuan, L. Ru, S. Wang, and C. Wu, “Wh-mavs: A novel dataset
and deep learning benchmark for multiple land use and land
cover applications,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. ,
vol. 15, pp. 1575–1590, 2022.
[323] M. Leenstra, D. Marcos, F. Bovolo, and D. Tuia, “Self-supervised
pre-training enhances change detection in sentinel-2 imagery,” in
ICPR . Springer, 2021, pp. 578–590.
[324] A. Toker, L. Kondmann, M. Weber, M. Eisenberger, A. Camero,
J. Hu, A. P . Hoderlein, C. Senaras, T. Davis, D. Cremers,
G. Marchisio, X. X. Zhu, and L. Leal-Tai ´e, “DynamicEarthNet:
Daily Multi-Spectral Satellite Dataset for Semantic Change Seg-
mentation,” in CVPR , 2022.
[325] H. Li, F. Zhu, X. Zheng, M. Liu, and G. Chen, “Mscdunet:
A deep learning framework for built-up area change detection
integrating multispectral, sar and vhr data,” IEEE J. Sel. Top. Appl.
Earth Obs. Remote Sens. , 2022.
[326] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient esti-
mation of word representations in vector space,” arXiv preprint
arXiv:1301.3781 , 2013.
[327] I. Dimitrovski, I. Kitanovski, P . Panov, N. Simidjievski, and D. Ko-
cev, “Aitlas: Artiﬁcial intelligence toolbox for earth observation,”
arXiv preprint arXiv:2201.08789 , 2022.
[328] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch:
An imperative style, high-performance deep learning library,”
NeurIPS , vol. 32, 2019.
[329] M. Lu, J. Liu, F. Wang, and Y. Xiang, “Multi-task learning of
relative height estimation and semantic segmentation from single
airborne rgb images,” Remote Sensing , vol. 14, no. 14, p. 3450,
2022.
[330] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
“Swin transformer: Hierarchical vision transformer using shifted
windows,” in ICCV , 2021, pp. 10 012–10 022.
[331] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie,
“A convnet for the 2020s,” in CVPR , 2022, pp. 11 976–11 986.
[332] G. Sumbul, J. Kang, T. Kreuziger, F. Marcelino, H. Costa, P . Bene-
vides, M. Caetano, and B. Demir, “Bigearthnet dataset with a new
class-nomenclature for remote sensing image understanding,”
arXiv preprint arXiv:2001.06372 , 2020.
[333] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai,
T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al. ,
“Mlp-mixer: An all-mlp architecture for vision,” NeurIPS , vol. 34,
pp. 24 261–24 272, 2021.
[334] X. Chen, H. Fan, R. Girshick, and K. He, “Improved base-
lines with momentum contrastive learning,” arXiv preprint
arXiv:2003.04297 , 2020.
[335] K. He, G. Gkioxari, P . Doll ´ar, and R. Girshick, “Mask r-cnn,” in
ICCV , 2017, pp. 2961–2969.
[336] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethink-
ing atrous convolution for semantic image segmentation,” arXiv
preprint arXiv:1706.05587 , 2017.
[337] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and
P . Luo, “Segformer: Simple and efﬁcient design for semantic
21
segmentation with transformers,” NeurIPS , vol. 34, pp. 12 077–
12 090, 2021.
[338] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional net-
works for semantic segmentation,” in CVPR , 2015, pp. 3431–3440.
[339] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam,
“Encoder-decoder with atrous separable convolution for seman-
tic image segmentation,” in ECCV , 2018, pp. 801–818.
[340] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing
network,” in CVPR , 2017, pp. 2881–2890.
[341] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, “Uniﬁed perceptual
parsing for scene understanding,” in ECCV , 2018, pp. 418–434.
1
EarthNets: Empowering AI in Earth Observation
Supplementary Materials
Zhitong Xiong, Member, IEEE , Fahong Zhang, Yi Wang, Yilei Shi, Member, IEEE ,
and Xiao Xiang Zhu, Fellow, IEEE
!
OUTLINE
In this supplementary material, we present the datasets
constructed for Earth observation that are not included in
the image classiﬁcation, object detection, semantic segmen-
tation, and change detection tasks. Next, the implementa-
tion details of the proposed EarthNets platform and the
benchmarking experiments are introduced. Finally, more
visualization results and detailed benchmarking results are
provided.
1 O THER EARTH OBSERVATION TASKS
Image classiﬁcation, object detection, semantic segmenta-
tion, and time-series modeling (change detection) are four
fundamental tasks in both the computer vision and remote
sensing research community. Although many real-world
applications can be framed as one or more of these four
tasks, there are still some important Earth observation tasks
that do not fall into these categories. To provide a compre-
hensive review of these datasets, in this study, we arrange
them into 33 different research domains, as presented in
Table 1. A detailed list of the datasets can be retrieved
via the website page1.Self-supervised learning (SSL) has
been a hot research topic in the computer vision community
[77], [78]. Recently, several datasets [17], [79] have been
constructed to learn representations using self-supervised
learning methods on remote sensing data. As there is a large
amount of data from satellites without labels, SSL has great
potential to improve performance on down-stream tasks
and real-world applications.
Image enhancement aims to convert an input image
to an enhanced one that has richer information and better
details. For instance, super resolution [44], [45], [46], [47],
image dehazing [15], and cloud removal [8], [9], [80] are
•Z. Xiong, F. Zhang and X. X. Zhu are with the Chair of Data Science
in Earth Observation, Technical University of Munich (TUM), 80333
Munich, Germany.
•Y. Wang is with the Chair of Data Science in Earth Observation, Tech-
nical University of Munich (TUM), Germany, and the Remote Sensing
Technology Institute, German Aerospace Center (DLR), Germany.
•Y. Shi is with the Chair of Remote Sensing Technology, Technical Univer-
sity of Munich (TUM), 80333 Munich, Germany.
1. https://earthnets.retool.com/embedded/public/
676aa812-0dca-4e3b-a596-b043d852571dresearch tasks that are useful for improving the image
quality for many downstream applications.
Multi-modal learning attempts to combine the strengths
of different modalities of data to enhance the representations
for different tasks. Image captioning [25], [26] and visual
question answering (VQA) [41], [68] combine natural lan-
guage and image data to enable a more natural interaction
between end-users and artiﬁcial intelligence systems. Apart
from that, there are also remote sensing datasets designed
for multi-modal data fusion [13], [14] and cross-view geo-
localization [19], [20], [21] tasks.
3D understanding is crucial for many real-world appli-
cations like urban planning, disaster monitoring, ﬂood man-
agement, and autonomous driving. Hence, many datasets
have been designed for different tasks, including 3D re-
construction, point cloud analysis [52], [53], [54], [57], and
multi-view stereo [32], [33], [34].
Earth system modeling seeks to study physical, chem-
ical, and biological processes in order to understand the
Earth planet with complex integration of environmental
variables. To this end, datasets are designed for plant/tree
[35], [36], [37] analysis, forest monitoring [16], soil parameter
estimation [43], geophysical [24], air quality monitoring [4],
and population estimation [38]. There are also Earth obser-
vation datasets constructed for climate variable estimation
[72] and weather forecasting [71].
2 I MPLEMENTATION OF EARTH NETSPLATFORM
In this section, we will detail the implementation of the main
libraries in the EarthNets platform Dataset4EO . At present,
there are some difﬁculties in loading RS datasets, especially
for researchers in other communities. 1) The datasets have
different downloading links, folder structures, ﬁle formats,
data modalities, research domains, and annotation levels.
It would be helpful if it were possible to download, de-
compress, and split the dataset automatically. 2) Images in
RS datasets usually contain multiple modalities and bands.
To accelerate the training speed, it is better to move the
data augmentation to GPU. 3) For datasets with very large
volumes or time-series streaming data, support iterable-
style data pipes would be useful to handle the dataset
loading process.
Remote Sensing Tasks Based on the Dataset4EO li-
brary, we build the RS Classiﬁcation, RS Detection, and RSarXiv:2210.04936v2  [cs.CV]  7 Dec 2022
2
TABLE 1
Detailed Information of 83 Other RS Datasets. These datasets are grouped into 33 different research domains in alphabetical order. The download
links for all these datasets can be found at https://earthnets.github.io.
Domain Name Year #Samples Sample Size #Classes Modailty Resolution Vol.(GB) #Cita.
Action Event Okutama-Action [1] 2017 77000 3840×2160 12 RGB UAV@10 ∼45m 25.9 130
AgriculturePaddy Rice Maps South Korea [2] 2022 12942 256 / Sentinel-1 10m 0.198 /
Paddy Rice Labeling South Korea [3] 2022 / / / Sentinel-2 / 0.0016 /
Air Quality Air Quality e-Reporting [4] 2021 / / / / / / /
Anomaly Objects Aerial Anomaly Detection [5] 2022 / / 2 RGB UAV / 0
Building Urban 3D Challenge [6] 2017 157,000 / 2 RGB 0.5m / 20
Chronology Draper Satellite Image Chronology [7] 2016 1,000 3100x2329 / RGB / 36 /
CloudSTGAN Cloud Removal [8] 2019 217190 256 2 Sentinel-2,RGBIR 10m 1.5 35
SEN12MS-CR [9] 2020 122218 256 / Sentinel-1,Sentinel-2 10 ∼60m 272 99
CountingDLR-ACD [10] 2019 33 4458 2 RGB 0.045 ∼0.15m / 28
DroneCrowd [11] 2020 3360 1920x1080 2 RGB UAV 1 8
RSOC [12] 2020 3057 2500 4 RGB / 0.082 14
Data FusionSEN1-2 [13] 2018 282384 256 / Sentinel-1,Sentinel-2 10m 42.68 122
QXS-SAROPT [14] 2021 40000 256 / SAR,RGB 1m 2.7 9
Dehazing SateHaze1k [15] 2017 1,200 512 / RGB 3m 1.2 14
Forest Forest Canopy Height [16] 2018 1105 / / LiDAR 1m 62.9 0
Self-supervisedSeCo [17] 2021 24000x5 264 / Sentinel-2 10m 43.6 38
SSL4EO-S12 [18] 2022 1004316 264 / Sentinel-1,Sentinel-2 10m ∼60m 1500 0
Geo-localizationCVUSA [19] 2017 44,416 750x750 0 RGB 0.3m 15.7 131
University-1652 [20] 2020 146,580 / / RGB / / 49
UCF Cross View Dataset [21] 2017 35404 1200x1200 0 RGB / 59.5 130
CVACT [22] 2019 128334 1200x1200 0 RGB 0.5m 152.8 71
University-1652 [20] 2020 50220 1024x1024 0 RGB / 58.4 49
VIGOR [23] 2021 90618 640x640 0 RGB 0.114m 94.2 16
Geophysical TenGeoP-SARwv [24] 2019 37000 / 10 SAR 5m 31.7 3
Image CaptioningUCM caption [25] 2016 2,100 256 21 RGB 0.3m 0.255 97
Sydney caption [25] 2016 613 500 7 RGB 0.5m 0.298 97
RSICD [25] 2017 10,921 224 30 RGB / 0.46 210
LEVIR-CC [26] 2022 10077 512 / RGB / / /
Image Matching EuroSDR Image Matching [27] 2014 / / / RGB 6 ∼13m / 116
Image Registration Dataset of thermal and visible [28] 2019 110 336, 4000 0 RGB,Thermal / 0.31 2
Image TranslationAerial to Map(Pixel2Pixel) [29] 2017 2,194 600 / RGB / 0.239 14625
WHU-SEN-City [30] 2019 18542 256 / Sentinel-1,Sentinel-2 10m 4.3 57
Multiview 3DThe IARPA Multi-View Stereo 3D [31] 2017 / / / MS,LiDAR 0.3m 75 70
SatStereo [32] 2019 144 / / Panchromatic 0.5m 127 10
WHU MVS/Stereo Dataset [33] 2020 1776 5376 / RGB 0.1m 95.7 29
WHU TCL SatMVS [34] 2021 300 5,120 / Panchromatic 2.1 ∼2.5m 29 2
WHU Multi-View Dataset [34] 2020 28400 768 / RGB 0.1m 12.3 29
WHU Stereo Dataset [34] 2020 21868 768 / RGB 0.1m 8.4 29
Plant/TreeCactus Aerial Photos [35] 2018 24,000 32 2 RGB / 0.053 26
ReforesTree [36] 2022 105 4000 / RGB 0.02m 7.5 14
Urban Tree Detection Data [37] 2020 60 256 2 RGB-NIR 0.6m 0.114 0
Population So2Sat Population [38] 2022 / / / DEM,Sentinel-2 10m / 0
Pose Satellite Pose Estimation [39] 2019 15303 1920x1200 / Grayscale / 4.6 77
Poverty Poverty in Africa [40] 2020 32823 256 / RGB / 5.58 /
RSVQARSVQA-LR [41] 2020 772 256 9 Sentinel-2 10m 0.15 38
RSVQA-HR [42] 2020 10659 512 89 RGB 0.15m 14.4 38
RSVQAxBEN [41] 2021 590,326 up to 120 26,875 Sentinel-2 10m 28.6 5
RSIVQA [41] 2021 37,264 256 ∼4000 864 RGB 0.1 ∼8m 29.5 33
Soil Parameter Hyperview Challenge [43] 2022 2,886 11 ∼250 / Hyperspectral 2m 1.85 0
Super ResolutionALSAT-2B [44] 2021 5518 256 / RGB-NIR 2.5m/10m 0.044 0
The WorldStrat [45] 2022 3449 1054 8 MS,Sentinel-2,RGB 1.5 ∼60m 109.5 2
SEN2VENmuS [46] 2022 132955 256 / Sentinel-1,Sentinel-2 10m ∼20m 87 /
Proba-V Super Resolution [47] 2018 1160 384 / RNIR 100 ∼300m 0.71 37
Unmixing DLR HySU [48] 2021 1 86x123 / Hyperspectral 0.3m ∼1m 0.005 2
Urban 3D Point CloudCORE3D [49] 2018 / / / Panchromatic,MS,PointCloud / / 0
benchmark ISPRS2021 [50] 2021 20 1024 / RGB 0.08m 3.22 1
SemanticKITTI [51] 2019 4549M 39.2 km2 25 PointCloud / 80.2 632
Toronto3D [52] 2020 78.3M 1.0 km2 8 PointCloud / 1.1 69
Swiss3DCities [53], [54] 2020 226M 2.7 km2 5 PointCloud / / 17
DALES [55] 2020 505.3M 10.0 km2 8 PointCloud / / 61
LASDU [56] 2020 3.12M 1.02 km2 5 PointCloud Aircraft @ 1200m / 23
Campus3D [57] 2020 937.1M 1.58 km2 14 PointCloud UAV / 18
SensatUrban [58] 2020 2847.1M 6 km2 13 PointCloud UAV 20.6 51
Heisenheim3D [59] 2021 125.7M 0.19 km2 11 PointCloud / 58.8 18
SUM-Helsinki [60] 2021 19M 4 km2 6 Mesh 0.075m 8.35 8
Oakland 3-D PointCloud [61] 2009 1.6 M 1.5 km 5 PointCloud / 0.033 365
District of Columbia LiDAR 2015 [62] 2015 / / 8 PointCloud / 34.4 /
Semantic3D.net [63] 2017 4000 M / 8 PointCloud / 24.5 526
District of Columbia LiDAR 2018 [62] 2018 / / 11 PointCloud / 279.8 /
Paris-Lille-3D [64] 2017 143 M 1.94 km2 50 PointCloud / 19.9 183
DublinCity [65] 2019 260M 2.0 km2 13 PointCloud / 166.8 49
Paris-rue-Madame [66] 2014 20 M 1 km 26 PointCloud / / 98
Vehicles AM3D-Real [67] 2022 1,012 720x480 2 RGB,PointCloud 0.25m / 0
VQA,Change Detection CDVQA [68] 2022 2968 512 19 RGB 0.5-3m / 0
Water Body Forel-Ule Index global inland waters [69] 2021 / / / MS 500m 0.004 /
WeatherHistorical Hourly Weather [70] 2017 / / / Weather Attributes / 0.075 /
DeepWeather [71] 2020 20 / 9 Weather Attributes / 2,965 58
ClimateNet [72] 2020 459 16 (768,1152) 9 NC variables / 28 22
EarthNet2021 [73] 2021 32000 128 / Sentinel-2 20m 614.4 7
ClimateEarth Surface Temperature [74] 2017 16 / / Climate variables / 0.09 /
Greenhouse Gas [75] 2017 / / / Gas Emission / 0.0001 /
Hurricane Wind Speed [76] 2020 114634 366 / LWIR / 2.24 12
Segmentation libraries. All these libraries share the same
dataset loading module. To establish a deep connection with
the CV community, we base these libraries for RS tasks
on the libraries from OpenMMlab [82]. EarthNets enables
an easy adaptation of modern deep learning models from
the CV community to the RS community. For example,
backbone models like ResNet [83], EfﬁcientNet [84], Con-
vNext [85], Vision Transformers [86], MLP-Mixer [87], Swin
Transformer [88], and so forth can be used. Numerous state-
of-the-art architectures designed for CV tasks like RetinaNet[89], UNet [90], Deeplab [91], YOLO [92], Upernet [93], and
SegFormer [94] can be applied to RS data. By this means,
EarthNets can serve as a bridge between the CV and RS
communities.
3 I MPLEMENTATION DETAILS FOR BENCHMARK -
INGEXPERIMENTS
Optimizer : For convolution-based models, SGD is used as
the optimizer. The AdamW [95] is used for optimizing the
Transformer-based models. Initialization : By default, we
3
Functional Map of the World
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityBigEarthNet
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityMillion AID
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
RSD46-WHU
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalitySo2Sat LCZ42
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalitySenseEarth classify
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
MLRSNet
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityMultiScene
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityRSI-CB128
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
NWPU-RESISC45
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityPatternNet
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityRSI-CB256
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
SAT-6
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalitySAT-4
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityAID
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
CLRS
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityEuroSAT
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityAiRound
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
HistAerial
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityUC Merced
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityCV-BrCT (Cross-View Brazilian Constru
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
NaSC-TG2
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityWHU-RS19
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalitySIRI-WHU (Google + USGS)
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
OPTIMAL-31
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalitySlovenia Land Cover
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityGaofen Image Dataset (GID)
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
RSC11
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityRSSCN7
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityAustin Zoning Satellite Image
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
Satellite Image Classiﬁcation
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityBijie Landslide
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityTG1HRSSC
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
OVERHEAD MNIST
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityTiSeLaC (Time Series Land Cover Classiﬁcation Challenge)
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityBotswana
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
Pavia Center
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityPavia University
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityMSLCC
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
Functional Map of the World
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityDIOR-R
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityxView
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
DOTA v2.0
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityTGRS HRRSD
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityAI-TOD - tiny object detection
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
DOTA v1.5
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityDOTA v1.0
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityUAVOD10
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
NWPU-VHR10
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityRemote Sensing Object Detection (RSOD)
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
SEASONET
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityOpenSentinelMap
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalitySEN12MS
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
DFC18
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityDFC20
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityWHU-OHS
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
Chesapeake Land Cover
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityMultiSenGE
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityGeoNRW
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
MiniFrance-DFC22
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityLoveDA
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityDLRSD
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
DSTL Feature Detection (16Band)
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityUrban Semantic 3D (DFC19)
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityWHDLD
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
Five-Billion-Pixels
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityAeroscapes
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityDeepGlobe (LandCover Classiﬁca
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
DSTL Feature Detection (3Band)
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityGID15
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityAIR-PolSAR-Seg
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
Semantic Drone Dataset-SemSeg
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalitySynthAer
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityUAVid
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
EvLab-SS Dataset
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalitySemCity Toulouse
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityISPRS 2D - Potsdam
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
Urban Drone Dataset (UDD)
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityDroneDeploy
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityISPRS 2D - Vaihingen
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
DFC2015 Zeebruges
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityRIT-18
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityZurich Summer Dataset
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
LandCoverAI
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityxiongAn
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityHyRANK
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
Kaggle Semantic segmentation of aerial imagery
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityAerial Image Segmentation
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityTimeSpec4LULC
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
AeroRIT
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality ModalityWashington DC MALL
00224466881010Resolution Resolution
Annotation Annotation
#Sample #Sample
Volume Volume#Class #ClassModality Modality
Fig. 1. Radar charts visualization of some top ranked RS datasets for three different tasks. Six different attributes are compared and displayed in
this ﬁgure. Note that all the attribute values are normalized to the range of 0 to 10.
use the ImageNet pre-trained weights for initializing the
models. For some architectures with no ImageNet [96] pre-
trained weights, we train them from scratch. Other hyper-
parameters including batch size and learning rate are set
differently for each dataset. More implementation details
are provided in the public codes at https://github.com/
EarthNets.
4 V ISUALIZATION RESULTS OF BENCHMARKING
EXPERIMENTS
In Fig. 1, we use radar charts to compare the attributes
of some top-ranked datasets. We compare six different at-
tributes of the datasets: the modality, the resolution, the
annotation level, the number of classes, the number of
samples, and the volume. Using radar charts makes it easy
to compare and rank different datasets. A complete set of
the charts is displayed on https://earthnets.github.io.
For object detection and semantic segmentation tasks,
we provide the visualization results to directly compare
the results of different network architectures on large-scale
datasets in Fig. 2. The ﬁrst two rows present the object
detection results on the DIOR dataset. The second two rows
show the semantic segmentation results on the SEASONET
dataset. The ﬁnal two rows are the semantic segmentation
results on the GeoNRW dataset. From the visualization re-
sults we can further verify that Transformer-based networks
work well on large-scale remote sensing datasets. This is
consistent with the conclusions on the computer vision
datasets. However, it is worth mentioning that in some local
regions of the image, the CNN-based method, i.e, ResNet-
50, works better than Swin-Transformer. Hence, an effective
combination of these two architectures could yield better
performance. In Table 2, Table 3 and Table 4, we provide
detailed experimental results for each semantic category.
For the image classiﬁcation task on the FMoW dataset,
we also show the confusion matrices to compare the per-
formance of different network architectures in Fig. 3. From
left to right, the ﬁgure displays the confusion matrix for the
ResNet-50, ConvNext, and Swin-Transformer.REFERENCES
[1] M. Barekatain, M. Mart ´ı, H.-F. Shih, S. Murray, K. Nakayama,
Y. Matsuo, and H. Prendinger, “Okutama-action: An aerial view
video dataset for concurrent human action detection,” in CVPRW ,
2017, pp. 28–35.
[2] H.-W. Jo and W.-K. Lee, “Paddy rice maps south korea
(2017-2021),” Jan. 2022. [Online]. Available: https://doi.org/10.
5281/zenodo.5845896
[3] K. Chanwoo, J. Hyun-Woo, L. Sujong, K. Whijin, Y. Yan,
K. Joon, S. Minju, and W.-K. Lee, “Paddy rice labeling
sites in south korea (2018),” Jan. 2022. [Online]. Available:
https://doi.org/10.5281/zenodo.5846018
[4] E. E. Agency. (2021) Air quality e-reporting (aq e-
reporting). [Online]. Available: https://www.eea.europa.eu/
data-and-maps/data/aqereporting-9
[5] P . Jin, L. Mou, G.-S. Xia, and X. X. Zhu, “Anomaly detection in
aerial videos with transformers,” IEEE TGRS , vol. 60, pp. 1–13,
2022.
[6] H. Goldberg, M. Brown, and S. Wang, “A benchmark for building
footprint classiﬁcation using orthorectiﬁed rgb imagery and digi-
tal surface models from commercial satellites,” in AIPRW , 2017.
[7] (2016) Draper satellite image chronology. [Online]. Available:
https://www.kaggle.com/c/draper-satellite-image-chronology
[8] V . Sarukkai, A. Jain, B. Uzkent, and S. Ermon, “Dataset From:
Cloud Removal from Satellite Images using Spatiotemporal
Generator Networks,” 2019. [Online]. Available: https://doi.org/
10.7910/DVN/BSETKZ
[9] P . Ebel, A. Meraner, M. Schmitt, and X. X. Zhu, “Multisensor
data fusion for cloud removal in global and all-season sentinel-
2 imagery,” IEEE TGRS , vol. 59, no. 7, pp. 5866–5878, 2020.
[10] R. Bahmanyar, E. Vig, and P . Reinartz, “Mrcnet: Crowd counting
and density map estimation in aerial and ground imagery,” arXiv
preprint arXiv:1909.12743 , 2019.
[11] P . Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, and H. Ling, “De-
tection and tracking meet drones challenge,” IEEE Trans. Pattern
Anal. Mach. Intell. , pp. 1–1, 2021.
[12] G. Gao, Q. Liu, and Y. Wang, “Counting from sky: A large-scale
data set for remote sensing object counting and a benchmark
method,” IEEE TGRS , vol. 59, no. 5, pp. 3642–3655, 2020.
[13] M. Schmitt, L. H. Hughes, and X. X. Zhu, “The sen1-2 dataset
for deep learning in sar-optical data fusion,” arXiv preprint
arXiv:1807.01569 , 2018.
[14] M. Huang, Y. Xu, L. Qian, W. Shi, Y. Zhang, W. Bao, N. Wang,
X. Liu, and X. Xiang, “The qxs-saropt dataset for deep learning in
sar-optical data fusion,” arXiv preprint arXiv:2103.08259 , 2021.
[15] B. Huang, L. Zhi, C. Yang, F. Sun, and Y. Song, “Single satellite op-
tical imagery dehazing using sar image prior based on conditional
generative adversarial networks,” in WACV , 2020, pp. 1806–1813.
[16] J. M. R. Mullor, “jmrmcode/Lidar-Landsat-data-fusion: lidar-
landsat- data-fusion,” Oct. 2019. [Online]. Available: https:
//doi.org/10.5281/zenodo.3468645
4
RGB Images Ground Truth RN-50,MaskRCNNConvNext,MaskRCNNSwinT,MaskRCNN
RGB Images Ground Truth RN-50,Deeplabv3+ConvNext,Deeplabv3+SwinT,Deeplabv3+
RGB Images Ground TruthResNet,DeeplabV3ConvNext,DeeplabV3SwinT,DeeplabV3
Fig. 2. Object detection and semantic segmentation results on three datasets. The ﬁrst two rows present the object detection results on the
DIOR dataset. The second two rows show the semantic segmentation results on the SEASONET dataset. The ﬁnal two rows are the semantic
segmentation results on the GeoNRW dataset.
5
Fig. 3. Confusion matrices of different methods on the FMoW dataset [81]. From left to right are the confusion matrix for the ResNet-50, ConvNext,
Swin-Transformer models.
TABLE 2
Detailed Comparison Results on the GeoNRW [97] Dataset
Methods DeepLabv3+/RN50 DeepLabv3+/ConvNext-Tiny DeepLabv3+/Swin-Tiny
Metrics IoU Acc IoU Acc IoU Acc
forest 72.49 84.57 71.07 84.93 72.22 83.97
water 90.14 92.91 91.4 94.54 91.69 95.07
agricultural 86.66 92.33 86.2 92.56 86.48 92.66
residential,commercial,industrial 69.67 85.5 67.96 84.8 68.38 85.2
grassland,swamp,shrubbery 38.92 58.25 35.72 53.96 36.98 56.23
railway,trainstation 59.03 73.76 56.66 77.29 58.89 78.58
highway,squares 43.97 51.96 39.89 46.17 41.31 48.79
airport,shipyard 55.37 79.18 59.35 73.98 58.77 75.34
roads 43.05 53.14 39.32 48.3 38.86 47.95
buildings 70.75 81.04 68.76 79.54 68.24 78.97
TABLE 3
Detailed Comparison Results on the SEASONET [98] Dataset
Methods DeepLabv3+/RN50 DeepLabv3+/ConvNext-Tiny DeepLabv3+/Swin-Tiny
Metrics IoU Acc IoU Acc IoU Acc
Continuous Urban Fabric 36.38 44.72 33.28 39.83 36.02 44.87
Discontinuous Urban Fabric 74.01 87.85 71.22 86.83 73 87.06
Industrial or Commercial Units 53.94 67.27 47.52 60.65 52.11 65.89
Road and Rail Networks and Associated Land 42.94 57.27 33.73 44.09 39.06 53.54
Port Areas 33.28 46.01 25.28 29.79 30.77 38.1
Airports 56.54 74.48 44.64 55.86 56.74 68.67
Mineral Extraction Sites 59.36 76.3 52.06 69.84 57.53 75.17
Dump Sites 25.4 32.51 8.65 9.79 22.39 28.24
Construction Sites 8.52 9.7 2.42 2.53 7.12 7.84
Green Urban Areas 30.58 43.06 23.23 29.66 27.62 37.05
Sport and Leisure Facilities 39.7 48.86 29.98 36.82 36.18 44.6
Non-irrigated Arable Land 83.67 91.7 80.99 90.23 82.65 91.87
Vineyards 75.34 88.8 66.46 82.52 73.73 85.66
Fruit Trees and Berry Planatations 42.95 53.5 33.82 43.59 40.44 51
Pastures 64.1 78.73 59.46 75.49 61.89 76.15
Broad-leaved Forest 65.09 80.71 62 79.62 63.72 80.45
Coniferous Forest 76.01 89.41 74.02 88.6 75.16 88.92
Mixed Forest 28.56 36.48 24.2 30.31 26.54 33.45
Natural Grasslands 34.23 42.65 28.07 84.97 32.66 40.26
Moors and Healthland 59.66 78.55 54.33 73.16 58.86 76.18
Transitional Woodland / Shrub 25.39 31.29 20.46 25.17 23.03 28.03
Beaches, Dunes, Sands 38.39 67.53 40.91 53.36 43.94 59.31
Bare Rock 0 0 29.26 36.52 34.67 50.25
Sparsely Vegetated Areas 10.13 18.06 7.24 8.71 7.45 9.57
Inland Marshes 31.86 44.94 23.77 31.19 28.2 38.27
Peat Bogs 56.68 72.8 50.81 63.44 54.5 69.69
Salt Marshes 70.42 86.18 60.76 77.6 66.55 80.6
Intertidal Flats 76.26 86.21 77.72 89.63 80.02 90.66
Water Courses 64.86 77.79 54.93 68.02 60.61 72.87
Water Bodies 77.94 86.09 73.38 83.31 78.25 87.76
Coastal Lagoons 79.77 90.37 80.55 89.6 83.7 92.3
Estuaries 59.04 80.07 60.1 69.98 65.3 77.48
Sea and Ocean 95.16 97.5 95.54 97.52 96.19 97.75
6
TABLE 4
Detailed Comparison Results on the FMoW [99] Dataset
Methods RN50 ConvNext-Tiny Swin-Tiny
Metrics Precision Recall Precision Recall Precision Recall
Airport 77.18 90.29 83.22 93.72 86.89 90.18 85.71 90.29 87.94
Airport hangar 58.25 57.82 58.03 66.50 58.55 62.27 67.71 66.81 67.26
Airport terminal 68.45 72.54 70.43 74.20 71.57 72.86 80.34 75.93 78.07
Amusement park 53.25 82.80 64.81 73.10 83.24 77.84 79.85 85.35 82.51
Aquaculture 76.71 81.28 78.93 80.08 85.53 82.72 82.92 84.68 83.79
Archaeological site 54.40 43.49 48.34 66.11 51.82 58.10 63.08 53.39 57.83
Barn 55.02 57.89 56.42 61.58 60.66 61.12 61.71 69.40 65.33
Border checkpoint 47.54 31.69 38.03 70.00 26.78 38.74 64.55 38.80 48.46
Burial site 58.39 60.23 59.30 66.73 63.06 64.84 64.35 70.88 67.46
Car dealership 61.86 66.49 64.09 60.92 76.61 67.87 67.22 74.38 70.62
Construction site 27.70 16.78 20.90 32.13 15.47 20.88 29.34 22.44 25.43
Crop ﬁeld 80.07 89.51 84.53 79.63 92.35 85.52 86.96 91.98 89.40
Dam 75.58 74.31 74.94 81.38 75.54 78.35 79.02 83.49 81.19
Debris orrubble 54.84 19.92 29.23 49.61 25.00 33.25 40.72 26.56 32.15
Educational institution 36.43 35.57 35.99 36.46 41.88 38.98 48.22 46.46 47.32
Electric substation 57.33 59.22 58.26 62.71 65.08 63.87 67.14 69.84 68.46
Factory orpowerplant 46.00 32.80 38.29 56.73 34.58 42.97 59.69 47.77 53.07
Fire station 24.54 17.25 20.26 26.26 16.09 19.95 27.73 24.84 26.21
Flooded road 56.28 40.12 46.85 57.64 40.74 47.74 69.83 50.00 58.27
Fountain 47.86 33.22 39.22 51.65 39.27 44.62 55.20 46.69 50.59
Gas station 38.32 42.08 40.11 41.84 43.58 42.70 52.45 56.99 54.63
Golf course 84.54 72.74 78.20 84.35 72.58 78.02 83.97 83.42 83.69
Ground transportation station 53.03 59.32 56.00 55.28 65.34 59.89 62.47 71.12 66.51
Helipad 45.77 37.36 41.14 65.27 31.87 42.83 56.98 46.15 51.00
Hospital 30.14 30.69 30.41 30.28 36.58 33.13 35.13 33.69 34.40
Impoverished settlement 69.20 83.17 75.55 77.46 90.87 83.63 84.07 91.35 87.56
Interchange 77.08 79.49 78.27 69.93 83.59 76.16 85.34 81.84 83.55
Lake orpond 30.14 16.67 21.46 43.56 33.33 37.77 45.28 36.36 40.34
Lighthouse 72.89 73.29 73.09 78.96 73.83 76.31 78.92 81.77 80.32
Military facility 57.71 51.35 54.35 58.67 60.65 59.65 61.80 59.62 60.69
Multi-unit residential 40.95 32.17 36.03 47.38 37.57 41.91 51.17 45.39 48.11
Nuclear powerplant 75.86 75.86 75.86 92.00 79.31 85.19 83.87 89.66 86.67
Ofﬁce building 13.12 8.76 10.51 17.76 9.29 12.20 18.84 9.72 12.83
Oilorgas facility 73.04 76.55 74.75 79.38 83.07 81.18 77.78 81.52 79.61
Park 27.81 20.08 23.32 36.67 26.69 30.89 37.57 27.24 31.58
Parking lotorgarage 28.77 35.33 31.71 35.28 41.92 38.31 46.46 47.19 46.82
Place ofworship 42.18 51.19 46.25 46.87 54.06 50.21 51.96 61.25 56.22
Police station 14.13 12.90 13.48 17.33 15.07 16.12 20.59 18.77 19.64
Port 73.44 79.72 76.45 73.53 88.97 80.52 78.66 87.90 83.03
Prison 65.23 46.52 54.31 61.42 54.32 57.65 71.61 53.76 61.42
Race track 90.18 81.83 85.80 85.81 87.65 86.72 87.27 88.72 87.99
Railway bridge 58.35 50.15 53.94 59.11 57.56 58.33 64.69 63.89 64.29
Recreational facility 68.37 78.95 73.28 72.19 80.23 76.00 75.11 84.55 79.55
Road bridge 54.50 69.25 61.00 62.75 63.15 62.95 72.13 74.33 73.22
Runway 74.15 86.05 79.66 86.35 86.58 86.47 90.49 92.63 91.55
Shipyard 56.86 52.73 54.72 64.71 40.00 49.44 66.28 51.82 58.16
Shopping mall 50.89 49.30 50.08 53.69 57.28 55.43 62.32 60.30 61.29
Single-unit residential 58.07 64.63 61.17 57.34 73.22 64.31 65.39 69.23 67.25
Smokestack 56.40 43.56 49.16 53.53 60.08 56.62 58.82 64.63 61.59
Solar farm 83.18 83.51 83.34 80.76 84.84 82.75 84.70 86.84 85.75
Space facility 89.13 93.18 91.11 88.64 88.64 88.64 87.76 97.73 92.47
Stadium 80.22 79.83 80.02 80.78 81.74 81.26 83.81 87.11 85.43
Storage tank 75.00 79.95 77.40 77.59 79.95 78.75 80.77 85.28 82.96
Surface mine 81.86 75.00 78.28 80.50 78.85 79.67 77.84 82.01 79.87
Swimming pool 62.86 64.35 63.60 68.84 63.78 66.21 76.08 74.90 75.48
Toll booth 84.85 88.33 86.56 87.37 94.87 90.96 89.46 95.77 92.51
Tower 47.76 32.20 38.47 64.29 36.54 46.60 55.32 43.72 48.84
Tunnel opening 82.05 82.05 82.05 83.20 86.75 84.94 86.95 89.00 87.96
Waste disposal 45.78 29.44 35.84 61.81 31.41 41.66 56.98 49.01 52.70
Water treatment facility 68.49 69.04 68.77 73.69 75.77 74.72 74.12 79.41 76.67
Wind farm 89.82 94.20 91.96 91.82 96.18 93.95 90.69 96.31 93.42
Zoo 51.40 37.86 43.60 61.18 42.80 50.36 59.26 46.09 51.85
7
[17] O. Ma ˜nas, A. Lacoste, X. Giro-i Nieto, D. Vazquez, and P . Ro-
driguez, “Seasonal contrast: Unsupervised pre-training from un-
curated remote sensing data,” arXiv preprint arXiv:2103.16607 ,
2021.
[18] Y. Wang, N. Ait Ali Braham, C. M. Albrecht, Z. Xiong, C. Liu,
and X. Zhu, “Ssl4eo-s12: A large-scale multimodal multitemporal
dataset for self-supervised learning in earth observation,” 2022.
[Online]. Available: https://mediatum.ub.tum.de/1660427
[19] S. Workman, R. Souvenir, and N. Jacobs, “Wide-area image ge-
olocalization with aerial reference imagery,” in ICCV , 2015, pp.
3961–3969.
[20] Z. Zheng, Y. Wei, and Y. Yang, “University-1652: A multi-view
multi-source benchmark for drone-based geo-localization,” ACM
Multimedia , 2020.
[21] Y. Tian, C. Chen, and M. Shah, “Cross-view image matching for
geo-localization in urban environments,” in CVPR , 2017, pp. 3608–
3616.
[22] L. Liu and H. Li, “Lending orientation to neural networks for
cross-view geo-localization,” in CVPR , June 2019.
[23] S. Zhu, T. Yang, and C. Chen, “Vigor: Cross-view image geo-
localization beyond one-to-one retrieval,” in CVPR , 2021, pp.
3640–3649.
[24] C. Wang, A. Mouche, P . Tandeo, J. E. Stopa, N. Long ´ep´e, G. Erhard,
R. C. Foster, D. Vandemark, and B. Chapron, “A labelled ocean sar
imagery dataset of ten geophysical phenomena from sentinel-1
wave mode,” Geoscience Data Journal , vol. 6, no. 2, pp. 105–115,
2019.
[25] X. Lu, B. Wang, X. Zheng, and X. Li, “Exploring models and data
for remote sensing image caption generation,” IEEE TGRS , vol. 56,
no. 4, pp. 2183–2195.
[26] Chen-Yang-Liu. (2022) Remote sensing image change captioning
(rsicc). [Online]. Available: https://github.com/Chen-Yang-Liu/
RSICC
[27] S. Cavegn, N. Haala, S. Nebiker, M. Rothermel, and P . Tutzauer,
“Benchmarking high density image matching for oblique airborne
imagery,” ISPRS Archives , vol. 40, no. 3, p. 45, 2014.
[28] J. L. H. R. A. Garc ´ıa, Lina; D ´ıaz. (2019) Thermal and visible
aerial imagery. [Online]. Available: https://data.mendeley.com/
datasets/ffgxxzx298/1
[29] P . Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image
translation with conditional adversarial networks,” CVPR , 2017.
[30] WHU. (2019) Whu-sen-city. [Online]. Available: https://github.
com/whu-csl/WHU-SEN-City
[31] G. Facciolo, C. De Franchis, and E. Meinhardt-Llopis, “Automatic
3d reconstruction from multi-date satellite images,” in CVPRW ,
2017, pp. 57–66.
[32] T. P . Sonali Patil, Bharath Comandur and A. C. Kak, “A New Stereo
Benchmarking Dataset for Satellite Images,” arXiv:1907.04404 ,
2019. [Online]. Available: http://arxiv.org/abs/1907.04404
[33] J. Liu and S. Ji, “A novel recurrent encoder-decoder structure for
large-scale multi-view stereo reconstruction from an open aerial
dataset,” in CVPR , 2020, pp. 6050–6059.
[34] J. Gao, J. Liu, and S. Ji, “Rational polynomial camera model warp-
ing for deep learning based satellite multi-view stereo matching,”
inICCV , October 2021, pp. 6148–6157.
[35] E. L ´opez-Jim ´enez, J. I. Vasquez-Gomez, M. A. Sanchez-Acevedo,
J. C. Herrera-Lozada, and A. V . Uriarte-Arcia, “Columnar cactus
recognition in aerial images using a deep learning approach,”
Ecological Informatics , vol. 52, pp. 131–138, 2019.
[36] G. Reiersen, D. Dao, B. L ¨utjens, K. Klemmer, K. Amara, A. Steineg-
ger, C. Zhang, and X. Zhu, “Reforestree: A dataset for estimating
tropical forest carbon stock with deep learning and aerial im-
agery,” arXiv preprint arXiv:2201.11192 , 2022.
[37] J. Ventura, M. Honsberger, C. Gonsalves, J. Rice, C. Pawlak, N. L.
Love, S. Han, V . Nguyen, K. Sugano, J. Doremus et al. , “Individ-
ual tree detection in large-scale urban environments using high-
resolution multispectral imagery,” arXiv preprint arXiv:2208.10607 ,
2022.
[38] S. Doda, Y. Wang, M. Kahl, E. J. Hoffmann, H. Taubenb ¨ock,
and X. X. Zhu, “So2sat pop–a curated benchmark data set for
population estimation from space on a continental scale,” arXiv
preprint arXiv:2204.08524 , 2022.
[39] T. H. Park, M. M ¨artens, G. Lecuyer, D. Izzo, and S. D’Amico,
“Speed+: Next-generation dataset for spacecraft pose estimation
across domain gap,” in AERO . IEEE, 2022, pp. 1–15.[40] (2020) Satellite images to predict poverty. [Online].
Available: https://www.kaggle.com/datasets/sandeshbhat/
satellite-images-to-predict-povertyafrica
[41] S. Lobry, D. Marcos, J. Murray, and D. Tuia, “Rsvqa: Visual
question answering for remote sensing data,” IEEE TGRS , vol. 58,
no. 12, pp. 8555–8566, 2020.
[42] S. Lobry, J. Murray, D. Marcos, and D. Tuia, “Visual question
answering from remote sensing images,” in IGARSS . IEEE, 2019,
pp. 4951–4954.
[43] (2022) Hyperview challenge. [Online]. Available: https://
platform.ai4eo.eu/seeing-beyond-the-visible
[44] A. Djerida, K. Djerriri, M. S. Karoui et al. , “A new public alsat-
2b dataset for single-image super-resolution,” in IGARSS . IEEE,
2021, pp. 8095–8098.
[45] J. Cornebise, I. Or ˇsoli´c, and F. Kalaitzis, “The WorldStrat
Dataset: Open High-Resolution Satellite Imagery With Paired
Multi-Temporal Low- Resolution,” Jul. 2022. [Online]. Available:
https://doi.org/10.5281/zenodo.6810792
[46] J. Michel, J. Vinasco-Salinas, J. Inglada, and O. Hagolle,
“SEN2VENµS, a dataset for the training of Sentinel-2 super-
resolution algorithms,” May 2022. [Online]. Available: https:
//doi.org/10.5281/zenodo.6514159
[47] M. M ¨artens, D. Izzo, A. Krzic, and D. Cox, “Proba-
v super-resolution dataset,” Nov. 2018. [Online]. Available:
https://doi.org/10.5281/zenodo.6327426
[48] D. Cerra, M. Pato, K. Alonso, C. K ¨ohler, M. Schneider, R. de los
Reyes, E. Carmona, R. Richter, F. Kurz, P . Reinartz et al. , “Dlr
hysu—a benchmark dataset for spectral unmixing,” Remote Sens-
ing, vol. 13, no. 13, p. 2559, 2021.
[49] M. Brown, H. Goldberg, K. Foster, A. Leichtman, S. Wang,
S. Hagstrom, M. Bosch, and S. Almes, “Large-scale public lidar
and satellite image data set for urban semantic labeling,” in Laser
Radar Technology and Applications , vol. 10636. SPIE, 2018, pp. 154–
167.
[50] T. Wu, B. Vallet, M. Pierrot-Deseilligny, and E. Rupnik, “A new
stereo dense matching benchmark dataset for deep learning,”
ISPRS Archives , vol. 43, pp. 405–412, 2021.
[51] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stach-
niss, and J. Gall, “SemanticKITTI: A Dataset for Semantic Scene
Understanding of LiDAR Sequences,” in ICCV , 2019.
[52] W. Tan, N. Qin, L. Ma, Y. Li, J. Du, G. Cai, K. Yang, and J. Li,
“Toronto-3d: A large-scale mobile lidar dataset for semantic seg-
mentation of urban roadways,” in CVPRW , 2020, pp. 202–203.
[53] G. Can, D. Mantegazza, G. Abbate, S. Chappuis, and
A. Giusti, “Swiss3dcities: Aerial photogrammetric 3d pointcloud
dataset with semantic labels,” Dec. 2020. [Online]. Available:
https://doi.org/10.5281/zenodo.4390295
[54] ——, “Semantic segmentation on swiss3dcities: a benchmark
study on aerial photogrammetric 3d pointcloud dataset,” Pattern
Recognition Letters , 2021. [Online]. Available: https://www.
sciencedirect.com/science/article/pii/S0167865521001938
[55] N. Varney, V . K. Asari, and Q. Graehling, “Dales: A large-scale
aerial lidar data set for semantic segmentation,” in CVPRW , 2020,
pp. 186–187.
[56] X. Li, G. Cheng, S. Liu, Q. Xiao, M. Ma, R. Jin, T. Che, Q. Liu,
W. Wang, Y. Qi et al. , “Heihe watershed allied telemetry exper-
imental research (hiwater): Scientiﬁc objectives and experimental
design,” Bulletin of the American Meteorological Society , vol. 94, no. 8,
pp. 1145–1160, 2013.
[57] X. Li, C. Li, Z. Tong, A. Lim, J. Yuan, Y. Wu, J. Tang, and R. Huang,
“Campus3d: A photogrammetry point cloud benchmark for hi-
erarchical understanding of outdoor scene,” in ACM Multimedia ,
2020, pp. 238–246.
[58] Q. Hu, B. Yang, S. Khalid, W. Xiao, N. Trigoni, and A. Markham,
“Sensaturban: Learning semantics from urban-scale photogram-
metric point clouds,” International Journal of Computer Vision , vol.
130, no. 2, pp. 316–343, 2022.
[59] M. K ¨olle, D. Laupheimer, S. Schmohl, N. Haala, F. Rottensteiner,
J. D. Wegner, and H. Ledoux, “The hessigheim 3d (h3d) bench-
mark on semantic segmentation of high-resolution 3d point clouds
and textured meshes from uav lidar and multi-view-stereo,” IS-
PRS Open Journal of Photogrammetry and Remote Sensing , vol. 1, p.
100001, 2021.
[60] W. Gao, L. Nan, B. Boom, and H. Ledoux, “Sum: A benchmark
dataset of semantic urban meshes,” ISPRS J. Photogramm. Remote
Sens. , vol. 179, pp. 108–120, 2021.
8
[61] D. Munoz, J. A. Bagnell, N. Vandapel, and M. Hebert, “Contextual
classiﬁcation with functional max-margin markov networks,” in
CVPR . IEEE, 2009, pp. 975–982.
[62] (2015) District of columbia – classiﬁed point cloud lidar. [Online].
Available: https://github.com/awslabs/open-data-docs/tree/
main/docs/dc-lidar-2018
[63] T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner, K. Schindler, and
M. Pollefeys, “SEMANTIC3D.NET: A new large-scale point cloud
classiﬁcation benchmark,” in ISPRS Annals , vol. IV-1-W1, 2017, pp.
91–98.
[64] X. Roynard, J.-E. Deschaud, and F. Goulette, “Paris-lille-3d: A
large and high-quality ground-truth urban point cloud dataset
for automatic segmentation and classiﬁcation,” The International
Journal of Robotics Research , vol. 37, no. 6, pp. 545–557, 2018.
[65] S. Zolanvari, S. Ruano, A. Rana, A. Cummins, R. E. da Silva,
M. Rahbar, and A. Smolic, “Dublincity: Annotated lidar point
cloud and its applications,” arXiv preprint arXiv:1909.03613 , 2019.
[66] A. Serna, B. Marcotegui, F. Goulette, and J.-E. Deschaud, “Paris-
rue-madame database: a 3d mobile laser scanner dataset for
benchmarking urban detection, segmentation and classiﬁcation
methods,” in ICPRAM , 2014.
[67] (2022) Aerial monocular 3d object detection dataset. [Online].
Available: https://sjtu-magic.github.io/dataset/AM3D/
[68] Z. Yuan, L. Mou, Z. Xiong, and X. X. Zhu, “Change detection meets
visual question answering,” IEEE TGRS , 2022.
[69] S. Wang, J. Li, W. Zhang, C. Cao, F. Zhang, Q. Shen,
X. Zhang, and B. Zhang, “A dataset of remote-sensed Forel-Ule
Index for global inland waters during 2000–2018,” 12 2020.
[Online]. Available: https://ﬁgshare.com/articles/dataset/A
dataset ofremote-sensed Forel-Ule Index forglobal inland
waters during 2000 2018/13014299
[70] S. Yadav, C. Fu, and A. v. Wyk. (2017) Historical hourly weather
data 2012-2017. [Online]. Available: https://www.kaggle.com/
datasets/selﬁshgene/historical-hourly-weather-data
[71] P . Gr ¨onquist, C. Yao, T. Ben-Nun, N. Dryden, P . Dueben, S. Li, and
T. Hoeﬂer, “Deep learning for post-processing ensemble weather
forecasts,” 2020.
[72] K. Kashinath, M. Mudigonda, S. Kim, L. Kapp-Schwoerer,
A. Graubner, E. Karaismailoglu, L. Von Kleist, T. Kurth, A. Greiner,
A. Mahesh et al. , “Climatenet: an expert-labeled open dataset and
deep learning architecture for enabling high-precision analyses of
extreme weather,” Geoscientiﬁc Model Development , vol. 14, no. 1,
pp. 107–124, 2021.
[73] C. Requena-Mesa, V . Benson, M. Reichstein, J. Runge, and J. Den-
zler, “Earthnet2021: A large-scale dataset and challenge for earth
surface forecasting as a guided video prediction task.” in CVPR ,
2021, pp. 1132–1142.
[74] (2017) Climate change: Earth surface temperature
data. [Online]. Available: https://www.kaggle.com/datasets/
berkeleyearth/climate-change-earth-surface-temperature-data
[75] (2017) International greenhouse gas emissions. [Online].
Available: https://www.kaggle.com/datasets/unitednations/
international-greenhouse-gas-emissions
[76] M. Maskey, R. Ramachandran, M. Ramasubramanian, I. Gurung,
B. Freitag, A. Kaulfus, D. Bollinger, D. J. Cecil, and J. Miller,
“Deepti: Deep-learning-based tropical cyclone intensity estimation
system,” IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. , vol. 13, pp.
4271–4281, 2020.
[77] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast
for unsupervised visual representation learning,” in Proceedings of
the IEEE/CVF conference on computer vision and pattern recognition ,
2020, pp. 9729–9738.
[78] J.-B. Grill, F. Strub, F. Altch ´e, C. Tallec, P . Richemond,
E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo, M. Ghesh-
laghi Azar et al. , “Bootstrap your own latent-a new approach to
self-supervised learning,” Advances in neural information processing
systems , vol. 33, pp. 21 271–21 284, 2020.
[79] Y. Wang, N. A. A. Braham, Z. Xiong, C. Liu, C. M. Albrecht, and
X. X. Zhu, “Ssl4eo-s12: A large-scale multi-modal, multi-temporal
dataset for self-supervised learning in earth observation,” arXiv
preprint arXiv:2211.07044 , 2022.
[80] S. Ji, P . Dai, M. Lu, and Y. Zhang, “Simultaneous cloud detection
and removal from bitemporal remote sensing images using cas-
cade convolutional neural networks,” IEEE TGRS , vol. 59, no. 1,
pp. 732–748, 2020.
[81] G. Sumbul, M. Charfuelan, B. Demir, and V . Markl, “Bigearthnet:A large-scale benchmark archive for remote sensing image under-
standing,” in IGARSS . IEEE, 2019, pp. 5901–5904.
[82] K. Chen, J. Wang, J. Pang, Y. Cao, Y. Xiong, X. Li, S. Sun, W. Feng,
Z. Liu, J. Xu et al. , “Mmdetection: Open mmlab detection toolbox
and benchmark,” arXiv preprint arXiv:1906.07155 , 2019.
[83] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for
image recognition,” in CVPR , 2016, pp. 770–778.
[84] M. Tan and Q. Le, “Efﬁcientnet: Rethinking model scaling for
convolutional neural networks,” in ICML . PMLR, 2019, pp. 6105–
6114.
[85] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie,
“A convnet for the 2020s,” in CVPR , 2022, pp. 11 976–11 986.
[86] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly
et al. , “An image is worth 16x16 words: Transformers for image
recognition at scale,” arXiv preprint arXiv:2010.11929 , 2020.
[87] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai,
T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al. ,
“Mlp-mixer: An all-mlp architecture for vision,” NeurIPS , vol. 34,
pp. 24 261–24 272, 2021.
[88] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
“Swin transformer: Hierarchical vision transformer using shifted
windows,” in ICCV , 2021, pp. 10 012–10 022.
[89] T.-Y. Lin, P . Goyal, R. Girshick, K. He, and P . Doll ´ar, “Focal loss for
dense object detection,” in CVPR , 2017, pp. 2980–2988.
[90] O. Ronneberger, P . Fischer, and T. Brox, “U-net: Convolutional net-
works for biomedical image segmentation,” in MICCAI . Springer,
2015, pp. 234–241.
[91] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethink-
ing atrous convolution for semantic image segmentation,” arXiv
preprint arXiv:1706.05587 , 2017.
[92] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look
once: Uniﬁed, real-time object detection,” in CVPR , 2016, pp. 779–
788.
[93] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, “Uniﬁed perceptual
parsing for scene understanding,” in ECCV , 2018, pp. 418–434.
[94] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P . Luo,
“Segformer: Simple and efﬁcient design for semantic segmentation
with transformers,” NeurIPS , vol. 34, pp. 12 077–12 090, 2021.
[95] I. Loshchilov and F. Hutter, “Decoupled weight decay regulariza-
tion,” arXiv preprint arXiv:1711.05101 , 2017.
[96] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Ima-
genet: A large-scale hierarchical image database,” in CVPR . Ieee,
2009, pp. 248–255.
[97] G. Baier, A. Deschemps, M. Schmitt, and N. Yokoya, “Synthesizing
optical and sar imagery from land cover maps and auxiliary raster
data,” IEEE TGRS , vol. 60, pp. 1–12, 2021.
[98] D. Koßmann, V . Brack, and T. Wilhelm, “Seasonet: A seasonal
scene classiﬁcation, segmentation and retrieval dataset for satellite
imagery over germany,” in IGARSS . IEEE, 2022, pp. 243–246.
[99] G. Christie, N. Fendley, J. Wilson, and R. Mukherjee, “Functional
map of the world,” in CVPR , 2018.
 
 
Adapting Vision Transformer for  Efficient Change  
Detection  
 
Yang Zhao , Yuxiang  Zhang , Yanni Dong , Senior  Member, IEEE , and Bo Du , Senior Member, IEEE  
 
 
Abstract—Most change detection models based on vision 
transformers currently follow a “pretraining  then fine -tuning” 
strategy. This involves initializing the model weights using large -
scale classification datasets , which can be either natural images 
or remote sensing images. However, fully tuning such a model 
requires significant time and resources. In  this paper, we propose 
an efficient tuning approach that involves freezing the 
parameters of the pretrained image encoder and introducing 
additional training parameters. Through this approach, we have 
achieved competitive or even better results while main taining 
extremely low resource consumption across six change detection 
benchmarks. For example, training time on LEVIR -CD, a change 
detection benchmark, is only half an hour with 9  GB memory 
usage, which could be very convenient for most researchers. 
Addit ionally, the decoupled tuning framework can be extended to 
any pretrained model for semantic change detection and multi -
temporal change detection as well. We hope that our proposed 
approach will serve as a part of foundational model to inspire 
more unified  training approaches on change detection in the 
future. Code will be  available at URL . 
 
Index Terms —Efficient parameters fine-tuning , change 
detection , transformer . 
 
I. INTRODUCTION  
hange Detection (CD) is one of the fundamental tasks in 
earth observation  [1]. By interpreting images of the same 
region from different periods, we can understand the economic 
development and human activities via observing changes such 
as building demolition and farmland  conversion . For example , 
Microsoft researchers employed  images before and after 
geological disasters  for change detection  to evaluate the post -
disaster damage of large -scale artificial buildings in Turkey, 
providing assistance for rescue  effort . This work  is a typical 
examp le applying deep learning method  to extract spatial and 
temporal features for change detection.  Generally, most deep 
learning -based change detection methods  [2], [3]  are based on 
convolution neural network. The first deep learning -based 
change detect ion method FC -Net [4] based on U -Net [5], 
originally developed  for medical applications , has the ability 
to capture small change s and is an important baseline. Its three 
fusion methods including  early concatenation,  late 
 
This work  was supported in part by the National Natural Science 
Foundation of China under Grants 62071438, 62222116 , 62225113, and 
62171417 . (Corresponding author: Y uxiang Zhang ) 
Y. Zhang, Y. Zhao , and Y. Dong are with School of Geophysics and 
Geomatics, China University of Geosciences, Wuhan 430074, China (e -mail: 
zhangyx@cug.edu.cn; zyang6194@gmail.com ; dongyanni@cug.edu.cn ). 
Bo Du is with the School of Computer Science, Wuhan University, Wuhan  
430079, China (e -mail: gunspace@163.com).  concatenation , and late difference , have also been incorporated  
into newer models.  For example,  [6] and [7] use the late 
difference as their fusion operation by comparing the output of 
two corresponding layers and calculating the difference.  
Transformers  [8], [9] are a popular topic in architecture 
across various media, including image  [8], video  [10], [11] , 
text [12] and audio. This is due to their ability to model 
globally and act as unified feature extractors. Recently, many 
transformer -based methods [6], [7], [13], [14]  have shown 
promising results in change detection benchmarks. However, 
the success of these methods heavily relies  on large datasets 
due to their lack of image bias , compared to convolutional 
neural networks (CNNs)  [15]. Unfortunately, change  detection 
task suffer s from a lack of sufficient training samples. This 
results in training transformers from  scratch  taking  longer to 
converge and being more  prone to overfitting. Therefore, 
vision transformer base d change detection methods  typically 
load pretrained model parameters before starting the target 
task’s training process. Although loading pretrained models 
can lead to better performance, it requires a significant 
investment of time and resources due to the large size of these 
models . For example, the training in [16] on a single Tesla 
RTX8000 GPU  can take up to more  than 40 hours . The large 
models including transformers have shown strong 
generalization ability  not only in change detection  [4] but also  
in natural language  processing  (NLP) [12] and computer 
vision domains  [13], albeit  at the expense  of expensive  
training time and resource s. 
To tackle t he issues of training consumption , a new research 
direction called parameter -efficient transfer learning  [17]–[22] 
has emerged in the field of NLP . This rese arch aims  to achieve 
optimal performance by only fine -tuning a small number of 
additional parameters, while keeping large pretrained language 
models unchanged.  Such  techniques  have been recently 
introduced to image classification  [23], [24]  (image -to-image ) 
and video understanding [25] (image -to-video) in computer 
vision. First of all, under this parameter -efficient transfer 
learning  paradigm, all downstream task s share a set of 
parameters for the  pretrained  model. This requires the 
pretrained model to have extensive feature extraction 
capabilities, which has recently been addressed by self -
supervised learning  [26]–[29] of computer vision. Secondly, a 
framework that can specifically solve downstream tasks and 
keep the backbone model consistent with the upstream model 
is needed. To our knowledge, directly leveraging a pretrained 
model for change detection tasks (image -to-temporal image) is 
less explored,  which is likely due to the absence of a 
standardized change detection framework for transferring C 
 
 
image models  as well as the underwhelming performance of  
the existing frameworks.  
Inspired by the above ideas, we propose a unified 
framework for change detection that involves adapting the 
parameters of a pretrained vision transformer model by 
training a small number of  additional layers. Behind of the 
pretrained vision transformer, we add our fusion module  to 
detect changes between  two temporal features  and a n 
upsampling module  to complete  the framework . The inserted 
additional layer consists of two fully connected layers and a 
non-linear activation function. This approach significantly 
reduces training time and r esource costs while achieving 
competitive or even superior results compared to state -of-the-
art methods. Furthermore, we compared several mainstream 
parameter -efficient transfer learning methods  [18], [21], [22], 
[24] for their effectiveness in change detection.  Our findings 
suggest that a well -pretrained natural image model is 
sufficient for spatial modeling in change detection tasks. 
Additionally, we discover that early temporal modeling  in the 
feature encoder  is unnecessary for the current two -temporal 
change detection task. To summarize, we make the following 
contributions:  
1) We propose a new way to adapt pretrained image  
transformer models for efficient change detection. Our 
Adapter -CD method is a general, simple, and cost -
effective solution for adapting differ ent image 
pretrained models.  
2) Our method  is significantly more efficient than full fine -
tuning a change detection model, e.g., on DINO v2 
backbone with the flash attention, training time can be 
reduced to half an hour and memory footprint can be 
decreased to  9GB. In contrast, it takes more than 40 
hours to train on a single RTX 8000 with a memory 
capacity of 48G  in Change Former  [16]. 
3) We compare the effectiveness of mainstream parameter 
efficient fine -tuning methods in change detection. We 
found that fine -tuning a small number of parameters on 
the natural image model can achieve good results in 
change detection.  
4) Our method also achieves the state -of-the-arts on four 
change detection benchmarks with lower tunable 
parameters.  
II. RELATED WORK 
A. Transformer in Vision Field 
Transformer  [9], which has unified the model structure of 
natural language processing, has gradually demonstrated its 
dominance in the field of vision in recent years, especially in 
tasks that require advanced semantic expression . In some 
dense prediction tasks, transformer s [30], [31]  have achieved 
impressive results even without  hierarchical feature extraction. 
The hierarchical feature can be utilized  for subsequent 
operations to achieve the fusion of spatial and advanced 
semantic information, which can improve  the accuracy of 
semantic segmentation and change detection. However, it is 
not mand atory to employ hierarchical feature extraction while loading the pretrained transformer model for  some dense 
prediction tasks . One possible reason is that self-supervised 
learning particularly with transformer  not only benefit s 
semantic  extraction but also enhance s the extraction of multi -
level information . Transformer has been applicated in various 
vision tasks, including image classification  [8], [32] , object 
detection  [33], instance segmentation  [30], video 
understanding [10], [11] , and change detection  [6], [13], [16] . 
Due to its extensive fitting and generalization capabilities, 
transform er can be  combine d with self -supervised learning to 
learn more general features from large -scale data  [28], [29], 
[34], [35] . In this case, the large data depen dency issue of the 
transformer can be resolved and its accuracy can also be 
improved. Then, the trained model could also serve as good 
initialization for transfer learning to downstream tasks  [31]. 
Therefore, in nowadays, loading a pretrained transformer 
model to achieve better performance especially for tasks with 
insufficient data is very common . Add itionally, there exist s 
more than thousands of pretrained transformer model s in the 
web, e.g., Huggingface [36] and Timm [37] . By fully tapping 
into the generalization ability of these pretrained models in 
some downstream tasks, it can reduce the computing resources 
consumption and the greenhouse gas emission.  In this work, 
we aim to take advantage of these well pretrained model s and 
adapt them efficiently in change detection.  
 
B. Interaction with Temporal I mages  
The integrati on of temporal images was first proposed in 
video understanding  [38], where the featur e dimensions of 
multi -temporal images  were concatenated . There are three 
version s of concatenation based on the time of fusing image : 
Early Fusion (fusing the input data ), Slow Fusion (fusing  
temporal  information  throughout  the networ k), and Late 
Fusion (fusing the features after the networ k). This 
concatenation  operation  has inspir ed the development of the 
first change detection method based on deep learning, known 
as FC -Net [4]. Furthermore, the difference operation is 
utilized in certain methods when identifying the type of 
change in an object is the objective of change de tection. 
Several methods  [6], [16]  have been developed based on this 
difference operation to enhance feature extraction by util izing 
a more robust spatial encoder. Recently, [39] propose s a new 
temporal fusion operation that involves  exchang ing feature 
dimension and temporal dimension between two input 
datasets.   
Due to the popularity of transform er, some works  [10], [11]  
have also employe d self -attention for temporal fusion  
modeling. However, training self -attention directly on patches 
across multiple frames can be time -consuming and resource -
intensive. A more effective approach is to model 
corresponding spatial positions across frames in order to 
integrate temporal information. The most relevant work to us 
is [40], which involves identifying and selecting objects that 
have changed in two temporal natural ima ges by using cross -
attention. To clearly explain the specific fusion method, let ’s 
take the first image as an example, which is identical to the 
 
 
second image. This method first uses the first image as a query 
and the second image as key and value. Query, k ey, and value 
are linear transformations of input data. Then , the method  will 
perform cross -attention calculation to retrieve features related 
to the first image from querying the second image. The 
weighted result will then be concatenated with the first 
image ’s features to obtain the first feature map containing 
information from both images, thus enabling identification of 
changes in location.  
By introducing the above fusion method, we can 
summarize that all of the above methods operate on two input 
data.  In our work, we use masked cross -attention to detect 
changes between two temporal images, and it can be extended 
to inputs with multiple temporal data while maintaining low 
computation. More information is included in section III.A . 
 
C. Parameter efficient fine -tuning  
Since the appearance of BERT  [12] with 350 million 
parameters, models are becoming larger in natural la nguage 
processing while computational resources are increasing less 
than 10 times  due to the high cost of high bandwi dth memory 
(HBM ) memory. In the vision field, the parameters of vision 
transformer  [41] are extended to 22 billion. And Meta 
proposes a large self -supervised learning method, namely 
DINO v2 [28] , which has 1.1 billion parameters. The above 
large models have achieved good  performance in various 
downstream task s. However, it makes fine -tuning such a large 
model to downstream tasks infeasible for most  researche rs. To 
solve the above problem , parameters efficient fine -tuning 
(PEFT)  [19] was first proposed in natural language processing . 
PEFT  only trains part of existing backbone or newly -added 
parameters, resulting in a more efficient  training  process with 
a smaller memory footprint and time consumption . Even 
though most parameters are frozen during the training process, 
the performance of the model  is comp arable  or even superior 
to that achieved through  full fine -tuning. Recently, parameter 
efficient transfer learning is also studied in the vision field  
[23]–[25]. They mainly focus on in -domain transfer learning, 
e.g., natural image classification.  To our best knowledge, we 
are the first to adapt pretrained image model to remote sensing 
image, especially for change detection task. Additionally, we 
conduct a comparison of the main parameter efficient fine -
tuning method s on change  detection. Furthermore, our method 
is compatible with different image model s, and we fine-tuned 
the state-of-the-art in self -supervised learning , DINOv2  [28]. 
Our results achieve competitive  or state -of-the-art 
performance on six change detection benchmark  dataset s. 
III. METHOD OLOGY  
In this section , we first de scribe the pretraining model 
vision transformer  (ViT) [8] in section III.A  and then 
introduce the unified change detection framework in section 
III.B that could efficient ly transfer the pretrained image 
model. Finally , we interpret  the adapter layer s to change 
detection in  section III.C , which can adapt  the parameters of 
the pretrained model.  A. Preliminary  
Since vision  transformer was developed  from natural 
language processing, transformer -based models have been 
widely adopted in various remote sensing tasks, such as 
change detection, hyperspectral  classification. In our work, we 
use the original vision transformer as the backbone  of the 
change detection framework allowing for compatib ility with a 
wider range of  pretrained models. This method differs from 
existing transformer -based chan ged detection methods that 
primarily focus on multi -scale feature extraction . Additionally , 
this method  enables the framework to potentially  handle  more 
complex change detection tasks involving  multimodal tasks 
that incorporate  adding text  or elevation ima ges.  
Given the input image 
3HWxR , vision transformer 
first split s it into 
N  non-overlap patches. Here, 
H and 
W are 
the height and width of the input image  respectively . We 
denote the patch size as 
P and the number of patches is
2HWNP
. Then , a linear layer  called  patch embedding is 
operated on these patches, resulting in ou tput token 
ND
PxR
, where D is feature dimension, usually 384 or 768. 
Then , a learnable class token is prepended to 
Px  as 
( 1)
0[ ; ]ND
class p x x x R
 and 
classx  can be used as the output 
of the final global feature. Due to the los s of position 
information when ad apting the 2D image to 1D sequences, a 
learnable position embedding 
1 ND
posER  are added to 
0x  
as 
00 pos z x E . At this point, the total operations of dividing 
the image into sequences is complete. Then 
0z  is input to the 
sequences of transformer layers to do self -attention.  Each 
transformer layer is composed of multi -head self -attention 
layer (MSA), multilayer perception (MLP), layernorm  (LN) , 
and residual layer . The computation is listed as following.  
1
11 z ( ( ))l l lz MSA LN z
 (1) 
2( ( ))l l lz z MLP LN z
 (2) 
Here, 
l  and 
1l  are the index es of transformer layer 
where 
0z  is the first input to transformer layer. The number of 
layers is 12 in both the small and base versions of the vision 
transformer, which is also our default choice. Class token 
classx
 is removed before the output feature when the 
downstream tasks are some dense prediction tasks.  
 
B. Change Detection Framework  
Except for spatial modeling on single image, temporal 
fusion method is the most important  for change detection . 
Some traditional temporal fusion operation is introduced in  
section II.B. Our fusion method  is inspired  by masked cross -
attention.  Masked cross -attention  (MCA)  was initially  
proposed for predict ing the next word in a single sentence  in 
natural language processing  [9]. It was then  applied  to object  
 
 
 
 
Fig. 1. Change detection framework.  
 
 
 
detection  [33] in the vision field , leading to the development 
of the first end -to-end transforme r object detection model.  
This success prompts masked cross -attention as a unified 
detection head neural network for  semantic and instance 
detection  [42]. SAM  [43], the first  fundamental vision mod el, 
has implemented masked cross -attention for object 
segmentation based on input dot, box , and even text. The 
methods mentioned above mainly detect objects in spatial 
features , whereas our approach focus on  detect ing object 
changes in the temporal dimension. We achieve this by 
implementing masked cross -attention for identify changes 
between  two temporal images. Our method can potentially 
scale to three or more temporal images  and we have yet to 
explore this possibility  in this work . 
Given t he embedding 
T N DzR from the output of 
backbone, we first reshape it to  
T N T DzR , where T is the 
number of frames, usually 2, and 
2HWNP  is the number 
of patches because the class token 
classx  is removed. Then, the 
learned temporal position embeddings 
2ND
TPzR  are added 
to 
Tz . Finally , we randomly initialize a token, namely mask  
query 
1ND
mzR  to do interaction with the output features 
Tz
 in the temporal dimension. To implement  masked cross -
attention, we first do the  query  transformation on 
mz  and 
separately do the  key and value transformation on 
Tz . Here, 
, ,  are all linear layer. query key value
Then , the obtained query, 
key, and value will do scale dot attention computation. The 
masked cross -attention is computed as follows.  
( ); ; ( )TT
m Q query z K key z V value z   （） (3) 
(softmax( ) )TQKO out Vs
 (4) 
Here, 
out is another linear layer to transform the feature 
dimension  to original dim ension  and 
sD  is the scale. 
Furthermore, we also add norm layer, skip connections, and 
feed forward layer in the masked cross -attention (MCA) block. 
Therefore, the complete calculation is as follows.  
()Tz tranpose z
 (5) 
TT
TP z z z
 (6) 
12( ); ( );TT
mm z LN z z LN z
 (7) 
( , )T
m m mz z MCA z z
 (8) 
3( ( ))m m mz z MLP LN z
 (9) 
The complete pipeline of the change detection framework is 
shown in Fig 1. The two temporal images 
3
12,HWx x R are 
firstly concatenated in temporal dim ension , resulting  in 
3T H W
TxR  
, where T is commonly 2 and can also scale to 
more temporal image inputs as stated in former paragraph. 
Secondly, temporal image 
Tx  is forward ed to  the vision 
transformer, outputting spatial features 
T C H W
SxR   . 
Thirdly, we add our MCA  block behind vision transformer to 
do temporal fusion modeling. Finally , we use UperNet  [44], 
commonly used in change detection and semantic  
segmentation,  as our last module to upsample the  change  
feature to original input image size. Compared to Uper Net 

 
 
                   
 
Fig. 2. Adapter layer .     Fig. 3. Prefix -tuning .      Fig. 4. Lora . 
 
 
 
[44], we notice that MaskFormer [42] has been  utilized  in 
several  state-of-the-art methods for other dense prediction 
tasks . However, it s computation and resource requirements are  
expensive. Additionally,  most change detection benchmarks 
only involve  two classes while the advantage of MaskFormer  
is to deal with multi -class classification.  
 
C. Adapter Layer 
Even though pretrained image mode ls trained from large 
scale natural dataset ha ve shown strong transferability, using 
the encoded feature directly for remote sensing change 
detection can lead to sub-optimal performance due to the 
variability in tasks and data domains. Recently, [24] adopted 
an parameter efficient fine-tuning method  in the  vision filed 
and close d the g ap with full tuning in terms of accuracy. 
Therefore, we adopt this adapter layer method  to change 
detection due to its simplicity and good performance. In 
addition , we also evaluate the efficacy  of several  other 
parameter efficient fine-tuning techniques that have been  
successfully implemented  in the domains  of video 
understanding and image generation. Speci fically, w e compare 
the performance of three such  methods , prefix -tuning  [21], 
Lora [18] and IA3 [22], when employed  for change detection 
task in our experiment.  
As stated in sec tion III.A, image is split into many non -
overlap tokens before inputting to the transformer layer. 
Prefix -tuning p repends some learnable tokens to these image 
tokens, the output features can be aligned to target task. And 
the learnable token can be prepended to any transformer layers. 
Lora hypothesize s that change of weights are low rank when 
tuning the pretrained mod el for downstream task. Then , a low 
rank dense layer is insert ed into self-attention layer as a 
substitute for updating the parameters of the model during 
transfer learning, while keeping the pretrained model 
parameters unchanged.
dkWR , the low rank layer composed of weight matrix 
drAR  and weight matrix  
rkBR
 is adopted to replace the behavior of updating the 
pretrained weights 
W . Among them, 
A  uses Gaussian 
initialization and 
B  uses zero initialization. This can ignore 
the newly inserted parameters in the early stage of training and 
reduce the instability.  IA3 can be seen as a special export form 
of Lora, and its direct purpose is to control the scale of 
pretrain ed weights.  IA3 sets the low rank dense layer  as a 
single matrix 
A  with rank  one, and multiplies the output of 
pretrained weights and new inserted layers element -wise. 
Because the inserted layer has low rank and the most 
parameters of the model are frozen  in the above two methods , 
the computation is very low. And it is very hotly used for 
model fine -tuning in the field of image generation.  The sketch 
maps of prefix -tunin g and Lora are shown in Fig. 3 and Fig. 4.  
The adapter layer is an effective tuning method that has 
demonstrated superior performance in image classification  
[21]. As a result, we choose  it as our default option.  Adapter 
layer is composed of a down -project ion layer, an activate 
function , and an up-projection layer, as shown in Fig. 2. 
Experiments in several papers  [20], [24]  have found that 
inserting such a bottleneck layer after feed forward layer in 
parallel is the most effective. In the early sta ge of training, the 
inserted layer s do not disturb the output feature when 
initializing to zero. To be more precise, scale s is used to 
weight the feature from the adapter layer. Given the feature 
z  
from the output of the self -attention, the computations are as 
follows.  
( ( )) ( ( ( ))up down z z MLP LN z s W GELU W z   
 (10) 
Here, 
MLP , 
LN , GELU  are feed forward layer, 
LayerNorm , and activate function , respectively . 
DD
upWR  
and 
DD
downWR  are up -projection layer and down -projection  
layer, where 
D  is bottleneck dim and 
DD . As similar to  

 
 
Lora, most parameters of pretrained model are frozen.  
 
IV. EXPERIMENT  
In our experiment, we fist evaluate the effectiveness  of 
adapti ng image model with linear -prob and some other 
parameter  efficient fine-tuning method s on remote sensing 
change detection. Then we evaluate our strategy with state -of-
the-art change detection m ethods on six change detection 
benchmarks. Finally , we conduct  experiments to ablate the 
pretrained model, model size and hyperparameters about some 
tuning methods. We introduce the six change detection 
benchmarks in section  IV.A, training details in section  IV.B, 
the evaluation of e fficient tuning method s in section IV.C, 
comparation with SOTA method s in section IV.D and more 
ablation studies in section IV.E.  
A. Datasets  
S2looking  [45]: a building change detection dataset 
containing large -scale side -view satellite images taken at 
different off -ground angles. The dataset comprises 5000 
registered bit -time image pairs with a size of 1024× 1024 and a 
resolution ranging from 0.5 to 0.8 m/pixel. Additionally, it 
contains over 65,920 annotated instances of change in rural 
areas worldwide that are classified into three categories: newly 
constructed buildings, demolished buildings, and background.  
LEVIR -CD [46]: this change detection dataset consists of 
637 pairs of Google Earth (GE) image patches, each with a 
high resolution of 0.5 m/pixel and a size of 1024× 1024 pixels. 
These images were captured from 20 different regions in 
various cities across Texas, USA between the years 2002 and 
2018. The dataset includes various types of buildings such as 
villa residence s, high -rise apartments, small garages, and large 
warehouses.  
LEVIR -CD+  [45]: The dataset is a continuation of the 
LEVIR -CD dataset and has over 985 bi temporal Google Earth 
images with a very high resolution of 0.5m/pixel. The images 
are 1024× 1024 pixels in size and were collected  from 20 
different regions in various Texas cities, spanning from 2002 
to 2020.  
DSIFN  [47]: this dataset for change detection consists of 
high-resolution bi -temporal images that were manually 
collected from Google Earth. It covers six cities in China, 
including Beijing, Chengdu, Shenzhen, Chongqing  and 
Wuhan. Five of the large image -pairs have been divided into 
394 sub -image pairs with dimensions of 512× 512 pixels. The 
dataset includes a total of 3600 image pairs for training, 340 
for validation and 48 for testing purposes.  
WHU -CD [48]: this dataset includes an area that 
experienced a 6.3 -magnitude earthquake in February 20 11 and 
was subsequently rebuilt over the following years. It 
comprises aerial images captured in April 2012, depicting 
12,796 buildings across a span of 20.5 km²  (compared to the 
16,077 buildings present in the same area as depicted by the 
2016 dataset).  
CDD  [49]: This dataset contains 7 pairs of season -varying 
images with a resolution of 4725× 2700 pixels . It also includes 4 pairs of season -varying images with minimal change s and a 
resolution of 1900× 1000 pixels, which were used to manually 
add additional objects. The dataset was generated by cropping 
the images to a size of 256× 256 pixels. The dataset consists of 
16,000 image sets with an image size of 256× 256 pixels: 
10,000  for training and 3,000 each for testing and validation.  
 
B. Training details  
We use PyTorch and PyTorch Lightning to build model 
training framework . And the pipeline part of data preprocessing 
is modified from TorchGeo . For the default settings of all 
experiments, we use a batch size of 32, 100 number of epochs, 
AdamW optimizer, weight decay of 0.05 , and betas of (0.9, 
0.999). We set learning rate of 0.0004 , patch sizes of 512× 512, 
and a training/validation split of 0.2  for datasets which did not 
have a predefined validation split based on baseline model for 
the dataset. In dataset preprocessing, training samples are all 
random crop ped by 512, and augmentation contains only 
horizontal flip and vertical Flip. We use the c ombined loss of 
focal and Jaccard as the loss function. Specially, for the WHU -
CD, LEVIR -CD, and CDD dataset, we crop image to 256 
following Change Former.  
 
C. Parameter Tuning methods on change detection  
This section  evaluate s the effectiveness  of the prop osed 
adapter on three datasets. The experiments compare the 
adapter , other parameter  efficient fine-tuning method s and 
linear -prob that training just a linear classifier on WHU -CD, 
LEVIR -CD+, and CDD change detection datase ts. We use the 
DINO as our pretrained model , which is the first framework to 
use a vision transformer for self -supervised learning and 
achieves better performance compared with other self -supervised 
learning  method s. Results on different pretrained weight s are 
shown in section IV.E. The backbone is small version of vision 
transformer and we report evaluation metrics on test dataset.  The 
F1 score (F 1), Intersection over Union ( IoU), and Overall 
Accuracy ( OA) performance of  compassion  parameter  tuning 
methods  on three datasets are illu strated in Table I.  The best 
values of each evaluation metric are highlighted in bold.  
Most parameter  efficient fine -tuning methods achieve better 
performance compared to linear -probe, as shown in Table I. This 
indicates that adjusting the features can effectively improve the 
accuracy of change detection tasks . The reason is that the 
features for upstream and downstream tasks  should be different  
due to the differences in task and data domains. On the 
evaluation metric of IoU, we achieve an improvement of 
approximately 2%, 4%, and 3% compared to linear -probe on the 
WHU -CD, LEVIR -CD+, and CDD benchmarks, respectively.  In 
our proposed change detection framework, even if only the linear 
layer is trained, we f ind that features from natural image 
pretrained models can also perform well for change detection  
tasks. Besides, a ll experiments were conducted on a single  
RTX3090 device, demonstrating that parameter  efficient fine - 
tuning methods can reduce resource and training time  
requirements when applied to change detection.  
 
 
Table I. Parameter Tuning methods on three benchmarks.  
Method  WHU -CD  LEVIR -CD+   CDD  
F1 IoU OA  F1 IoU OA  F1 IoU OA 
Linear -probe  91.40  84.17  99.35   81.08  68.18  98.58   90.38  82.44  97.64  
Prefix -tuning  90.56  82.75  97.69   81.63  68.96  98.62   90.25  82.23  97.62  
Lora  91.84  84.91  99.37   84.22  72.74  98.77   91.39  84.14  97.88  
IA3 91.91  85.03  99.38   83.92  72.30  98.75   91.78  84.81  97.97  
Adapter  92.53  86.11  99.43   84.49 72.98  98.78   92.81 85.59 98.22 
 
Table II. Evaluation with SOTA methods on six benchmarks  (Color convention: best, 2nd-best, and 3rd-best). 
Method  LEVIR -CD  WHU -CD  DSIFN   CDD   S2Looking LEVIR -
CD+  
F1 IoU OA  F1 IoU OA  F1 IoU OA  F1 IoU OA  F1 F1 
FC-EF 83.40  71.53  98.39   76.73  62.24  98.31   61.09  43.98 88.59   66.93  50.30  93.28   7.65 66.48  
FC-SC 83.69  75.92  98.67   78.95  65.23  99.48   62.54 45.50  86.63   70.61  54.57  94.33   13.54 72.97 
FC-SD 86.31  71.96  98.49   79.85  66.46  98.50   59.71  42.56  87.57   75.11  60.14  94.95   13.19 73.48 
DTC -SCN  87.67  78.05  98.77   91.43  84.21  99.35   63.72 46.76 84.91   92.09  85.34  98.16   57.27  77.60  
STANet  87.26  77.40  98.66   82.32  69.95  98.52   64.56  47.66  88.49   84.12  72.22  96.13   45.97 79.31  
IFNet  88.13  78.77  98.87   83.40  71.52  98.83   60.10  42.96  87.83   84.00  71.91  96.03     
SNUNet  88.16  78.83  98.82   83.50  71.67  98.71   66.18  49.45  87.34   83.89  72.11  96.23     
BIT 89.31  80.68  98.92   90.53  83.39  99.34   69.26  52.97  89.41   88.90  80.01  97.47   61.85  82.80  
ChangeFormer  90.40  82.48  99.04   88.57  79.49  99.12       94.63  89.80  98.74   63.39   
DINO -S 88.47  79.33  98.88   92.53  86.11  99.43  72.91  57.37  91.51   92.81  86.59  98.22   64.09  84.49  
DINOv2 -S 89.55  81.08  98.98   92.38  85.84  99.40   68.50  52.09  89.39   93.93  88.55  98.49   67.29  85.66  
 
 
 
D. Evaluation with SOTA methods  
This section evaluates our strategy with state -of-the-art 
change detection methods on six change detection benchmarks.  
The experiments consider  the DINO  ViT-small and DINO v2 
ViT-small  as our pretrained model , which are named DINO -S 
and DINOv2 -S in Table II.  DINOv2 expands on the basis of 
DINO by increasing the model scale and using larger datasets in 
the context of large -scale models . Table II present the 
compari son with the state-of-the-art change detection models 
on six benchmarks.  We chose FC -Net [4], DTC -SCN  [50], 
STANet  [46], IFNet  [47], SNUNet  [51], BIT  [6], and 
Change Former  [7] as the state -of-the-art baseline models for 
comparison.  The results  on six datasets are illustrated in Table II. 
The values of each evaluation metric ranked best, second , third 
are highlighted in red, blue, and dark bold, respectively . 
Based on DINO -S and DINO v2-S, our method achieves 
competitive or better performance than most prior arts.  In 
change detection tasks, the F1 and IoU metrics are the most 
important evaluation metrics . However, previous methods 
have not reported IoU results on the S2Looking and L EVIR -
CD+ datasets.  On the evaluation metric of F1 score, we 
achieve the improvement approximate ly by 2%, 3%, 4% and 3% 
on WHU -CD, DSIFN, S2Looking and LEVIR -CD+ 
benchmarks compared with existing state-of-the-art methods.  
On other datasets, we are also able to achieve good 
performance with a difference in accuracy of less than 1%.  Note that ChangeFormer  fully tunes a segmentation 
pretrained model  based on vision transformer , taking more 
than 40 hours to train on a single RTX 8000 with a memory 
capacity of 48G  in Change Former . While our method simply 
adapts the pretrained vision transformer by training a small 
number of  additional layers  to change detection , taking half an 
hour to train o n a single RTX3090  with a memory footprint of 
9GB . The results show that o ur method is significantly more 
efficient than full fine -tuning a change detection model . This 
can also show that the proposed change detection framework 
is more general and can be loaded with more pretrained 
weight s.  
However, our method falls behind Change Former on 
LEVIR -CD and CDD dataset s. One reason is that LEVIR -CD 
is the smallest dataset among these six benchmarks and then 
the model may be  overfitting. On some large dataset s, e.g., 
S2Looking and DSIFN, our method shows large improvement.  
CDD is a dataset with seasonal variations, which m ay result in 
pseudo changes caused by seasonal factors. One potential 
explanation for this is that our experiment only employed 
simple data augmentation on the spatial scale, including 
horizontal and vertical flipping.  
E. Ablation studies  
In this section, we ablate tuning methods to study what 
properties make for a good tuning strategy and observe  several 
intriguing properties.  
 
 
Ablation on Adapter.  As stated in Section III.C, the 
adapter layer is a bottleneck layer. The bottleneck  dimension 
controls the number of introduced parameters by adapter layer. 
Lower bottleneck dimensions introduce fewer parameters at a 
possible cost to performance. We conducted an ablation study 
on the bottleneck feature dimension to investigate this effect 
on LEVIR -CD+ dataset. We denote the factor  r as the ratio of 
the feature dimension to the bottleneck dimension. As shown 
in Table III, the accuracy consistently improves when the 
factor  r increases up to 6 and reaches the saturation point 
when the bottleneck dimension is about 64. Since we set 
DINO -S as our default backbone, the feature dimension is 384.  
The scaling factor s is introduced to balance the task -
agnostic features (generated by the original frozen branch) and 
the task -specific features (generated by the tunable bottl eneck 
branch). We evaluate the adapter layer with multiple s values 
and the results are also summarized in Table III. We found 
that it achieves optimal performance with  s = 1.0. A larger or 
smaller s value results in a slight drop in performance. Thus, 
we choose  s = 1.0 as the default setting.  
 
Table I II. Factor study of  Adapter on LEVIR -CD+ dataset . 
Factor  Factor Value  F1 IoU 
Feature 
factor r 4 84.46  73.10  
6 84.49  73.14  
8 83.28  71.34  
Scale  
factor s 0.1 82.08  69.60  
0.5 83.94  72.32  
1.0 84.49  73.14  
2.0 83.67  71.92  
4.0 83.75  72.04  
Learned  84.01  72.43  
 
Ablation on Lora . The aim of Lora is to add extra layers to 
replace the behavior of updating parameters when do full fine -
tuning. We insert linear layers in the self -attention layer as 
similar as Lora.  As stated in section III.A, there are four linear 
transformation layers in the self -attention layer , namely q, k, v, 
and o transformations.  We denote 
, , ,q k v oW W W W  as the 
reparameterization  position of q, k, v, and o weight matrix . We 
ablate Lora on the position and the low rank dim to study 
these effects  on LEVIR -CD+ dataset . As shown in Table IV, 
using all 
, , ,q k v oW W W W  layers can achieve the best 
performance.  This shows that adjusting more parameters of 
the pretrain ed model can result in higher performance for 
downstream tasks such as change detection.  Besides, when the 
low rank dim ension is set to 1 , it achieve s the best 
performance and also reduces the tunable parameters and 
training time.  One possible reason is that the change detection 
task is relatively simple and the amount of data is too small. 
When fine -tuning the model, many feature dimensions are 
redundant and only a few features are beneficial. Therefore, an extremely low -rank layer can achieve good resul ts. 
 
Table  IV. Weight type study of Lora  on LEVIR -CD+ dataset . 
Weight type  r=1 r=2 r=4 
qW
 83.29  79.61  80.94  
q,v WW
 83.13  81.56  82.88  
q, , ,k v o W W W W
 84.22  84.03  83.61  
 
Scale the model size . We further conduct experiments to 
compare the accuracy between DINO and DINO v2 when 
scaling the model size  on four benchmarks . The ViT -base is 
chosen as backbone. We report F1 and IoU metric in Table V . 
Compared to DINO -S, DINO -B can achieve better F1 and IoU  
performance on the LEVIR -CD, WHU -CD, and CDD datasets. 
DINO v2-B can achieve better F1 and IoU performance on the 
LEVIR -CD, WHU -CD, and DSIFN  datasets . DINO does well 
in DSIFN and CDD dataset.  Compared with ViT-small , we 
observe that increasing the number of parameters in the ViT-
base model can improve the performance  of DINO  and 
DINOv2 . However, in most change detection datasets, ViT-
small can achieve relatively good performance while requiring 
less training time and  resource consumption. Therefore, we 
choose  ViT-small as our default option.  
 
Table V. Different m odel size s on four benchmarks . 
 Method LEVIR -CD WHU -CD DSIFN  CDD  
F1 DINO -S 88.47  92.53  72.91  92.81  
DINO -B 88.96  92.98  71.19  94.07  
DINOv2 -S 89.55  92.38  68.50 93.93  
DINOv2 -B 89.90 93.77  70.58  93.58  
IoU DINO -S 79.33  86.11  57.37  86.59  
DINO -B 80.13  86.89  54.53  88.81  
DINOv2 -S 81.08  85.84  52.09  88.55  
DINOv2 -B 81.66  88.27  54.53  87.94  
 
Different pretrained models . Here , we demonstrate the 
effectiveness of change detection framework with adapter 
layer on different pre -trained models , including SUP [8], 
DINO  [29], MAE  [34], BEIT -v2 [52]. In these methods, SUP 
refers to supervised training of the transformer on ImageNet, 
while the other methods are well -known self-supervised 
learning approaches  on ImageNet . The change detection 
framework  is performed on the LEVIR -CD+ dataset . In Table 
VI, we show the framework based on ViT -B backbone  
because most methods have a base version . DINO achieves 
the best performance and then we set it as our default choice  
in our experiments . Meanwhile, this also indicates that our 
method is compatible with most vision  transformer -based 
methods, demonstrating the versatility of our approach and 
enabling it to benefit from more powerfu l image pretrained 
models . 
 
 
Table  VI. Different pretrained models  on LEVIR -CD+ dataset . 
Arch.  Method  Pretrain data F1 IoU OA 
ViT-
B/16  SUP IN-1K 83.30  71.38  98.73  
DINO  IN-1K 85.22  74.24 98.84 
MAE  IN-1K 80.70  67.64  98.51  
BEIT -v2 IN-1K 66.07  49.33  97.34  
 
V. CONCLUSION  
In this work, we propose a new way to efficiently transfer 
pretrained image models for remote sensing change detection 
task. We compare the main efficient tuning methods and find 
that adapter layer is the best. Since only new added parameters 
are updated, the training cost is substantially lower than 
previous transformer -based change detection method. And we 
also achieve competitive and better performance on six 
benchmarks. The proposed framework is simple enough and it 
can also scal e to different pretrained image model s and multi -
temporal change detection task. Therefore, in the future work, 
we might be able to detect object changes guided by text  [53] 
instead of using one modal pretrained weight. And more 
temporal image s will be also used to verify the generality of 
the proposed framework.  
 
REFERENCES  
[1] L. Ru, B. Du, and C. Wu, “Multi -Temporal Scene Classification and 
Scene Change Detection With Correlation Based Fusion,” IEEE Trans. 
Image Process. , vol. 30, pp. 1382 –1394, 202 1. 
[2] M. Hu, C. Wu, B. Du, and L. Zhang, “Binary Change Guided 
Hyperspectral Multiclass Change Detection,” IEEE Trans. Image 
Process. , vol. 32, pp. 791 –806, 2023 . 
[3] M. Lin, G. Yang, and H. Zhang, “Transition Is a Process: Pair -to-Video 
Change Detection Networks for Very High Resolution Remote Sensing 
Images,” IEEE Trans. Image Process. , vol. 32, pp. 57 –71, 2023 . 
[4] R. Caye Daudt, B. Le Saux, and A. Boulch, “Fully Convolutional 
Siamese Networks for Change Detection,” in Proc. 25th IEEE Int. Conf. 
Image  Process.  (ICIP) , Oct. 2018, pp. 4063 –4067.  
[5] O. Ronneberger, P. Fischer, and T. Brox, “U -Net: Convolutional 
Networks for Biomedical Image Segmentation,” in Medical Image 
Comput . Computer -Assisted Interv .(MICCAI ), 2015, pp. 234 –241. 
[6] H. Chen, Z. Qi, an d Z. Shi, “Remote Sensing Image Change Detection 
with Transformers,” IEEE Trans. Geosci. Remote Sens. , vol. 60, 2021 , 
Art. no. 5607514 . 
[7] W. G. C. Bandara and V. M. Patel, “A Transformer -Based Siamese 
Network for Change Detection,” in IEEE Int. Geosci . Remote Sensi . 
Symp . (IGARSS ), Jul. 2022, pp. 207 –210. 
[8] A. Dosovitskiy et al. , “An Image is Worth 16  × 16 Words: Transformers 
for Image Recognition at Scale,” in Proc. Int. Conf. Learn. Represent . 
(ICLR ), 2021 , pp. 1 –12. 
[9] A. Vaswani et al. , “Attention is All you Need,” in Proc. Adv. Neural Inf. 
Process. Syst ., vol. 30, 2017, pp. 1 –11 
[10] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, and C. Schmid, 
“ViViT: A Video Vision Transformer,” in Proc. IEEE/CVF Int. Conf. 
Comput. Vis. (ICCV) , Oct. 2021, pp. 6816 –6826.  
[11] G. Bertasius, H. Wang, and L. Torresani, “Is Space -Time Attention All 
You Need for Video Understanding?” in Proc. 38th Int. Conf. Mach. 
Learn ., 2021, pp. 813 –824. 
[12] J. Devlin, M. -W. Chang, K. Lee, and K. Toutanova, “BERT:  Pre-training 
of Deep Bidirectional Transformers for Language Understanding ,” 2019, 
arXiv:1810.04805 v2. 
[13] C. Zhang, L. Wang, S. Cheng, and Y. Li, “SwinSUNet: Pure 
Transformer Network for Remote Sensing Image Change Detection,” 
IEEE Trans. Geosci. Remote  Sens. , vol. 60, 2022,  Art. no. 5224713  [14] Y. Zhang, Y. Zhao, Y. Dong, and B. Du, “Self -Supervised Pretraining 
via Multimodality Images With Transformer for Change Detection,” 
IEEE Trans. Geosci. Remote Sens. , vol. 61, 2023,  Art. No. 5402711 . 
[15] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image 
Recognition,” in Proc. IEEE Conf . Comput . Vis. Pattern Recognit ., 2016, 
pp. 770 –778. 
[16] W. G. C. Bandara and V. M. Patel, “A Transformer -Based Siamese 
Network for Change Detection ,” 2022, arxiv:2201.01293v7 .  
[17] V. Lialin, V. Deshpande, and A. Rumshisky, “Scaling Down to Scale Up: 
A Guide to Parameter -Efficient Fine -Tuning ,” 2023 , arxiv:2303.15647 . 
[18] E. J. Hu , Y. Shen, P. Wallis,  et al. , “LoRA: Low -Rank Adaptation of 
Large Language Models ,”2021 , arXiv : 2106.09685 . 
[19] N. Houlsby , A. Giurgiu , S. Jastrzebski,  et al. , “Parameter -Efficient 
Transfer Learning for NLP,” in Proc . 36th Int. Conf . Mach . Learn ., Jun. 
2019, vol. 97, pp. 2790 –2799.  
[20] J. He, C. Zhou, X. Ma, T. Berg -Kirkpatrick, and G. Neubig, “Towards a 
Unified View of Parameter -Efficient Transfer Learning ,” Feb. 2022 , 
arXiv :2110.04366 . 
[21] X. L. Li and P. Liang, “Prefix -Tuning: Optimizing Continuous Prompts 
for Generation ,” Jan. 2021 , arXiv:2101.00190 . 
[22] H. Liu et al. , “Few -Shot Parameter -Efficient Fine -Tuning is Better and 
Cheaper than In -Context Learning,” in Proc. Adv. Neural Inf. Process. 
Syst., vol. 35,  2022, pp. 1950 –1965.  
[23] M. Jia et al. , “Visual Prompt Tuning,” in Proc. Eur. conf. Comput . Vis., 
2022, pp. 709 –727. 
[24] S. Chen et al. , “AdaptFormer: Adapting Vision Transformers for 
Scalable Visual Recognition ,” Oct. 2022 , arXiv:2205.13535 . 
[25] T. Yang, Y. Zhu, Y. Xie, A. Zhang, C. Chen, and M. Li, “AIM: Adapting 
Image Models for Efficient Video Ac tion Recognition ,” Feb. 2023 , 
arxiv :2302.03024 . 
[26] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A Simple 
Framework for Contrastive Learning of Visual Representations,” in Proc . 
37th Int. Conf . Mach . Learn ., Nov. 2020, pp. 1597 –1607.  
[27] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum Contrast for 
Unsupervised Visual Representation Learning,” in Proc . IEEE/CVF Conf . 
Comput . Vis. Pattern Recognit ., 2020, pp. 9729 –9738.  
[28] M. Oquab et al. , “DINOv2: Learning Robust Visual Features without 
Supervision ,” Apr. 2023 , arXiv:2304.07193 . 
[29] M. Caron et al. , “Emerging Properties in Self -Supervised Vision 
Transformers,” in Proc . IEEE/CVF Conf . Comput . Vis. Pattern Recognit ., 
2021, pp. 9650 –9660.  
[30] S. Zheng et al. , “Rethinking Semantic Segmentation From a Sequence -
to-Sequence Perspective With Transformers,” in Proc . IEEE/CVF Conf . 
Comput . Vis. Pattern Recognit ., Jun. 2021, pp. 6881 –6890.  
[31] Y. Li, H. Mao, R. Girshick, and K. He, “Exploring Plain Vision 
Transforme r Backbones for Object Detection,” in Proc. Eur. conf. 
Comput . Vis., 2022, pp. 280 –296. 
[32] Z. Liu et al. , “Swin Transformer: Hierarchical Vision Transformer using 
Shifted Windows,” Mar. 2021,  arXiv: 2103.14030 . 
[33] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. 
Zagoruyko, “End -to-End Object Detection with Transformers,” in Proc. 
Eur. conf. Comput . Vis., 2020, pp. 213 –229. 
[34] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, “Masked 
Autoe ncoders Are Scalable Vision Learners,” in Proc . IEEE/CVF Conf . 
Comput . Vis. Pattern Recognit ., Jun. 2022, pp. 15979 –15988.  
[35] Z. Huang et al. , “Contrastive Masked Autoencoders are Stronger Vision 
Learners ,” Nov. 2022 , arXiv:2207.13532 . 
[36] T. Wolf et al., “Transformers: State -of-the-Art Natural Language 
Processing,” in Proc . Conf . Empirical Meth . Natural Lang . Process .: Sys . 
Demons , Oct. 2020, pp. 38 –45. 
[37] R. Wightman, “PyTorch Image Models,” GitHub repository . GitHub, 
2019.  
[38] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei -
Fei, “Large -Scale Video Classification with Convolutional Neural 
Networks,” in Proc . IEEE/CVF Conf . Comput . Vis. Pattern Recognit ., 
Jun. 2014, pp. 1725 –1732.  
[39] S. Fang, K. Li, and Z. Li, “Changer: Feature Interaction is What You 
Need for Change Detection ,” Sep. 2022 , arXiv:2209.08290 . 
[40] R. Sachdeva and A. Zisserman, “The Change You Want To See,” in Proc . 
IEEE/CVF Winter Conf . Appl . Comput . Vis., 2023, pp. 3993 –4002.  
[41] M. Dehghani et al. , “Scaling Vision Transformers to 22 Billion 
Parameters ,” Feb. 2023 , arXiv:2302.05442 . 
[42] B. Cheng, A. Schwing, and A. Kirillov, “Per -Pixel Classification is Not 
All You Need for Semantic Segmentation,” Adv. Neural Inform . Process . 
Sys., vol. 34, 2021, pp. 17864 –17875.  
 
 
[43] A. Kirillov et al. , “Segment Anything ,” Apr. 2023 , arXiv:2304.02643 . 
[44] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, “Unified Perceptual 
Parsing for Scene Understanding,” in Proc. Eur. conf. Comput . Vis., 
2018, pp. 418 –434. 
[45] L. Shen et al. , “S2Looking: A Satellite Side -Looking Dataset for 
Building Change Detection,” Remote Sens. , vol. 13, no. 24, p. 5094, Dec. 
2021,  
[46] H. Chen and Z. Shi, “A Spatial -Temporal Attention -Based Method and a 
New Dataset for Remo te Sensing Image Change Detection,” Remote 
Sens. , vol. 12, no. 10, Art. no. 10, Jan. 2020,  
[47] C. Zhang et al. , “A deeply supervised image fusion network for change 
detection in high resolution bi -temporal remote sensing images,” ISPRS J. 
Photogramm. Remo te Sens. , vol. 166, pp. 183 –200, Aug. 2020,  
[48] S. Ji, S. Wei, and M. Lu, “Fully Convolutional Networks for Multisource 
Building Extraction From an Open Aerial and Satellite Imagery Data Set,” 
IEEE Trans. Geosci. Remote Sens. , vol. 57, no. 1, pp. 574 –586, Jan. 2019,  
[49] M. A. Lebedev, Y. V. Vizilter, O. V. Vygolov, V. A. Knyaz, and A. Y. 
Rubis, “C hange  Detection  in Remote  Sensing  Images  Using Conditional  
Adversarial  Networks ,” Int. Arch. Photogramm. Remote Sens. Spat. Inf. 
Sci., vol. XLII –2, pp. 565 –571, May 2018,  
[50] Y. Liu, C. Pang, Z. Zhan, X. Zhang, and X. Yang, “Building Change 
Detection for Remote Sensing Images Using a Dual -Task Constrained 
Deep Siamese Convolutional Network Model,” IEEE Geosci. Remote 
Sens. Lett. , vol. 18, no. 5, pp. 811 –815, May 2021,  
[51] S. Fang, K. Li, J. Shao, and Z. Li, “SNUNet -CD: A Densely Connected 
Siamese Network for Change Detection of VHR Images,” IEEE Geosci. 
Remote Sens. Lett. , vol. 19, pp. 1 –5, 2022,  
[52] H. Bao, L. Dong, S. Piao, and F. Wei, “BEiT: BERT Pre -Training  of 
Image Transformers ,” Sep. 2022 , arXiv:2106.08254 . 
[53] A. Radford et al. , “Learning Transferable Visual Models From Natural 
Language Supervision,” in Proc . 38th Int Conf . Mach . Learn ., Jul. 2021, 
pp. 8748 –8763.  
 
 
 
 
TIML: Task-Informed Meta-Learning for Agriculture
Gabriel Tseng1 2Hannah Kerner1 3David Rolnick2
Abstract
Labeled datasets for agriculture are extremely spa-
tially imbalanced. When developing algorithms
for data-sparse regions, a natural approach is
to use transfer learning from data-rich regions.
While standard transfer learning approaches typ-
ically leverage only direct inputs and outputs,
geospatial imagery and agricultural data are rich
in metadata that can inform transfer learning algo-
rithms, such as the spatial coordinates of data-
points or the class of task being learned. We
build on previous work exploring the use of meta-
learning for agricultural contexts in data-sparse re-
gions and introduce task-informed meta-learning
(TIML), an augmentation to model-agnostic meta-
learning which takes advantage of task-speciﬁc
metadata. We apply TIML to crop type classiﬁ-
cation and yield estimation, and ﬁnd that TIML
signiﬁcantly improves performance compared to
a range of benchmarks in both contexts, across a
diversity of model architectures. While we focus
on tasks from agriculture, TIML could offer bene-
ﬁts to any meta-learning setup with task-speciﬁc
metadata, such as classiﬁcation of geo-tagged im-
ages and species distribution modelling.
1. Introduction
Machine learning is useful for inferring comprehensive
geospatial information from sparsely labelled data. This
is applicable to a wide range of uses, from vegetation height
mapping (Lang et al., 2019) to building footprint detection
(Zhu et al., 2021). In particular, learning from geospatial
data is crucial to better understanding, mitigating, and re-
sponding to climate change, with applications ranging from
hurricane forecasting (Boussioux et al., 2021) to methane
detection (Kumar et al., 2020). Geospatial data is especially
useful to better understand agricultural practices; data such
as agricultural land use or yield is extremely incomplete,
1NASA Harvest2McGill University and Mila – Quebec AI
Institute3University of Maryland, College Park. Correspondence
to: Gabriel Tseng <gabriel.tseng@mail.mcgill.ca >.
Preprint. Copyright 2022 by the authors.especially when considered on a global scale, and machine
learning is critical in helping ﬁll the gaps in the data. A
complete picture of global agricultural practices is vital to
mitigate and adapt to the effects of climate change, including
by assessing food security in the event of extreme weather,
more rapidly responding to food crises, and increasing pro-
ductive land without sacriﬁcing carbon sinks.
Certain parts of the world collect plentiful ﬁeld-level agri-
cultural data, but many regions are extremely data-sparse
(with this data imbalance reﬂecting a eurocentric and amero-
centric bias as in other labeled datasets in machine learning
(Shankar et al., 2017)). While previous work has investi-
gated transfer learning from data-rich areas to improve per-
formance in data-sparse areas (Wang et al., 2018; Rußwurm
et al., 2020), geospatial datasets (and agricultural data in par-
ticular) are rich in metadata that can inform transfer learning
algorithms by enabling models to learn useful context be-
tween datapoints, such as the relative geographic locations
of datapoints or the higher-level category of the class label
(Turkoglu et al., 2021).
We propose a new method for passing such auxiliary infor-
mation to the model to improve overall performance and
equitable generalization. Speciﬁcally, we build on previous
work applying Model-Agnostic Meta-Learning (MAML)
(Finn et al., 2017) to geospatial data. Meta-learning aims to
learn a model that can quickly learn a new task from a small
amount of new data by optimizing over many training tasks.
When using geospatial data, tasks are created by partitioning
samples based on agro-ecological (Rußwurm et al., 2020)
or political (Tseng et al., 2021a;b) boundaries.
We summarize the main contributions of this paper below1:
•We introduce Task-Informed Meta-Learning (TIML),
an algorithm designed to augment MAML by incorpo-
rating task metadata and removing memorized tasks.
•We show that TIML improves performance for both
regression (yield estimation) and classiﬁcation (crop
type classiﬁcation) tasks across a diversity of neural
network architectures.
•We highlight TIML’s ability to learn from very few
positive labels and to perform well on tasks where
1Seegithub.com/nasaharvest/timl for code & dataarXiv:2202.02124v1  [cs.LG]  4 Feb 2022
Task-Informed Meta-Learning for Agriculture
Figure 1. Remote sensing data from the CropHarvest dataset
(Tseng et al., 2021b), collapsed to two dimensions using t-SNE
(van der Maaten & Hinton, 2008) and colored according to the
continent in which the datapoint is located. Datapoints appear
to cluster according to their continent, suggesting that datapoints
from the same geographic region share similarities that could be
learned by a model. This clustering provides the intuition for the
TIML method (that nearby tasks will be more informative during
ﬁne-tuning than far-away tasks).
other transfer-learned models do poorly.
While we motivate our method and focus our experiments
on agricultural applications, we highlight that TIML is not
speciﬁc to agriculture and could be applied to any meta-
learning problem that includes task-speciﬁc metadata, such
as classiﬁcation of geo-tagged images (Mac Aodha et al.,
2019) or species distribution modelling (Beery et al., 2021).
2. Related Work
2.1. Transfer learning for remote sensing
In prior work using machine learning for remote sensing
data, there have been numerous efforts to learn from data-
rich geographies and transfer the resulting model to data-
sparse regions or underrepresented classes. Wang et al.
(2018) found that training a yield estimation model using
data from Argentina boosted the performance of that model
in Brazil. In other studies, the source and target tasks are not
geographically deﬁned. For example, Jean et al. (2016) used
transfer learning to improve performance on a data-sparse
task (wealth estimation) by ﬁrst learning a data-rich task
(nighttime light estimation).
Prior work has also used multi-task learning to improve
model performance for data-sparse tasks. Kerner et al.
(2020) trained a multi-task model where one task classi-
ﬁes crops in the data-sparse target region (Togo) and the
second task classiﬁes crops elsewhere in the world, and
showed that augmenting the data-sparse task with global
data improved the model performance in Togo. Chang et al.
(2019) trained a multi-task neural network to simultaneously
classify forest cover type and regress forest structure vari-ables such as biomass to improve model performance on
both tasks.
Rußwurm et al. (2020) introduced the idea of geographically
deﬁned tasks for meta-learning and applied it to remote sens-
ing data (speciﬁcally for land cover classiﬁcation). Tseng
et al. (2021a) and Tseng et al. (2021b) used meta-learning
for agricultural land cover classiﬁcation.
These approaches often fail to capture important metadata
and expert knowledge about the data-sparse tasks of fo-
cus, such as their geographic location relative to the pre-
training data or the high-level category of crops being classi-
ﬁed. This metadata can be useful for learning relationships
between samples or tasks that can improve classiﬁcation
performance—for example, remote sensing observations of
maize will be more similar between Kenya and Mali than
between Kenya and France (Figure 1). In this work, we
consider the metadata inherent to agricultural classiﬁcation
tasks, such as the spatial relations between tasks, and how
this can inform the model’s predictions.
2.2. Meta-learning
Meta-learning, or learning to learn , consists of learning a
function for a task given other example tasks (Thrun & Pratt,
1998). Recent work in this area has focused on few-shot
learning, i.e., learning a function for a task given few train-
ing datapoints (Snell et al., 2017; Ravi & Larochelle, 2017).
In particular, model-agnostic meta-learning (MAML) (Finn
et al., 2017) is a few-shot meta-learning algorithm that uses
the other example tasks to learn a set of initial weights that
can rapidly generalize to a new task. MAML can be used
with any neural network architecture.
A more complex variant to few-shot generalization is few-
shot dataset generalization, where a single model is used
to learn from multiple datasets (as opposed to tasks, which
can be drawn from the same dataset) (Triantaﬁllou et al.,
2020). In this regime, one solution is to learn an encoder
which can modulate the distribution of weights learned by
MAML depending on the dataset being considered (Vuorio
et al., 2019; Triantaﬁllou et al., 2021).
In this work, we consider how the MAML weights may be
modulated even when all the tasks are drawn from the same
dataset. In particular, we consider the special case when
there is task-level metadata that can inform the modulation
of the MAML weights for speciﬁc tasks.
3. Methods
Our proposed approach, Task-Informed Meta-Learning
(TIML), builds on Model-Agnostic Meta-Learning
(MAML) (Finn et al., 2017). Speciﬁcally, we modulate the
meta-weights learned by MAML depending on the task
Task-Informed Meta-Learning for Agriculture
Figure 2. An illustration of the encoder, and the modulation of the
MAML learner’s hidden vectors using the encoder’s output. We
highlight the differing optimization regimes for the encoder and
the MAML learner – the encoder’s output remains static through
the MAML learner’s inner loop optimization.
of interest. This allows different weight initializations to
be learned depending on the tasks, allowing the model
to learn a wide distribution of tasks. Model-Agnostic
Meta-Learning (MAML) learns a set of model weights θ
which is close to optimal for each of a variety of different
tasks, allowing the optimal weights for a speciﬁc task to be
reached with little data and/or few gradient steps. These
initial weights θare updated by ﬁne-tuning them on a
training task (inner loop training), yielding updated weights
θ′. A gradient for θis then computed with respect to the
loss of the updated model, Lθ′. This gradient is then used
to updateθ(outer loop training).
In the following sections, we ﬁrst describe how we imple-
mented MAML in a geospatial context (Section 3.1) (fo-
cusing on how tasks are constructed) before describing the
TIML method in more detail (Section 3.2).
3.1. MAML in a geospatial context
As in previous work applying meta-learning to geospatial
data (Tseng et al., 2021b; Wang et al., 2020), we deﬁne
tasks spatially. Speciﬁcally, given a particular task we use
political boundaries (counties or countries) to separate a sin-
gle dataset into many different tasks. The intuition for this
is that agricultural practices (and land use) are inﬂuenced
by the policies and cultural practices of a region, which are
often deﬁned along political boundaries (e.g., teff is a pop-
ular crop grown in Ethiopia and Eritrea but not bordering
countries). This makes political territories useful units when
conducting spatial analysis of a region (e.g. (Kimenyi et al.,
2014)). In addition, deﬁning tasks in this way allows for
region-speciﬁc tasks to be deﬁned. For example, some crops
may only be grown in certain parts of the world (e.g., cacao
is typically grown within 20 degrees of the equator), or data
collection efforts for speciﬁc crops may only have occurredin certain areas. Spatially deﬁned tasks mean that the model
can be trained to identify these regional crops when it is
looking at that region, and not elsewhere.
3.2. Task-Informed Meta-Learning
We build on model-agnostic meta-learning (Finn et al.,
2017), considering the case where there is additional task-
speciﬁc information that could inform the model, such as
the spatial relationships between tasks. Information such as
the spatial coordinates of a task remains static for all data-
points in the task, so is not useful to differentiate positive
and negative instances within tasks. However, it may be
useful to condition the model prior to inner loop training.
Algorithm 1 Task-Informed Meta-Learning
1:Require:p(T): Distribution over tasks
2:Require:α,β: step size hyperparameters
3:randomly initialize meta model θm, task encoder θe
4:while not done do
5: Sample batch of tasks Ti∼p(T)with task informa-
tionti
6: for allTi,tido
7: Generate task embeddings µi=f(ti;θe)
8: Evaluate∇θmLTi(fθm,µi)with respect to K ex-
amples
9: Compute adapted meta parameters with gradient
descent:θ′
mi←θm−α∇θmLTi(fθm,µi)
10: end for
11: Updateθm←θm−β∇θmΣTi∼p(T)LTi(fθ′
mi,µi)
12: Updateθe←θm−β∇θeΣTi∼p(T)LTi(fθ′
mi,µi)
13:end while
We introduce Task-Informed Meta-Learning (TIML) (Algo-
rithm 1), which modulates the hidden vectors in the meta-
model based on embeddings calculated using task informa-
tion. We encode the task-speciﬁc information into a set of
vectors – two for each hidden layer to be modulated in the
meta-model, ti
γandti
β. We use feature-wise linear modu-
lation (FiLM (Perez et al., 2018)) to modulate the hidden
vector outputs of the meta-model using these task encodings.
Given a hidden vector output h, we compute the Hadamard
product of ti
γandhiand add ti
βto calculate the modulated
vector which is passed to the next layer in the network:
hi
out= (ti
γ⊙hi) +ti
β (1)
These task embeddings are updated in the outer loop during
training. This means that when the meta-model is being ﬁne-
tuned for a speciﬁc task, the embeddings remain constant
for all datapoints in that task. We illustrate this in Figure 2.
Task encoder We use a task encoder to learn the em-
beddings. This encoder consists of linear blocks, where
Task-Informed Meta-Learning for Agriculture
each block contains a linear layer with a GeLU activation
(Hendrycks & Gimpel, 2016), group normalization (Wu &
He, 2018) and dropout (Srivastava et al., 2014). The task
information is encoded into a hidden task vector. Indepen-
dent blocks are then used to generate an embedding for each
hidden vector in the classiﬁer to be modulated.
3.3. Forgetful Meta-Learning
Due to the spatial imbalance of data globally (de Vries
et al., 2019; Shankar et al., 2017), spatially partitioned meta-
learning tasks may not be geographically well distributed.
They can also be semantically imbalanced. In the CropHar-
vest dataset (Tseng et al., 2021b), a large fraction of the
tasks are crop vs. non-crop tasks, reﬂecting the large num-
ber of binary crop vs. non-crop datapoints in the dataset
(65.8% of all instances only have crop vs. non-crop labels).
We ﬁnd that in such settings the model is liable to memorize
many similar tasks to the detriment of its ability to learn
more difﬁcult or rarer tasks, thus hurting generalization per-
formance for the ﬁne-tuning tasks. This limitation is not
limited to geospatial or agricultural datasets and could occur
for any dataset with imbalanced classes or task difﬁculty.
Although complex meta-learning methods have been de-
signed to optimize for performance on highly challenging
tasks (Jamal & Qi, 2019; Collins et al., 2020), we take
advantage of the large number of similar tasks in the geospa-
tial data setting to introduce a simple method to prevent
memorization of certain tasks: removing training tasks the
model has memorized, where memorization is deﬁned as
having exceeded a performance threshold for a task over
a continuous set of epochs. We call this method “forget-
fulness.” Reducing the training set size before training has
been previously explored (Ohno-Machado et al., 1998; Han
et al., 2021) to reduce training time - we do it dynamically
to improve performance.
4. Datasets
TIML is designed for transfer learning regimes which can
be structured in terms of meta-learning (learning from many
different tasks). In addition, TIML expects metadata that
inform how the model should consider each task and remain
constant for all datapoints within the task. We consider two
datasets well suited to this regime.We consider a regression
and a classiﬁcation task to demonstrate the suitability of
TIML in both contexts.
4.1. Crop Type Classiﬁcation
Up to date cropland maps are critical to understanding the
climate impacts of agriculture (Song et al., 2021). Crop type
classiﬁcation consists of predicting whether or not a given
instance contains a crop of interest. Speciﬁcally, givena remote sensing-derived pixel time series for a speciﬁc
latitude and longitude and a crop of interest, the goal is to
output a binary value describing whether the crop of interest
is being grown at that pixel location.
4.1.1. D ATA DESCRIPTION
We use the CropHarvest dataset (Tseng et al., 2021b). This
dataset consists of 90,480 globally distributed datapoints
with the associated satellite pixel time series for each point.
Of these datapoints, 30,899 (34.2%) contain multi-class
agricultural labels; the remaining datapoints contain binary
“crop” or “non-crop” labels. Each datapoint is accompa-
nied by a pixel time series from 4 remote sensing products:
Sentinel-2 L1C optical observations, Sentinel 1 synthetic
aperture radar observations, ERA5 climatology data (pre-
cipitation and temperature), and topography (slope and el-
evation) from a Digital Elevation Model (DEM). The time
series includes 1 year of data at monthly timesteps.
4.1.2. T ASK CONSTRUCTION FOR META -LEARNING
As with the CropHarvest benchmarks, we deﬁned tasks spa-
tially using bounding boxes for countries drawn by Natural
Earth (Patterson & Kelso). Tasks consist of binary classiﬁ-
cation of pixels as either crop vs. non-crop or a speciﬁc crop
type vs. rest. This yielded 525 tasks, which were randomly
split into training and validation tasks. Three evaluation
tasks (described in Section 4.1.3) were withheld from the
initial training. For each evaluation task, we ﬁne-tuned
the model on that task’s training data before evaluating the
model on that task’s test data.
Task Information Task information is encoded in a
13-dimensional vector. Three dimensions are used to
encode spatial information, consisting of latitude and
longitude transformed to [cos(lat)×cos(lon),cos(lat)×
sin(lon),sin(lat)]. This transforms the spatial information
from spherical to Cartesian coordinates, ensuring that trans-
formed values at the extreme longitudes are close to each
other. The remaining 10 dimensions are used to communi-
cate the type of task the model is being asked to learn. This
consists of a one-hot encoding of crop categories from the
UN Food and Agriculture Organization (FAO) indicative
crop classiﬁcation (fao, 2020), with an additional class for
non-crop. For crop vs. non-crop tasks, positive examples are
given the value1
nacross all the n= 9crop type categories.
4.1.3. E VALUATION
The CropHarvest dataset is accompanied by 3 evaluation
tasks which test the ability of a pre-trained model to learn
from a small number of in-distribution datapoints in a va-
riety of agroecologies. These test tasks cover a variety of
agroecologies. We describe each task and the accompanying
Task-Informed Meta-Learning for Agriculture
training data below.
Togo crop vs. non-crop : The goal of this task is to classify
datapoints as crop or non-crop in Togo. The training set
consists of 1,319 datapoints and the test set consists of 306
datapoints – 106 (35%) positive and 200 (65%) negative –
sampled from random locations within the country.
The two other evaluation tasks consist of classifying a spe-
ciﬁc crop. Thus, “rest” below includes all other crop and
non-crop classes. For both tasks, entire polygons delineating
a ﬁeld (as opposed to single pixels within a ﬁeld) were col-
lected, allowing evaluation across the polygons. However,
during training, only the polygon centroids were used.
Kenya maize vs. rest : The training set consists of 1,345
imbalanced samples (266 positive and 1,079 negative sam-
ples). The test set consists of 45 polygons containing 575
(64%) positive and 323 (36%) negative pixels.
Brazil coffee vs. rest : The training set consists of 794 im-
balanced samples (21 positive and 773 negative samples).
The test set consists of 66 polygons containing 174,026
(25%) positive and 508,533 (75%) negative pixels.
4.2. Yield Estimation
Accurate and timely yield estimates are a key input to food
security forecasts (Becker-Reshef et al., 2020), and to better
understand how food production can be sustainably man-
aged (Lark et al., 2020). Yield estimation is a regression task
which consists of estimating the yield – the amount of crop
harvested per unit of land – of an area, given remote sensing
data of that area. Speciﬁcally, we estimate soybean yield in
the top soybean-producing states in the United States.
4.2.1. D ATA DESCRIPTION
We recreate the yield prediction dataset originally collected
by You et al. (2017). This dataset consists of county-level
soybean yields for the 11 US states accounting for over
75% of national soybean production from 2009 to 2015.
MODIS reﬂectance (Vermote, 2015) and temperature data
(Wan et al., 2015) are used to construct the remote sens-
ing inputs. Since counties cover large areas, inputting the
raw satellite data to the model would create extremely high-
dimensional inputs. To handle this, You et al. (2017) as-
sumed permutation invariance , meaning the positions of
farmland pixels in a county do not affect yield, since they
only indicate the positions of cropland. This allows all crop-
land pixels in a county (based on the MODIS land cover map
(Friedl et al., 2010)) to be mapped to a histogram of pixel
values, signiﬁcantly reducing the dimensionality of the in-
put. This is the predictand for soybean yields in each county.
Since the original You et al. (2017) paper was released, the
MODIS data product version has incremented from ver-
sion5.1to6.0. Therefore, our histograms are similar butnot identical to those in You et al. (2017). We note that
this dataset was also released by (Yeh et al., 2021) but we
chose to recreate the dataset from You et al. (2017) since we
wanted to maintain the temporal validation originally used
(as opposed to the random split used in Yeh et al. (2021)).
4.2.2. T ASK CONSTRUCTION FOR META -LEARNING
We deﬁne tasks to be individual counties, with task (X,y)
pairs consisting of histograms and yields for different years.
Task Information As with the crop type classiﬁcation
task (Section 4.1), we use 3 dimensions in the task infor-
mation vector to encode transformed latitude and longitude
values describing the location of the county. In addition, we
include a one-hot encoding communicating which state the
county is in to the model based on the intuition that agricul-
tural practices vary enough across states that it may help the
model to have this difference explicitly communicated.
4.2.3. E VALUATION
We use temporal validation; speciﬁcally, for each year in
{2011,2012,2013,2014,2015}, we train a model using all
the data prior to that year, and evaluate the performance of
the model for the unseen year.
5. Experiments
5.1. Crop Type Classiﬁcation
We evaluate TIML by training it on the CropHarvest dataset
and ﬁne-tuning it on the evaluation tasks, as was done for the
benchmark results released with the dataset in Tseng et al.
(2021b). MAML (and by extension, TIML) can be applied
to any neural network architecture. We use the same base
classiﬁer and hyperparameters as in Tseng et al. (2021b): a
1-layer LSTM model followed by a linear classiﬁer.
5.1.1. A BLATIONS
We perform 3 ablations to understand the effects of different
components of TIML on overall model performance:
•No forgetfulness : A TIML model trained without for-
getfulness; no tasks are removed in the training loop.
•No encoder : A TIML model with no encoder. The
task information is instead appended to every raw input
timestep and passed directly to the classiﬁer.
•No task information or encoder : No task informa-
tion passed to the model at all. This model is effectively
a normal MAML model, trained with forgetfulness.
Task-Informed Meta-Learning for Agriculture
Table 1. Results for the crop type classiﬁcation evaluation tasks. All results are averaged from 10 runs and reported with the accompanying
standard error. We report the area under the receiver operating characteristic curve (AUC ROC) and the F1 score using a threshold of 0.5
to classify a prediction as the positive or negative class. We highlight the ﬁrst andsecond best metrics for each task. TIML achieves the
highest F1 score of any model on the Brazil task and the best AUC ROC and F1 scores when averaged across the 3 tasks. We highlight the
improvement of TIML relative to other transfer-learning models, showing it is able to leverage task structure to signiﬁcantly increase
performance on the CropHarvest dataset.
Model Kenya Brazil Togo MeanAUC ROCRandom Forest 0.578±0.006 0.941±0.004 0.892±0.001 0.803
No pre-training 0.329±0.011 0.898±0.010 0.861±0.002 0.700
Crop pre-training 0.694±0.001 0.820±0.002 0.894±0.000 0.801
MAML 0.729±0.001 0.831±0.005 0.878±0.001 0.843
TIML 0.794±0.003 0 .988±0.001 0.890±0.000 0.890
no forgetfulness 0.779±0.003 0.877±0.003 0.893±0.001 0.850
no encoder 0.712±0.001 0.977±0.002 0 .895±0.000 0 .862
no task info or encoder 0.690±0.001 0.977±0.002 0.876±0.001 0.848F1 scoreRandom Forest 0.559±0.003 0.000±0.000 0.756±0.002 0.441
No pre-training 0.782±0.000 0.764±0.012 0.720±0.005 0.734
Crop pre-training 0.819±0.001 0.619±0.005 0.713±0.002 0.613
MAML 0.828±0.001 0.496±0.001 0.662±0.001 0.652
TIML 0.838±0.000 0 .835±0.012 0.732±0.002 0.802
no forgetfulness 0.840±0.000 0.537±0.002 0.764±0.002 0.724
no encoder 0.840±0.000 0.473±0.002 0.691±0.001 0.691
no task info or encoder 0.837±0.001 0.473±0.001 0.645±0.002 0.652
5.1.2. B ASELINES
We compare the TIML architecture to 4 baselines. As with
TIML, we ﬁne-tune these models on each benchmark task’s
training data and then evaluate them on the task’s test data:
•MAML : A model-agnostic meta-learning classiﬁer
without the task information.
•Crop pre-training : A classiﬁer pre-trained to classify
all data as crop or non-crop (without task metadata),
then re-trained on each test task.
•No pre-training : A randomly initialized classiﬁer,
which is not pre-trained on the global CropHarvest
dataset but instead is trained directly on the test task
training data.
In addition, we trained a Random Forest baseline imple-
mented using scikit-learn (Pedregosa et al., 2011) with the
default hyperparameters.
5.2. Yield Estimation
We apply TIML to the original network architectures used by
You et al. (2017) – a 1-layer LSTM and a CNN-based regres-
sor. In addition to the remote sensing input, the Deep Gaus-
sian Process baseline model (described in Section 5.2.1)receives as input the year of each training point. We there-
fore append the year to each timestep of the input to the
TIML LSTM, so that the model has comparable inputs to
the Deep Gaussian Process. The CNN-based models receive
only the remotely sensed data as input.
5.2.1. B ASELINES
We compared TIML to 2 baselines: the Deep Gaussian
Process models (proposed by You et al. (2017) alongside
the yield estimation dataset) and standard MAML.
Deep Gaussian Process To train a Deep Gaussian Pro-
cess, a deep learning model is ﬁrst trained to estimate yield
given the remote sensing dataset described above. The ﬁnal
hidden vector h(x)of the model (for each input) is used as
input to a Gaussian process:
y(x) =f(x)+h(x)Twheref(x)∼GP (0,k(x,x′))(2)
where the kernel function kis conditioned on both the loca-
tion of the datapoint (deﬁned by its latitude and longitude),
gl, and the year of the datapoint, gy:
k(x,x′) =σ2exp[−∥gl−g′
l∥2
2
2rl−∥gy−g′
y∥2
2
2ry] +σ2
eδg,g′(3)
We include baselines with and without a Gaussian process
(i.e., using the outputs of the deep learning models directly
Task-Informed Meta-Learning for Agriculture
(a) Kenya: Maize vs. Rest
(b) Togo: Crop vs. Non Crop
Figure 3. Results of TIML and the benchmark models when trained
on a subset of the evaluation training datasets for the Crop Type
Classiﬁcation Task. Speciﬁcally, we plot results for (Figure 3a) the
Kenya Maize vs. rest evaluation task and (Figure 3b) Togo Crop
vs. Non Crop evaluation task. All results are averaged from 10
runs, and reported with standard error bars. For both tasks, the
subset is balanced so that it contains an equal number of positive
and negative samples. TIML is the best performing model in
Kenya, and - alongside the crop vs. non crop pretrained model - is
among the best performining models in Togo for all subset sizes,
indicating TIML’s ability to learn from limited datast sizes. We
highlight that for the smallest training sample size, consisting of
20 samples, TIML is the best performing algorithm.
instead of passing the ﬁnal hidden vectors to a Gaussian pro-
cess). We note that this implementation of Deep Gaussian
Processes differs from Damianou & Lawrence (2013).
As noted in Section 4.2.1, the MODIS datasets have been
updated since the original Deep Gaussian Process models
were run. We therefore retrain them to obtain our baseline re-
sults. We use the same hyperparameters as You et al. (2017),
with the addition of early stopping when training. You et al.
(2017)’s original results are included for comparison.
Additional implementation details for the crop classiﬁcation
and yield estimation datasets are available in Appendix A.6. Results & Discussion
6.1. Crop Type Classiﬁcation
We show the model results for TIML, its ablations and all
baseline models when trained on the CropHarvest dataset
in Table 1. Like Tseng et al. (2021b), we report the AUC
ROC score and the F1 score calculated using a threshold
of 0.5. Overall, TIML is the best performing algorithm
on the CropHarvest dataset, achieving the highest F1 and
AUC ROC scores when averaged across all tasks. TIML
is consistently the best performing algorithm on every task.
In particular, TIML is the only transfer learning model that
outperforms a randomly-initialized model in the challenging
Brazil task, where there are only 26 positive datapoints.
6.1.1. E FFECTS OF TRANSFER LEARNING
Standard transfer learning from the global dataset is not
guaranteed to confer advantages to the model. For example,
ﬁrst training using MAML or crop pre-training results in
lower performance on the Brazil task compared to an LSTM
initialized with random weights. We hypothesize this may
be due to the difference in distribution of the Brazil task data
relative to the other tasks the models are trained on. TIML
is the only model to see signiﬁcant improvements in per-
formance compared to the randomly initialized model, sug-
gesting conditioning the model with prior, domain-speciﬁc
information about the tasks can help to model the diversity
of samples in the CropHarvest dataset.
6.1.2. F ORGETFULNESS
Forgetfulness – when coupled with task information – im-
proves model performance in more challenging tasks with-
out penalizing performance elsewhere. Training TIML with
forgetfulness signiﬁcantly boosts performance in the Brazil
task without substantially impacting performance on the
other tasks, and yields signiﬁcantly higher mean F1 and
AUC ROC scores when measured across all tasks. How-
ever, training TIML forgetfully without the task information
(TIML with no task information or encoder) yields compa-
rable results to the baseline MAML model trained without
forgetfulness. We therefore hypothesize that task informa-
tion provides useful context around which tasks are being
kept and forgotten during training, allowing TIML to learn
from more difﬁcult tasks in the “forgetful” regime without
forgetting easier tasks it has already learned.
6.1.3. E FFECT OF TASK INFORMATION
Including task information in the model improves perfor-
mance, both when it is concatenated to the input data and
when it is passed to the model through TIML. However,
there are signiﬁcant differences in performance depending
onhow this information is passed to the model: passing the
Task-Informed Meta-Learning for Agriculture
Table 2. The RMSE of county-level model performance for the yield estimation task. We use temporal validation to evaluate the model.
Speciﬁcally, for each year, models are trained with data up to that year and evaluated with that year’s data. All models are calculated from
an average of 10 runs, with the standard error reported. We highlight the ﬁrst andsecond best metrics for each task. For completeness,
we include the results reported by (You et al., 2017), but highlight that these results were obtained on the MODIS 5.1 dataset (whilst
all other models were trained on the MODIS 6.0 dataset) and are the result of 2 runs, compared to 10 runs for all other models. TIML
improves on the Deep Gaussian Process models for both architectures, even though MAML performs signiﬁcantly worse than all other
models. This suggests that in some cases, the task information is necessary for meta-learning to work.
Model 2011 2012 2013 2014 2015 Mean
LSTM 5.62±0.10 6.60±0.29 5.57±0.21 6.63±0.13 6.69±0.31 6.22
+ GP 5.32±0.10 5.83±0.18 5.70±0.19 5.61±0.12 5.24±0.14 5 .54
+ MAML 26.90±0.01 30.97±0.01 29.57±0.01 30.84±0.01 32.02±0.01 30.06
+ TIML 5.16±0.03 5 .77±0.05 5 .39±0.02 5.24±0.04 4.89±0.04 5 .29
CNN 6.08±0.77 6.94±1.83 6.42±1.23 4.80±0.83 5.57±0.38 5.96
+ GP 5.55±0.14 6.18±0.49 6.44±0.67 4.87±0.31 6.02±0.26 5.81
+ MAML 12.93±0.05 8.28±0.07 7.98±0.04 12.05±0.05 7.69±0.06 9.79
+ TIML 5.23±0.02 6.59±0.02 5.34±0.01 4.93±0.02 6.35±0.01 5.69
(You et al., 2017)
LSTM + GP 5.77 6.23 5.96 5.70 5.49 5.83
CNN + GP 5.70 5.68 5.83 4.89 5.67 5.55
task information directly to the classiﬁer (TIML with no en-
coder) yields mixed results (lower AUC ROC in Kenya, and
lower F1 scores in Brazil and Togo compared to MAML or
crop-pretraining models). Training TIML with the encoder
signiﬁcantly boosts performance in these regimes, yielding
the highest mean AUC ROC and F1 scores. We hypothe-
size that because the task information remains static for all
datapoints in a task, it is challenging for the model to learn
from them during the inner loop optimization – the encoder
architecture allows it to only be optimized in the outer loop.
6.1.4. E FFECTS OF FINE -TUNING DATASET SIZES
TIML excels at learning from small dataset sizes. We plot
the performance of the models as a function of training set
size in Figure 3 for the Kenya and Togo evaluation tasks (the
Brazil task has only 26 positive examples, and is therefore
already in the small-dataset size regime). In both the Togo
and Kenya tasks, the TIML model is amongst the most
performant algorithms (as measured by AUC ROC score)
for all subset sizes. We highlight that for the smallest sample
size (20 ﬁne-tuning samples), TIML is the best performing
algorithm for both the Togo and Kenya evaluation tasks.
6.2. Yield Estimation
We share yield estimation results in Table 2. Like You
et al. (2017), we report the RMSE score averaged across all
counties and use temporal validation to evaluate the models.
The TIML and MAML LSTM receive the year as input (to
reﬂect the data available to the Deep Gaussian Process), but
the TIML and MAML CNN do not.For both the LSTM and CNN architectures, TIML is the
most performant model. This is the case even though the
Deep Gaussian Process is much more memory intensive,
since it requires all predictions and hidden vectors (for the
training and test data) to be computed together for the Gaus-
sian process modelling step; this may be infeasible for larger
datasets. TIML requires substantially less memory since it
considers each county independently.
It is also worth noting that while TIML achieves the best
result of all models, MAML performs signiﬁcantly worse
than all other models. This suggests that in some contexts,
the task information is necessary for meta-learning to work.
7. Conclusion
In conclusion, we introduce task-informed meta-learning
(TIML), a method for conditioning the model with prior
information about a speciﬁc task. Speciﬁcally, the task in-
formation is encoded into a set of vectors which are used
to modulate the weights learned by a MAML learner prior
to task-speciﬁc ﬁne-tuning. In addition, we introduce the
concept of “forgetful” meta-learning, which can improve
meta-learning performance when there are many similar
tasks to learn from. We apply TIML to a range of tasks
(classiﬁcation and regression) and a range of model archi-
tectures (RNNs and CNNs), demonstrating its utility in
a variety of regimes (including those with very few data
points, and regimes in which standard MAML fails com-
pletely). Although we focus on agriculture-related tasks,
TIML is not speciﬁc to agriculture and can be applied to any
meta-learning setup with task-level metadata.
Task-Informed Meta-Learning for Agriculture
References
Programme, concepts and deﬁnitions. In World Programme
for the Census of Agriculture . FAO, 2020.
Antoniou, A., Edwards, H., and Storkey, A. How to train
your MAML. In International Conference on Learning
Representations (ICML) , 2019.
Arnold, S. M. R., Mahajan, P., Datta, D., Bunner, I., and
Zarkias, K. S. learn2learn: A library for Meta-Learning
research. 2020.
Becker-Reshef, I., Justice, C. J., Barker, B., Humber, M. L.,
Rembold, F., Bonifacio, R., Zappacosta, M., Budde,
M., Magadzire, T., Shitote, C., Pound, J., Constantino,
A., Nakalembe, C., Mwangi, K., Sobue, S., Newby, T.,
Whitcraft, A., Jarvis, I., and Verdin, J. Strengthening agri-
cultural decisions in countries at risk of food insecurity:
The geoglam crop monitor for early warning. Remote
Sensing of Environment , 2020.
Beery, S., Cole, E., Parker, J., Perona, P., and Winner, K.
Species distribution modeling for machine learning prac-
titioners: A review. In ACM SIGCAS Conference on
Computing and Sustainable Societies , 2021.
Boussioux, L., Zeng, C., Bertsimas, D., and Guenais, T. J.
Hurricane forecasting: A novel multimodal machine
learning framework. In NeurIPS 2021 Workshop on Tack-
ling Climate Change with Machine Learning , 2021.
Chang, T., Rasmussen, B. P., Dickson, B. G., and Zachmann,
L. J. Chimera: A multi-task recurrent convolutional neu-
ral network for forest classiﬁcation and structural estima-
tion. Remote Sensing , 11(7):768, 2019.
Collins, L., Mokhtari, A., and Shakkottai, S. Task-robust
model-agnostic meta-learning. In Advances in Neural
Information Processing Systems (NeurIPS) , 2020.
Damianou, A. and Lawrence, N. D. Deep Gaussian pro-
cesses. In Proceedings of the Sixteenth International
Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS) , 2013.
de Vries, T., Misra, I., Wang, C., and van der Maaten, L.
Does object recognition work for everyone? In Proceed-
ings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) Workshops , June 2019.
Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In Interna-
tional Conference on Machine Learning (ICML) , 2017.
Friedl, M. A., Sulla-Menashe, D., Tan, B., Schneider, A.,
Ramankutty, N., Sibley, A., and Huang, X. MODIS
collection 5 global land cover: Algorithm reﬁnements
and characterization of new datasets. Remote Sensing of
Environment , 2010.Han, R., Liu, C. H., Li, S., Chen, L. Y ., Wang, G., Tang, J.,
and Ye, J. Slimml: Removing non-critical input data in
large-scale iterative machine learning. IEEE Transactions
on Knowledge and Data Engineering , 2021.
Hendrycks, D. and Gimpel, K. Gaussian error linear units
(GELUs). arXiv preprint arXiv:1606.08415 , 2016.
Jamal, M. A. and Qi, G.-J. Task agnostic meta-learning
for few-shot learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , June 2019.
Jean, N., Burke, M., Xie, M., Davis, W. M., Lobell, D. B.,
and Ermon, S. Combining satellite imagery and machine
learning to predict poverty. Science , 2016.
Kerner, H., Tseng, G., Becker-Reshef, I., Nakalembe, C.,
Barker, B., Munshell, B., Paliyam, M., and Hosseini, M.
Rapid response crop maps in data sparse regions. In ACM
SIGKDD Conference on Data Mining and Knowledge
Discovery Workshops , 2020.
Kimenyi, M., Adibe, J., Djir ´e, M., Jirgi, A. J., Kergna, A.,
Deressa, T. T., Pugliese, J. E., and Westbury, A. The
impact of conﬂict and political instability on agricultural
investments in Mali and Nigeria. 2014.
Kumar, S., Torres, C., Ulutan, O., Ayasse, A., Roberts, D.,
and Manjunath, B. Deep remote sensing methods for
methane detection in overhead hyperspectral imagery. In
2020 IEEE Winter Conference on Applications of Com-
puter Vision (WACV) , 2020.
Lang, N., Schindler, K., and Wegner, J. D. Country-wide
high-resolution vegetation height mapping with sentinel-
2.Remote Sensing of Environment , 2019.
Lark, T. J., Spawn, S. A., Bougie, M., and Gibbs,
H. K. Cropland expansion in the united states produces
marginal yields at high costs to wildlife. Nature Commu-
nications , 2020.
Mac Aodha, O., Cole, E., and Perona, P. Presence-Only Ge-
ographical Priors for Fine-Grained Image Classiﬁcation.
InInternational Conference on Computer Vision (ICCV) ,
2019.
Ohno-Machado, L., Fraser, H. S., and Ohrn, A. Improving
machine learning performance by removing redundant
cases in medical data sets. Proceedings. AMIA Sympo-
sium , 1998.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
Bai, J., and Chintala, S. Pytorch: An imperative style,
Task-Informed Meta-Learning for Agriculture
high-performance deep learning library. In Advances in
Neural Information Processing Systems (NeurIPS) , 2019.
Patterson, T. and Kelso, N. V . Natural earth. https:
//www.naturalearthdata.com/ .
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V .,
Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
Weiss, R., Dubourg, V ., Vanderplas, J., Passos, A., Cour-
napeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research , 2011.
Perez, E., Strub, F., De Vries, H., Dumoulin, V ., and
Courville, A. Film: Visual reasoning with a general con-
ditioning layer. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence , 2018.
Ravi, S. and Larochelle, H. Optimization as a model for few-
shot learning. In International Conference on Learning
Representations (ICLR) , 2017.
Rußwurm, M., Wang, S., Korner, M., and Lobell, D. Meta-
learning for few-shot land cover classiﬁcation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) Workshops , 2020.
Shankar, S., Halpern, Y ., Breck, E., Atwood, J., Wilson, J.,
and Sculley, D. No classiﬁcation without representation:
Assessing geodiversity issues in open data sets for the
developing world. In NIPS 2017 workshop: Machine
Learning for the Developing World , 2017.
Snell, J., Swersky, K., and Zemel, R. Prototypical networks
for few-shot learning. In Advances in Neural Information
Processing Systems (NeurIPS) , 2017.
Song, X.-P., Hansen, M. C., Potapov, P., Adusei, B., Picker-
ing, J., Adami, M., Lima, A., Zalles, V ., Stehman, S. V .,
Di Bella, C. M., Conde, M. C., Copati, E. J., Fernandes,
L. B., Hernandez-Serna, A., Jantz, S. M., Pickens, A. H.,
Turubanova, S., and Tyukavina, A. Massive soybean ex-
pansion in south america since 2000 and implications for
conservation. Nature Sustainability , 2021.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
and Salakhutdinov, R. Dropout: a simple way to prevent
neural networks from overﬁtting. Journal of Machine
Learning Research , 2014.
Thrun, S. and Pratt, L. Learning to Learn . Springer Science
& Business Media, 1998.
Triantaﬁllou, E., Zhu, T., Dumoulin, V ., Lamblin, P., Evci,
U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Man-
zagol, P.-A., and Larochelle, H. Meta-dataset: A dataset
of datasets for learning to learn from few examples. In
International Conference on Learning Representations
(ICLR) , 2020.Triantaﬁllou, E., Larochelle, H., Zemel, R., and Dumoulin,
V . Learning a universal template for few-shot dataset
generalization. In Proceedings of the 38th International
Conference on Machine Learning (ICML) , 2021.
Tseng, G., Kerner, H., Nakalembe, C., and Becker-Reshef,
I. Learning to predict crop type from heterogeneous
sparse labels using meta-learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops , 2021a.
Tseng, G., Zvonkov, I., Nakalembe, C., and Kerner, H.
CropHarvest: a global satellite dataset for crop type clas-
siﬁcation. In Neural Information Processing Systems
(NeurIPS) Datasets and Benchmarks Track , 2021b.
Turkoglu, M. O., D’Aronco, S., Perich, G., Liebisch, F.,
Streit, C., Schindler, K., and Wegner, J. D. Crop mapping
from image time series: Deep learning with multi-scale
label hierarchies. Remote Sensing of Environment , 2021.
van der Maaten, L. and Hinton, G. Visualizing high-
dimensional data using t-SNE. Journal of Machine Learn-
ing Research , 2008.
Vermote, E. MODIS/terra surface reﬂectance 8-day l3
global 500m SIN grid v006, 2015. URL https://
doi.org/10.5067/MODIS/MOD09A1.006 .
Vuorio, R., Sun, S.-H., Hu, H., and Lim, J. J. Multimodal
model-agnostic meta-learning via task-aware modulation.
InNeural Information Processing Systems (NeurIPS) ,
2019.
Wan, Z., Hook, S., and Hulley, G. MODIS/aqua land surface
temperature/emissivity 8-day l3 global 1km SIN grid
v006, 2015. URL https://doi.org/10.5067/
MODIS/MYD11A2.006 .
Wang, A. X., Tran, C., Desai, N., Lobell, D., and Ermon,
S. Deep transfer learning for crop yield prediction with
remote sensing data. In Proceedings of the 1st ACM SIG-
CAS Conference on Computing and Sustainable Societies
(COMPASS) , 2018.
Wang, S., Chen, W., Xie, S. M., Azzari, G., and Lobell,
D. Weakly supervised deep learning for segmentation of
remote sensing imagery. Remote Sensing , 2020.
Wu, Y . and He, K. Group normalization, 2018.
Yeh, C., Meng, C., Wang, S., Driscoll, A., Rozi, E., Liu,
P., Lee, J., Burke, M., Lobell, D. B., and Ermon, S. Sus-
tainbench: Benchmarks for monitoring the sustainable
development goals with machine learning. In Thirty-ﬁfth
Conference on Neural Information Processing Systems
(NeurIPS) Datasets and Benchmarks Track , 2021.
Task-Informed Meta-Learning for Agriculture
You, J., Li, X., Low, M., Lobell, D., and Ermon, S. Deep
gaussian process for crop yield prediction based on re-
mote sensing data. Proceedings of the AAAI Conference
on Artiﬁcial Intelligence , 2017.
Zhu, Q., Liao, C., Hu, H., Mei, X., and Li, H. Map-net: Mul-
tiple attending path neural network for building footprint
extraction from remote sensed imagery. IEEE Transac-
tions on Geoscience and Remote Sensing , 2021.A. Implementation Details
We implement TIML in PyTorch (Paszke et al., 2019), using
the learn2learn library (Arnold et al., 2020). All MAML
and TIML models are trained using the same optimizer hy-
perparameters. Speciﬁcally, we use an inner loop learning
rate of 10−4. We use an Adam optimizer on the outer loop
(for both the classiﬁer and the encoder), with a Cosine An-
nealing Learning rate (as per Antoniou et al. (2019)). For
both the classiﬁer and encoder, we use an initial learning
rate of 10−4and a minimum learning rate of 10−5.
When ﬁne-tuning, we use the same learning rate as the inner
loop learning rate ( 10−4) for all models with the excep-
tion of the yield-estimation standard-MAML CNN. The
standard-MAML CNN experienced an exploding loss using
this learning rate, so we reduced the learning rate to 10−5
when ﬁne-tuning it.
Both MAML and TIML are trained for 1000 epochs - we
selected the model checkpoint with the best performance on
the validation set (consisting of 10% of the training tasks,
up to a maximum of 50 tasks).
For the crop type clasiﬁcation dataset, all LSTM-based
classiﬁers were ﬁne-tuned on the evaluation tasks for 250
gradient steps with batches containing 10 positive and 10
negative examples (as in Tseng et al. (2021b)). We show
the variety of agro-ecologies represented in the crop type
classiﬁcation evaluation tasks in Figure 4.
For the yield estimation dataset, all models were ﬁne-tuned
on each county for 15 gradient steps, with batches of size
10. The reduced ﬁne-tuning steps relative to the crop clas-
siﬁcation dataset is due to the much lower amount of data
available for each county (compared to the crop classiﬁ-
cation evaluation tasks). Some counties did not have any
ﬁne-tuning data available – the results for these zero-shot
counties are shared in Appendix B.
A.1. Forgetfulness
We use the following thresholds to deﬁne task-
memorization:
•Crop Type Classiﬁcation : An AUC ROC of 0.95 or
above
•Yield Estimation An RMSE of 4 or less
In both cases, a training task was forgotten if it met the
threshold for forgetfulness continuously over the last 20
epochs.
For the crop type classiﬁcation, we note that the training
batches were balanced to contain 10 positive and 10 negative
examples, making AUC ROC appropriate.
Task-Informed Meta-Learning for Agriculture
(a) Togo
 (b) Kenya
 (c) Brazil
Figure 4. Example 1km×1km satellite images of the evaluation
regions, demonstrating the variety in ﬁeld sizes and agroecologies
being evaluated. (Images were obtained from Google Earth Pro
basemaps comprised primarily of high resolution Maxar images,
and are reproduced with permission from (Tseng et al., 2021b))
Model 2011 2012 2013 2014 2015
# counties 7 9 5 6 5
LSTM + TIML 8.99 12.93 17.19 9.97 11.22
CNN + TIML 10.44 7.02 9.81 7.25 11.89
Table 3. Zero-shot learning results: RMSE of the TIML model
when measured only on counties not present during training (or
ﬁne-tuning). We note that these results were obtained with no
training data about the county, in a zero-shot learning regime. The
number of counties being tested is additionally recorded.
A.2. Task augmentation for geospatial MAML
Deﬁning tasks according to their geospatial boundaries al-
lows for a form of weak task augmentation, by including
nearby datapoints which are not explicitly within the bound-
ary. For example, using a rectangular bounding box instead
of a polygon when deﬁning a political boundary includes
nearby points which may not be inside the polygon. Sim-
ilarly, for the yield estimation dataset we include nearby
counties in tasks for MAML and TIML.
B. Zero-shot learning
For the Yield estimation task, some counties did not appear
in the training data but were present in the evaluation data
(i.e. if the ﬁrst year of data for a county is 2011, then there
will be no training data for that county for the evaluation
year 2011).
For these counties, the model is therefore evaluated in a
zero-shot learning regime (the county is not present when
training the meta-model, or during ﬁne-tuning).
We record the results of the yield model in a zero-shot
learning regime below in Table 3. These results are included
in the overall results reported in Table 2.
We highlight that very few counties are in this zero-shot
regime, but include these results for completeness.
Learning to Interpret Satellite Images in Global Scale Using Wikipedia
Burak Uzkent1∗, Evan Sheehan1, Chenlin Meng1, Zhongyi Tang2, David Lobell2, Marshall Burke2,
Stefano Ermon1
1Department of Computer Science, Stanford University
2Department of Earth Systems Science, Stanford University
buzkent@cs.stanford.edu, {esheehan, chenlin, zztang, dlobell, mburke }@stanford.edu,
ermon@cs.stanford.edu
Abstract
Despite recent progress in computer vision, ﬁne-
grained interpretation of satellite images remains
challenging because of a lack of labeled training
data. To overcome this limitation, we construct a
novel dataset called WikiSatNet by pairing geo-
referenced Wikipedia articles with satellite imagery
of their corresponding locations. We then propose
two strategies to learn representations of satellite
images by predicting properties of the correspond-
ing articles from the images. Leveraging this new
multi-modal dataset, we can drastically reduce the
quantity of human-annotated labels and time re-
quired for downstream tasks. On the recently re-
leased fMoW dataset, our pre-training strategies
can boost the performance of a model pre-trained
on ImageNet by up to 4.5%in F1 score.
1 Introduction
Deep learning has been the driving force behind many recent
improvements in computer vision tasks, including image clas-
siﬁcation, image segmentation, object detection and tracking,
etc.[21; 12; 4; 29; 27; 28 ]. These deep models, however, re-
quire training on high quality, large-scale datasets, and build-
ing these datasets is typically very costly. Satellite images are
particularly difﬁcult and expensive to label because of hu-
mans’ unfamiliarity with aerial perspectives [1].
One effective way to reduce the amount of training data
needed is to perform pre-training on an existing, previ-
ously annotated dataset, such as ImageNet [3], and trans-
fer the learned weights to the domain of interest [17; 2;
30]. However, the success of this approach diminishes if
the underlying distributions and/or compositions of the pre-
training and target datasets are not sufﬁciently similar. Such a
problem is exceptionally pronounced in the satellite imagery
space, as the entire frame of reference and perspective of an
aerial image is altered compared to a natural image. This has
the unfortunate effect of rendering natural image datasets,
such as ImageNet, less useful as pre-training mechanisms for
downstream computer vision tasks in the satellite domain [16;
7].
∗Contact AuthorBecause direct annotation is expensive, researchers have
considered many creative ways to provide supervision with-
out explicit labels. These include unsupervised [9], label-free
[19; 25 ], and weakly supervised learning methods [18]. A
particularly effective strategy is to leverage co-occurrence
statistics in a dataset, e.g., predict the next frame in a video, a
missing word in a sentence [14], or predict relationships be-
tween entities such as images and text co-occurring together.
For example, leveraging images and their hashtags on Insta-
gram, [13]build a large scale image recognition dataset con-
sisting of more than 3 billion images across 17,000 weak la-
bels obtained from textual hashtags and their WordNet [15]
synsets. After pre-training on this extremely large dataset,
they report almost 5%improvement over the same model
trained from scratch on ImageNet.
Because satellite images are geolocated, i.e., they corre-
spond to speciﬁc locations (and times), they can be paired
with other geolocated datasets (e.g., OpenStreetMap [7]), ex-
ploiting spatial co-occurrence statistics as a source of super-
vision [23; 22 ]. Following this strategy, we construct a novel
multi-modal dataset by pairing geo-referenced Wikipedia ar-
ticles with their corresponding satellite images. By treating an
article as an information-rich label, we obtain highly detailed
physical and qualitative context for each image. For exam-
ple, the ﬁrst sentence of the John. F. Kennedy International
Airport article contains excerpts such as “ JFK is the primary
international airport serving New York City ”. Wikipedia ar-
ticles additionally contain demographic, environmental, and
social information in structured form. To the best of our
knowledge, this is the ﬁrst time that Wikipedia has been used
in conjunction with satellite images, and with 888,696 article-
image entries, our approach yields the largest satellite image
dataset to date.
In this paper, we demonstrate the effectiveness of pairing
Wikipedia articles to satellite images for pre-training CNNs
for satellite image recognition. We propose two pre-training
methods to learn deep representations. First, similar to [13],
we weakly label satellite images with curated summarization
tags extracted from the article via an automated process. We
then train a deep convolutional network to predict these weak
labels directly from the images, learning useful representa-
tions in the process. In the second approach, we propose a
novel joint architecture where we ﬁrst obtain a textual em-
bedding of each article using document summarization tech-arXiv:1905.02506v3  [cs.CV]  11 Aug 2019
Three Gorges Dam
Hagia SophiaPort of Boston Chrysler Building Huvudstabron
Taj Mahal Maracanã Stadium  JFK Airport  
Niagara Falls
Colosseum
Figure 1: Left: Scatter plot of the distribution of geo-tagged Wikipedia articles together with some images ( right ) matched to
the articles shown as green dots on the left plot. The title of the Wikipedia articles are written under each image. Zooming-in is
recommended for visualization.
niques from NLP [10]and then train a deep convolutional
network to produce an embedding for each image that is “sim-
ilar” to the textual one. The ﬁrst approach is a crude way of
getting a single weak label for each article whereas the second
learns representations without weak labels. The pre-trained
networks are then evaluated on a downstream hand-labeled
dataset, as in [6], where we obtain 4.5%higher accuracy com-
pared to networks pre-trained on ImageNet, the standard ap-
proach for computer vision tasks.
2 Pairing Rich Crowdsourced Annotations
from Wikipedia to Satellite Images
Wikipedia is a large-scale, crowdsourced database spanning
302 languages with over 47 million articles [32]. Of these 47
million articles, about 11 %are contained in the English ver-
sion. Out of these approximately 5 million articles, we found
that roughly 1 million, or nearly 20 %, are geolocated, mean-
ing there is a latitude and longitude ci={clat
i,clon
i}associ-
ated with the article’s text yi. Our key idea is to use the ar-
ticle’s coordinates to acquire a satellite image of its location
from space (see Fig. 1).
There is often a strong correlation between the article’s
text,yi, and the visual content of the corresponding image,
xi. Indeed, we can think of the article as an extremely de-
tailed “caption” for the satellite image, providing an often
comprehensive textual representation of the satellite image,
or an information-rich label . This label often contains struc-
tured data in the form of tables, called infoboxes, as well as
raw text, allowing for the extraction of information about the
physical state and features of the entity (e.g., elevation, age,
climate, population).
2.1 Acquiring Matching Satellite Imagery
For a given article’s coordinate ci, there are many sensors
that can provide imagery, with different tradeoffs in terms of
spatial and temporal resolution, wavelengths, and costs. In
this paper we acquire high resolution images from Digital-
Globe satellites. The images have a ground sampling distance
(GSD) of 0.3-0.5m. These are among the highest resolution
images available commercially, and were also used in the re-
cently released functional map of the world (fMoW) dataset[1]. Note that one could also use the same strategy to build
a similar multi-modal dataset using lower-resolution (10 me-
ter), publicly available Landsat and Sentinel-2 images. For a
given coordinate ci, there are usually multiple images avail-
able, captured at different times. We acquired the latest image
available. Another important design choice is the size of the
acquired images. In this study, we use 1000 ×1000 pixels im-
ages covering approximately an area of 900 m2. In aerial im-
ages, objects occupy drastically different numbers of pixels,
as shown in Fig. 1. Based on preliminary manual examina-
tion, we found that 1000 ×1000 pixels images can typically
cover most of the relevant objects. Finally, we prioritized col-
lecting RGB images and only acquired grayscale images if an
RGB image was not available. We did not perform any ﬁlter-
ing to remove cloudy images, as our goal is to learn robust
representations on a noisy dataset.
Our resulting WikiSatNet multi-modal dataset is a set of
tuplesD={(c1,x1,y1),(c2,x2,y2),···,(cN,xN,yN)}
where each tuple (ci,xi,yi)represents a location ( ci), corre-
sponding DigitalGlobe image ( xi) and Wikipedia article text
(yi).WikiSatNet containsN= 888,696article-image pairs.
To the best of our knowledge, this is the largest dataset to
date consisting of satellite images and about 2times larger
than the recently released large scale fMoW dataset. Note that
our procedure is highly scalable and fully automated. It could
be used to generate even larger datasets by considering other
Wikipedia languages and other sensors in addition to Digi-
talGlobe. In the next section, we propose two novel methods
to pre-train a convolutional neural network (CNN) to extract
information about images xiusing information from yi.
3 Learning Visual Representations using
Wikipedia Textual Information
Exemplifying the diverse application possibilities highlighted
in the previous sections, we construct a general Wikipedia
article-satellite image framework for pre-training CNNs. We
then explore whether we can learn to interpret satellite im-
ages using knowledge extracted from Wikipedia articles via
two approaches: weakly-supervised [18]labelling and a novel
textual embedding method that attempts to match textual and
visual embeddings.
North Queensland
CowboysHighland
AviationIserbrook
(event) (school) (incident)
Figure 2: Some of the extracted weak labels representing
ﬂipped label noise. Corresponding Wikipedia article titles are
written above the images. Though the words stadium ,airport ,
andwater are mentioned 19, 6, and 23 times in the articles,
our weak label extraction pipeline generates wrong labels.
Using image to text matching helps alleviate this ﬂipped label
noise.
(city) (town) (town) (county)
Figure 3: Visually similar examples where the extracted weak
labels cause adversarial label noise. Here the CNN is penal-
ized for errors even when the predicted label is visually sim-
ilar to assigned weak label. In contrast, our document sum-
marization model projects the embeddings of the articles of
these images to a similar space to avoid penalizing the CNN
when predicting a similar label.
3.1 Weakly Supervised Learning
We ﬁrst propose learning visual features using a data-
programming pipeline [18]to label our dataset. We begin by
extracting a weak label ˆw(yi)for each article yiin our dataset.
In our context, a weak label is a noisy, machine-generated
classiﬁcation of an article from a set of pre-deﬁned labels.
Because of space constraints, we only provide a high-level
description of the approach, and will add more details by pur-
chasing extra pages in the ﬁnal version. As a ﬁrst step, we
manually compile a list of 97 potential categories that an ar-
ticle could fall under (e.g., city,lake,event , etc.) and use
regular expressions to search for the terms throughout spe-
ciﬁc areas of the article’s text where article meta-data is con-
tained. We then rank the categories which are matched to the
article in a manually-constructed hierarchical fashion from
speciﬁc to general (e.g., building−→town−→county ,
etc.) and choose the one which comes ﬁrst to label the article.
Because many of these category labels are very detailed, we
then merge certain similar categories together to create more
general labels. We also discard articles that are assigned la-
bels which cannot be determined from a satellite image (e.g.,
person ,event , etc.). Weak labels represented by less than
100 samples are also removed, reducing the ﬁnal set of labels
to 55.
Given the ﬁnal set of weak labels and corresponding im-
ages, we train a classiﬁer to predict ˆw(yi)fromxi. The clas-
siﬁer is composed of a convolutional neural network fv:X ↦→ RMthat embeds images into an Mdimensional fea-
ture space, followed by fully connected and softmax layers
as shown in Fig. 4a. In this study, we parameterize fvus-
ing the DenseNet121 [5]architecture which was previously
shown to perform well across a range of tasks. The classiﬁer
is trained using the cross entropy loss function. The features
learned by the convolutional embedding fvon this large-scale
pre-training task can then be transferred to downstream tasks,
e.g., object detection or land cover classiﬁcation.
Extracting weak labels is a noisy process that leads to a sig-
niﬁcant number of ﬂipped labels as shown in Fig. 2. Addition-
ally, the process leads to adversarial label noise because of
visually similar labels such as city, country, populated place,
building, town etc., as shown in Fig. 3. One can apply a sim-
ple merging step to place such visually similar labels into a
general category, e.g., populated place . However, it leads to
a class imbalance problem where almost 40% of the dataset
is dominated by populated places. Exploring the trade-off be-
tween adversarial label noise and class imbalance problems is
a very time-consuming process due to the nature of working
with a large-scale dataset. For this reason, in the next sec-
tion, we propose a novel, and practical method to learn deep
representations using multi-modal data without manual pre-
processing.
3.2 Image to Text Matching Learning
In this section, we propose a novel method to learn deep con-
volutional features without using hand-crafted labeling func-
tions. This not only substantially reduces human effort, but
also tackles the adversarial label noise by softening the loss
function for the images that can fall into multiple visually
similar categories. Our method relies on the idea of image to
text matching [11; 31 ]. In this direction, we propose a novel
network shown in Fig. 4b with two branches: a visual and
atextual one. We design a loss function that encourages the
CNN ( visual branch) to produce image embeddings that are
close to a suitable vector representation of the corresponding
article’s text ( textual branch).
The proposed architecture uses satellite images, X, and
Wikipedia articles, Y, as input. In the textual branch, we learn
a functionft:Y↦→RK, to project an article, yi, to a textual
embedding space zt
i∈RKusing a document summarization
model from natural language processing (NLP):
zt
i=ft(yi). (1)
In the visual branch, we use a function fv:X ↦→ RMpa-
rameterized using a convolutional neural network to extract
features from an image as
zv
i=fv(xi) (2)
whereirepresents the index of the image paired to article yi.
We parameterize fvusing the DenseNet121 architecture [5]
as in the weak supervision method. Next, we use a function
fm:Zv↦→RKto mapzv
ito the same dimension as the
textual feature vector zt
i. The function fmis parameterized
using a fully connected layer with ReLU activations. The ﬁnal
feature vectors, zv
iandzt
i∈RK, are then compared with a
loss function that enforces similarity.
Fine­tune W eigths
on  Human Labeled
Dataset  CNNGlobal
Pooling  
1024Loss  
Function
W eak
 Label
Extraction
Bridge
FCL
55CNNGlobal
Pooling  
1024 300Loss
Function
FCLFCL
Doc2V ecDBridge Mandela
­ ­ ­
300
FCL(a) (b)
Figure 4: The workﬂow of the proposed weakly supervised learning method (a): (1) Extract labels from articles using our
labeling pipeline. (2) Match articles with images of their coordinates. (3) Pre-train on a large-scale dataset using 55 weak
labels. (4) Transfer learned weights to a down-stream task. In (b) we show the workﬂow of the image to text matching learning.
Our method enforces the CNN to learn features similar to raw textual features learned by Doc2Vec .
Pre-training the Doc2Vec Model
Our image to text matching method uses textual descriptors
Ztto learn deep visual representations. In our study, we use
theDoc2Vec network [10]which can summarize variable
length articles in a uniﬁed framework. Doc2Vec is a docu-
ment summarization method that can take a variable length
piece of text, yi, and mapyi∈ Y to a paragraph vector
zt
i=ft(yi)∈RKin a ﬁxed-length vector space, where
Kis speciﬁed by the user. Documents that possess simi-
lar meanings are mapped to nearby points in the embedding
space, allowing a comparison between any two documents.
In contrast to ﬁxed length vector representations using Bag-
of-Words, Doc2Vec can capture the orderings and seman-
tics of the words, which is highly beneﬁcial for our unsu-
pervised learning task. For example, learning a textual em-
bedding space where we can closely map article categories
such as country ,city,town etc. is desired considering that
their corresponding visual data contain similar structures (see
Fig. 5). Another advantage of the Doc2Vec model is that it
is an unsupervised learning model. This allows us to learn
Wikipedia-speciﬁc descriptors by training it on the full ge-
olocated Wikipedia article corpus.
−0.5 0.0 0.5 1.0−0.75−0.50−0.250.000.250.500.751.00
citycity
lakelaketowntown
roadroad
riverriverisland
islandperson
City - Middletown, Connecticut City - Milton, Georgia Lake - Timothy Lake Lake - Tinquilco Lake Town - Mingona Township, Barber County,  Kansas Town - Moon Township, Allegheny County,  Pennsylvania Road - Morehampton Road, Dublin Road - Motorway M10 Pakistan River - Motru River River - Mousam River Island - Aupaluktok Island Island - Avatanak Island 
Figure 5: Visualization of PCA components of the randomly
chosen articles learned by Doc2Vec . Notice that visually simi-
lar objects such as city,town are closely mapped while differ-
ent objects are projected far away. Corresponding Wikipedia
article titles are shown on the right.Cosine Similarity Loss Function
After learning feature vectors, zv
iandzt
i∈RK, from the
two branch network, we apply a loss function to measure the
similarity of the two vectors. We propose using the cosine
similarity metric, which measures the angle, θi, between two
vectors as
D(xi,yi) =cos(θi) =fv(xi)Tft(yi)
∥fv(xi)∥2∥ft(yi)∥2. (3)
Wikipedia has varying lengths of articles, which makes the
cosine similarity function ideal since it measures the similar-
ity between the direction rather than the magnitude of two
vectors.
One can apply some other loss functions for our pre-
training task. For example, [31]proposed triplet loss function
where the anchor is a phrase paired to its corresponding posi-
tive visual data. The negative image is then sampled from the
neighborhood of the positive sample. [6]adapted triplet loss
function for unsupervised learning on satellite images. They
assign a positive image for each anchor image from its spatial
neighborhood following the assumption that nearby images
contain similar visual structures. The negative sample is then
sampled from the areas outside the anchor’s neighborhood
circle. In our case, we lack explicit knowledge that can help
us sample negative image given an article, yi, as anchor and
its corresponding image, xi, as positive. In this direction, one
can compute the similarity in the visual, zv1, or textual, zt,
embedding space between a positive sample and other sam-
ples in a certain spatial neighborhood to get a negative sam-
ple.
Another interesting aspect of our architecture is the di-
mensionality of the textual embeddding space. We believe
that 300 dimensional feature vector can capture all the ﬁne-
grained visual structures in an aerial image. However, dur-
ing our experiments we observed that visually similar fea-
tures can lead to more uniform textual descriptors slowing
down the learning process. Thus, using a smaller dimensional
embedding space can lead to more discriminative visual fea-
tures that can potentially speed up the learning process. On
the other hand, this can also prevent the CNN from learn-
ing ﬁne-grained in- formation. We leave the task of exploring
this trade-off as a future work of our study. Another future
dimension of our image to text matching work is generating
a Wikipedia article, yi, using an NLP decoder f′
tgiven visual
representations xias
yi=f′
t(fv(xi)). (4)
Training on WikiSatNet
In our pre-training experiments, we use similar hyper-
parameters in both weak supervision and image to text match-
ing to train the DenseNet121 for optimizing the weights for
fv. We initialize weights randomly, however, we observed
faster convergence when initializing with pre-trained weights.
After experimentation, we set the learning rate and batch size
to 0.0001 and 64, respectively, and the Adam optimizer is
used to train the model [8]. Finally, we resize the 1000 ×1000
pixels images to 224 ×224 pixels images to compare with
publicly available datasets.
Figure 6: Some of the cloudy images in WikiSatNet. The
cloudy images amount to roughly 5-7%of the dataset.
In the initial steps of image to text training, we observe
an angle of approximately 90o(D(xi,yi)≈0) betweenzt
i
andzv
i. This is consistent with the fact that random vectors in
high dimensional spaces are likely to be orthogonal to each
other. After several epochs, the angle decreases to about 45o
(D(xi,yi)≈0.5) and stops decreasing further. We believe
that this is partially due to articles that do not contain any
visual cue, e.g culture andperson , and also cloudy images
(see Fig. 6), which amount to roughly 5%of the dataset. We
did not observe over-ﬁtting in our experiments. While we are
not able to achieve zero loss, we qualitatively ﬁnd that our
approaches learn meaningful representations. To verify this,
after pre-training the CNN on WikiSatNet using the image to
text matching, we visualize the cosine similarities between zt
i
andzv
ias shown in Fig. 7. In the same ﬁgure, we keep zt
ﬁxed and use embeddings from images at different locations.
The CNN learns to project embedding zv
icloser to its corre-
sponding article embedding zt
i. We will publicly release the
code for our image to text matching and weak supervision
methods upon publication. Additionally, we expect to release
a substantial fraction of the high resolution images in Wik-
iSatNet (negotiations on the license with the image provider
are ongoing). This will encourage further research into jointly
utilizing Wikipedia and satellite images.
Figure 7: Visualization of cosine similarities learned by the
CNN. The cosine similarities between the CNN embeddings
and the Doc2Vec embedding are computed and overlaid on
the images. The CNN learns to embed AT &T Stadium’s im-
age closer to the its corresponding article.
4 Transfer Learning Experiments
After pre-training CNNs on WikiSatNet using the proposed
methods, we test them on three target tasks: (1) single image
classiﬁcation on the fMoW dataset, (2) temporal view clas-
siﬁcation using multiple images over an area on the fMoW
dataset, and (3) land cover classiﬁcation. In these tasks, we
compare our pre-training strategies to the following base-
lines: (1) pre-training on ImageNet [21], (2) pre-training on
CIFAR10, and (3) training from scratch. Our goal is to eval-
uate whether we learn satellite-speciﬁc representations that
outperform the ones obtained using out-of-domain bench-
marks with human labels.
Fine-tuning
There are two classical approaches in ﬁne-tuning a deep net-
work on the target task: (1) training all layers, and (2) freez-
ing all the layers other than the ﬁnal classiﬁcation layer. In
our experiments, we present results from both strategies. The
learning rates for the weakly supervised and image to text
matching model are set to 1e-4 and 1e-5 after experimenta-
tion. On the other hand, the learning rate for the ImageNet
model is set to 1e-4, while it is set to 1e-3 for both the CI-
FAR10 pre-trained and trained from scratch models. These
were the best performing hyper-parameters in our experi-
ments. Finally, resized 224 ×224 pixel RGB images are used
as input to the model as in the pre-training task. We follow
the same approach for the models pre-trained on CIFAR10
and ImageNet.
4.1 Experimenting on the fMoW Dataset
To quantify the quality of the representations learned in the
pre-training step, we ﬁrst use a recently released large-scale
satellite image recognition dataset named fMoW [1]. The
fMoW dataset consists of both multispectral and RGB im-
ages and contains 83,412 unique training bounding boxes
from large satellite images representing 62 different objects.
The validation and test sets contain 14,241 and 16,948 bound-
ing boxes and are left unchanged in our experiments. It also
comes with temporal views from the same scenes, making
classiﬁcation of some classes such as construction site and
ﬂooded road easier. [1]proposes a multi-modal architecture
that uses a DenseNet161 pre-trained on ImageNet and an
1050100 200 350
Number of Samples (*103)10203040506070Top 1 AccuracyFrom Scratch
CIFAR10
ImageNet
WikiSatNet 
Weak Supervision
WikiSatNet 
Image2Text
10 50 100 200 350
Number of Samples (*103)30405060708090Top 5 AccuracyFrom Scratch
CIFAR10
ImageNet
WikiSatNet 
Weak Supervision
WikiSatNet 
Image2TextFigure 8: The top-1 and 5 classiﬁcation accuracies of the pro-
posed pre-training and baseline strategies on fMoW’s test set
when ﬁne-tuning all layers on fMoW’s training set. Monte-
Carlo experiments were conducted when sampling a subset
of the full training set.
LSTM to learn from images and their corresponding meta-
data. Their DenseNet161 model has a number of parameters
similar to the DenseNet121 model we use in our experiments.
Since our pre-training framework learns from visual data, it
can be easily applied to any CNN model to boost performance
as well as reduce the number of labeled samples needed for a
target task.
Reasoning on Single Images
In the ﬁrst task, we perform experiments on the fMoW dataset
for the task of classifying individual images using features
extracted by the visual branchfv(·)as
L(xi) = argmax
j(p(fv(xi))) (5)
whereprepresent the fully connected and softmax layers
whereasjdenotes the index of the assigned label. We experi-
ment with 2000, 10000, 50000, 100000, 200000, and 350000
training images. As shown in Fig. 8, our pre-training strate-
gies outperform the other pre-training strategies by large mar-
gins in top-1 and top-5 classiﬁcation accuracy when using
small amounts of labeled data. For example, when using 2000
labeled images, both our training strategies outperform Ima-
geNet and CIFAR10 by 10% and30%, respectively. As ex-
pected, this number goes down to about 5%and20% when
increasing the number of labeled images to 50000. Interest-
ingly, at this point, the model trained from scratch starts to
outperform the model pre-trained on CIFAR10. When using
the full training set, our proposed pre-training strategies out-
perform ImageNet by about 2%and outperform the model
trained from scratch by about 10%. These results demon-
strate that our proposed approach produces features that are
highly beneﬁcial in down-stream tasks involving satellite im-
ages, even when large numbers of human labeled samples are
available. When ﬁne-tuning only the ﬁnal layer, the proposed
pre-training methods outperform ImageNet features by about
13% on the test set as shown in Table 1.Model CIFAR10 ImageNetWikiSatNet
Weak LabelsWikiSatNet
Image2Text
Top-1 Acc.
(Fixedfv)13.98 (%) 37.73 (%) 50.73 (%) 51.02 (%)
Top-1 Acc.
(Fine-tuned fv)55.79 (%) 68.61 (%) 70.62 (%) 70.72 (%)
Table 1: Top-1 accuracies on the fMoW test set for pre-trained
models. All the models are ﬁne-tuned on the full fMoW train-
ing set. Fixed fvrepresents the ﬁne-tuning method where the
pre-trained weights are ﬁxed whereas the second method ﬁne-
tunes all the layers.
Reasoning on Temporal Views
In this section, we evaluate our representations on the task of
temporal view classiﬁcation across 62 classes from the fMoW
dataset. This way, we can understand if our pre-training meth-
ods also boost performance on tasks that use temporal data as
input. [1]trains the network on single labeled images and at
test time averages the softmax predictions of the network on
different images from the same area to assign the label with
the maximum average score. We follow their training and test
methods and at test time average predictions from Timages
over the same area, again using features extracted from fv(·)
as input. This can be formulated as
L(X) = argmax
j(mean (T∑
t=1p(fv(xt)))) (6)
wherejdenotes the index of the assigned label and fvrep-
resents the pre-trained network ﬁne-tuned on the fMoW. Dif-
ferent from the previous section, we now report results in F1-
scores to compare our models to the ones proposed by [1].
Model CIFAR10 ImageNetWikiSatNet
Weak LabelsWikiSatNet
Image2Text
F1 Score
(Single View )55.34 (%) 64.71 (%) 66.17 (%) 67.12 (%)
F1 Score
(Temporal Views )60.45 (%) 68.73 (%) 71.31 (%) 73.02 (%)
Table 2: F1 scores of different pre-training methods on
fMoW’s test set when ﬁne-tuning all the layers on fMoW’s
training set.
We ﬁrst compare our pre-training methods to ImageNet
and CIFAR10 pre-training in Table 2. The proposed pre-
training methods outperform the ImageNet pre-trained model
by up to 4.5%in F1 Score when performing reasoning on
temporal views. Among the proposed methods, the image
to text matching approach outperforms the weak supervision
with handcrafted labels method by about 1.7%in F1 Score.
These results prove that the importance of pre-training does
not diminish when switching from single to temporal views.
On the other hand, [1]proposes ﬁve different models for the
fMoW classiﬁcation task. Three of them use meta-data and
images jointly, whereas the remaining two only employ an
ImageNet pre-trained DenseNet on images. Their visual data-
only models are named CNN-I-1 andCNN-I , where the for-
mer is a single view model and the latter performs tempo-
Upsampler  
(224x224 )  
1x1DenseNet Block
( 1 12x1 12 )DenseNet
Block   
( 56x56 )DenseNet  
Block
( 28x28 )Upsampler  
( 56x56 )  Upsampler  
(112x112 )  Pre­trained
1x1
Figure 9: The proposed segmentation architecture that uses the pre-trained weights in the encoder stage.
ral reasoning. We can improve these models with our pre-
training strategy by about 4.5%in F1 score while perform-
ing similarly to their top performing model, LSTM-IM , which
uses meta-data and visual data jointly to perform temporal
reasoning. Although this is outside the scope of this paper,
our pre-trained models can replace the DenseNet model, pre-
trained on ImageNet, used in LSTM-IM to improve its results
as well.
Our experiments demonstrate that pre-training with weak
or no supervision is very useful for the target task as reported
by[13]both in terms of (1) boosting accuracy and (2) reduc-
ing the required human labeled dataset size on the target task.
Unlike the pre-training framework proposed by [13], we do
not necessarily need to have billions of images to overcome
noise to learn useful representations.
4.2 Experiments on Land Cover Classiﬁcation
Additionally, we perform classiﬁcation across 66 land cover
classes using remote sensing images with 0.6mGSD ob-
tained by the USDA’s National Agriculture Imagery Program
(NAIP). We focus on the images from the California’s Central
Valley near the city of Fresno for the year 2016. The corre-
sponding land cover map, named the Cropland Data Layer
(CDL), is collected by the USDA for the continental United
States [26]. The CDL is provided at 30mGSD, and we up-
sample them to match 0.6mGSD to use as ground truth. The
ﬁnal dataset consists of 100000 training and 50000 valida-
tion and test images. We only ﬁne-tune the classiﬁcation layer
while keeping fvﬁxed.
Model CIFAR10 ImageNetWikiSatNet
Weak LabelsWikiSatNet
Image2Text
Top 1 Acc. 42.01 (%) 40.11 (%) 46.16 (%) 47.65 (%)
Top 5 Acc. 74.73 (%) 80.15 (%) 88.66 (%) 88.77 (%)
Table 3: Performance of different pre-training methods on the
land cover classiﬁcation task.
As shown in Table 3, our pre-training strategies lead to
substantially higher performance than the ImageNet and CI-
FAR10 features. This demonstrates the robustness and wide
range of applications our pre-training strategies possess.
4.3 Experiments on Semantic Segmentation
Previously, we explored image recognition in both pre-
training and target tasks. In this section, we change the target
task type to semantic segmentation to understand if image
recognition pre-training can still boost the performance onsemantic segmentation target task. Image recognition focuses
on global features to associate an image xiwith a label wi.
On the other hand, in semantic segmentation, local features
play more important role in associating a pixel xi(m,n)with
a labelwiwheremandnrepresent the column and row in-
dex.
We ﬁrst build a semantic segmentation network using the
DenseNet121 model pre-trained on WikiSatNet. A typical
segmentation model consists of encoder-decoder steps to gen-
erate segmentation maps with the same size to input images.
In this case, we use the pre-trained weights fvas encoder
and design a decoder architecture on top of fvas shown in
Fig. 9. The proposed architecture is similar to U-Net archi-
tecture [20], however, we use the DenseNet121 pre-trained
on WikiSatNet in the decoder stage. This is important as it
requires a complex network to learn from large-scale datasets
such as WikiSatNet. On the other hand, the U-Net employs a
shallow encoder, preventing us to pre-train it on the WikiSat-
Net. We perform experiments on the SpaceNet [24]semantic
segmentation task on the satellite images. The SpaceNet con-
tains training and test set from six different cities, and build-
ing and road masks for corresponding high resolution ( 0.3-
0.5m GSD) DigitalGlobe images. In this study, we focus on
theRioset and building masks. There are about 5000 training
and 800 test images coming from the city of Rio de Janeiro.
We experiment with varying number of training samples to
quantify the learned representations in the case of using dif-
ferent amount of labeled samples in the target task. However,
we keep the test set unchanged in our experiments. Table 4
shows the Intersection-over-Union (IoU) scores of the pro-
posed segmentation architecture (see Fig. 9) when pre-trained
on ImageNet and WikiSatNet.
Figure 10: An example of a building mask for a satellite im-
age in SpaceNet Riodataset.
As shown in Table 4, the pre-training provides signiﬁcant
boost when ﬁne-tuning on small amount of training samples
Model From Scratch ImageNetWikiSatNet
Image2Text
200 Samples 42.11 (%) 50.75 (%) 51.70 (%)
500 Samples 48.98 (%) 54.63 (%) 55.41 (%)
5000 Samples 57.21 (%) 59.63 (%) 59.74 (%)
Table 4: The IoU scores of different pre-training methods on
building segmentation task.
(200, and 500 samples). However, pre-training on the Wik-
iSatNet only achieves slightly higher IoU than ImageNet.
These results are consistent with the previous studies where
pre-training and target datasets contain different level tasks
[13]. For example, [13]explored the idea of pre-training on
image recognition task and transferring the learned weights
for the task of object detection. They report that such set up
does not lead to signiﬁcant performance increase as in the
case where both pre-training and target tasks are the same-
level tasks (image recognition).
5 Conclusion
In this study, we proposed a novel combination of satellite
images and crowdsourced annotations from geo-referenced
Wikipedia articles. To the best of our knowledge, this is the
ﬁrst time that Wikipedia has been used this way. Our ap-
proach yields a large scale, multi-modal dataset combining
rich visual and textual information for millions of locations
all over the world — including additional languages beyond
English will likely improve coverage even more. Leverag-
ing paired multi-modal data, we proposed two different pre-
training methods: (1) learning with weak labels, and (2) learn-
ing without weak labels using image to text matching. Both
pre-training strategies lead to improved results on the recently
released fMoW dataset consisting of large numbers of labeled
samples. Our image to text matching model outperformed one
pre-trained on ImageNet by 4.5%when using around 350000
labeled samples; this increase in performance is substantially
higher when there are fewer labeled samples available.
References
[1]Gordon Christie, Neil Fendley, James Wilson, and Ryan
Mukherjee. Functional map of the world. In Proc. IEEE
Conference on Computer Vision and Pattern Recogni-
tion, Salt Lake City, Utah , 2018.
[2]Wenyuan Dai, Ou Jin, Gui-Rong Xue, Qiang Yang, and
Yong Yu. Eigentransfer: a uniﬁed framework for trans-
fer learning. In Proceedings of the 26th Annual Inter-
national Conference on Machine Learning , pages 193–
200. ACM, 2009.
[3]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical im-
age database. In Computer Vision and Pattern Recog-
nition, 2009. CVPR 2009. IEEE Conference on , pages
248–255. Ieee, 2009.
[4]Junwei Han, Dingwen Zhang, Gong Cheng, Nian Liu,
and Dong Xu. Advanced deep-learning techniques forsalient and category-speciﬁc object detection: a survey.
IEEE Signal Processing Magazine , 35(1):84–100, 2018.
[5]Gao Huang, Zhuang Liu, Laurens Van Der Maaten,
and Kilian Q Weinberger. Densely connected convolu-
tional networks. In 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 2261–
2269. IEEE, 2017.
[6]Neal Jean, Sherrie Wang, George Azzari, David Lobell,
and Stefano Ermon. Tile2vec: Unsupervised represen-
tation learning for remote sensing data. arXiv preprint
arXiv:1805.02855 , 2018.
[7]Pascal Kaiser, Jan Dirk Wegner, Aur ´elien Lucchi, Mar-
tin Jaggi, Thomas Hofmann, and Konrad Schindler.
Learning aerial image segmentation from online maps.
IEEE Transactions on Geoscience and Remote Sensing ,
55(11):6054–6068, 2017.
[8]Diederik P Kingma and Jimmy Ba. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[9]Diederik P Kingma, Shakir Mohamed, Danilo Jimenez
Rezende, and Max Welling. Semi-supervised learning
with deep generative models. In Advances in Neural In-
formation Processing Systems , pages 3581–3589, 2014.
[10]Quoc Le and Thomas Mikolov. Distributed represen-
tations of sentences and documents. arXiv preprint
arXiv:1405.4053 , 2014.
[11]Jimmy Lei Ba, Kevin Swersky, Sanja Fidler, et al. Pre-
dicting deep zero-shot convolutional neural networks
using textual descriptions. In Proceedings of the IEEE
International Conference on Computer Vision , pages
4247–4255, 2015.
[12]Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and
C Lawrence Zitnick. Microsoft coco: Common objects
in context. In European conference on computer vision ,
pages 740–755. Springer, 2014.
[13]Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,
Kaiming He, Manohar Paluri, Yixuan Li, Ashwin
Bharambe, and Laurens van der Maaten. Exploring the
limits of weakly supervised pretraining. arXiv preprint
arXiv:1805.00932 , 2018.
[14]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. Efﬁcient estimation of word representations in
vector space. CoRR , abs/1301.3781, 2013.
[15]George A Miller. Wordnet: a lexical database for en-
glish. Communications of the ACM , 38(11):39–41,
1995.
[16]Sinno Jialin Pan, Qiang Yang, et al. A survey on transfer
learning. IEEE Transactions on knowledge and data
engineering , 22(10):1345–1359, 2010.
[17]Rajat Raina, Alexis Battle, Honglak Lee, Benjamin
Packer, and Andrew Y Ng. Self-taught learning: trans-
fer learning from unlabeled data. In Proceedings of
the 24th international conference on Machine learning ,
pages 759–766. ACM, 2007.
[18]Alexander Ratner, Stephen H. Bach, Henry Ehrenberg,
Jason Fries, Sen Wu, and Christopher R. Snorkel: Rapid
training data creation with weak supervision. arXiv
preprint arXiv:1711.10160 , 2017.
[19]Hongyu Ren, Russell Stewart, Jiaming Song,
V olodymyr Kuleshov, and Stefano Ermon. Adver-
sarial constraint learning for structured prediction.
CoRR , abs/1805.10561, 2018.
[20]Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
U-net: Convolutional networks for biomedical image
segmentation. In International Conference on Medi-
cal image computing and computer-assisted interven-
tion, pages 234–241. Springer, 2015.
[21]Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej
Karpathy, Aditya Khosla, Michael Bernstein, et al. Im-
agenet large scale visual recognition challenge. Inter-
national Journal of Computer Vision , 115(3):211–252,
2015.
[22]Evan Sheehan, Chenlin Meng, Matthew Tan, Burak
Uzkent, Neal Jean, David Lobell, Marshall Burke, and
Stefano Ermon. Predicting economic development
using geolocated wikipedia articles. arXiv preprint
arXiv:1905.01627 , 2019.
[23]Evan Sheehan, Burak Uzkent, Chenlin Meng, Zhongyi
Tang, Marshall Burke, David Lobell, and Stefano Er-
mon. Learning to interpret satellite images using
wikipedia. arXiv preprint arXiv:1809.10236 , 2018.
[24]SpaceNet. Spacenet on amazon web services (aws).
‘datasets.’ the spacenet catalog, April 30, 2018.
[25]Russell Stewart and Stefano Ermon. Label-free supervi-
sion of neural networks with physics and domain knowl-
edge. In AAAI , volume 1, pages 1–7, 2017.
[26]USDA National Agricultural Statistics Service Crop-
land Data Layer. published crop-speciﬁc data layer [on-
line], 2016.
[27]Burak Uzkent, Matthew J Hoffman, and Anthony V o-
dacek. Real-time vehicle tracking in aerial video us-
ing hyperspectral features. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion Workshops , pages 36–44, 2016.
[28]Burak Uzkent, Matthew J Hoffman, Anthony V odacek,
John P Kerekes, and Bin Chen. Feature matching
and adaptive prediction models in an object tracking
DDDAS. Procedia Computer Science , 18:1939–1948,
2013.
[29]Burak Uzkent, Aneesh Rangnekar, and Matthew Hoff-
man. Aerial vehicle tracking by adaptive fusion of hy-
perspectral likelihood maps. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion Workshops , pages 39–48, 2017.
[30]Burak Uzkent, Aneesh Rangnekar, and Matthew J Hoff-
man. Tracking in aerial hyperspectral videos using deep
kernelized correlation ﬁlters. IEEE Transactions on
Geoscience and Remote Sensing , 57(1):449–461, 2018.[31]Liwei Wang, Yin Li, Jing Huang, and Svetlana Lazeb-
nik. Learning two-branch neural networks for image-
text matching tasks. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 2018.
[32]Wikipedia. Wikipedia, the free encyclopedia, 2018.
CROMA: Remote Sensing Representations with
Contrastive Radar-Optical Masked Autoencoders
Anthony Fuller1,∗, Koreen Millard2, James R. Green1
1Department of Systems and Computer Engineering
2Department of Geography and Environmental Studies
Carleton University, Ottawa, Canada
Abstract
A vital and rapidly growing application, remote sensing offers vast yet sparsely
labeled, spatially aligned multimodal data; this makes self-supervised learning
algorithms invaluable. We present CROMA: a framework that combines contrastive
and reconstruction self-supervised objectives to learn rich unimodal and multi-
modal representations. Our method separately encodes masked-out multispectral
optical and synthetic aperture radar samples—aligned in space and time—and
performs cross-modal contrastive learning. Another encoder fuses these sen-
sors, producing joint multimodal encodings that are used to predict the masked
patches via a lightweight decoder. We show that these objectives are comple-
mentary when leveraged on spatially aligned multimodal data. We also introduce
X- and 2D-ALiBi, which spatially biases our cross- and self-attention matrices.
These strategies improve representations and allow our models to effectively ex-
trapolate to images up to 17.6×larger at test-time. CROMA outperforms the
current SoTA multispectral model, evaluated on: four classification benchmarks—
finetuning (avg. ↑1.8%), linear (avg. ↑2.4%) and nonlinear (avg. ↑1.4%) probing,
kNN classification (avg. ↑3.5%), and K-means clustering (avg. ↑8.4%); and three
segmentation benchmarks (avg. ↑6.4%). CROMA’s rich, optionally multimodal
representations can be widely leveraged across remote sensing applications.
1 Introduction
Deep learning has led to rapid advances in remote sensing, augmenting our ability to understand and
monitor our planet. The remote sensing community has developed many application-specific deep
learning models, specifically for satellite imagery: identifying heavily polluting brick kilns [ 1,2] or
illegal airstrips [ 3]; monitoring deforestation [ 4,5,6,7] or crops [ 8,9,10]; detecting floods [ 11,12]
or wildfires [ 13,14,15]; even estimating household income [ 16,17] or poverty [ 18,19,20]. Deep
learning-based remote sensing is playing a growing role in tackling our climate crisis [ 21,22,23].
Recently, researchers leveraged self-supervised learning to pretrain remote sensing models that can
be employed on these tasks, and more [ 24,25,26,27]. Self-supervised methods are invaluable for
remote sensing, as there are petabytes of publicly available raw data from which to learn general
representations, while only limited annotated data exists for downstream applications.
Self-supervised representations are often learned via contrastive approaches [ 28,29,30] or recon-
struction approaches [ 31,32,33]. Contrastive approaches encourage the representations of positive
pairs of samples—built by producing another view of a sample, for instance, from another sensor [ 34]
or time [ 35], or by augmentations [ 30]—to be similar, and the representations of negative pairs to be
dissimilar; this process can learn descriptive, object-focused representations. Models trained with a
∗All correspondence should be addressed to Anthony Fuller: anthony.fuller@carleton.ca
37th Conference on Neural Information Processing Systems (NeurIPS 2023).
contrastive objective learn to discard information not shared between views [ 36,37], regardless of
the information’s usefulness on downstream tasks; this makes the representations they learn sensitive
to how positive pairs are built [ 30,38,39,40]. Conversely, models trained with reconstruction or
autoencoding objectives—for instance, predicting hidden pixels [ 31,41]—learn to capture as much
information as possible [ 42,43,44]. Reconstruction approaches do not rely on multiple views and
scale incredibly well [ 31,45,46], but they learn representations that require significant finetuning to
be useful on downstream tasks [ 43,45]. Park et al. [ 47] show vision transformers (ViTs [ 48]) trained
with contrastive learning focus more on shapes and low-frequency information than ViTs trained with
reconstruction approaches, which focus more on textures and high-frequency information. They show
that combining both objectives may achieve a sweet spot that learns better representations than either
objective alone. Several other frameworks have been developed that leverage both objectives to learn
SoTA representations [ 49,50,51,52]; however, none are designed for spatially aligned multimodal
data.
Researchers developing foundation models for remote sensing have yet to take advantage of the
multimodal data ubiquitous to remote sensing. For instance, the Sentinel missions—imaging the
Earth’s landmass multiple times per month since 2015—consist of multispectral optical imagery
acquired by Sentinel-2 and Synthetic Aperture Radar (SAR) data acquired by Sentinel-1. By
exploiting differences in how electromagnetic radiation interacts with Earth surface materials and
measuring the radiation at many wavelengths (ranging from 440to2,200 nm ), Sentinel-2 multispectral
optical imagery can be used to characterize the material composition of objects [ 53]. By actively
transmitting and receiving longer wavelength ( 5.5 cm ) electromagnetic pulses, Sentinel-1 SAR can
be used to characterize the geometry, roughness, and electrical properties of objects [ 54]. These
modalities have proven complementary across remote sensing applications [ 55,56,57]. Importantly
for our work, they are spatially aligned, allowing multiple views of the same feature on the ground.
Moreover, because data from the Sentinel missions are freely available, they have become the most
widely used source of satellite imagery in research; thus, models with useful representations of
Sentinel-1 & 2 imagery can be immediately leveraged in scientific research, improving our ability to
understand our planet.
These observations motivate C ontrastive R adar-O ptical M asked A utoencoders ( CROMA ): a frame-
work for learning rich representations of multimodal, spatially aligned, 2D data; which we leverage
to pretrain ViT models on Sentinel-1 & 2 data, providing the most valuable foundation models for
Earth Observation, to date. We highlight three contributions: 1CROMA significantly outperforms
the current SoTA multispectral model, SatMAE [ 26], under an extensive evaluation. 2CROMA
learns representations that are optionally multimodal (i.e., they can be effectively leveraged when
either or both modalities are available) and extrapolate to larger images at test-time (i.e., they can
be effectively leveraged on images larger than those on which the model was trained). 3CROMA’s
effectiveness is driven by two innovations: our complementary pretraining objectives and our novel
relative position encoding (RPE) strategies that contribute to the quality of learned representations.
2 Method
In this section, “optical” refers to 12-channel multispectral optical imagery acquired by Sentinel-2,
and “radar” refers to 2-channel SAR backscatter data acquired by Sentinel-1.
Architecture Background. Most work combining contrastive and reconstruction objectives learning
joint multimodal representations is in the image-text domain [ 51,58,59,60,61,62,63,64,65];
heavily inspiring our architecture, Contrastive Captioning (CoCa [ 49]) learns SoTA image-text
representations that are optionally multimodal. The CoCa framework consists of two unimodal
encoders—one for images, one for text—and a multimodal decoder that receives text encodings
at the bottom of the network and cross-attends to image encodings. CoCa is trained with two
objectives: an image ↔text contrastive objective between unimodal encoders and an image-captioning
objective at the output of the multimodal decoder. Our framework significantly adapts CoCa to
aligned multimodal 2D data by masking both modalities, introducing a multimodal encoder, a
lightweight decoder (only used during pretraining, inspired by masked autoencoders [ 31]), and novel
cross-attention and self-attention positional biases.
Model Architecture. CROMA consists of three encoders (Fig. 1). 1A unimodal radar ViT fR
that encodes radar inputs IR∈R2×H×WintoLpatch encodings ER∈RL×D, i.e.,ER=fR(IR).
2A unimodal optical ViT fOthat encodes optical inputs IO∈R12×H×WintoLpatch encodings
2
Multispectral
Optical EncoderEO
Learning Representations
→radar↔optical contrastive learning
→multimodal masked autoencoding
R1
O R1
R
R2
O R2
R
R3
O R3
R......
RN
O RN
R
Contrastive
Losspositive
negativesRadar EncoderER
Multimodal EncoderMMM
 M
 MM
Multimodal DecoderReconstruction Loss
cross-attention with spatial biases
pool + FFN pool + FFN
Multispectral
Optical EncoderRO EO
Leveraging Representations
→finetuning, probing, clustering, and nearest-neighbor
→classification and segmentation
→optionally multimodal
→larger images at test-time
Radar EncoderRR ER
Multimodal Encoder
ERO
cross-attention with
spatial biases
pool
+
FFNpool
+
FFN
Figure 1: (Left) Our CROMA framework jointly leverages radar ↔optical contrastive learning and
masked autoencoding to learn rich, self-supervised representations. (Right) Leverage representations
on: unimodal or multimodal data, larger images at test-time, and diverse tasks and methods.
EO∈RL×D, i.e.,EO=fO(IO).3A multimodal radar-optical transformer fROthat encodes Lradar-
optical patches, ERO∈RL×D, i.e.,ERO=fRO(ER,EO). For each set of unimodal encodings, we
build full-image representations R ∈RDby processing the mean pooled patch encodings through a
feedforward network (FFN), i.e., RR= FFN R(MeanPool( ER))andRO= FFN O(MeanPool( EO)).
Our patch size is 8 × 8 pixels for both modalities, and our default image size is 120 × 120 pixels. Our
radar encoder has N/2transformer layers, and our optical encoder has Ntransformer layers ( N
is12for ViT-B and 24for ViT-L backbones). All unimodal encoder layers are composed of self-
attention and FFN sublayers. Our multimodal N/2-layer encoder—composed of self-attention, cross-
attention, and FFN sublayers—encodes both modalities into a single sequence of Lpatch encodings;
this encoder receives radar patch encodings at the bottom of the network and learns multimodal
representations by cross-attending to optical patch encodings. Multimodal representations can be
built via pooling multimodal patch encodings, i.e., RRO= MeanPool( ERO). Our ViT backbones
do not use sinusoidal position embeddings; instead, we bias the self-attention and cross-attention
matrices with the distances between patches.
ALiBi Background. ALiBi [ 66] is a simple and intuitive RPE method for transformers that biases
the self-attention matrix based on the distance between tokens in a 1D sequence. Each self-attention
head receives positional biases with different strengths, called slopes m. With 16attention heads, the
geometric sequence defines these scalar slopes starting at1√
2, i.e.,1
20.5,1
21,1
21.5, . . . ,1
28. Biases are
subtracted from the attention matrix before the softmax is calculated. Specifically, the pre-softmax
attention matrix A∈Rh×L×L(his the number of heads, Lis the sequence length) is populated with
attention scores ahijfor the ith query qhi∈Rdandjth key khj∈Rd(dis the head dimension):
ahij=√
d·qhi·khj, without ALiBi; and ahij=√
d·qhi·khj−distance( i, j)·m(h), with ALiBi.
Crucially, ALiBi does not add position embeddings at the bottom of the network; instead, the relative
positions between tokens are encoded in the attention matrix itself. To date, ALiBi is the only position
encoding method for transformers that has been demonstrated to extrapolate at test-time to sequences
far longer than those on which it was trained.
0.01.02.01.01.42.22.02.22.8
1.00.01.01.41.01.42.22.02.2
2.01.00.02.21.41.02.82.22.0
1.01.42.20.01.02.01.01.42.2
1.41.01.41.00.01.01.41.01.4
2.21.41.02.01.00.02.21.41.0
2.02.22.81.01.42.20.01.02.0
2.22.02.21.41.01.41.00.01.0
2.82.22.02.21.41.02.01.00.0k1
q1 q1k2
q2 q2k3
q3 q3k4
q4 q4k5
q5 q5k6
q6 q6k7
q7 q7k8
q8 q8k9
q9 q9·m
Figure 2: 2D-ALiBi matrix for an
image with 9 patches ( 32before flat-
tening); qis for query, kis for key,
andmis a fixed scalar that biases
attention heads at different rates.2D-ALiBi and X-ALiBi. We extend ALiBi to 2D inputs by
biasing the self-attention matrix based on the Euclidean distance
between query-key pairs in a ViT—we call this 2D-ALiBi (Fig.
2). We also extend ALiBi to cross-attention by biasing the
cross-attention matrix based on the Euclidean distance between
cross-modal query-key pairs—we call this X-ALiBi. (In cross-
attention [ 67], queries are built from the previous layer, whereas
keys and values are built from optical encodings.) For both 2D
and X-ALiBi, we calculate our attention-head slopes with the
same geometric sequence as ALiBi. Since our two modalities
3
are aligned 2D sensor data, our 2D-ALiBi and X-ALiBi matrices are identical. The primary motivation
of 2D-ALiBi is to learn representations that can generalize across image sizes; this is particularly
useful in remote sensing and is likely useful in other domains. The primary motivation of X-ALiBi is
to improve sensor fusion by inserting positional information in the cross-attention sublayer of our
multimodal encoder, fRO. These position encoding techniques are rotation and translation invariant,
which are desirable properties for the overhead imagery in Earth Observation.
MAE Background. A masked autoencoder (MAE [ 31]) rearranges an image into a sequence of
non-overlapping patches, then randomly samples a large portion of patches to be held-out. The
“visible” patches, that are notheld-out, are encoded by a ViT. This MAE-style masking is ingenious:
it leverages sparse computation while only requiring dense operations that run efficiently on modern
hardware. MAE introduces a lightweight decoder that receives visible patch encodings and hidden
mask embeddings, which are both added to 2D-sinusoidal position embeddings. The decoder outputs
predictions of the pixel values of the held-out patches. Both the encoder and decoder are pretrained
end-to-end to minimize the mean squared error between patch predictions and the originally held-out
patches. The pretrained encoder can then be leveraged on downstream applications.
Reconstruction Objective. We independently mask 75% of radar and optical patches and en-
code the unmasked patches with our three encoders, i.e., Eum
R=fR(Ium
R),Eum
O=fO(Ium
O),
andEum
RO=fRO(Eum
R,Eum
O); where um means unmasked. We introduce a lightweight 1-
layer transformer decoder fDEC that receives multimodal patch encodings and mask embed-
dings after adding 2D-sinusoidal position embeddings and predicts a target image, i.e., ˆI=
fDEC(CONCAT[ Eum
RO,Embmask] + Embpositions). We split the channels of ˆIto form predictions
for each sensor, ˆIOandˆIR; the loss is only applied at the locations of the masked-out patches:
LMAE =1
NNX
i
PM
j
ˆIij
O−Norm( Iij)O2
M+PM
j
ˆIij
R−Norm( Iij)R2
M

optical reconstruction radar reconstruction
where Nis the batch size, Mis the number of masked patches, and Norm sets the mean to 0and
standard deviation to 1for target patches (following MAE). Along with learning unimodal repre-
sentations, this objective spatially fuses our two sensors, i.e., it builds multimodal patch encodings
that represent information from both sensors in the patch, corresponding to an 80 m × 80 m square
on the ground ( 8 × 8 patches at 10 m resolution). Finally, 2D and X-ALiBi can be easily adapted to
MAE-style masking by removing the masked-out columns and rows from the bias matrix.
Contrastive Learning Background. Contrastive learning aims to classify the correct pairings of
samples derived from a batch. Logits are formed by measuring the similarity between the projected
representations of samples. As a result, the representations of positive pairs are pulled together, and
the representations of negative pairs are pushed apart. Very recently, FLIP [ 68] performs contrastive
learning with masked-out representations via MAE-style masking. This speeds up pretraining and
enables larger batches due to the reduced memory per sample. FLIP performs on par with CLIP [ 69]—
the foundational work that learns rich representations via image ↔text contrastive learning—but can
be pretrained at half the cost.
Contrastive Objective. We perform radar ↔optical contrastive learning across the unimodal repre-
sentations of our masked-out sensor data using the InfoNCE loss [ 28]. For an optical anchor image,
the positive sample is the geographically and temporally matched radar sample, and the negative
samples are all other radar samples from the batch; likewise, our radar representations are pulled
towards (positives) or pushed apart (negatives) from optical representations:
LCon=−1
2N
NX
ilogexp 
zi⊤
Rzi
O/σ
PN
jexp
zi⊤
Rzj
O/σ+NX
ilogexp 
zi⊤
Ozi
R/σ
PN
jexp
zi⊤
Ozj
R/σ

radar-to-optical optical-to-radar
where zRandzOareℓ2normalized linear projections of radar and optical representations, respectively,
i.e.,zR= Norm(Linear R(RR))andzO= Norm(Linear O(RO)).σis the softmax temperature, and
4
Nis the batch size. Crucially, we only encode a small portion of input patches, which form our
representations. This masking provides advantages: it enables larger batches, speeds up pretraining,
and enables our multimodal reconstruction objective with the same computational graph. This
radar↔optical contrastive objective encourages representations to be sensor-invariant, i.e., to capture
information shared between sensors.
Combined Pretraining Objective. We combine contrastive learning and masked sensor modeling
pretraining objectives: L=λConLCon+λMAELMAE . We set both task weights (i.e., λConandλMAE )
to 1 and ablate them in Appendix §A.1.
3 Experiments
Pretraining. We pretrain CROMA models on the SSL4EO dataset [ 70]—a large geographically
and seasonally diverse unlabeled dataset. SSL4EO consists of 1million paired Sentinel-1 GRD &
Sentinel-2 L2A samples of 264 × 264 pixels. Sentinel-1 channels consist of VV and VH backscatter.
Sentinel-2 channels consist of 12surface reflectance multispectral bands (the cirrus band is removed).
We pretrain CROMA-B (ViT-B backbone) for 300epochs and CROMA-L (ViT-L backbone) for
600epochs. Crucially, a single pretraining run trains all three encoders (optical, radar, and joint
radar-optical) end-to-end; users can then finetune one or multiple encoders, depending on their
task and data availability. We perform all pretraining experiments on an NVIDIA DGX server ( 8×
A100–80 GB), including ablations. Please see Appendix §A.3 for more details.
Comparisons. We compare CROMA to all available multispectral optical foundation models,
which include two models pretrained by [ 71] using radar ↔optical contrastive learning; two models
pretrained by [ 70] using the MAE [ 31] and DINO [ 72] frameworks; and two models pretrained by
[26] using their multispectral representation learning framework, SatMAE. We also compare CROMA
to a SoTA method for learning visual representations of natural images—image joint embedding
predictive architecture (I-JEPA, [ 73])—that we leverage to pretrain a ViT-B model on SSL4EO’s
optical imagery for 300epochs. To enable a fair comparison between models, we evaluate all models
under identical conditions and hyper-parameter budgets (please see Appendix §A.4.2 for details).
This is necessary because the originally reported results of these models occurred under inconsistent
evaluation conditions—for instance, data splits or training data amounts. We use the latest publicly
available models for all evaluations and preprocess data according to official repositories. For radar
and radar-optical datasets, we compare CROMA to SatViT-V2 [ 74], a model pretrained using MAE
[31] on stacked Sentinel-1 & 2 imagery; and DeCUR, a model pretrained—concurrently with this
work—by [75] using their multimodal representation learning framework.
3.1 Multispectral Optical Experiments
Classification Setup. We evaluate CROMA by finetuning, frozen linear and nonlinear probing, kNN
classifying, and K-means clustering pretrained representations across four Sentinel-2 classification
benchmarks. 1The multi-label BigEarthNet dataset [ 76] (35,420 train samples and 118,065 vali-
dation samples); this is 10% of the complete BigEarthNet training set that is now used by default
[25,26] to reduce the costs of finetuning and is better suited for a remote sensing benchmark [ 22].
2The fMoW-Sentinel dataset [ 26] (71,287 train samples and 84,939 validation samples); this is also
10% of the complete training set. Following BigEarthNet’s use, we believe this smaller training set is
a more appropriate benchmark for model evaluation, but we show results on the complete training set
in Appendix §A.4.1. 3The EuroSAT dataset [ 77] (16,200 train samples and 5,400 validation sam-
ples). 4The Canadian Cropland dataset [ 78] (53,884 train samples and 23,088 validation samples);
this is a new benchmark inspired by EuroSAT but is more challenging, as the crop types (barley,
canola, corn, etc.) can be visually similar. For finetuning and linear probing, we add a linear layer
for these tasks atop the full-image representations, i.e., Linear( RO). For nonlinear probing, we use
an MLP with one hidden 2048 -d layer, i.e., Linear(ReLU(Linear( RO))). Additionally, we perform
non-parametric kNN classification ( k= 20 ) and K-means clustering for single-label benchmarks
to evaluate frozen representations. [ 79] shows that no single method of evaluating representations
is the best; they recommend including kNN and K-means alongside linear probing. Other studies
[80,72,81] show a rank mismatch between kNN and linear probing evaluations—indicating they
offer complementary estimates of representation quality. Please see Appendix §A.4.1 and A.4.2 for
implementation, data splits, and hyper-parameter details.
5
Table 1: Classification results on four benchmarks, under finetuning (FT), and frozen linear
(LP) and nonlinear (MLP) probing. * denotes originally reported results; we obtain all other results
under identical conditions.
BigEarthNet (10%) fMoW-Sentinel (10%) EuroSAT Canadian Cropland
mAP Top 1 Acc. Top 1 Acc. Top 1 Acc.
Method Backbone FT MLP LP FT MLP LP FT MLP LP FT MLP LP
radar↔optical [71] ResNet50 77.65 78.79 77.44 32.03 6.46 6.01 96.31 86.35 78.81 57.44 58.09 55.55
radar↔optical [71] Swin-T 86.41 78.65 77.93 52.01 28.54 31.06 98.09 93.50 94.78 70.98 60.36 57.05
MAE [31, 70] ViT-S 86.15 81.70 75.94 51.79 31.70 27.69 98.78 94.46 91.80 74.02 59.07 48.38
DINO [72, 70] ViT-S 87.04 84.96 81.58 52.79 35.62 32.64 98.63 97.07 96.07 75.27 67.35 59.94
I-JEPA [73] ViT-B 85.92 84.27 80.80 53.54 35.76 32.35 99.20 96.60 95.63 75.13 66.69 60.17
SatMAE [26] ViT-B 85.94 83.48 79.36 57.20 37.28 35.17 99.20 97.28 96.61 73.58 66.02 60.40
CROMA ViT-B 87.58 86.29 85.04 54.47 39.67 38.42 99.22 97.89 97.59 76.17 67.62 63.39
SatMAE [26] ViT-L 86.18/
82.62*84.01 80.29 58.19 38.18 36.76 99.35/
98.98*97.67 97.65 74.06 67.03 61.75
CROMA ViT-L 88.29 86.46 85.01 59.02 40.07 39.17 99.46 98.04 98.02 78.07 67.94 64.02
Table 2: Non-parametric kNN classification and K-
means clustering results. * denotes 10% of the training
set.
fMoW-Sent.* EuroSAT Can. Crop.
Top 1 Acc. Top 1 Acc. Top 1 Acc.
Method Backbone kNNK-
meanskNNK-
meanskNNK-
means
radar↔optical [71] ResNet50 7.07 3.94 52.09 23.52 49.82 23.15
radar↔optical [71] Swin-T 19.46 7.93 85.07 56.91 48.22 21.57
MAE [31, 70] ViT-S 22.25 7.54 87.33 41.50 56.01 18.31
DINO [72, 70] ViT-S 28.89 8.23 94.20 45.83 60.70 20.22
I-JEPA [73] ViT-B 25.45 8.01 89.02 42.89 57.48 18.06
SatMAE [26] ViT-B 26.76 8.98 89.28 44.87 56.20 20.14
CROMA ViT-B 32.03 11.35 95.26 76.06 61.25 22.55
SatMAE [26] ViT-L 26.77 9.17 89.57 38.48 56.93 19.65
CROMA ViT-L 29.54 10.12 94.70 61.24 59.41 21.27Classification Results. CROMA ranks
first, averaged across four Sentinel-2 (mul-
tispectral optical) benchmarks, under fine-
tuning, frozen linear and nonlinear probing,
kNN classification, and K-means cluster-
ing (Table 1, 2). The only case where a Sat-
MAE model outperforms a CROMA model
of the same backbone is finetuning on
the fMoW-Sentinel dataset. However, Sat-
MAE was pretrained on fMoW-Sentinel—
meaning there is no distribution shift be-
tween pretraining and finetuning. This
gives SatMAE an advantage on the fMoW-
Sentinel benchmark since downstream per-
formance is impacted by the similarity be-
tween upstream and downstream data [ 82,83,84]. Despite this observation, CROMA-B outperforms
SatMAE-B on fMoW-Sentinel under linear probing ( ↑3.3%), nonlinear probing ( ↑2.4%),kNN
(↑5.3%), and K-means ( ↑2.4%). Additionally, CROMA models are more than 4×faster during fine-
tuning and inference than their SatMAE counterparts (please see Appendix §A.4.2 for a comparison).
On all evaluations, CROMA outperforms a ViT-B model pretrained using the I-JEPA framework on
the SSL4EO dataset—I-JEPA is the current SoTA framework for learning self-supervised represen-
tations of ImageNet [ 73]. We also plot UMAP [ 85] embeddings (Fig. 3); CROMA shows a strong
separation of EuroSAT classes.
CROMA (ViT-B): 76.0% SatMAE (ViT-B): 44.9% DINO (ViT-S): 45.8% radar↔optical (Swin-T): 56.9%
Permanent Crop Residential Industrial Highway Pasture
Sea & Lake Forest River Annual Crop Herbaceous Vegetation
Figure 3: UMAP embeddings and K-means clustering accuracies of CROMA (ViT-B), SatMAE
(ViT-B) [26], DINO (ViT-S) [70], and radar ↔optical (Swin-T) [71] models on EuroSAT [77].
6
10010110210300.20.40.6(A) Agriculture
(B) Beaches,
dunes, sandsCROMA
SatMAE
I-JEPA
kF1 Score
Figure 4: Sparse probing two
BigEarthNet classes.Sparse Probing. Inspired by the sparse probing of language
models [ 86], we sparsely probe the 768-dimensional (ViT-B
backbone) image representations by restricting the linear probe
tokdimensions. For each class, we rank all dimensions and
then train a linear probe on the top kto perform binary classifi-
cation (this setup follows [ 86], please see Appendix §A.4.2 for
more details). This experiment helps us understand the infor-
mation contained in the learned representations. For example,
in Fig. 4 (B), there is a large drop in F1 score when decreas-
ingk—indicating that BigEarthNet’s “beaches, dunes, sands”
class is not well represented by individual dimensions. Rather,
this class is represented by a composition of many features
that are correlated with beaches. Conversely, BigEarthNet’s
“agriculture with natural vegetation” class maps well to a single
dimension of CROMA’s representations, achieving an F1 score
of49% when k= 1(Fig. 4 (A)). In Appendix §A.6, we show
sparse probing results for all classification tasks and for all classes.
Segmentation Setup. We evaluate all ViT-based models on three Sentinel-2 segmentation bench-
marks. 1The DFC2020 dataset [ 87] (46,152 train samples and 8,874 validation samples). 2A
subset of the Dynamic World dataset [ 88] that was annotated by experts; hence we call it DW-Expert
(20,422 train samples and 51,022 validation samples). DW-Expert is a new, high-quality benchmark
that was annotated with the help of high-resolution satellite and street-level imagery. 3The MARIDA
dataset [ 89] (1,682 train samples and 1,615 validation samples), which is a small, sparsely labeled
dataset of marine debris. For all tasks, we linear probe frozen patch encodings, i.e., Linear( EO). We
crop images of 96 × 96 from the original images ( 256 × 256 for DFC2020, 510 × 510 for DW-Expert,
and256 × 256 for MARIDA). We train and evaluate all models on these 96 × 96 images, which is the
default image size for SatMAE, enabling a fair comparison with the SoTA.
Table 3: Semantic segmentation (mIoU) re-
sults on three Sentinel-2 benchmarks.
Method Backbone DFC2020 DW-Expert MARIDA
MAE ViT-S 35.63 46.63 48.06
DINO ViT-S 32.34 48.34 49.38
I-JEPA ViT-B 36.72 50.82 53.85
SatMAE ViT-B 45.53 51.03 58.17
CROMA ViT-B 46.67 58.55 65.56
SatMAE ViT-L 44.13 51.50 57.12
CROMA ViT-L 49.78 58.71 63.32Segmentation Results. CROMA outperforms Sat-
MAE by averages of 5.4%and6.4%for ViT-B and
ViT-L backbones, respectively (Table 3). These re-
sults demonstrate that CROMA effectively learns fine-
grained patch-level features useful for dense predic-
tion tasks like semantic segmentation.
3.2 Radar and Radar-Optical Experiments
Table 4: Multimodal (Sentinel-
1 & 2) linear probing results
on classification and segmen-
tation benchmarks.
BigEarthNet DFC2020
Model mAP mIoU
SatViT-V2 [74] 79.80 46.20
CROMA-B 86.24 51.58
CROMA-L 86.20 53.24Multimodal Setup. BigEarthNet and DFC2020 also contain spa-
tially and temporally aligned Sentinel-1 samples alongside Sentinel-
2. This allows us to evaluate our multimodal representations and
directly compare them with optical-only representations on the
same benchmark. We linear probe frozen image representations for
BigEarthNet, i.e., Linear(CONCAT( RR,RO,RRO)), and patch en-
codings for DFC2020, i.e., Linear(CONCAT( ER,EO,ERO)). Con-
current with this work, DeCUR [ 75] learns radar and radar-optical
representations via their novel multimodal framework. They fit a
linear probe on top of DeCUR’s representations on 1% of the BigEarthNet training set and report
results on the BigEarthNet validation set; we follow this experimental procedure with our CROMA-B
model for a direct comparison.
Table 5: Linear prob-
ing radar and radar-
optical representations.
BigEarthNet (1%)
mAP
Method Radar Radar-Opt.
DeCUR 73.7 79.4
CROMA 75.7 81.8Multimodal Results. CROMA significantly outperforms the joint multi-
modal representations learned by SatViT-V2 [ 74] (Table 4). Our joint multi-
modal representations outperform our optical-only representations by 1.2%
on BigEarthNet (both CROMA-B and CROMA-L) and by 4.9%(CROMA-
B) and 3.5%(CROMA-L) on DFC2020 (Table 1, 3, 4)—justifying our
multimodal approach. CROMA’s radar-only and radar-optical represen-
tations also outperform the concurrent DeCUR under a linear probing
experiment (Table 5).
7
Table 6: Linear probing ablation results on radar-only (“R”), optical-only (“O”), and joint radar-
optical (“RO”) inputs. We consider both performance and cost to select our design. All rows below
“all default” report the performance differences between ablated cases and the default.
Case (default) Ablationbatch
sizecostClassification (mAP) Segmentation (mIoU)
R O RO R O RO Avg
all default 7.2k 1.0× 78.2 84.5 84.8 40.8 56.0 56.5 66.8
1objectives
(both)MAE-only 7.2k 1.0× −10.4 −6.0 −5.6 −9.2 −8.4 −5.1 −7.5
contrast-only 14k 0.6× −3.2 −3.0 — −2.9 −3.7 — —
2position encoding
(2D-ALiBi + X-ALiBi)2D-ALiBi 7.2k 1.0× −0.2 −0.2 −0.3 0.1 0.0 −0.9 −0.2
PEG [90] 6.9k 1.0× −0.3 0.0 −0.1 −0.6 −1.0 −1.9 −0.7
2D-sinusoidal 7.2k 1.0× −3.8 −2.9 −1.3 −2.2 −0.1 −0.2 −1.7
3masking
(independent 75%)shared 25% 2.4k 3.0× −0.5 −1.0 −1.1 1.1 0.5 −0.2 −0.2
shared 50% 3.7k 1.8× −0.3 −0.7 −0.7 1.0 0.6 0.1 0.0
shared 75% 7.2k 1.0× −0.8 −0.5 −0.3 0.0 0.1 0.0 −0.2
independent 50% 3.7k 1.8× −0.2 −0.5 −0.5 0.9 0.7 0.3 0.1
4MAE target
(radar & optical)radar-only 7.2k 1.0× −0.1 −0.2 −0.6 0.3 −0.2 −1.4 −0.5
optical-only 7.2k 1.0× −0.1 −0.2 −0.2 −0.1 −0.1 −0.6 −0.2
5 MAE decoder (depth=1, dim=512) depth=6, dim=768 4.3k 1.7× 0.1 −0.1 0.1 0.4 0.2 0.2 0.1
6scale
(ViT-B, epochs=100)ViT-B, epochs=300 7.2k 3.0× 1.6 0.5 0.4 1.7 1.3 0.5 1.0
ViT-L, epochs=600 3k 15× 2.5 0.4 0.5 2.8 0.9 0.1 1.2
4 Ablation Analysis
4.1 CROMA Design
Ablation Setup. We ablate the CROMA design by pretraining CROMA-B models for 100epochs
unless stated otherwise. For each ablation, we set the maximum batch size that can fit into 640GB
of VRAM (with bfloat16 precision); adjusting the batch size results in better comparisons between
approaches. For classification on BigEarthNet, we linear probe frozen unimodal and multimodal
representations, i.e., Linear( RR),Linear( RO),Linear( RRO). For segmentation on DW-Expert-120,
we linear probe frozen patch encodings, i.e., Linear( ER),Linear( EO),Linear( ERO). For BigEarthNet,
we train on the same 10% split as §3.1, but report results on the combined validation and test sets
(236,130 samples). For DW-Expert-120, we select 120 × 120 images (CROMA’s default image
size) from Dynamic World’s [ 88] original Sentinel-2 images and match them, in space and time,
with Sentinel-1 images—forming a high-quality multimodal segmentation dataset. DW-Expert-120
consists of 10,200 train and 45,367 validation samples.
Ablation Results. 1Removing either self-supervised objective significantly degrades accuracy on
both classification and segmentation tasks. This justifies a fundamental design choice, i.e., combining
contrastive and reconstruction approaches for aligned multimodal data. 2Although our primary
motivation for 2D-ALiBi is to enable input size extrapolation (see §4.2), we find it also improves linear
probing accuracy over a SoTA RPE method for ViTs, even when no extrapolation occurs. Averaged
across six evaluations, 2D-ALiBi without X-ALiBi outperforms a Position Encoding Generator (PEG,
[90]) by 0.5%and 2D-sinusoidal embeddings by 1.5%. (We adapt PEG to MAE-style masking
by zero-filling the masked-out patches during pretraining, inspired by [ 91]). Leveraging X-ALiBi
with 2D-ALiBi further improves average performance by 0.2%(Table 6)—particularly improving
multimodal performance, which is our motivation for X-ALiBi. We believe there are two reasons for
2D-ALiBi’s superior performance—evidence for both is provided in Appendix §A.2. First, 2D-ALiBi
learns rotation-invariant representations despite not being trained with rotation-invariant objectives;
this is a desirable property of satellite imagery that likely improves classification performance.
Second, 2D-ALiBi prevents patch-wise representational collapse (i.e., the representations of patches
within an image become similar, losing local information) often observed with contrastive objectives
[47]; preserving patch-wise diversity likely improves segmentation performance. 3Lower mask
ratios hurt classification and help segmentation but increase costs. For both 50% and75% mask ratios,
independently masking our modalities (i.e., optical and radar samples are masked differently) slightly
outperforms shared masking. Although 50% independent masking outperforms 75% masking by
0.1%, we select the higher mask ratio because it offers a 1.8×speedup. 4A multimodal target ( 14
channels) outperforms an optical-only target ( 12channels) by 0.2%, which outperforms a radar-only
8
target ( 2channels) by 0.3%(Table 6); this implies that reconstructing more information leads to more
general representations. A multimodal target is especially important for learning rich multimodal
representations. 5A larger decoder improves segmentation but costs 1.7×more. MAE showed
that a deep decoder improves linear probing accuracy [ 31], but CROMA is very robust to decoder
size. Therefore, we select the most efficient, i.e., a 1-layer, 512-d decoder. 6We design CROMA by
pretraining ViT-B models for 100epochs and consider linear probing performance and cost. Thus,
our design may not be optimal when scaling up. Nevertheless, scaling to more epochs and a larger
model improves representations, especially radar-only representations.
In Appendix §A.1 we also experiment with—all showing minimal or negative impacts—task weights,
more decoder sizes, VICReg [ 92], mean squared error loss between cross-modal patch encodings
(inspired by [ 93]), InfoNCE loss between cross-modal patch encodings (inspired by [ 94]), hard-
negative mixing [95], and lower-masked tuning at the end of pretraining [68].
4.2 Extrapolating Representations to Larger Images
Extrapolation Setup. For three CROMA-B models (2D-sinusoidal, PEG [ 90], and 2D-ALiBi)
pretrained for 100epochs, we finetune all parameters (along with a linear head) on Sentinel-2 images
from DW-Expert-120 ( 120 × 120 pixels). Then, we directly evaluate the models on images of varying
sizes to test their ability to extrapolate at test-time. We create validation sets with different image
sizes by cropping from the original 510 × 510 images in Dynamic World [ 88]. Regardless of image
size, we retain the 10 m per pixel spatial resolution on which the models were trained—extrapolating
to smaller or larger geographic areas at test-time. For 2D-sinusoidal embeddings, we also evaluate an
embedding interpolation algorithm often used when applying ViTs on larger images [ 31,26,48,96].
Table 7: Segmentation results (mIoU) of models trained
on120 × 120 resolution and evaluated on various reso-
lutions.
48 96 120 224 384 448 504
2D-Sinusoidal 54.4 58.5 59.2 38.2 24.9 21.8 19.5
2D-Sinusoidal w/ interp. 50.7 58.6 59.2 58.3 56.2 55.5 54.9
PEG [90] 55.1 58.0 58.3 58.5 57.7 57.4 57.1
2D-ALiBi 56.3 59.0 59.3 59.5 59.1 59.0 58.8Train Res.
Test Res.MethodExtrapolation Results. 2D-ALiBi outper-
forms PEG by 1.7%on504 × 504 pixel
images (Table 7). Amazingly, we only see
a0.7%drop in mIoU when testing on ar-
eas17.64×larger than those on which our
model was trained—effectively generaliz-
ing from 225to3,969 patches per image.
We achieve this by extending ALiBi [ 66]
to 2D inputs by penalizing attention scores
based on the Euclidean distance between
query-key pairs. We may achieve even better results by encoding directions in a subset of attention
heads or learning scalars [ 97]; we leave these investigations to future work. We believe X- and
2D-ALiBi have tremendous potential beyond our CROMA framework, for instance, by extending
these methods to additional modalities, viewing angles, or 3D data.
5 Related Work
Remote Sensing Representations. Deep learning for remote sensing has been an active research
area for many years. Researchers have leveraged self-supervised learning frameworks in the last few
years to learn representations of remote sensing data that can be widely leveraged across societally
important downstream tasks. In designing contrastive pretext tasks, remote sensing researchers built
positive pairs by leveraging spatial [ 98,99,100,101,102,103,104,105,106,107,108], temporal
[24,25,109], spectral [ 110], or cross-modal [ 111,112,113,114,115,116] information. In designing
reconstructive pretext tasks, researchers held-out spectral bands [ 117], pixels [ 118,119,120,121], and
resolutions [ 27]. However, these studies—including the concurrent Scale-MAE [ 27] and influential
studies tile2vec [ 108], GASSL [ 24], and SeCo [ 25]—were either performed at smaller scales or
only included wavelengths from the visible (red, green, and blue) spectrum; this limits their utility
on downstream applications since wavelengths from the non-visible spectrum contain information
critical to many remote sensing tasks [ 53,122,123,124]. For example, the ability to measure
the reflectance of objects in the near-infrared portion of the wavelength is extremely valuable in
applications related to vegetation identification [ 125], health and productivity [ 126], as well as
identifying water bodies [127], soil moisture [128], and vegetation water content [129].
Relative Position Encoding for ViTs. SoTA transformers in natural language processing use RPE
[130,131,132], but fixed 2D-sinusoidal embeddings still predominate ViTs. The improved inductive
9
bias of RPE over absolute position encoding can offer improved performance and, sometimes, the
ability to extrapolate to larger images at test-time, i.e., directly applying a model trained on one image
size to another without further training. This extrapolation ability would add considerable value to
remote sensing foundation models since image sizes vary widely—images are often cropped from
large scenes down to smaller sizes chosen idiosyncratically. Positional Encoding Generator (PEG,
[90]) is a SoTA RPE method that uses a convolution between ViT layers; PEG was demonstrated to
significantly outperform other RPE methods when tested on larger images. iRPE [ 133] is another,
more complex, RPE method for ViTs; however, it demonstrates no extrapolation ability. We found no
prior work that leverages RPE in cross-attention.
6 Conclusion
We propose a novel framework for learning unimodal and multimodal representations for Earth
Observation by jointly leveraging contrastive and reconstruction self-supervised objectives. We
extend a SoTA position encoding method for 1D sequences to 2D inputs and cross-attention; to
the best of our knowledge, this is the first time explicit position encoding has been leveraged in
cross-attention. These strategies allow our models to extrapolate to larger images at test-time and
improve performance on both unimodal and multimodal data. We extensively evaluate our pretrained
models on diverse tasks and methods, outperforming the previous SoTA. Although our method is
designed for multimodal satellite imagery, it can be leveraged on other applications that offer spatially
aligned multimodal data, for instance, medical imaging or autonomous vehicles.
The main limitation of our work is our focus on static-in-time Sentinel-1 & 2 data; in the future,
we will explore other sensors that offer higher spatial or spectral resolutions and time-series data.
Despite this limitation, Sentinel-1 & 2 are the most widely used sources for satellite imagery in
research—making our models an incredible resource for the remote sensing community and users of
remote sensing-derived products, for instance, geographers, economists, or environmental scientists.
7 Acknowledgements
This work was made possible with compute provided by Cyxtera Technologies and the NVIDIA
Academic Hardware Grant Program. Anthony thanks the Vector Institute for their financial support
during his master’s program.
References
[1]Jihyeon Lee, Nina R. Brooks, Fahim Tajwar, Marshall Burke, Stefano Ermon, David B. Lobell,
Debashish Biswas, and Stephen P. Luby. Scalable deep learning to identify brick kilns and aid
regulatory capacity. Proceedings of the National Academy of Sciences , 2021.
[2]Arati Paul, Soumya Bandyopadhyay, and Uday Raj. Brick kiln detection in remote sensing
imagery using deep neural network and change analysis. Spatial Information Research , 2021.
[3]Fen Chen, Ruilong Ren, Tim Van de V oorde, Wenbo Xu, Guiyun Zhou, and Yan Zhou. Fast
Automatic Airport Detection in Remote Sensing Images Using Convolutional Neural Networks.
Remote Sensing , 2018.
[4]Raian V . Maretto, Leila M. G. Fonseca, Nathan Jacobs, Thales S. Körting, Hugo N. Bendini,
and Leandro L. Parente. Spatio-Temporal Deep Learning Approach to Map Deforestation in
Amazon Rainforest. IEEE Geoscience and Remote Sensing Letters , 2021.
[5]Seong-Hyeok Lee, Kuk-Jin Han, Kwon Lee, Kwang-Jae Lee, Kwan-Young Oh, and Moung-Jin
Lee. Classification of Landscape Affected by Deforestation Using High-Resolution Remote
Sensing Data and Deep-Learning Techniques. Remote Sensing , 2020.
[6]Pablo Pozzobon de Bem, Osmar Abílio de Carvalho Junior, Renato Fontes Guimarães, and
Roberto Arnaldo Trancoso Gomes. Change Detection of Deforestation in the Brazilian Amazon
Using Landsat Data and Convolutional Neural Networks. Remote Sensing , 2020.
10
[7]Bruno Menini Matosak, Leila Maria Garcia Fonseca, Evandro Carrijo Taquary, Raian Vargas
Maretto, Hugo do Nascimento Bendini, and Marcos Adami. Mapping Deforestation in Cerrado
Based on Hybrid Deep Learning Architecture and Medium Spatial Resolution Satellite Time
Series. Remote Sensing , 2022.
[8]Hannah Kerner, Gabriel Tseng, Inbal Becker-Reshef, Catherine Nakalembe, Brian Barker,
Blake Munshell, Madhava Paliyam, and Mehdi Hosseini. Rapid Response Crop Maps in Data
Sparse Regions. CoRR , abs/2006.16866, 2020.
[9]Nataliia Kussul, Mykola Lavreniuk, Sergii Skakun, and Andrii Shelestov. Deep Learning
Classification of Land Cover and Crop Types Using Remote Sensing Data. IEEE Geoscience
and Remote Sensing Letters , 2017.
[10] Foyez Ahmed Prodhan, Jiahua Zhang, Fengmei Yao, Lamei Shi, Til Prasad Pangali Sharma,
Da Zhang, Dan Cao, Minxuan Zheng, Naveed Ahmed, and Hasiba Pervin Mohana. Deep
Learning for Monitoring Agricultural Drought in South Asia Using Remote Sensing Data.
Remote Sensing , 2021.
[11] R. Bentivoglio, E. Isufi, S. N. Jonkman, and R. Taormina. Deep learning methods for flood
mapping: a review of existing applications and future research directions. Hydrology and
Earth System Sciences , 2022.
[12] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari,
and Robin Roberson Murphy. FloodNet: A High Resolution Aerial Imagery Dataset for Post
Flood Scene Understanding. IEEE Access , 2021.
[13] Fantine Huot, R. Lily Hu, Nita Goyal, Tharun Sankar, Matthias Ihme, and Yi-Fan Chen.
Next Day Wildfire Spread: A Machine Learning Dataset to Predict Wildfire Spreading From
Remote-Sensing Data. IEEE Transactions on Geoscience and Remote Sensing , 2022.
[14] Dmitry Rashkovetsky, Florian Mauracher, Martin Langer, and Michael Schmitt. Wildfire
Detection From Multisensor Satellite Imagery Using Deep Semantic Segmentation. IEEE
Journal of Selected Topics in Applied Earth Observations and Remote Sensing , 2021.
[15] Panagiotis Barmpoutis, Periklis Papaioannou, Kosmas Dimitropoulos, and Nikos Grammalidis.
A Review on Early Forest Fire Detection Systems Using Optical Remote Sensing. Sensors ,
2020.
[16] Arman Khachiyan, Anthony Thomas, Huye Zhou, Gordon Hanson, Alex Cloninger, Tajana
Rosing, and Amit K. Khandelwal. Using Neural Networks to Predict Microspatial Economic
Growth. American Economic Review: Insights , 2022.
[17] Esra Suel, Samir Bhatt, Michael Brauer, Seth Flaxman, and Majid Ezzati. Multimodal deep
learning from satellite and street-level imagery for measuring income, overcrowding, and
environmental deprivation in urban areas. Remote Sensing of Environment , 2021.
[18] Christopher Yeh, Anthony Perez, Anne Driscoll, George Azzari, Zhongyi Tang, David Lobell,
Stefano Ermon, and Marshall Burke. Using publicly available satellite imagery and deep
learning to understand economic well-being in Africa. Nature Communications , 2020.
[19] Michael Xie, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon. Transfer Learning
from Deep Features for Remote Sensing and Poverty Mapping. In AAAI , 2016.
[20] Luna Yue Huang, Solomon M Hsiang, and Marco Gonzalez-Navarro. Using Satellite Imagery
and Deep Learning to Evaluate the Impact of Anti-Poverty Programs. Working paper, National
Bureau of Economic Research, 2021.
[21] David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris
Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-
Brown, Alexandra Sasha Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik Mukkavilli,
Konrad P. Kording, Carla P. Gomes, Andrew Y . Ng, Demis Hassabis, John C. Platt, Felix
Creutzig, Jennifer Chayes, and Yoshua Bengio. Tackling Climate Change with Machine
Learning. ACM Comput. Surv. , 2022.
11
[22] Alexandre Lacoste, Evan David Sherwin, Hannah Kerner, Hamed Alemohammad, Björn
Lütjens, Jeremy Irvin, David Dao, Alex Chang, Mehmet Gunturkun, Alexandre Drouin, Pau
Rodríguez, and David Vázquez. Toward Foundation Models for Earth Monitoring: Proposal
for a Climate Change Benchmark. In NeurIPS Workshops , 2021.
[23] Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, and
Friedrich Fraundorfer. Deep Learning in Remote Sensing: A Comprehensive Review and List
of Resources. IEEE Geoscience and Remote Sensing Magazine , 2017.
[24] Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell,
and Stefano Ermon. Geography-Aware Self-Supervised Learning. In ICCV , 2021.
[25] Oscar Mañas, Alexandre Lacoste, Xavier Giró-i Nieto, David Vazquez, and Pau Rodríguez.
Seasonal Contrast: Unsupervised Pre-Training From Uncurated Remote Sensing Data. In
ICCV , 2021.
[26] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall
Burke, David B. Lobell, and Stefano Ermon. SatMAE: Pre-training Transformers for Temporal
and Multi-Spectral Satellite Imagery. In NeurIPS , 2022.
[27] Colorado J. Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian Clipp,
Kurt Keutzer, Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scale-MAE: A
Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning. CoRR ,
abs/2212.14532, 2023.
[28] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. Representation Learning with Contrastive
Predictive Coding. CoRR , abs/1807.03748, 2018.
[29] Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning Representations by
Maximizing Mutual Information Across Views. In NeurIPS , 2019.
[30] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A Simple Framework
for Contrastive Learning of Visual Representations. In ICML , 2020.
[31] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
Autoencoders Are Scalable Vision Learners. In CVPR , 2022.
[32] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT Pre-Training of Image
Transformers. In ICLR , 2022.
[33] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. In NAACL , 2019.
[34] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive Multiview Coding. In ECCV ,
2020.
[35] Michael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby,
Sylvain Gelly, and Mario Lucic. Self-Supervised Learning of Video-Induced Visual Invariances.
InCVPR , 2020.
[36] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola.
What Makes for Good Views for Contrastive Learning? In NeurIPS , 2020.
[37] Tete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Darrell. What Should Not Be
Contrastive in Contrastive Learning. In ICLR , 2021.
[38] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum Contrast for
Unsupervised Visual Representation Learning. In CVPR , 2020.
[39] Xinlei Chen and Kaiming He. Exploring Simple Siamese Representation Learning. In CVPR ,
2021.
12
[40] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap Your Own Latent -
A New Approach to Self-Supervised Learning. In NeurIPS , 2020.
[41] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and
Han Hu. SimMIM: A Simple Framework for Masked Image Modeling. In CVPR , 2022.
[42] Baruch Epstein and Ron Meir. Generalization Bounds For Unsupervised and Semi-Supervised
Learning With Autoencoders. CoRR , abs/1902.01449, 2019.
[43] Johannes Lehner, Benedikt Alkin, Andreas Fürst, Elisabeth Rumetshofer, Lukas Miklautz, and
Sepp Hochreiter. Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget.
CoRR , abs/2304.10520, 2023.
[44] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. Contractive
Auto-Encoders: Explicit Invariance during Feature Extraction. In ICML , 2011.
[45] Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav Aggarwal, Aaron
Adcock, Armand Joulin, Piotr Dollár, Christoph Feichtenhofer, Ross Girshick, Rohit Girdhar,
and Ishan Misra. The effectiveness of MAE pre-pretraining for billion-scale pretraining. CoRR ,
abs/2303.13496, 2023.
[46] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and
Saining Xie. ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders.
InCVPR , 2023.
[47] Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What Do
Self-Supervised Vision Transformers Learn? In ICLR , 2023.
[48] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale. In ICLR , 2021.
[49] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui
Wu. CoCa: Contrastive Captioners are Image-Text Foundation Models. Transactions on
Machine Learning Research , 2022.
[50] Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky,
Hilde Kuehne, and James R. Glass. Contrastive Audio-Visual Masked Autoencoder. In ICLR ,
2023.
[51] Haoxuan You, Mandy Guo, Zhecan Wang, Kai-Wei Chang, Jason Baldridge, and Jiahui Yu.
CoBIT: A Contrastive Bi-directional Image-Text Generation Model. CoRR , abs/2303.13455,
2023.
[52] Po-Yao Huang, Vasu Sharma, Hu Xu, Chaitanya Ryali, Haoqi Fan, Yanghao Li, Shang-Wen
Li, Gargi Ghosh, Jitendra Malik, and Christoph Feichtenhofer. MA ViL: Masked Audio-Video
Learners. CoRR , abs/2212.08071, 2022.
[53] Gary A Shaw and Hsiaohua K Burke. Spectral imaging for remote sensing. Lincoln laboratory
journal , 2003.
[54] Alberto Moreira, Pau Prats-Iraola, Marwan Younis, Gerhard Krieger, Irena Hajnsek, and
Konstantinos P Papathanassiou. A tutorial on synthetic aperture radar. IEEE Geoscience and
remote sensing magazine , 2013.
[55] Cle Pohl and John L Van Genderen. Review article multisensor image fusion in remote sensing:
concepts, methods and applications. International journal of remote sensing , 1998.
13
[56] Pedram Ghamisi, Behnood Rasti, Naoto Yokoya, Qunming Wang, Bernhard Hofle, Lorenzo
Bruzzone, Francesca Bovolo, Mingmin Chi, Katharina Anders, Richard Gloaguen, Peter M.
Atkinson, and Jon Atli Benediktsson. Multisource and Multitemporal Data Fusion in Remote
Sensing: A Comprehensive Review of the State of the Art. IEEE Geoscience and Remote
Sensing Magazine , 2019.
[57] Michael Schmitt and Xiao Xiang Zhu. Data Fusion and Remote Sensing: An ever-growing
relationship. IEEE Geoscience and Remote Sensing Magazine , 2016.
[58] Sunan He, Taian Guo, Tao Dai, Ruizhi Qiao, Chen Wu, Xiujun Shu, and Bo Ren. VLMAE:
Vision-Language Masked Autoencoder. CoRR , abs/2208.09374, 2022.
[59] Ziyang Luo, Yadong Xi, Rongsheng Zhang, Gongzheng Li, Zeng Zhao, and Jing Ma. Condi-
tioned Masked Language and Image Modeling for Image-Text Dense Retrieval. In EMNLP ,
2022.
[60] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven
Chu Hong Hoi. Align before Fuse: Vision and Language Representation Learning with
Momentum Distillation. In NeurIPS , 2021.
[61] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul
Chilimbi, and Junzhou Huang. Vision-Language Pre-training with Triple Contrastive Learning.
InCVPR , 2022.
[62] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang,
Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics Aligned Pre-training for
Vision-Language Tasks. In ECCV , 2020.
[63] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping Language-Image
Pre-training for Unified Vision-Language Understanding and Generation. In ICML , 2022.
[64] Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale,
Luowei Zhou, Andrew Dai, Zhifeng Chen, et al. MaMMUT: A Simple Architecture for Joint
Learning for MultiModal Tasks. CoRR , abs/2303.16839, 2023.
[65] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-Grained Vision Language Pre-Training:
Aligning Texts with Visual Concepts. In ICML , 2022.
[66] Ofir Press, Noah Smith, and Mike Lewis. Train Short, Test Long: Attention with Linear Biases
Enables Input Length Extrapolation. In ICLR , 2022.
[67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In NeurIPS , 2017.
[68] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling
Language-Image Pre-training via Masking. In CVPR , 2023.
[69] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable
Visual Models From Natural Language Supervision. In ICML , 2021.
[70] Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad M Albrecht, and
Xiao Xiang Zhu. SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for
Self-Supervised Learning in Earth Observation. CoRR , abs/2211.07044, 2022.
[71] Linus Scheibenreif, Joëlle Hanna, Michael Mommert, and Damian Borth. Self-Supervised
Vision Transformers for Land-Cover Segmentation and Classification. In CVPR Workshops ,
2022.
[72] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,
and Armand Joulin. Emerging Properties in Self-Supervised Vision Transformers. In ICCV ,
2021.
14
[73] Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael
Rabbat, Yann LeCun, and Nicolas Ballas. Self-Supervised Learning from Images with a
Joint-Embedding Predictive Architecture. In CVPR , 2023.
[74] Anthony Fuller, Koreen Millard, and James R. Green. Transfer Learning with Pretrained
Remote Sensing Transformers. CoRR , abs/2209.14969, 2022.
[75] Yi Wang, Conrad M Albrecht, Nassim Ait Ali Braham, Chenying Liu, Zhitong Xiong, and
Xiao Xiang Zhu. DeCUR: decoupling common & unique representations for multimodal
self-supervision. CoRR , abs/2309.05300, 2023.
[76] Gencer Sumbul, Arne de Wall, Tristan Kreuziger, Filipe Marcelino, Hugo Costa, Pedro
Benevides, Mário Caetano, Begüm Demir, and V olker Markl. BigEarthNet-MM: A Large-
Scale, Multimodal, Multilabel Benchmark Archive for Remote Sensing Image Classification
and Retrieval [Software and Data Sets]. IEEE Geoscience and Remote Sensing Magazine ,
2021.
[77] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel
dataset and deep learning benchmark for land use and land cover classification. IEEE Journal
of Selected Topics in Applied Earth Observations and Remote Sensing , 2019.
[78] Amanda A Boatswain Jacques, Abdoulaye Baniré Diallo, and Etienne Lord. Towards the
Creation of a Canadian Land-Use Dataset for Agricultural Land Classification. In 42nd
Canadian Symposium on Remote Sensing: Understanding Our World: Remote Sensing for a
Sustainable Future , 2021.
[79] Matthew Gwilliam and Abhinav Shrivastava. Beyond Supervised vs. Unsupervised: Represen-
tative Benchmarking and Analysis of Image Representation Learning. In CVPR , 2022.
[80] Jae-Hun Lee, Doyoung Yoon, ByeongMoon Ji, Kyungyul Kim, and Sangheum Hwang. Re-
thinking Evaluation Protocols of Visual Representations Learned via Self-supervised Learning.
CoRR , abs/2304.03456, 2023.
[81] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong.
Image BERT Pre-training with Online Tokenizer. In ICLR , 2022.
[82] Colorado J Reed, Xiangyu Yue, Ani Nrusimha, Sayna Ebrahimi, Vivek Vijaykumar, Richard
Mao, Bo Li, Shanghang Zhang, Devin Guillory, Sean Metzger, et al. Self-Supervised Pretrain-
ing Improves Self-Supervised Pretraining. In WACV , 2022.
[83] Klemen Kotar, Gabriel Ilharco, Ludwig Schmidt, Kiana Ehsani, and Roozbeh Mottaghi.
Contrasting Contrastive Self-Supervised Representation Learning Pipelines. In ICCV , 2021.
[84] Samira Abnar, Mostafa Dehghani, Behnam Neyshabur, and Hanie Sedghi. Exploring the
Limits of Large Scale Pre-training. In ICLR , 2022.
[85] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Großberger. UMAP: Uniform
Manifold Approximation and Projection. Journal of Open Source Software , 2018.
[86] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris
Bertsimas. Finding Neurons in a Haystack: Case Studies with Sparse Probing. CoRR ,
abs/2305.01610, 2023.
[87] Naoto Yokoya, Pedram Ghamisi, Ronny Hänsch, and Michael Schmitt. 2020 IEEE GRSS
Data Fusion Contest: Global Land Cover Mapping With Weak Supervision. IEEE Geoscience
and Remote Sensing Magazine , 2020.
[88] Christopher F Brown, Steven P Brumby, Brookie Guzder-Williams, Tanya Birch, Saman-
tha Brooks Hyde, Joseph Mazzariello, Wanda Czerwinski, Valerie J Pasquarella, Robert
Haertel, Simon Ilyushchenko, et al. Dynamic World, Near real-time global 10 m land use land
cover mapping. Scientific Data , 2022.
[89] Katerina Kikaki, Ioannis Kakogeorgiou, Paraskevi Mikeli, Dionysios E Raitsos, and Konstanti-
nos Karantzalos. MARIDA: A benchmark for Marine Debris detection from Sentinel-2 remote
sensing data. PloS one , 2022.
15
[90] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, and Chunhua Shen. Conditional
Positional Encodings for Vision Transformers. In ICLR , 2023.
[91] Peng Gao, Teli Ma, Hongsheng Li, Ziyi Lin, Jifeng Dai, and Yu Qiao. MCMAE: Masked
Convolution Meets Masked Autoencoders. In NeurIPS , 2022.
[92] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-Invariance-Covariance
Regularization for Self-Supervised Learning. In ICLR , 2022.
[93] Adrien Bardes, Jean Ponce, and Yann LeCun. VICRegL: Self-Supervised Learning of Local
Visual Features. In NeurIPS , 2022.
[94] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense Contrastive
Learning for Self-Supervised Visual Pre-Training. In CVPR , 2021.
[95] Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus.
Hard Negative Mixing for Contrastive Learning. In NeurIPS , 2020.
[96] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Herve Jegou. Training data-efficient image transformers & distillation through attention. In
ICML , 2021.
[97] Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient Self-supervised
Learning with Contextualized Target Representations for Vision, Speech and Language. CoRR ,
abs/2212.07525, 2022.
[98] Gengchen Mai, Ni Lao, Yutong He, Jiaming Song, and Stefano Ermon. CSP: Self-Supervised
Contrastive Spatial Pre-Training for Geospatial-Visual Representations. In ICCV , 2023.
[99] Chao Tao, Ji Qi, Weipeng Lu, Hao Wang, and Haifeng Li. Remote Sensing Image Scene Clas-
sification With Self-Supervised Paradigm Under Limited Labeled Samples. IEEE Geoscience
and Remote Sensing Letters , 2022.
[100] Heechul Jung, Yoonju Oh, Seongho Jeong, Chaehyeon Lee, and Taegyun Jeon. Contrastive
Self-Supervised Learning With Smoothed Representation for Remote Sensing. IEEE Geo-
science and Remote Sensing Letters , 2022.
[101] Jian Kang, Ruben Fernandez-Beltran, Puhong Duan, Sicong Liu, and Antonio J. Plaza. Deep
Unsupervised Embedding for Remotely Sensed Images Based on Spatially Augmented Mo-
mentum Contrast. IEEE Transactions on Geoscience and Remote Sensing , 2021.
[102] Yuxing Chen and Lorenzo Bruzzone. Self-Supervised SAR-Optical Data Fusion of Sentinel-
1/-2 Images. IEEE Transactions on Geoscience and Remote Sensing , 2022.
[103] Lucas Kruitwagen. Towards DeepSentinel: An extensible corpus of labelled Sentinel-1
and -2 imagery and a general-purpose sensor-fusion semantic embedding model. CoRR ,
abs/2102.06260, 2021.
[104] Xin Huang, Mengjie Dong, Jiayi Li, and Xian Guo. A 3-D-Swin Transformer-Based Hierarchi-
cal Contrastive Learning Method for Hyperspectral Image Classification. IEEE Transactions
on Geoscience and Remote Sensing , 2022.
[105] Johan Bjorck, Brendan H. Rappazzo, Qinru Shi, Carrie Brown-Lima, Jennifer Dean, Angela
Fuller, and Carla Gomes. Accelerating Ecological Sciences from Above: Spatial Contrastive
Learning for Remote Sensing. In AAAI , 2021.
[106] Haifeng Li, Yi Li, Guo Zhang, Ruoyun Liu, Haozhe Huang, Qing Zhu, and Chao Tao. Global
and Local Contrastive Self-Supervised Learning for Semantic Segmentation of HR Remote
Sensing Images. IEEE Transactions on Geoscience and Remote Sensing , 2022.
[107] Xinye Wanyan, Sachith Seneviratne, Shuchang Shen, and Michael Kirley. DINO-MC: Self-
supervised Contrastive Learning for Remote Sensing Imagery with Multi-sized Local Crops.
CoRR , abs/2303.06670, 2023.
16
[108] Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, and Stefano Ermon.
Tile2Vec: Unsupervised Representation Learning for Spatially Distributed Data. In AAAI ,
2019.
[109] Haozhe Huang, Zhongfeng Mou, Yunying Li, Qiujun Li, Jie Chen, and Haifeng Li. Spatial-
Temporal Invariant Contrastive Learning for Remote Sensing Scene Classification. IEEE
Geoscience and Remote Sensing Letters , 2022.
[110] Vladan Stojnic and Vladimir Risojevic. Self-Supervised Learning of Remote Sensing Scene
Representations Using Contrastive Multiview Coding. In CVPR Workshops , 2021.
[111] Gencer Sumbul, Markus Müller, and Begüm Demir. A Novel Self-Supervised Cross-Modal
Image Retrieval Method in Remote Sensing. In ICIP , 2022.
[112] Isaac Corley and Peyman Najafirad. Supervising Remote Sensing Change Detection Models
With 3d Surface Semantics. In ICIP , 2022.
[113] Keumgang Cha, Junghoon Seo, and Yeji Choi. Contrastive Multiview Coding With Electro-
Optics for SAR Semantic Segmentation. IEEE Geoscience and Remote Sensing Letters ,
2022.
[114] L. Scheibenreif, M. Mommert, and D. Borth. Contrastive Self-supervised Data Fusion for Satel-
lite Imagery. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information
Sciences , 2022.
[115] Umangi Jain, Alex Wilson, and Varun Gulshan. Multimodal contrastive learning for remote
sensing tasks. CoRR , abs/2209.02329, 2022.
[116] Chenfang Liu, Hao Sun, Yanjie Xu, and Gangyao Kuang. Multi-Source Remote Sensing
Pretraining Based on Contrastive Self-Supervised Learning. Remote Sensing , 2022.
[117] S. Vincenzi, A. Porrello, P. Buzzega, M. Cipriano, P. Fronte, R. Cuccu, C. Ippoliti, A. Conte,
and S. Calderara. The color out of space: learning self-supervised representations for Earth
Observation imagery. In ICPR , 2021.
[118] Anthony Fuller, Koreen Millard, and James R. Green. SatViT: Pretraining Transformers for
Earth Observation. IEEE Geoscience and Remote Sensing Letters , 2022.
[119] Xian Sun, Peijin Wang, Wanxuan Lu, Zicong Zhu, Xiaonan Lu, Qibin He, Junxi Li, Xuee
Rong, Zhujun Yang, Hao Chang, Qinglin He, Guang Yang, Ruiping Wang, Jiwen Lu, and Kun
Fu. RingMo: A Remote Sensing Foundation Model with Masked Image Modeling. IEEE
Transactions on Geoscience and Remote Sensing , 2022.
[120] Yuan Gao, Xiaojuan Sun, and Chao Liu. A General Self-Supervised Framework for Remote
Sensing Image Classification. Remote Sensing , 2022.
[121] Hongmiao Wang, Cheng Xing, Junjun Yin, and Jian Yang. Land Cover Classification for
Polarimetric SAR Images Based on Vision Transformer. Remote Sensing , 2022.
[122] Kai Wang, Steven E. Franklin, Xulin Guo, and Marc Cattet. Remote Sensing of Ecology,
Biodiversity and Conservation: A Review from the Perspective of Remote Sensing Specialists.
Sensors , 2010.
[123] Sahel Mahdavi, Bahram Salehi, Jean Granger, Meisam Amani, Brian Brisco, and Weimin
Huang. Remote sensing for wetland classification: A comprehensive review. GIScience &
Remote Sensing , 2018.
[124] Freek D Van der Meer, Harald MA Van der Werff, Frank JA Van Ruitenbeek, Chris A Hecker,
Wim H Bakker, Marleen F Noomen, Mark Van Der Meijde, E John M Carranza, J Boudewijn
De Smeth, and Tsehaie Woldai. Multi-and hyperspectral geologic remote sensing: A review.
International Journal of Applied Earth Observation and Geoinformation , 2012.
[125] John Wilson Rouse, Rüdiger H Haas, John A Schell, Donald W Deering, et al. Monitoring
vegetation systems in the Great Plains with ERTS. NASA Spec. Publ , 1974.
17
[126] E Raymond Hunt, Jr. Relationship between woody biomass and PAR conversion efficiency
for estimating net primary production from NDVI. International Journal of Remote Sensing ,
1994.
[127] Bo-Cai Gao. NDWI—A normalized difference water index for remote sensing of vegetation
liquid water from space. Remote sensing of environment , 1996.
[128] Zhao-Liang Li, Pei Leng, Chenghu Zhou, Kun-Shan Chen, Fang-Cheng Zhou, and Guo-Fei
Shang. Soil moisture retrieval from remote sensing measurements: Current knowledge and
directions for the future. Earth-Science Reviews , 2021.
[129] Compton J Tucker. Remote sensing of leaf water content in the near infrared. Remote sensing
of Environment , 1980.
[130] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili ´c, Daniel Hesslow,
Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. BLOOM:
A 176B-Parameter Open-Access Multilingual Language Model. CoRR , abs/2211.05100, 2022.
[131] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katherine Millican, George van den Driessche, Bogdan Damoc,
Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack William Rae,
and Laurent Sifre. Training Compute-Optimal Large Language Models. In NeurIPS , 2022.
[132] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-
othée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA:
Open and Efficient Foundation Language Models. CoRR , abs/2302.13971, 2023.
[133] Kan Wu, Houwen Peng, Minghao Chen, Jianlong Fu, and Hongyang Chao. Rethinking and
Improving Relative Position Encoding for Vision Transformer. In ICCV , 2021.
[134] David Arthur and Sergei Vassilvitskii. K-Means++: The Advantages of Careful Seeding. In
Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms , 2007.
[135] H. W. Kuhn. The Hungarian method for the assignment problem. Naval Research Logistics
Quarterly , 1955.
18
A Appendix
Code and pretrained models: https://github.com/antofuller/CROMA
A.1 Other Pretraining Experiments
In this section, we experiment with additional CROMA settings. We use the same experimental
conditions as §4.1 of our paper; i.e., we linear probe representations on BigEarthNet [ 76] (reporting
mAP on the combined validation and test sets) and patch encodings on DW-Expert-120 [ 88] (reporting
mIoU on the validation set). We use the linear probing hyper-parameters listed in §A.4.2 of this
Appendix.
Table 8: Linear probing results on radar-only (“R”), optical-only (“O”), and joint radar-optical
(“RO”) inputs. Across all experiments, we use 2D-ALiBi with X-ALiBi, 75% shared masking, ViT-B
backbones, and 100pretraining epochs.
Cross-Modal Cross-Modal Decoder Obj. Weights HN Mixing Classification (mAP) Segmentation (mIoU)
Image Obj. Patch Obj. Depth, Dim λCon,λMAE (1024 ,0,n) Cost R O RO R O RO
InfoNCE MSE 1, 512 1, 1 ✗ 1× 77.4 83.9 84.3 40.5 56.0 56.7
InfoNCE MSE 1, 768 1, 1 ✗ 1× 77.4 83.9 84.3 40.7 56.1 56.6
InfoNCE MSE 3, 512 1, 1 ✗ 1.2× 77.5 83.9 84.4 40.5 56.3 57.1
InfoNCE MSE 3, 768 1, 1 ✗ 1.3× 77.5 83.9 84.4 40.7 56.2 56.6
InfoNCE MSE 6, 512 1, 1 ✗ 1.4× 77.5 83.9 84.4 40.3 56.0 56.7
InfoNCE MSE 6, 768 1, 1 ✗ 1.6× 77.6 83.8 84.5 40.6 56.2 56.7
InfoNCE ✗ 1, 512 1, 1 ✗ 1× 77.4 84.0 84.5 40.8 56.1 56.4
InfoNCE ✗ 1, 768 1, 1 ✗ 1× 77.5 84.2 84.5 40.8 56.1 56.2
InfoNCE ✗ 3, 512 1, 1 ✗ 1.2× 77.6 84.1 84.5 40.8 56.2 56.7
InfoNCE ✗ 3, 768 1, 1 ✗ 1.3× 77.0 83.9 84.5 40.6 56.1 56.5
InfoNCE ✗ 6, 512 1, 1 ✗ 1.4× 77.3 84.1 84.5 40.8 56.1 56.5
InfoNCE ✗ 6, 768 1, 1 ✗ 1.6× 77.5 84.1 84.6 40.6 56.5 56.8
InfoNCE InfoNCE 3, 512 1, 1 ✗ 2.2× 72.8 80.9 82.4 39.0 55.1 55.2
InfoNCE ✗ 1, 512 1, 2 ✗ 1× 77.5 84.3 84.2 40.7 55.9 56.2
InfoNCE ✗ 1, 512 1, 4 ✗ 1× 77.5 84.3 84.1 40.6 55.4 56.0
InfoNCE ✗ 1, 512 2, 1 ✗ 1× 77.5 84.1 84.5 40.4 55.9 56.3
InfoNCE ✗ 1, 512 4, 1 ✗ 1× 77.6 83.9 84.5 40.7 55.8 56.8
InfoNCE ✗ 1, 512 1, 1 128 1 × 73.6 81.6 83.0 38.0 53.2 55.0
InfoNCE ✗ 1, 512 1, 1 256 1 × 73.0 81.0 82.8 37.8 52.9 54.7
InfoNCE ✗ 1, 512 1, 1 512 1 × 72.5 80.2 82.4 37.6 52.6 54.4
VICReg MSE 1, 768 1, 1 ✗ 1.1× 70.7 78.7 83.3 40.0 55.5 55.1
Self-supervised Objectives. Inspired by the local objective of VICRegL [ 93], we experiment with a
mean squared error (MSE) objective between cross-modal patch encodings, i.e., Llocal=MSE (ER,EO).
This attracts patch encodings if they match locations, i.e., if they represent the same 80 m × 80 m
square on the ground. We find this does not improve representations. Next, we experiment with
the VICReg [ 92] objective (calculating VICReg statistics based on a batch size of 800) between
cross-modal image representations, i.e., RRandRO; we find it underperforms InfoNCE [ 28]. Finally,
we experiment with the InfoNCE objective between cross-modal patch encodings; positive pairs
are encodings that match locations across modalities, and negative pairs are all other encodings
from the matched sample and encodings from all other samples in the batch. This does not improve
representations and slows pretraining by 2.2×(Table 8).
Objective Weights. We find that weighting the contrastive loss term or MAE [ 31] loss term does not
uniformly improve representations; hence, we select equal weights.
Hard Negatives. We find that hard-negative mixing [ 95] (N=1024 ,s=0,s′=n,β=0.5, with nof128,
256, or512) degrades performance when used in our framework.
Decoder Sizes. At least in these experiments, CROMA is not sensitive to the decoder size; a
tiny decoder with a 1-layer, 512-d transformer performs similarly to a much larger 6-layer, 768-d
transformer.
19
Table 9: Linear probing results with shared
75% masking, ViT-B, 100epochs.
Classification Segmentation
Method mAP mIoU
R O RO R O RO
PEG [90] 67.9 75.9 79.0 32.6 49.8 51.0
2D-Sin. 69.4 75.6 79.8 29.0 44.1 50.7Position Encoding with Shared Masking. We find
that using 2D-sinusoidal embeddings or PEG [ 90]
with shared masking performs poorly. These two
methods of position encoding store positional infor-
mation in the internal representations, which can
help solve the contrastive objective if both modal-
ities share masks; 2D-ALiBi instead stores positional
information in the attention matrix, which may pre-
vent this from occurring. In our paper (Table 6), we show that 2D-sinusoidal or PEG can perform
well in our framework if modalities are masked independently, although 2D-ALiBi still outperforms
these approaches.
Table 10: Lower masked tuning for 5 epochs
after pretraining CROMA-L.
Classification Segmentation
Mask Ratio mAP mIoU
R O RO R O RO
10% 80.8 84.7 84.7 43.8 56.8 56.6
25% 80.8 84.7 84.8 43.9 56.8 56.6
50% 80.8 84.8 85.0 43.9 56.8 56.6Lower Masked Tuning. FLIP [ 68] performs con-
trastive learning using the representations of masked-
out samples; after this masked pretraining, it lever-
ages unmasked tuning to increase accuracy by 1.3%
on zero-shot ImageNet-1K. Unmasked tuning contin-
ues FLIP pretraining by performing contrastive learn-
ing using the representations of unmasked samples to
reduce the distribution gap between pretraining and
inference [ 68]. We cannot perform fully unmasked
tuning because we must mask patches for our reconstruction objective. However, we can lower
our mask ratio and perform lower masked tuning. Following FLIP, initializing parameters with our
pretrained CROMA-L model, we train for 5 additional epochs using a base learning rate of 8e-8,
warmup over the first epoch, and cooldown for 4 epochs using a cosine decay schedule. We explore
mask ratios {10%,25%,50%}and find that lower masked tuning does not improve linear probing
accuracy for CROMA (Table 10).
A.2 Two Reasons for 2D-ALiBi’s Performance
Our primary reason for introducing 2D-ALiBi is to enable the test-time extrapolation demonstrated
in §4.2. But even when no extrapolation occurs—training and testing on the same image size—2D-
ALiBi outperforms both the most commonly used position encoding method (2D-sinusoidal) and a
SoTA relative position encoding method (PEG [ 90]). We believe 2D-ALiBi outperforms these two
methods for two reasons.
Table 11: Cosine similarity between the representations of images and the representations of trans-
formed versions of the same images. Higher similarity means the representations are less influenced
by the image transformation.
Transformation (cosine similarity)
Method H-Flip V-Flip rotate(90°) rotate(180°) rotate(270°)
2D-ALiBi + X-ALiBi 0.992 0.992 0.992 0.992 0.992
2D-ALiBi 0.992 0.992 0.992 0.992 0.992
PEG [90] 0.992 0.992 0.988 0.992 0.988
2D-sinusoidal 0.641 0.629 0.523 0.559 0.523
12D-ALiBi learns image representations invariant to rotating and flipping . Learning represen-
tations that are invariant to certain transformations is often desirable. In CROMA, the contrastive
objective between optical and radar data encourages sensor-invariant representations. Invariances
to flipping and rotating are desirable properties of the representations of satellite imagery. How-
ever, CROMA’s pretraining objectives do not explicitly encourage these invariances. To investigate
if CROMA’s optical representations (i.e., RO) are invariant to flipping and rotating, we produce
representations of 5,000 optical images from DW-Expert-120 for four position encoding strategies—
2D-ALiBi, 2D-ALiBi with X-ALiBi, PEG, and 2D-sinusoidal. Then, for each model, we produce
representations of the same samples but transformed. We measure the cosine similarity between the
original and transformed images’ representations. When used in our CROMA framework, 2D-ALiBi
learns representations invariant to flipping and rotating (average cosine similarity of 0.992, Table
11). Notably, 2D-sinusoidal learns representations that are not invariant to flipping and rotating
20
(average cosine similarity of 0.575, Table 11); we believe this contributes to the poor performance of
2D-sinusoidal embeddings on image classification. PEG performs similarly to 2D-ALiBi on image
classification and also learns representations invariant to flipping and rotating.
Table 12: (Middle column) Cosine similarity between patch encodings at different locations within
an image, averaged across 5,000 images. (Right column) Cross entropy loss of an MLP probe trained
to predict patch locations given patch encodings.
Method Patch Encoding Similarity Patch Encoding Position Probing
(cosine similarity) (cross entropy loss)
2D-ALiBi + X-ALiBi 0.546 4.43
2D-ALiBi 0.582 4.41
PEG [90] 0.701 4.13
2D-sinusoidal 0.493 0.00
22D-ALiBi learns patch representations that retain local information . [47] show that models
trained with contrastive objectives can lose significant local information at deeper ViT layers that
harm performance on dense prediction tasks. They show that the cosine similarity between the
representations of patches at different locations within an image becomes high, indicating a patch-
wise representational collapse. At every ViT layer, 2D-ALiBi injects a local bias in each attention
head, for each patch location. To investigate if 2D-ALiBi can successfully prevent this collapse
from occurring, we measure the cosine similarity between different patch encodings of the same
sample and take the average across 5,000 optical images. We find that 2D-ALiBi learns to represent
patches with greater spatial diversity than PEG, and X-ALiBi further improves diversity (Table 12).
Interestingly, 2D-sinusoidal learns the most diverse patch representations. We also train MLP probes
on patch encodings to classify the patch location; this measures the amount of positional information
represented in patch encodings. We find that the patch encodings of models that use 2D-sinusoidal
embeddings fully specify the location of the patch within the image (Table 12); this is an undesirable
property of patch encodings, which should represent the content of the patch, rather than its location.
A.3 Pretraining Details
A.3.1 Data
We use the SSL4EO dataset [ 70], which consists of Sentinel-1 & 2 imagery acquired at 250K locations
around the world; each location (a 2.64 km × 2.64 km square) is imaged four times, spread out over a
year. We use these 1million samples of 264 × 264 pixels for pretraining. Please see the SSL4EO
paper [70] for more details.
A.3.2 Implementation
We use an NVIDIA DGX server (8 ×A100-80GB), the maximum batch size that can fit into 640GB
of VRAM ( 7,200 for our default ViT-B), bfloat16 precision, a base learning rate of 4e-6, warmup for
5% of the total epochs, and cooldown via a cosine decay schedule. We use the same normalization
procedure as SatMAE [ 26]. For data augmentation, we randomly crop 60-180pixel squares from
the original 264 × 264 pixels and resize the crops to 120 × 120 pixels (our default image size). We
also perform vertical and horizontal flipping, 90-degree rotations, and mixup= 0.3. Crucially, we
apply these transformations identically to both modalities; if we applied them to each modality
independently, our spatial alignment would break. We use the AdamW optimizer with β1=0.9and
β2=0.999, and a weight decay of 0.01.
A.4 Evaluation Details
The evaluation of foundation models for Earth Observation is less mature than in other fields. We
do our best to re-use the experimental conditions of the SoTA, i.e., SatMAE [ 26], and improve
upon them where possible. One such condition is to report results from a held-out validation set;
precisely, the best validation performance measured after each finetuning epoch is reported. No
test sets are used. To enable fair comparisons with prior work, we copy this approach. In trying to
improve the evaluation of foundation models for Earth Observation, we detail our approach in this
Appendix, share code and preprocessed datasets, re-evaluate all near-SoTA models under identical
21
conditions, and evaluate models in more ways than prior work (i.e., linear and nonlinear probing,
kNN classification, and K-means clustering).
We initialize parameters from publicly shared pretrained weights, evaluating all models ourselves
under identical conditions. Although this process is laborious, we believe it significantly improves
the value of our paper; several prior studies have often evaluated their models in different ways, using
different data splits that cannot be directly compared. When downloading pretrained weights, we use
the latest weights that are publicly available. For instance, SatMAE [ 26] released improved versions
of their multispectral ViT-B and ViT-L models, pretrained for 200epochs, after their manuscript
was accepted for publication (edited on arxiv on January 15th,2023 ). We exclusively evaluate these
improved models throughout our paper, ensuring we compare CROMA to the best models available.
For multispectral benchmarks with 13channels (with cirrus included), we simply drop the cirrus
band for models pretrained without it.
A.4.1 Data
BigEarthNet. [76] We use the same splits for training ( 10% of the complete training set) and
evaluating (the entire validation set) as SatMAE [ 26] and SeCo [ 25]. However, we use the combined
validation and test sets ( 236,130 samples) in our ablation studies to increase the reliability of our
findings with minimal added cost. Images are 120 × 120 pixels.
fMoW-Sentinel. [26] Inspired by how the BigEarthNet benchmark is used (i.e., training on 10% of
the complete training set of 354,200 samples), we create a 10% split of the complete fMoW-Sentinel
training set of 712,874 samples. We share the IDs of the 10% of fMoW-Sentinel training samples that
we randomly selected. We believe this smaller training set should be used in future work to reduce
the costs of hyper-parameter searches—a single finetuning run of SatMAE on the complete training
set requires 192hours on a V100 GPU [ 26]. Following SatMAE, we use the full validation set for
evaluation. Images vary in size; the mean height is 45 pixels, and the mean width is 60 pixels.
Table 13: fMoW-Sentinel results (top 1 ac-
curacy) using the complete training set. *
denotes results reported in SatMAE (updated
onarxiv on January 15th,2023 ).
Method Backbone Finetuning Linear Probing
SatMAE ViT-B 62.65* 37.40
CROMA ViT-B 61.00 40.94
SatMAE ViT-L 63.84* 39.19
CROMA ViT-L 63.59 41.96In our paper, we benchmark this new split. However,
we report results obtained by our CROMA models on
the complete training set in Table 13. Due to the costs
of finetuning on the complete training set ( 712,874
samples), we decide to allocate our resources else-
where and notperform any hyper-parameter tuning.
Instead, we select hyper-parameters we believe to be
reasonable and finetune CROMA-B and CROMA-L
once. For finetuning, we use a base learning rate of
1e-5and all other hyper-parameters from §A.4.2.
EuroSAT. [77] We use the same training and validation sets as SatMAE. Images are 64 × 64 pixels.
Canadian Cropland. [78] We are the first to benchmark this dataset of Canadian agricultural
croplands, consisting of 10classes (barley, canola, corn, mixedwood, oats, orchard, pasture, potato,
soybean, and spring wheat). We select this dataset because it is a large dataset that evaluates different
capabilities from the other benchmarks that typically consider croplands as a single class. Following
EuroSAT [ 77], the authors selected an image size of 64 × 64 pixels [ 78]; therefore, models evaluated
on EuroSAT can be evaluated on Canadian Cropland with minimal modifications. We use the training
set and combine their validation and test sets to form a single held-out set for evaluation. We
share these complete training and validation sets. The performance (Table 1) and representation
visualizations (Fig. 6 and 7 in this Appendix) indicate that the 10classes present in this dataset are
challenging to separate.
DFC2020. [87] This dataset is used for evaluation in diverse ways—both the choice of data split and
image size. The original dataset comprises 6,114 samples of 256 × 256 pixels. These samples are
typically split into two: a so-called “validation set” of 986samples and a so-called “test set” of 5,128
samples. Some studies use the “validation set” for training and the “test set” for validation; others use
the “test set” for training and the “validation set” for validation. Some studies use the full 256 × 256
pixels as inputs to their models, while others use smaller inputs. We select the split of 5,128 samples
for training, which we divide into 46,152 images of 96 × 96 pixels—leaving us with the split of 986
samples for validation, which we divide into 8,874 images of 96 × 96 pixels. We select this final
22
resolution because it is the default image size of SatMAE, enabling a fair comparison to the SoTA.
We share these complete training and validation sets.
DW-Expert. [88] The data collected by Dynamic World [ 88] is a new high-quality dataset annotated
by experts with the help of auxiliary information. Thus, it should be used in the future when
benchmarking models. Our work uses the expertly annotated data from Dynamic World, which we
split into 20,422 train samples and 51,022 validation samples. All images are 96 × 96 pixels to enable
a fair comparison with SatMAE. We share these complete training and validation sets. We also create
a version of this dataset that consists of 120 × 120 pixel images (i.e., DW-Expert-120) that we only
use for ablations because it is the default image size of CROMA.
MARIDA. [89] We use the training set and combine the validation and test sets to form a single held-
out set for evaluation. Following our approach for DFC2020 and DW-Expert, we divide the original
images into images of 96 × 96 pixels. Because it is a sparsely labeled dataset (i.e., only a fraction
of pixels per image are labeled), we include images with at least one labeled pixel. We select this
dataset because it evaluates different capabilities from the other semantic segmentation benchmarks.
It consists of the following classes: marine debris, dense Sargassum , sparse Sargassum , natural
organic material, ship, clouds, marine water, sediment-laden water, foam, turbid water, shallow water,
waves, cloud shadows, wakes, and mixed water. We share these complete training and validation sets.
A.4.2 Implementation
Finetuning. We select reasonable hyper-parameters that we use for all models and datasets unless
otherwise stated and sweep across learning rates. This learning rate sweep is essential to creating
fair evaluation conditions across models since each model is given the same search budget (in terms
of finetuning runs, not compute hours), and different models have different optimal learning rates.
Models pretrained with reconstruction approaches tend to require higher base learning rates during
finetuning than models pretrained with contrastive learning. For instance, MAE [ 31] lists a base
learning rate of 1e-3, FLIP [ 68] lists a base learning rate of 5e-5, CoCa [ 49] lists base learning rates
from 1e-5to5e-4, depending on the downstream dataset.
No single learning rate would enable a fair comparison across all models and datasets. Therefore, we
sweep learning rates across an extensive range { 3e-5,5e-5,8e-5,1e-4,3e-4,5e-4,8e-4,1e-3} and
report the best single evaluation result obtained for each dataset; this sweep is performed for CROMA
models and all other models. We convert these base learning rates to actual learning rates via the
widely used linear scaling rule: lr=base _lr×batch _size/ 256. We use the largest batch size
that can fit on an A100-40GB GPU (using bfloat16 precision), the AdamW optimizer with β1=0.9,
β2=0.999, and a weight decay of 0.01. We warmup for 5epochs and cooldown for 30epochs using
a cosine decay schedule (other than EuroSAT, which we cooldown for 150 epochs); this follows
SatMAE [ 26]. For classification tasks, we use mixup= 0.8, cutmix= 1.0, switch probability= 0.5, label
smoothing= 0.1, vertical and horizontal flipping, and 90-degree rotations. We enlarge images to the
default image size of the model we are finetuning (i.e., the image size on which the model was
pretrained), with one exception. The default image size of SatMAE is 96 × 96 ; however, BigEarthNet
images are 120 × 120 [76], requiring that we either crop BigEarthNet samples (losing information) or
adapt SatMAE to larger images. We achieve better performance by adapting SatMAE to 120 × 120
images via the widely used position embedding interpolation algorithm than cropping BigEarthNet
samples down to 96 × 96 . This allowed us to achieve an mAP of 86.18for SatMAE, a significant
improvement over the 82.62reported in the SatMAE paper. All other datasets use images of 96 × 96 ,
or smaller—thus, there is no reason to use this technique for other datasets.
Linear and Nonlinear Probing. We encode each image without data augmentation then train linear
and nonlinear probes on the frozen representations. Since each model only encodes each image
once, we can sweep through a large range of learning rates ({ 1,2,3,4,5,6,7,8,9}e{-4, -3, -2})
very quickly. Unlike finetuning, we do not evaluate probes after every epoch; instead, we evaluate
trained probes after all epochs are complete. We use a batch size of 1024 , bfloat16 precision, the
AdamW optimizer with β1=0.9,β2=0.999, and a weight decay of 0.01. We warmup for 5epochs
and cooldown for 100epochs using a cosine decay schedule.
Non-parametric kNN and K-means. ForkNN, we use the implementation from [ 27]. This consists
of encoding all training and validation samples and then using the representations of validation
samples as queries and training samples as keys to fetch training labels. These fetched training
labels are used to classify validation samples. We use k=20, other values for k(i.e., 10, 50) ranked
23
Table 14: CROMA vs SatMAE training and inference throughput on an A100-40GB GPU.
Model Backbone Image Size Train Imgs/s Inference Imgs/s
SatMAE ViT-B 96 ×96 249.3 692.5
CROMA ViT-B 96×96 1,079.3 2,957.7
CROMA ViT-B 120×120 555.0 1,532.1
SatMAE ViT-L 96 ×96 84.2 263.2
CROMA ViT-L 96×96 389.1 1,168.2
CROMA ViT-L 120×120 209.6 640.3
models in the same order as k=20. ForK-means, we use the implementation from [ 79]. This consists
of encoding all training and validation samples, then clustering training samples with K-means
(K-means++ [ 134] initialization run 10times). Then, we assign validation samples to clusters and
assign clusters to classes via the Hungarian matching algorithm [135].
Sparse Probing. For each model and dataset, we rank the dimensions of representations Rusing the
mean difference between classes—[ 86] shows that the mean difference performs on-par with more
complex ranking methods and is simple to implement. Specifically, this is our procedure to sparsely
probe the representations of CROMA for BigEarthNet’s “beaches, dunes, sands” class: (i) compute
the average representation, RBeaches
CROMA ∈R768, of all samples in the BigEarthNet training set that
contain the class “beaches, dunes, sands”; (ii) compute the average representation, RNoBeaches
CROMA ∈R768,
of all samples in BigEarthNet that do not contain the class “beaches, dunes, sands”; (iii) compute
the difference between these averaged representations, Rdiff∈R768; (iv) rank all 768dimensions in
Rdiffby the absolute value, i.e., the dimension with the greatest absolute difference between classes is
ranked first; (v) train a separate linear probe to perform binary classification on the top kdimensions,
we sweep many values of kbetween 1and768; (vi) using these trained probes, evaluate them on the
BigEarthNet validation set. Again, this procedure is performed for all three ViT-B models (CROMA,
SatMAE, and I-JEPA), all classification datasets (BigEarthNet, fMoW-Sentinel, Canadian Cropland,
and EuroSAT), and all classes—these plots are displayed at the end of this Appendix §A.6. We also
sparsely probe radar-only, RR, and radar-optical, RRO, representations for BigEarthNet, since we
have access to Sentinel-1 samples.
SatMAE Specifics. SatMAE [ 26] divides spectral bands into three groups and outputs patch encod-
ings for every group; thus, SatMAE outputs three patch encodings per patch location. To be as fair as
possible to SatMAE, we explore four ways of merging these co-located patch encodings to perform
segmentation: unnormalized spatial concatenation, normalized spatial concatenation, unnormalized
spatial pooling, and normalized spatial pooling. We find unnormalized spatial concatenation (i.e.,
concatenating the patch encodings of co-located patches before the LayerNorm) performed best.
Thus, we use the unnormalized spatially concatenated patch encodings for all segmentation datasets.
Conversely, CROMA does not divide spectral bands into groups—resulting in 3×shorter sequence
lengths. The computation required to process a sequence of tokens with a transformer increases
with increasing sequence lengths. This makes CROMA much more computationally efficient than
SatMAE for a given ViT backbone and image size (Table 14). We also pretrain a SatMAE-B model
for300epochs on the SSL4EO dataset. However, this model performs poorly, so we do not report
these results; this experiment indicates that SatMAE’s hyper-parameters may not transfer well to
different pretraining datasets.
A.5 Societal Impact
Since we pretrain our models on the SSL4EO dataset [ 70], our models may be biased towards the
distribution from which SSL4EO data were sampled. Although SSL4EO samples are geographically
diverse (please see Fig. 2 from the SSL4EO paper [ 70]), locations are sampled from areas surrounding
human settlements. As a result, large geographic areas that are sparsely populated—for instance, the
Amazon rainforest, the Sahara desert, and the Australian outback—are underrepresented. This could
negatively impact the quality of representations in these locations and any decisions made on their
basis.
Another distribution shift—this time, between finetuning and inference—is our primary concern. For
example, finetuning a model on the imagery of one geography and then making predictions on the
imagery of another geography creates a distribution shift. As a result, biases from the finetuning
geography may be realized in the predictions made by the finetuned model. This is particularly
24
problematic when these predictions are used in decision-making, for instance, allocating poverty
assistance. However, it is well-demonstrated that pretrained models are more robust to distribution
shifts than models trained from scratch. Additionally, as we develop better foundation models for
Earth Observation, we reduce the need for annotated data; this may allow practitioners to be more
selective of the data they wish to leverage during finetuning.
We do not expect our pretrained models to be particularly valuable for military applications, as
militaries likely have access to higher resolutions (spatially, spectrally, and temporally) than Sentinel-
1 & 2 provide. However, our framework may be leveraged to pretrain models on higher-resolution
imagery, which could be useful for military applications, although this is a risk of all novel learning
algorithms.
A.5.1 Compute
We approximate the computational resources we use for pretraining and finetuning (frozen represen-
tation evaluations are negligible in comparison). For pretraining, estimates are in A100-80GB GPU
hours; for finetuning, estimates are in A100-40GB GPU hours. Please see Table 15.
Table 15: Estimated GPU hours used for developing and validating CROMA.
Method Backbone Task GPU Hours
radar↔optical [71] ResNet50 Classification Finetuning 10
radar↔optical [71] Swin-T Classification Finetuning 25
MAE [31, 70] ViT-S Classification Finetuning 20
DINO [72, 70] ViT-S Classification Finetuning 20
SatMAE [26] ViT-B Classification Finetuning 75
CROMA ViT-B Classification Finetuning 35
SatMAE [26] ViT-L Classification Finetuning 215
CROMA ViT-L Classification Finetuning 90
CROMA ViT-B Pretraining 300 epochs 80
CROMA ViT-L Pretraining 600 epochs 380
CROMA ViT-B Pretraining Ablations 1,100
A.6 Visualizations
We visualize representations and patch encodings using UMAP and t-SNE. For both segmentation
datasets (DFC2020 [ 87] and DW-Expert [ 88]), we visualize patch encodings of 50,000 randomly
sampled patches and use the most dominant class in a patch as its label. We also plot sparse probing
results for binary classifiers trained on the top krepresentation dimensions.
CROMA (ViT-B) SatMAE (ViT-B) DINO (ViT-S) radar ↔optical (Swin-T)
Permanent Crop Residential Industrial Highway Pasture
Sea & Lake Forest River Annual Crop Herbaceous Vegetation
Figure 5: t-SNE plots of EuroSAT [77] representations.
25
CROMA (ViT-B) SatMAE (ViT-B) DINO (ViT-S) radar ↔optical (Swin-T)
Barley Canola Corn Mixedwood Oat
Orchard Pasture Potato Soybean Spring Wheat
Figure 6: UMAP plots of Canadian Cropland [78] representations.
CROMA (ViT-B) SatMAE (ViT-B) DINO (ViT-S) radar ↔optical (Swin-T)
Barley Canola Corn Mixedwood Oat
Orchard Pasture Potato Soybean Spring Wheat
Figure 7: t-SNE plots of Canadian Cropland [78] representations.
CROMA-B (UMAP) CROMA-B (t-SNE) SatMAE-B (UMAP) SatMAE-B (t-SNE)
Water Trees Grass Flooded vegetation Crops
Shrub & Scrub Built-up Barren Snow & Ice
Figure 8: UMAP and t-SNE plots of DW-Expert [88] patch encodings.
CROMA-B (UMAP) CROMA-B (t-SNE) SatMAE-B (UMAP) SatMAE-B (t-SNE)
Forest Shrubland Grassland Wetlands Croplands
Built-up Barren Water
Figure 9: UMAP and t-SNE plots on DFC2020 [87] patch encodings.
26
1001011021030.40.50.60.7
kF1 ScoreUrban fabric
1001011021030.10.20.3
kF1 ScoreIndustrial or
commercial units
1001011021030.60.70.8
kF1 ScoreArable land
1001011021030.20.30.4
kF1 ScorePermanent crops
1001011021030.40.50.60.7
kF1 ScorePastures
1001011021030.50.6
kF1 ScoreComplex cultivation
patterns
1001011021030.50.6
kF1 ScoreAgriculture with
natural vegetation
1001011021030.40.6
kF1 ScoreAgro-forestry areas
1001011021030.50.60.7
kF1 ScoreBroad-leaved forest
1001011021030.60.70.8
kF1 ScoreConiferous forest
1001011021030.60.70.8
kF1 ScoreMixed forest
1001011021030.10.20.3
kF1 ScoreNatural grassland
and sparsely vegetated areas
1001011021030.20.4
kF1 ScoreMoors, heathland
and sclerophyllous vegetation
1001011021030.50.550.60.65
kF1 ScoreTransitional
woodland/shrub
10010110210300.20.4
kF1 ScoreBeaches, dunes,
sands
1001011021030.20.30.4
kF1 ScoreInland wetlands
10010110210300.10.20.3
kF1 ScoreCoastal wetlands
1001011021030.50.60.70.8
kF1 ScoreInland waters
1001011021030.70.80.9
kF1 ScoreMarine waters
CROMA (radar)
CROMA (optical)
CROMA (radar-optical)
SatMAE
I-JEPA
Figure 10: Sparse Probing all classes in BigEarthNet
27
1001011021030.150.20.25
kF1 ScoreBarley
1001011021030.40.50.6
kF1 ScoreCanola
1001011021030.40.50.6
kF1 ScoreCorn
1001011021030.40.60.8
kF1 ScoreMixedwood
1001011021030.150.20.250.3
kF1 ScoreOat
1001011021030.40.60.81
kF1 ScoreOrchard
1001011021030.30.40.5
kF1 ScorePasture
1001011021030.20.30.4
kF1 ScorePotato
1001011021030.30.4
kF1 ScoreSoybean
1001011021030.250.30.350.4
kF1 ScoreSpring Wheat
CROMA
SatMAE
I-JEPA
Figure 11: Sparse Probing all classes in Canadian Cropland
28
1001011021030.40.60.8
kF1 ScorePermanent crop
1001011021030.40.60.81
kF1 ScoreResidential
1001011021030.60.81
kF1 ScoreIndustrial
1001011021030.20.40.60.8
kF1 ScoreHighway
1001011021030.40.60.81
kF1 ScorePasture
1001011021030.850.90.951
kF1 ScoreSea or Lake
1001011021030.60.81
kF1 ScoreForest
1001011021030.40.60.81
kF1 ScoreRiver
1001011021030.60.81
kF1 ScoreAnnual crop
1001011021030.40.60.81
kF1 ScoreHerbaceous vegetation
CROMA
SatMAE
I-JEPA
Figure 12: Sparse Probing all classes in EuroSAT
29
10010110210300.20.40.60.8
kF1 ScoreAirport
1001011021030.050.10.15
kF1 ScoreAirport hangar
1001011021030.10.2
kF1 ScoreAirport terminal
1001011021030.10.20.30.4
kF1 ScoreAmusement park
10010110210300.20.40.6
kF1 ScoreAquaculture
1001011021030.040.060.08
kF1 ScoreArchaeological site
1001011021030.10.15
kF1 ScoreBarn
1001011021030.020.040.06
kF1 ScoreBorder checkpoint
1001011021030.050.060.07
kF1 ScoreBurial site
1001011021030.050.10.15
kF1 ScoreCar dealership
1001011021030.020.030.040.05
kF1 ScoreConstruction site
1001011021030.30.40.50.6
kF1 ScoreCrop field
1001011021030.10.20.3
kF1 ScoreDam
1001011021030.020.04
kF1 ScoreDebris or rubble
1001011021030.080.10.12
kF1 ScoreEducational institution
1001011021030.040.050.060.07
kF1 ScoreElectric substation
1001011021030.040.06
kF1 ScoreFactory or powerplant
1001011021030.050.060.070.08
kF1 ScoreFire station
1001011021030.050.10.150.2
kF1 ScoreFlooded road
CROMA
SatMAE
I-JEPA
Figure 13: Sparse Probing all classes in fMoW-Sentinel (part i)
30
1001011021030.040.060.08
kF1 ScoreFountain
1001011021030.040.050.060.07
kF1 ScoreGas station
1001011021030.10.2
kF1 ScoreGolf course
1001011021030.080.10.12
kF1 ScoreGround transportation station
1001011021030.030.040.050.06
kF1 ScoreHelipad
1001011021030.040.050.060.07
kF1 ScoreHospital
10010110210300.20.40.6
kF1 ScoreImpoverished settlement
1001011021030.10.2
kF1 ScoreInterchange
1001011021030.050.1
kF1 ScoreLake or pond
1001011021030.10.20.3
kF1 ScoreLighthouse
1001011021030.10.150.2
kF1 ScoreMilitary facility
1001011021030.10.150.2
kF1 ScoreMulti-unit residential
10010110210300.20.40.6
kF1 ScoreNuclear powerplant
1001011021030.040.050.06
kF1 ScoreOffice building
1001011021030.10.150.20.25
kF1 ScoreOil or gas facility
1001011021030.040.06
kF1 ScorePark
1001011021030.10.120.14
kF1 ScoreParking lot or garage
1001011021030.150.20.25
kF1 ScorePlace of worship
1001011021030.050.060.07
kF1 ScorePolice station
CROMA
SatMAE
I-JEPA
Figure 14: Sparse Probing all classes in fMoW-Sentinel (part ii)
31
1001011021030.20.40.60.8
kF1 ScorePort
1001011021030.040.06
kF1 ScorePrison
1001011021030.040.06
kF1 ScoreRace track
1001011021030.050.1
kF1 ScoreRailway bridge
1001011021030.20.30.4
kF1 ScoreRecreational facility
1001011021030.040.060.080.10.12
kF1 ScoreRoad bridge
1001011021030.20.40.6
kF1 ScoreRunway
10010110210300.20.4
kF1 ScoreShipyard
1001011021030.050.10.15
kF1 ScoreShopping mall
1001011021030.150.20.250.3
kF1 ScoreSingle-unit residential
1001011021030.040.060.08
kF1 ScoreSmokestack
1001011021030.10.20.3
kF1 ScoreSolar farm
10010110210300.10.20.30.4
kF1 ScoreSpace facility
1001011021030.10.2
kF1 ScoreStadium
1001011021030.040.060.080.1
kF1 ScoreStorage tank
1001011021030.050.10.150.2
kF1 ScoreSurface mine
1001011021030.050.10.15
kF1 ScoreSwimming pool
1001011021030.050.10.15
kF1 ScoreToll booth
1001011021030.060.08
kF1 ScoreTower
CROMA
SatMAE
I-JEPA
Figure 15: Sparse Probing all classes in fMoW-Sentinel (part iii)
32
1001011021030.060.08
kF1 ScoreTower
1001011021030.20.4
kF1 ScoreTunnel opening
1001011021030.040.06
kF1 ScoreWaste disposal
1001011021030.060.080.10.12
kF1 ScoreWater treatment facility
1001011021030.10.20.30.4
kF1 ScoreWind farm
1001011021030.050.1
kF1 ScoreZoo
CROMA
SatMAE
I-JEPA
Figure 16: Sparse Probing all classes in fMoW-Sentinel (part iv)
33
1. Introduction
1.1. The Need for Data Labeling Tools for Earth Surface Processes Research
Automation of data-intensive tasks is increasingly important in Earth surface-processes research. Due to 
the availability of data at greater spatial and temporal coverages and resolutions (Farr et  al.,  2007; Gorelick 
et al.,  2017; Wulder et al.,  2019), and open-source geo-analytics tools (Richardson et al.,  2018; Schwanghart 
& Scherler,  2014), it is increasingly possible to automate the discovery of patterns in processes operating over 
complex landscapes (Larsen et al.,  2021; Walker et al.,  2017). Scoping feasible applications of analytical tools Abstract  Segmentation, or the classification of pixels (grid cells) in imagery, is ubiquitously applied 
in the natural sciences. Manual methods are often prohibitively time-consuming, especially those images 
consisting of small objects and/or significant spatial heterogeneity of colors or textures. Labeling complicated 
regions of transition that in Earth surface imagery are represented by collections of mixed-pixels, -textures, 
and -spectral signatures, can be especially error-prone because it is difficult to reliably unmix, identify and 
delineate consistently. However, the success of supervised machine learning (ML) approaches is entirely 
dependent on good label data. We describe a fast, semi-automated, method for interactive segmentation of 
N-dimensional (x, y, N) images into two-dimensional (x, y) label images. It uses human-in-the-loop ML to 
achieve consensus between the labeler and a model in an iterative workflow. The technique is reproducible; the 
sequence of decisions made by human labeler and ML algorithms can be encoded to file, so the entire process 
can be played back and new outputs generated with alternative decisions and/or algorithms. We illustrate the 
scientific potential of segmentation of imagery of diverse settings and image types using six case studies from 
river, estuarine, and open coast environments. These photographic and non-photographic imagery consist of 
1- and 3-bands on regular and irregular grids ranging from centimeters to tens of meters. We demonstrate high 
levels of agreement in label images generated by several labelers on the same imagery, and make suggestions 
to achieve consensus and measure uncertainty, ideal for widespread application in training supervised ML for 
image segmentation.
Plain Language Summary  Labeling pixels in scientific images by hand is time-consuming 
and error-prone, so we would like to train computers to do that for us. We can use automated techniques 
from Artificial Intelligence or AI, like one called Deep Learning, but it needs a lot of example images and 
corresponding labels that have been made by hand. So, we still need to label quite a lot of images at the pixel 
level—called image segmentation. We made a computer program called Doodler that speeds up the process; 
you label some pixels, and it labels the rest. It is the fastest method we know of for image segmentation because 
it is semi-automated. We also show that it produces accurate and precise labeling, as we demonstrated by 
having multiple people use this method to label the same images. Because it is so fast and accurate, it allows us 
to get enough data to train Deep Learning models to do segmentation on all the images we have, from the past 
and in the future. Doodler therefore enables geoscientists to use Artificial Intelligence to extract much more 
information from their imagery, in service of geoscience in general.
BUSCOMBE ET AL.© 2022 The Authors. This article has 
been contributed to by U.S. Government 
employees and their work is in the public 
domain in the USA.
This is an open access article under 
the terms of the Creative Commons 
Attribution License, which permits use, 
distribution and reproduction in any 
medium, provided the original work is 
properly cited.Human-in-the-Loop Segmentation of Earth Surface Imagery
D. Buscombe1 , E. B. Goldstein2 , C. R. Sherwood3 , C. Bodine4 , J. A. Brown5 , 
J. Favela6 , S. Fitzpatrick7 , C. J. Kranenburg8 , J. R. Over3 , A. C. Ritchie9 , 
J. A. Warrick9 , and P. Wernette9 
1Marda Science, LLC, Contracted to USGS Pacific Coastal and Marine Science Center, Santa Cruz, CA, USA, 2Department 
of Geography, Environment, and Sustainability, University of North Carolina at Greensboro, Greensboro, NC, USA, 3USGS 
Woods Hole Coastal and Marine Science Center, Woods Hole, MA, USA, 4School of Informatics, Computing and Cyber 
Systems, Northern Arizona University, Flagstaff, AZ, USA, 5USGS MD-DE-DC Water Science Center, Dover, DE, USA, 
6Department of Earth and Planetary Sciences, University of California Santa Cruz, Santa Cruz, CA, USA, 7Department of 
Computer Science, California State University, Sacramento, CA, USA, 8USGS St. Petersburg Coastal and Marine Science 
Center, St. Petersburg, FL, USA, 9USGS Pacific Coastal and Marine Science Center, Santa Cruz, CA, USAKey Points:
•  Methodology for image segmentation 
based on agreement between a labeler 
and a machine learning model
•  Faster and more accurate 
segmentation of interpretable imagery 
compared to traditional labeling
•  Large multi-labeler consensus 
facilitates reproducible scientific 
inference from Earth surface images
Supporting Information:
Supporting Information may be found in 
the online version of this article.
Correspondence to:
D. Buscombe,
dbuscombe@contractor.usgs.gov
Citation:
Buscombe, D., Goldstein, E. B., 
Sherwood, C. R., Bodine, C., Brown, 
J. A., Favela, J., et al. (2022). Human-
in-the-loop segmentation of Earth 
surface imagery. Earth and Space 
Science, 9, e2021EA002085. https://doi.
org/10.1029/2021EA002085
Received 15 OCT 2021
Accepted 25 JAN 2022
Author Contributions:
Conceptualization: D. Buscombe, E. B. 
Goldstein
Data curation: D. Buscombe, E. B. 
Goldstein, C. Bodine, J. A. Brown, J. 
Favela, S. Fitzpatrick, C. J. Kranenburg, J. 
R. Over, A. C. Ritchie
Formal analysis: D. Buscombe
Funding acquisition: D. Buscombe, C. 
R. Sherwood, J. A. Warrick
Investigation: D. Buscombe, C. R. 
Sherwood, C. Bodine, J. A. Brown
Methodology: D. Buscombe, E. B. 
Goldstein, J. A. Brown
Project Administration: C. R. 
Sherwood, J. A. Warrick10.1029/2021EA002085RESEARCH ARTICLE
1 of 31

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
2 of 31such as machine learning (ML) in the geosciences has become a useful way to rapidly explore and prototype ideas 
with data (Goldstein et al., 2019; Reichstein et al., 2019).
Given the wealth of available ML algorithms in open-access software, geomorphologists have an unprecedented 
set of available tools for data exploration and hypothesis testing. Machine learning allows us to teach a computer 
to learn by example, usefully approximating quantities from readily obtainable data that are otherwise hard to 
sense (Buscombe et al.,  2017), parameterize (Beuzen et al.,  2019; Ni et al.,  2021; Tinoco et al.,  2015), flag for 
quality control (Sugiura & Hosoda,  2020), or to visualize or make automated inference on high-dimensional 
datasets that a human could not (Chmiel et al.,  2021; Plant & Stockdon,  2012), especially for phenomena without 
well-developed theory (Fox et al.,  2015; Goldstein & Coco,  2015). However, the generation of the right type of 
examples for the machine to learn, or enough of sufficient quality, is a challenge that requires the development 
of specialist data labeling tools. These tools would allow Earth surface processes researchers to generate their 
own data representations for training ML to automate cleaning, distillation or classification of content, and make 
inference, on large geospatial data sets. An example is the segmentation of imagery.
1.2. The Need for Better Tools for Image Segmentation
What we hereafter call imagery is considered in the broadest sense as any data set on a regular grid that may or 
not have a regular spatial footprint, which is collected for scientific applications in the Earth and environmental 
sciences and in related scientific fields. This definition includes geospatial data sets or rasters, photographic 
imagery, imagery from satellites, sonar, radar, and other geophysical sensors, and any other gridded data that is 
visually interpretable (by a subject matter expert or otherwise). Such Earth surface imagery comes in a range of 
types, from single-band or greyscale commonly created by sensors used in geophysical applications that consist 
of interpretable textures and edges, to hyperspectral imagery where up to hundreds of coincident bands sense a 
different narrow portion of the electromagnetic spectrum. We use the term pixels to mean either pixels or voxels, 
depending on whether the imagery is two- or three-dimensional.
The increasing availability of imagery and increasing acceptance (Olhede & Wolfe,  2018), accessibility (Gil 
et al.,  2016), and sophistication of human-supervised computerized analyses and classification workflows (Cheng 
et al.,  2001; Hossain & Chen,  2019; Mi & Chen,  2020), mean that accurate image segmentation workflows —
involving the classification of all pixels in an image —are ubiquitous in need and application in the geosciences 
(Carleer et al.,  2005; Kotaridis & Lazaridou,  2021). Probabilistic segmentation of imagery using ML has vari-
ous uses in Earth surface processes research (Lang et al.,  2019) involving environmental monitoring (Anders 
et al.,  2011; Bayr & Puschmann,  2019; Gaddes et al.,  2019; Su et al.,  2020). Detection of change in geomorphic 
studies has traditionally involved differencing of elevation surfaces (James et al.,  2012). Segmentation of coin-
cident imagery allows for additional insight, for example, the classification/attribution of the change, evaluation 
of the agent of change (Grams et al.,  2019), the nature and persistence of change, and determination of impli-
cations (Barlow et al.,  2006; Drăguţ & Eisank,  2012). Understanding these insights is key to habitat monitoring 
(Chilson et al.,  2019; Gray et al.,  2019; Ridge et al.,  2019) and land use or cover (change) mapping (Buscombe & 
Ritchie,  2018; Carbonneau et al.,  2020; Lefsky,  2010; Pandey et al.,  2021) among many other examples (Chaud-
hary et al., 2019; Ching et al., 2018; Quinn et al., 2018; Weinstein, 2018).
State-of-the-art ML-based image segmentation requires at least some level of human supervision (Kotaridis & 
Lazaridou,  2021; Sultana et al.,  2020). Often the greatest challenge to developing an automated workflow can 
be the creation of model training data that is internally consistent (Serre,  2019). In the case of image segmenta-
tion, training data consists of label imagery where each pixel is categorized into any number of pre-determined 
discrete nominal or ordinal classes. Many applications of segmentation of Earth surface imagery by definition are 
concerned with surfaces, therefore the focus of many labeling workflows, and also the present contribution, is the 
generation of 2D label images by segmenting visually interpretable imagery, that is, up to three coincident bands.
Such label imagery is typically acquired by either hand-digitizing vector polygons that are subsequently raster -
ized (Kotaridis & Lazaridou,  2021), or raster editing, which is hand classification of pixels directly. Creating 
label images through digitization of hand-drawn polygons is time-consuming; raster-editing can offer a quicker 
alternative, and most commercial and non-commercial image-editing software also have built-in tools that can 
select entire regions via similar colors or edge-detection techniques. These tools are typically (a) not reproducible 
because the outputs are generated by a sequence of clicks that are not recorded in a file (a fact that precludes Resources: D. Buscombe
Software: D. Buscombe, E. B. Goldstein
Supervision: J. A. Warrick
Validation: D. Buscombe, C. Bodine, J. 
Favela, S. Fitzpatrick, C. J. Kranenburg, J. 
R. Over, A. C. Ritchie, P. Wernette
Visualization: D. Buscombe
Writing – original draft: D. Buscombe, 
E. B. Goldstein
Writing – review & editing: D. 
Buscombe, E. B. Goldstein, C. R. 
Sherwood, C. Bodine, J. A. Brown, J. A. 
Warrick
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
3 of 31many of the analyses of multi-labeler agreement we present here), (b) proprietary or restrictively licensed, and/
or c still require significant amounts of time and effort to achieve good results. The largest error is at boundaries 
between classes, and arises due to two factors: (a) indistinct areas of transition where it is not always possible 
to make an objective decision about the class, and (b) it is almost never feasible to click the shape of a polygon 
outline at the pixel level.
Labeling Earth surface imagery using these traditional methods is especially time-consuming if images consist of 
small or unfamiliar objects and/or colors or textures exhibiting significant spatial heterogeneity and/or ambiguity, 
necessitating a high zoom level, or viewing at a range of scales. Moreover, labeling transition regions is difficult 
to do reliably because of mixed-pixels, -textures, and -spectral signatures, which can lead to significant amounts 
of error. Earth surface imagery is more likely to have these properties than much imagery used to develop image 
segmentation models, labeling tools, and benchmark datasets in ML research and applications (Everingham 
et al.,  2010). In Earth surface imagery, and especially in transition areas, we argue that pixelwise classification 
needs a human for these transition regions and more complex textures, but could also be sped up by including 
techniques that aid the human labeler, such as ML models that are trained as a human annotates.
1.3. Human-in-the-Loop Image Segmentation
Here, we describe and evaluate a so-called "human-in-the-loop” (Monarch,  2021) machine learning workflow for 
fast image segmentation, encoded in a computer program called Doodler, and we demonstrate its use for geophys-
ical, photographic, and multispectral satellite images of natural environments. Doodler lies on the spectrum of 
what Monarch (2021) refers to as "assisted annotation,” which is interaction with raw data, with ML assisting the 
data labeling process, and "predictive annotation,” where ML generates outputs that can be edited. In fact, the 
program essentially does both, in a loop whose number of iterations for any given sample is dictated by the human 
labeler who acts to assure data quality.
As supervised ML workflows gain popularity in the geosciences (Bergen et al.,  2019; Zuo et al.,  2019) and related 
fields (Crisci et al.,  2012; Kashinath, Mustafa, et al.,  2021), Doodler could be used in numerous contexts to reach 
a target ML model accuracy by training on large amounts of data acquired relatively quickly. It also serves as a 
case study in how to combine human and machine intelligence to label scientific data with increased efficiency 
and accuracy. In section two we introduce the human-in-the-loop labeling principles and graphical (in the sense 
of Koller and Friedman,  2009 of models consisting of nodes connected by vertices) model framework, followed 
by a description of the image feature-extraction methods, and the ML classifier. In Section  3 we describe six data-
sets that we use to demonstrate the approach. These are chosen to quantify and discuss variability among label 
images made by several independent labelers, and further to examine variability in image segmentation outputs 
due to image size and resolution.
Comparisons between images labeled by the same labeler at different scales, and multiple labelers of the same 
imagery are presented in Section  4. This section serves a few purposes. First, for subjective tasks involving 
interpretation of ambiguous data, or even objective tasks or relatively simple tasks where random human blunder 
may be a factor, no simple heuristics exist for deciding the correct label (Monarch,  2021) however some practical 
recommendations can be made using statistical metrics of multi-labeled datasets (Goldstein et al.,  2021). Simi-
larly, we offer some methods for identifying and quantifying uncertainty based on agreement over segmentations 
of the same imagery by multiple labelers. Second, this section serves to demonstrate that the methodology and 
implementation we present are reproducible between labelers, at different times, and using different computa-
tional infrastructure (computers, browsers, etc.), despite the fact that the label image is a model estimate from 
sparse annotations that would vary considerably from labeler to labeler. In Section  5 we make suggestions on 
how to achieve consensus and measure error, and recommendations over usage of the Doodler program, before 
drawing conclusions.
2. Human-in-the-Loop Labeling Using Machine Learning
The image labeling task (Figure  1) involves a human labeler providing sparse annotations (informally called 
“doodles”) to inform and automate a process (“model”) that estimates the label for all pixels in that image, then 
the same labeler refine the model predictions using a combination of adding/removing doodles and/or changing 
model hyperparameter values. A workable system necessitates a graphical user interface and a fast and accurate 
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
4 of 31image segmentation process. Each image is classified according to a set of pre-determined classes; we use the 
term label to refer to a single instance of an annotation of a specific class, such that each class present in every 
image is exemplified with numerous labels.
The images are segmented semi-interactively, one-by-one, so there is no need to specify an underlying prior 
statistical model, and we need not assume pixel values are conditionally independent of a given label. Therefore 
ML is ideally suited to the task; because it could learn how to map the features that may be readily extracted from 
imagery, to class labels, from a small proportion of labeled pixels. That model could then be used to estimate the 
class of the remaining pixels not labeled. More formally, we use a discriminative ML model, f, that has learned 
the conditional distribution P(y|θ, x) directly, which reads as the probability of y, given θ and x, where x are the Figure 1. Schematic of the approach encoded into the Doodler program. Most images-class set pairings trialed to date have 
been segmented successfully within one or two loops. Doodler also facilitates the user to modify the model hyperparameters 
that may be used iteratively the same way as adding or removing annotations (“doodles”). The human adjusts the 
hyperparameters and they feed into the Multilayer Perceptron and Conditional Random Field models.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
5 of 31image features associated with annotated pixels y, and θ are learned parameters. This approach is highly suited 
to task-specific prediction such as here; the models need not be portable among images, therefore no attempt is 
made to capture the distributions over x or model the correlations among x. The model then predicts the class 
𝐴𝐴 𝐴𝐴𝐴  
of the unlabeled pixels 
𝐴𝐴 𝐴𝐴𝐴  by 
𝐴𝐴 𝐴𝐴𝐴 =𝑓𝑓(𝐴𝑥𝑥)  , essentially by assuming 
𝐴𝐴𝐴𝐴 (̂𝑦𝑦|𝜃𝜃 𝜃̂𝜃𝜃)≈𝐴𝐴(𝑦𝑦|𝜃𝜃𝜃 𝜃𝜃 )  .
The system consists of (a) a human annotator providing sparse examples of each class of interest in a graph-
ical user interface running in a web browser, (b) a Multi-Layer Perceptron (MLP) model (Bishop,  2006) for 
per-pixel class, based on a probabilistic model of how classes relate to a stack features extracted from standard-
ized imagery based on intensity, texture, edges, and relative location, controlled by parameters learned during 
a discrete training period, and (c) a graphical model called a fully connected conditional random field (CRF; 
Kumar & Hebert,  2006) that refines estimates of the per-pixel class based on a probabilistic assessment of how 
classes relate to features extracted from imagery based on both color (if three or more dimensions) or intensity 
(if imagery is 2D) and relative location, controlled by hyperparameters set/tuned by the human labeler, who also 
acts to assess quality, and iterate as needed. We use the two ML models in conjunction with human annotations 
to classify each pixel of the scene and segment the image. At least one of the classes must exist in a given image, 
but otherwise there are no restrictions on the number of classes (other than practical considerations such as avail-
able time). Often models need the most detailed annotations or "doodles” near the boundaries where one class 
transitions to another.
The program facilitates human labeling, which also provides quality control. In effect, the labeler interacts with 
a machine to collectively decide on the most accurate and precise label image for any given image. Doodles are 
used to update the ML model iteratively, by adding/removing annotations, and also optionally changing hyper -
parameters for optimal segmentation on individual images, and retraining and implementing the model. The 
program relies on the labeler having the patience, dedication, and interest to do a good job, which may require 
a few iterations of the workflow (Figure  1). The design of the program would also be amenable to labeling in 
stages, with each stage perhaps employing people with different levels of expertise. We now describe the two 
ML models embedded within the doodler workflow, namely the Conditional Random Field (2.1) that uses a 
Multilayer Perceptron or MLP (2.3) as a sub-component. We conclude by describing our implementation of the 
Doodler workflow in 2.4.
The methodology not only facilitates much faster segmentation, which makes multiple labeler datasets more 
obtainable (affordable, and completed in a reasonable time), but also results in more accurate segmentations. That 
is because the labeler is asked only to provide true and unambiguous positive examples of each class. Errors at 
boundaries between classes that arise due to hand digitization, which can be significant because of mixed pixels 
or due to coarse digitization, are significantly reduced. That is because the program predicts at the pixel level 
much faster than a human could ever label at that scale, and also because our approach models the likelihood 
of uncertain regions. The latter is crucially important for class assignment in particularly difficult regions of 
imagery in a deterministic manner.
2.1. Conditional Random Field for Image Segmentation
We adopt a widely used approach to such task-specific probabilistic image segmentation, which is a Conditional 
Random Field or CRF model (Kumar & Hebert,  2006; Vosselman et al.,  2017; Zhong et al.,  2014) to estimate 
per-pixel class likelihoods (Figure  2). We use the similar CRF implementation of Krähenbühl and Koltun (2011) 
that was previously used by Buscombe and Ritchie (2018). Whereas Buscombe and Ritchie (2018) used a trained 
convolutional network to label regions of images that were used as unary potentials for a CRF model for pixel-
level refinement, and Buscombe and Grams  (2018) used sparse instrumental observations from the field in 
conjunction with geospatial imagery, here (Figure  2) labels of some regions of images are provided by humans, 
which are used to ascribe a probability of each class per pixel using a Multilayer Perceptron. Those outputs 
(per-pixel class likelihoods) are used as unary potentials for a CRF model for pixel-level refinement; the CRF 
model additionally models the joint likelihood of each pair of pixels, essentially checking for internal consistency 
of the MLP outputs.
The unary potentials define a log-likelihood over the label assignment y, and therefore represent the cost of 
assigning label yi to grid node i. They are called “unary” potentials because they describe feature-class relations 
at every pixel, and to distinguish them from pairwise potentials, dependent on feature-class relations over pairs 
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
6 of 31of feature-class relations, which are also used in the CRF model and defined later. Here we use a Multilayer 
Perceptron (Bishop,  2006) as a classifier to generate unary potentials. In CRFs based on “local” connectivity, 
nodes connect adjacent pixels in x (Kumar & Hebert,  2006), whereas in the fully connected definition such as 
here (Figure  2f), each node is linked to every other (Krähenbühl & Koltun,  2011). Linking each node of the graph 
created from x to every other enables modeling of the long-range connections within the data by considering both 
proximal and distal pairs of nodes, resulting in refined labeling at boundaries and transitions between different 
classes. We use a global probability prior pu of the unary potentials, that is, a prior probability that any random 
sample correctly labels the underlying image features. It is exposed to the user as a seldom-varied hyperparame-
ter, defaults to 0.9, and generally has limited effect unless provided annotations are actually of poor quality, which 
we assume is rarely the case.
There are two non-dimensional hyperparameters exposed to labelers using the Doodler program. The first is θβ 
(default = 1) is used by the CRF feature extractor to extract color image features and map them to classes. These 
features are engineered, by convolving Gaussian kernels with the imagery (in much the same way as features 
are extracted as inputs to the MLP model –see Section  2.2). Hyperparameter θβ controls the degree of allowable 
similarity in image features among classes, therefore θβ = 1 only tolerates image features with small differences 
in intensity being assigned the same class label.
The second hyperparameter, μ, is used within a Potts label “compatibility” function (Krähenbühl & Koltun,  2011) 
to define pairwise potentials used by the model to encourage adjacent pixels to be the same class label, defined as 
Λ(i, j) = μ if i =  j and 0 otherwise. By default, Doodler uses μ = 1, meaning Λ is simply a k ×  k identity matrix, 
whereby all classes are equally “compatible” (as likely as each other to be adjacent in either image or feature 
space). Values greater than 1 weight the pairwise potentials more than the unary potentials, which might be useful 
when the MLP prediction is poor, in which case the pairwise potentials count by a factor of μ greater than the 
unary potentials.Figure 2. An illustration of how image data (a) are used to extract features (b) that are used in conjunction with sparse 
annotations (c) to train an initial Multilayer Perceptron classifier (d) to extract unary potentials (e) that are refined by a 
Conditional Random Field (f) to create a refined label image (g).
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
7 of 31By definition, θβ and μ are task-specific, so their respective effects are hard to generalize, but it can be said that, 
in general, larger values of μ tend to give the model greater independence, resulting in the reclassification of 
more pixels. The importance of pairwise potentials becomes much greater than unary potentials, and spatial 
inconsistencies in feature-label pairings have greater likelihood of being reclassified. In general, θβ has a more 
muted effect and generally controls the sharpness of the class boundaries in the label image. Note that neither 
effect necessarily improves the result. Please refer to Figure S1 in Supporting Information S1 for visualizations 
of the effects of varying θβ and μ on sample imagery from the Sandwich data set, expressed in terms of where the 
labels of pixels are altered by the CRF compared to the MLP output. The reader is also referred to the Supporting 
Information  S1 section entitled “Fully Connected Conditional Random Field for Image Segmentation” for more 
technical details about its implementation and interpretation of parameters.
By design, the CRF solution is not overly sensitive to hyperparameter values. First, imagery is standardized 
therefore the model does not need to use parameters for brightness (related to non-zero image mean) and contrast 
(related to non-unit image variance). Second, we use spatial logic to filter CRF inputs, which eliminates a major 
source of uncertainty for the CRF solution employing pairwise potentials, because the CRF model will be given 
more consistent spatial pairs of feature-class-pairings to make inference from. Finally, hyperparameter sensitivity 
increases if the sparse annotations are used alone (Buscombe & Grams,  2018), and/or if the unary potentials 
estimated by the MLP model are spatially sparse (Buscombe & Ritchie, 2018).
2.2. Image Standardization and Feature Extraction
Each input image, I(i, j, d), where i and j describe 2D pixel locations and d indicates the number of coincident 
data layers, is standardized such that it has zero mean and unit variance (see Supporting Information  S1 section 
entitled "Image Standardization and Feature Extraction”). This ensures the values are distributed within the range 
−1 and 1, which helps numerical stability and builds insensitivity to outliers, as well as removing any bias from 
any channel as a function of the mean image intensity.
Raw pixel values are not used as inputs to the MLP classification model described in Section  2.3. Instead, features 
are extracted in a prescribed way that is, the image features are extracted in the same way each time, known as 
feature engineering. Features relating to image intensity, edges, texture, and relative location are extracted, all at 
a range of scales. Then a stack of features are provided to the classifier. We use kernel convolution methods for 
feature extraction because they are already common in numerous geophysical applications concerning interpre-
tation and quantification of spatially distributed imagery. Image intensity features If(i, j) are extracted from Is(i, 
j, d) by convolving with filter bank Σs, or If = Σs ∗ Is where ∗ denotes convolution, and where Σs consists of s 2D 
Gaussian kernels.
Edge features are extracted using the Sobel operator, computing an approximation of the gradient magnitude of 
If, 
𝐴𝐴 ∇𝐼𝐼𝑓𝑓(𝑖𝑖𝑖 𝑖𝑖)  . Location is encoded as the kernel-convolved bank of 2D features given by 
𝐴𝐴𝐴𝐴 (𝑖𝑖𝑖 𝑖𝑖)=Σ 𝑠𝑠∗√
(𝑖𝑖2+𝑖𝑖2  ). 
Finally, texture features are computed as the first and second eigenvalues of Hessian matrix of If(i, j), or H1(i, j) 
and H2(i, j). Eigenvalue analysis of the Hessian is commonly used in geophysical and medical image feature-ex-
traction (Bishop,  2006) because of its formalized relationship to physical quantities, extracting the principal 
directions in which the local second order structure of the image, that is, its spatial covariance structure, can be 
decomposed. The eigenvectors and eigenvalues of the Hessian are known as principal directions and principal 
curvatures respectively (Koenderink & Van Doorn,  1992). The first two eigenvalues are the magnitudes of the 
maximum and minimum curvature, respectively.
2.3. Initial Segmentation Using a Multilayer Perceptron
The feature stack used for initial segmentation consists of a set of 3D (i, j, d) grids, each flattened to 1D (1, ijd), 
then stacked columnwise to create a model input vector. The feature stack is then subsampled row-wise by a factor 
defined by the user. For larger imagery, this subsampling factor may be as large as six, but typically it is one (i.e., 
no subsampling) to three, and depends on the available computer memory and processing time.
Our entire model framework implementation (see Supporting Information  S1 section entitled "Multilayer Percep-
tron") consists of an input layer of ijd  neurons, two hidden layers, the first consisting of 100 neurons and the 
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
8 of 31second of 60 neurons, each linked to each other (i.e., fully connected), and finally a classifying layer consisting of 
k neurons, where k is the set of classes with labels, that is, present in the scene, determined a priori for the scene. 
Through extensive experimentation, we are satisfied that model outputs are not overly sensitive to the specifi-
cation of the number of neurons in each of the two hidden layers. However, hidden layers or neurons could be 
added for greater discriminative power at the expense of model parsimony and computational efficiency. MLPs 
have previously been successfully used for Earth surface image segmentation (Kurnaz et al.,  2005; Villmann 
et al., 2003), as have other types of artificial neural networks (Buscombe & Ritchie, 2018; Kemker et al., 2018).
While any number of similar deterministic ML algorithms could have been used, MLPs are attractive due to their 
relative simplicity and longevity which has created a widespread use of them among many geoscience and related 
fields (Gardner & Dorling,  1998). Because this is task-specific prediction, 90% of the input feature data are used 
for training and only 10% for validation, for iteratively adjusting w and b through back-propagation and solved 
using stochastic gradient descent, with a maximum of 2,000 training epochs. We use the Adam stochastic gradi-
ent-based optimizer method proposed by Kingma and Ba (2014), using early stopping to terminate training when 
the validation score does not improve by 1e −4 for at least 10 consecutive training epochs. Model outputs are not 
very sensitive to hyperparameters, that is, choices about percentage of data used for validation, number of training 
epochs, or criteria for terminating the training (see for example, Figure S1 in Supporting Information  S1). For 
brevity, this sensitivity analysis is not shown here but the program documentation explains where these hyperpa-
rameters may be adjusted and their resulting outputs compared.
Whereas there is no drop-in replacement for the CRF, the MLP could be switched to a different ML framework. 
In fact, we have also extensively trialled a Random Forest model framework but decided that the MLP performed 
better; see Figure S2 in Supporting Information S1 for an example, based on data set A.
2.4. Implementation: The Doodler Program
In a human-in-the-loop data labeling system, the design of the front-end annotation interface is as or more impor -
tant than the back-end ML model framework. At a minimum, the user interface must allow for image annotation 
and a mechanism for launching the image segmentation process (Figure  3). Optionally, it can also expose controls 
to facilitate image curation and class (label) definition, mechanisms to adjust hyperparameters, and controls for 
re-segmentation. We have created several versions of the program, including some that store images locally, and 
others that retrieve imagery from a remote server. The latter case is useful for collaborative labeling projects, 
because the application can be hosted on the worldwide web and the results can be stored centrally.
The default version of the program that we have made publicly available allows the user to place images for 
classification in a local "assets” folder. The program tracks images that have been classified, therefore the list of 
files available for classification gets smaller during a labeling session. Users can also modify hyperparameters 
and redo segmentations as many times as desired, as well as the “pen” width (width in pixels to ascribe each 
annotation). These controls can optionally be hidden from the user in order to only collect the sparse annotations, 
and/or (pixelwise) label images with a fixed set of default hyperparameters.
Each MLP prediction is a matrix of dimension ijk  encoding the probabilities of each pixel i, j and each class k. 
The discrete class is found as the maximum over i, j in the k dimension, or argmax, resulting in a label matrix 
of integer values, each integer corresponding to a unique class. Often there can be high-frequency noise in the 
resulting 2D discrete label image of pixelwise predictions, that is, small islands of misclassified pixels. Since 
classifier outputs are probabilistic, instead of using argmax we could choose to filter these islands based on logic 
or some other process operating on the probabilities themselves, or we could filter islands by operating in the 
spatial domain on the label image. Doodler implements the latter, using two complementary filtering procedures. 
Therefore we implement an additional, but optional, step is performed in which the label matrix output from the 
MLP model is spatially filtered. The filtered label is then used as input to the CRF model. The reader is referred 
to Supporting Information  S1 section "Spatial Filtering of Initial Segmentation” for more details. An illustration 
of the full workflow described in Section  2.1 through to the present section, including the spatial filtering of the 
initial segmentation, is presented as Figure S3 in Supporting Information S1.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
9 of 312.5. Comparison of Segmentations
In order to quantify inter-labeler differences, the canonical metric to evaluate the difference between two thematic 
maps or label images (Costa et al.,  2018) is the mean Intersection over Union score (IOU, or Jaccard Index) 
averaged over k classes. For a collection of overlapping regular shapes, an IOU value of 0.5 would imply average 
overlapping by 50%, but in this context contributions are summed over fields, therefore when IOU reflects 50% 
average overlap between each contiguous region of labeled pixels. However, many label images are class-imbal-
anced, which is to say there tends to be a majority class and one or more minority classes.
The mean Dice score is relatively insensitive to the number of pixels total in each class, because the numerator 
is the number of correctly classified pixels, and the denominator is the total number of pixels in a class that is in 
both estimated and observed. It has therefore been suggested to be a more accurate metric for the overall agree-
ment between two label images for class-imbalanced label images, whereas an IOU score is not as sensitive to 
contributions from the smaller class (Csurka et al.,  2004). The reader is referred to Figure S4 in Supporting Infor -
mation  S1 for the functional relationship between mean Dice and mean Intersection over Union, and to Figure S5 
in Supporting Information  S1 for an illustration of the behavior of these metrics for a sample comparison using 
one data set, and to the in Supporting Information  S1 section entitled “Comparison of Segmentations” for math-
ematical details about the two metrics.
For both IOU and Dice scores, where different numbers of unique classes exist, that is, two different candidates 
for k, we could choose to set k as the minimum number of the two respective class sets, or the maximum number. 
We chose the maximum, therefore scores are conservative in these situations. It might be surmised that Dice 
measures average accuracy, while IOU measures something closer to the worst-case accuracy. However, they 
vary nonlinearly and, due to averaging over classes, exhibit independently useful properties. We present both 
scores for each data set, and also use them to discuss ways to detect class imbalance, outlier labelers, and label Figure 3. The graphical user interface of the program Doodler for a simple two-class (water and land) labeling task. (a) Initial view of the primary interface tab; (b) 
view of the second tab showing the (optional) input for name or identifier that gets appended to every output filename (to help identify labelers in multi-labeler trials 
such as presented here), and the drop-down list of image filenames yet to label (lists are cross-checked every second or some user-defined amount); (c) view of the 
primary tab with doodles; (d) view of the result of the initial segmentation; and (e) detailed view of the control panel. The control panel shows classes as different color 
buttons (i), pen width (ii), a checkbox for computing the image segmentation (iii) (exposed) hyperparameters that relate to the Conditional Random Field (CRF) (iv: 
θβ; v: μ; vi: CRF downsample factor; vii: the prior global probability of the unary potentials, pu), and (exposed) hyperparameters that relate to the feature extraction 
(viii: feature downsample factor; ix: number of scales over which to extract features). The feature downsample factor downsamples the entire feature stack before 
being classified with an MLP, and the resulting outputs are upsampled using nearest-neighbor interpolation back to the original size. The CRF downsample factor 
downsamples the 3D one-hot encoded stack of unary potentials that result from the MLP, before being used as input to the CRF model, and again the resulting CRF 
outputs are upsampled using nearest-neighbor interpolation.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
10 of 31images in multi-labeler contexts, as well as reporting mean agreement for multi-labeled datasets as an uncertainty 
and quality metric.
3. Datasets and Case Studies
We demonstrate our approach using several case studies from riverine, estuarine, and coastal environments of 
the United States, chosen to illustrate the scientific potential of image segmentation in diverse environments 
and image types, and more specifically to quantify inter-labeler-agreement under various contexts. The datasets 
(Table  1) consist of one- and three-band imagery on regular and irregular grids ranging from centimeters to tens 
of meters, including photographic and non-photographic imagery. Segmentation of this imagery can be used to 
answer a range of scientific questions concerning landscape change, which we exemplify for each data set below. 
In each case, the labelers were issued instructions only verbally, rather than demonstrating with examples. The 
task was discussed, then attempted once and not redone.
3.1. Sedimentary Mapping of a Mixed-Sand-Gravel Beach From Visible-Band Aerial Orthomosaic 
Imagery
Data set A (Sherwood et al.,  2021) consists of one, three-band orthomosaic image (Figure  4a, Table  1), at 5-cm 
and also downsampled to a resolution of 25-cm, for mapping beach substrates of Sandwich Town Neck Beach 
on Cape Cod, Massachusetts. The orthomosaics are created from photographs collected from a low-altitude 
Uncrewed Aircraft System (UAS) on 21 September 2016, using a structure-from-motion workflow similar to 
that described by Over et al. (2021) for high-resolution elevation mapping of coasts from aerial imagery (Warrick 
et al.,  2019). The 5-cm and 25-cm pixel imagery are divided into 1,024 × 1,024 pixel, 3-band (RGB) tiles for 
annotation, which results in 99 and six tiles for the respective resolutions. The two data sets were labeled by 
different individuals. The reader is referred to Figures S1, S2, and S3 in Supporting Information  S1 for more 
example imagery. The following categories are used; (a) water, (b) sand, (c) gravel, (d) cobble/boulder, (e) vege-
tated, (f) development.
The orthomosaics are used to evaluate the products resulting from labeling images at two resolutions. They are 
also used to illustrate how to determine optimal image and pixel size for annotation. Such imagery is used for 
tracking changes to the beach morphology and sedimentology, such as tracking the position of the shoreline, 
berm, and scarp to indicate the nature of morphological change, as well as individual sediment fractions such as 
gravel patches that may have a morphodynamic role or could be sensitive coastal state indicators. Segmentation is 
also useful for determining which parts of the scene are useable data for subsequent analyses. In some situations 
when working with large imagery, it is difficult to know a priori what image size to use when annotating using 
the methods described here; while the program facilitates zooming and panning (see Section  2.4 for details on our Data set Name Type Classes Labelers Source
A Beach sedimentology Orthomosaic 6 2 U.S. Geological Survey data release a.
Sherwood et al. (2021)
B Post-hurricane assessment Oblique aerial 4 2 National Geodetic Survey emergency 
response imagery b
C Shoreline identification Nadir aerial 4 5 U.S Geological Survey data release c
Kranenburg et al. (2020)
D Riverbed structure Sidescan 9 2 Used with permission from U.S. Fish 
and Wildlife Service
E Barrier breach False-color satellite 7 2 Sentinel-2 imagery courtesy of 
European Space Agency (ESA)
F Coastal evolution Visible-band satellite 4 1 Landsat-8 imagery courtesy of U.S. 
Geological Survey
 ahttps://doi.org/10.5066/P9BFD3YH.  bhttps://storms.ngs.noaa.gov .  chttps://doi.org/10.5066/P9CA3D8P .Table 1 
Case Study Datasets
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
11 of 31program implementation), sometimes it is more efficient to use smaller image tiles. In other situations, there is a 
choice over what grid size to use when making the imagery, such as when converting from ungridded to gridded 
data. The orthomosaics are created from color-attributed 3D point clouds (Over et al.,  2021), therefore we use 
data set A (Table  1) to discuss a workflow designed to experimentally determine optimal grid size and image size 
ahead of a large labeling task.
3.2. Flood Detection in Post-Hurricane Aerial Photographic Imagery
Data set B (Figure  4b and Table  1) consists of a non-continuous spatial series of 80, three-band image tiles 
(1,000 × 750 × 3 pixels), which are from Emergency Response Imagery collected by the National Geodetic 
Survey Remote Sensing Division of the US National Oceanographic and Atmospheric Administration, NOAA 
(NOAA,  2021), that have been each divided into four tiles. The imagery is from North and South Carolina 
taken after Hurricane Florence (2018). Post storm imagery can be used to monitor the effects of hurricanes on 
coastal communities (Chen et al.,  2018) and ecosystems (Barnard et al.,  2021) and coastal change (Goldstein 
et al.,  2020). The images are labeled using the following classes: (a) water, (b) sand, (c) vegetated surface, and 
(d) development. We compare the segmentations from two labelers labeling the same complex imagery that is 
readily interpretable without specialist knowledge, but nevertheless difficult to interpret all classes consistently. 
The reader is referred to Figure S6 in Supporting Information S1 for more example imagery.Figure 4. One example image from each of the six data sets used in this study, from left to right; (a) a portion of an orthomosaic image of a beach, (b) an aerial image 
of a marsh environment, (c) an aerial image of a backbarrier coastal dune environment, (d) a portion of a sidescan echogram from a coastal plain river, and (e) a false-
color multispectral satellite image of a coastal lagoon and vicinity. Geospatial imagery on regular grids are shown with latitude and longitude grids and labels.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
12 of 313.3. Delineating Land From Water in Intertidal Areas of Aerial Photographic Imagery
Data set C (Figure  4c and Table  1) consists of a series of 10, three-band arbitrary images of shoreline environ-
ments such as could be collected from a low-altitude aircraft in numerous locations, each labeled by five people 
using the following four classes; (a) deep water, (b) whitewater, (c) intertidal area (including all visibly shallow 
water where the surface below the water is visible, swash regions, and wet sand), and (d) dry land. The reader is 
referred to Figure S7 in Supporting Information  S1 that depicts all 10 images. Such imagery is useful for basic 
monitoring and photogrammetric reconstruction of shoreline environments.
Five labelers examined the same complex imagery that is readily interpretable without specialist knowledge but 
like data set B, is not necessarily straightforward to consistently interpret. It is a complex labeling task involving 
identification and lumping of intertidal areas of what are in fact two distinct classes, namely wet sand and shallow 
water, into a single "shallow” class. The task is made even more complex by asking the labelers to distinguish 
between that shallow class and “water”, a subjective choice requiring identification of water that is deep enough 
so as not to be confused with shallow water through which the underlying surface is visible. On this occasion, 
the labeling team of five people discussed the challenges of reliably distinguishing among these four classes 
beforehand, and this labeling exercise was to determine the utility of the class set before a larger labeling exercise 
was conducted.
3.4. Benthic Physical Habitat Mapping in Sidescan Sonar Data
Data set D (Figure  4d and Table  1) consists of a non-continuous spatial series of 51, one-band (greyscale) image 
tiles, each a short section of port or starboard scan consisting of 1,024 consecutive sonar pings stacked as image 
columns. The length of each ping varied due to sonar range, resulting in the number of image rows varying 
between 1,300 and 2,000 pixels. The scans are collected using a Humminbird® Solix sidescan sonar emitting 
a frequency modulated sound pulse with a nominal carrier frequency of 1.2 MHz, from sections of the Pearl 
River and its tributary the Bogue Chitto, and from the Chickasawhay, Buoy and Leaf tributaries of the Pasca-
goula River, in Spring 2021, for mapping in-stream physical habitats in coastal plain rivers of Louisiana and 
Mississippi. Data set D (Table 1) consists of 10 example scans from the Bogue Chitto River, four from the Buoy 
River, two from the Chickasawhay River, 12 from the Leaf River, and the remaining 23 from the main stem Pearl 
River. The samples are selected for a variety of substrate types, water depths, and turbidities. Data are decoded 
and processed following Buscombe (2017). The reader is referred to Figure S8 in Supporting Information  S1 for 
more example images.
In these so-called “waterfall” images, the pixels represent acoustic backscatter intensity (brighter = higher inten-
sity) of the 80-ms pulse, mapped in a non-linear coordinate system representing two-way travel time on the 
y-axis, and pulse number on the x-axis. Because the transducer moves, pulse number corresponds to along-track 
distance, but the scale varies with boat and current speed. The top portion of the y-axis records backscatter from 
the water column and represents a nearly vertical domain between the transducer and the river bed. The lower 
portion records backscatter from the river bed at increasing distances from nadir. As the distance increases (lower 
in the images), the sound-path angle of incidence increases, changing the distance scale. The pixels representing 
the water column are oriented perpendicular to the bed, and the remaining pixels representing the riverbed and 
shadows in the lee of the bed and other objects. The water column pixels are therefore 2D (x, z) and the remaining 
pixels are 2D (x,y) representations of the 3D (x,y,z) bed relief; objects on the bed cast shadows in their lee, the 
length of which depends on the geometry of the object with respect to the sonar (Buscombe et al.,  2016). The 
length of each ping is variable, depending on the characteristics of the sound pulse that collectively determine 
range, and the fact that the amount of useable data also varies strongly across-track (the vertical image dimension) 
due to attenuation of sound by water and the bed (Buscombe, 2017).
For these data, we found it preferable to annotate the waterfall imagery rather than the georectified mosaic 
images, which would account for heading, slant-range, and time-varying gain. Visual identification of features 
is easier and more consistent when applied to the unrectified scans rather than the georectified mosaics, because 
they are collected in fast, shallow water, boat speed is variable, course is difficult to maintain exactly, and navi-
gating meanders can obscure features. Time-varying gain is minimal because the water depths are much less than 
10 m. Therefore we have found it preferable, in this case, to segment the unrectified imagery, and the subsequent 
label images would then be georectified.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
13 of 31Like many scientific images, there are unusable portions of the imagery that would need to be removed through 
classification and removal by an automated process; in this case, they are the bank shadows and water classes, 
because the others are mappable in 2D space. There are many low-signal-to-noise (dark, grainy) textures that at 
small scale are not distinguishable without some spatial context - such as water, and shadows cast by variously 
sized objects. The full class list is as follows: (a) water; (b) shadow/riverbank; (c) shadows cast by instream 
objects and morphologies; (d) submerged wood; (e) fine sediment bedforms; (f) flat, fine sediment; (g) coarse 
sediment (gravel through boulders), bedrock, and vegetation; (h) anthropogenic (human-made objects); and (i) 
unknown (rare blank regions where the sonar recording cut out). Of the above, all but "anthropogenic” are present 
in the data set used for this study.
Such imagery is used to compare the products resulting from two labelers annotating the same complex imagery 
requiring specialist interpretation. Such imagery is used for mapping riverbed sediments (Buscombe,  2017; 
Buscombe et al.,  2016) to provide basic information for benthic habitat mapping, and morphodynamic and sedi-
ment transport studies in rivers. It is also an example of a geophysical data set with features in common with other 
Earth surface imagery, such as slices from 3D tomography data, Synthetic Aperture Radar (SAR), multibeam 
sonar backscatter, seismic reflection and refraction, to name but a few. The sidescan data set requires the most 
training and expertise to interpret. It is the only data set used here that is actively sensed (using an emitted sound 
wave and recording the echo).
Other than the false-color satellite imagery, this sidescan imagery is the only data set that requires special-
ist knowledge to even sensibly interpret. Those data are therefore labeled by two experts with extensive prior 
experience in visual/manual interpretation of fluvial morphosedimentary forms. The other datasets (aerial and 
orthomosaic imagery) are passively sensed (photographic) and readily interpretable in the visible color spectrum 
(Table  1), requiring no special training however, that does not necessarily mean the labeling task is less difficult.
3.5. Coastal Lagoon and Barrier Beach Dynamics in False-Color Satellite Imagery
Data set E (Figure  4e and Table  1) consists of a time-series of 40, three-band false-color 10-m (122 × 342 × 3 
pixels) cloud-free Sentinel-2 satellite images of coastal lagoon environments in Salinas Rivermouth Natural 
Preserve and National Wildlife Refuge in Monterey, California, collected between 31 December 2018 and 19 
May 2021. The false color images consist of near infrared (band eight), red (band four), and green (band three). 
This three-band combination is commonly used for visual landscape classification where vegetation is present 
(Vuolo et al.,  2016) because plant-covered land appears deep red, and denser plant growth is darker red. Water 
appears blue/black. The spatio-temporal time-series depicts various changes on the landscape, including the 
dynamics of the Salinas River mouth into the coastal ocean, surfzone and riverplume characteristics, changes to 
marsh and dune vegetation, and agricultural crop rotation. Therefore we defined the following classes: (a) water, 
(b) whitewater, (c) bare sand, (d) marsh veg, (e) dune veg, (f) crop/woody, (g) soil. The reader is referred to Figure 
S9 in Supporting Information S1 for more example imagery.
This imagery is further used to study the dynamics of beach breaching by a coastal river, and to compare the vari-
ability in geomorphic interpretation resulting from automated analysis of labels from three labelers labeling the 
same relatively complex imagery. Such imagery could be useful for opportunistic monitoring of coastal change 
from, among many potential uses, shoreline detection and characterization to assess trends in erosion and deposi-
tion, to assessments of habitat loss, flooding, surf zone hydrodynamics, agricultural development, bluff and sand 
dune dynamics. The frequency of important change at the coast is often greater than the frequency of available 
aerial platforms to provide imagery, especially in remote locations at short notice, and this makes the vertical 
and time-varying components of these landscapes especially difficult to unravel from opportunistic surveying/
sampling. Satellite imagery with its regular timestamp therefore has a crucial role to play in linking time and 
spatial scales at coasts (McCarthy et al.,  2017), and will play an increasingly important role in facilitating coastal 
science as imagery becomes higher resolution and better quality, and new sensors provide capabilities to sense 
new quantities (Vos et al., 2020).
3.6. Coastal Evolution in Satellite Imagery
Data set F (Figure  4f and Table  1) consists of a time-series of 43, three-band visible-band pan-sharpened 15-m 
Landsat-8 satellite images (768 × 768 × 3 pixels) of Cape Hatteras, Cape Hatteras National Seashore, North 
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
14 of 31Carolina, collected between 15 February 2015 and 27 September 2021. Data set F differs from data set E in three 
important respects; (a) imagery represent a larger area of over 10 km in each horizontal dimension; (b) imagery 
is visible-band; and (c) the dynamics captured, consisting of changing sandbars, sandwaves, beaches and wave 
breaking patterns, manifest over a larger timescale (79 months compared to 18 months of data set E). We labeled 
the following classes: (a) water, (b) whitewater (surf), (c) sand, (d) land (all dry land that is not sand). There are 
also some small clouds and shadows of clouds in the scene, all occurring above water, therefore they are labeled 
“water”. However, separate classes for clouds and shadows might also be a valid strategy. The reader is referred 
to Figure S10 in Supporting Information  S1 for more example imagery. This larger-scale (multi-km) imagery is 
used to demonstrate the utility in segmenting natural features at relatively large scales, and is also used to compare 
hand-digitization workflows with the methodology presented here.
4. Case Study Results
4.1. Image Size and Resolution
A comparison of label images at the two different grid sizes helps us understand at what grid size, and perhaps 
more importantly image size, we should ideally use for a given scene. A region of the 5-cm and 25-cm pixel 
imagery in data set A (Sherwood et al.,  2021) are divided into 1,024 × 1,024 × 3 pixel tiles for annotation, 
which resulted in 99 and six tiles for the respective resolutions. It is more difficult to accurately label the larger, 
coarser resolution imagery for two reasons: the 25-cm imagery covers a much greater spatial extent than the 
5-cm imagery, so features are smaller, and the imagery is less well resolved, therefore features are less distinct. 
However, images can be over-resolved for the task, and the time it takes to label a set scales approximately propor -
tionally, at best, with the number of images in the set.
Each of the image tiles are labeled, then merged back into large label orthomosaics on the same spatial grids as 
the original orthomosaic images (Figure  5). In this case, errors are more readily observed when image tiles are 
merged, and assessed visually. We found this for both the 25-cm imagery and the 5-cm imagery; in Figure  5, those 
regions appear as abrupt changes in label values and are indicated by white boxes in Figures  5e through  5h. This 
artifact is more common for the coarser-resolution 25-cm imagery. The purpose of tiling of large imagery is to 
make the labeling tasks more manageable, and it also typically makes labeling faster. The disadvantage is that 
many of the errors in the higher resolution imagery occur or become apparent at tile boundaries. These errors are 
generally either caused by (a) a relatively low spatial density of annotations compared to the higher-resolution 
imagery, or (b) by annotations omitted by the labeler due to the larger size of the imagery. The majority of such 
errors occur at label boundaries and could be ameliorated through use of a spatial low-pass filter.
Other errors are due to misidentifications due to the lower resolution of the imagery; note how in Figure  5e the 
wrack line is labeled green (cobble/boulder), whereas in Figure  5f it is labeled “vegetated.” The latter is perhaps 
more correct, because it is composed of dead vegetation. The task became ambiguous, because wrack is rough 
like cobbles but composed of organic matter. In addition, the wrack is much better resolved and identifiable in 
the 5-cm imagery. For this class set, we would use moderately low resolution imagery for this segmentation task, 
but small image tiles. However, the decision is dependent on the processes of interest. In this example, spatially 
less extensive, higher-resolution image tiles would be useful for delineating subtle differences in sedimentary 
grade or texture that only manifests at that scale, such as the difference between fine and coarse sand. Coarser 
resolution imagery may be sufficient for delineating the more obvious sedimentary transitions, such as gravel to 
boulders. Before embarking on segmentation tasks where image grid size can be varied it is recommended to use 
an exercise similar to this to determine a grid resolution and image size that is a good compromise for available 
time, required spatial density of annotations, and ideal image size where the smallest important features are visi-
ble (e.g., higher resolution may be needed for identifying animals or distinguishing between subtle sediment or 
vegetation types).
4.2. Inter-Labeler Differences
Data set B is used to compare the products resulting from two labelers labeling the same complex data set. The 
mean agreement is high (Figure  6), as evidenced by a median of mean Dice scores of 0.76, and Dice scores are 
generally only marginally higher than equivalent IOU scores, suggesting class imbalance is not too much of a 
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
15 of 31factor for this data set. There are many more examples of where Dice ≫ IOU (i.e., IOU-Dice residual in Figure  6c 
is greater than, for example, 0.075), than where Dice and IOU are close.
4.3. Class Selection
An analysis of the labels generated from data set C presents an opportunity to discuss labeler agreement when a 
classification task is somewhat subjective, and how to achieve consensus by identifying which classes to lump 
together, and which to keep separate. IOU and Dice scores are surprisingly good (Dices scores range from 0.87 to 
0.93) when evaluated over the full set of 4 classes (Figure  7a) and show greatest improvement (Dice scores range 
from 0.94 to 0.97) when the whitewater class is included with the deep water class and shallow is lumped with the 
dry land class, to create a binary or two-class set (Figure  7b). Any remaining low scores are partially the result of 
confusion over whether to include swash foam as whitewater. All Dice and IOU scores increased when evaluated 
over two classes instead of four, although not uniformly (Figure  7), suggesting class imbalance is variable. Anal-
ysis of a set of labels in this way from multiple labelers could also be used to identify any outlier labelers whose 
interpretations are different from the rest of the group. As in evident in Figure  7, there are no individuals among 
the five labelers who have a noticeably lower agreement.Figure 5. (a) A region of orthomosaic of Sandwich Town Beach (data set A); (b) 25-cm label imagery as a semi-transparent color overlay; (c) 5-cm label imagery as 
a semi-transparent color overlay; (d) geographic location of the site; (e) closer detail of (b); (f) closer detail of (c); (g) yet closer detail of (e); and (h) yet closer detail 
of (f). In (b), (c), (e) and (f), label imagery consists of small 1024 × 1024 pixel label tiles that have been combined into a raster of full extent in a GIS. Classes are also 
depicts as colorful buttons (the same buttons used in the program Doodler when used to make the label tiles). White boxes highlight regions discussed in the text.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
16 of 314.4. Specialized Labeling
Data set D used to compare the products resulting from two labelers labeling the same complex imagery requir -
ing specialist interpretation. In this case, the mean agreement is lower than for the NOAA aerial imagery, as 
evidenced by a comparatively low median of mean Dice score of 0.43 compared to 0.76 for the NOAA aerial 
imagery (compare Figures  8a and  6a). This is possibly due to the task being more difficult, meaning large areas 
can be legitimately called two different classes (examples are shown in Figures  8d and  8e), and because there 
are more classes (eight instead of four), meaning the class-averaged IOU or Dice is affected by outlier classes.
Another major reason for the generally lower scores is that having more classes presents greater opportunity 
for a mismatch in the number of respective classes in each of a pair of label images. Recall that where different 
numbers of unique classes exist, that is, two different candidates for k, we choose k as the maximum length of 
the two respective class sets. The sidescan label set has, among those used in the present study, a greater percent-
age of images like this where there are unequal numbers of labels per image, therefore a greater percentage of 
conservative scores, which further decreases the class-averaged score.
Set-averaged Dice and IOU scores (i.e., the scalar mean of a distribution of mean scores) are close (Figure  8a), 
suggesting any class imbalance is not affecting the comparison between labels. Class imbalance may not be Figure 6. (a) Sample image from the data set; (b) Label image associated with (b); (c) Histograms of Intersection over Union (IOU) and Dice scores for the 80 pairs 
of labeled aerial images; (d) IOU-Dice comparison; (e) Examples where mean Dice >0.075 than mean IOU; (f) Examples where mean Dice and mean IOU are within 
0.075.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
17 of 31avoidable if specific classes must be used for the scientific purpose the labeled imagery serves, however the 
effects of class imbalance can be reduced by merging appropriate classes, that is, a minority class into a majority 
class, where possible. If a class is infrequent, but deemed too important to miss, imagery could be cropped so the 
class imbalance issue is ameliorated, or the algorithms could be modified to use class weights.
The two examples shown in Figure  8e with relatively poor agreement do so for different reasons; in the upper 
example the two labelers have disagreed over the two shadow classes, and in the lower example the two labelers 
have disagreed where one identifies a region as coarse whereas the other identifies it as wood. In these examples, 
consensus could be achieved through some rules-based process, or by redoing the labels with lower-than-average 
IOU and/or Dice scores in order to achieve greater label precision through consensus (Goldstein et al.,  2021; 
Monarch, 2021).
4.5. Multi-Labeler Comparison of Quantifying a Geomorphic Process
Data set E is used to compare the products resulting from three labelers labeling the same complex imagery of a 
geomorphic process. The overall agreement between Labelers 2 and 3 is very high, as evidenced by a mean Dice 
of 0.9 (Figure  9a). Additionally, the distribution of scores between Labeler 1 and Labelers 2 and 3 are almost 
identical.
In this case, mean Dice scores always exceed mean IOU scores (Figures  9b and  9c), suggesting class imbal-
ance does affect the comparison between labels (water is by far the dominant class in every image). The two 
largest discrepancies between mean Dice and IOU scores are shown in Figure  9d; in each case, the white arrow 
highlights the major error, which in both cases is the mislabeling of water, which, as the dominant class, has a 
disproportionately negative affect on mean IOU compared to mean Dice. A comparison between IOU and Dice 
can also be used to detect outliers. The highlighted outlier in Figure  9e corresponds to the pair of labels shown 
in Figure  9f, in which the one from labeler 3 is missing one category, whitewater, which the program has called 
sand and which would have to be relabeled.
As for the geomorphic event we wished to describe using the segmentation data, namely the barrier breaching 
and “resealing” event that happened between 25 January 2019 and 10 April 2019, captured by seven cloud-free 
images, Figure 10 depicts the breach vicinity in each of the seven images, with the contoured outline of the sand 
category of the image segmentation created by each of the three labelers overlain. In all but one case, shown by 
the white rectangle in Figure  10g, all three labelers captured the outline of the barrier correctly, in the vicinity of Figure 7. Matrices quantifying agreement among five labelers numbered one through five. The upper-right half of each 
matrix shows Dice scores, and the lower-left have shows Intersection over Union (IOU) scores. Two labeling experiments 
are shown: left (a) used four classes (deep, white, shallow, and dry); right (b) used two classes, combining "deep” and 
"whitewater” as one class, and "shallow” and "dry’ as the other.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
18 of 31the breach, plus the back barrier and shoreline areas. There are two additional images showing more temporary 
breaching events (on 24 April 2020 and 28 February 2021) in which all three labelers captured the outline of the 
barrier correctly (not shown). The average horizontal variability between outlines for the three respective labelers 
is within two pixels (20-m horizontal ground distance).
Aside from specific cases like those described above, a potential more generic downside of using highly discrim-
inative models optimized for specific tasks is that they do not necessarily transfer well to out-of-distribution data. 
This is why Doodler works well to generate training data for other types of models that carry out segmentation on 
datasets at scale (i.e., with much more variety than a single image). To demonstrate how the MLP model frame-
work does not transfer well to unseen data, and hence why for fully automated segmentation of unseen sample 
imagery requires a more powerful approach such as a deep neural network trained on thousands of examples, we 
use data set E once again. For each of the 40 images, we used the MLP model built on the small annotated scene 
to apply to a scene with an extent twice as large, extending down coast. The MLP model trained on each half 
image is able to extrapolate the broad categories that are significant at the boundary of the extent of annotations 
well, that is, at the bottom edge of the top half of the image (Figure  11) such as water, dune, and crops. However, 
it tends to under-predict the less dominant classes whitewater (surf), soil and sand, and predictions get worse 
the farther away from the boundary. The CRF model cannot fix all the errors in these under-predicted classes Figure 8. (a) Sample image from the data set; (b) Label image associated with (b); (c) Histograms of mean (class-averaged) Intersection over Union (IOU) and Dice 
scores for the 51 pairs of labeled sidescan images; (d) sample mean IOU –mean Dice comparison; (e) two examples of average/good agreement; and (f) two examples 
of relatively bad agreement.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
19 of 31however, the Doodler program itself results in annotations that could be used within alternative ML frameworks 
and it is likely that annotations with sufficient density for a good MLP solution would easily be sufficient for a 
more sophisticated model (perhaps at greater computational expense) because MLPs are relatively simple ML 
architectures. The fact that the annotations have been optimized through guided iteration towards a solution for a 
particular ML algorithm, does not mean they cannot be repurposed for, after all, they are simply example pixels 
of each class. And, as we mentioned above, Doodler is designed for both one-time data set segmentation and 
for generation of label imagery for training ML models such as deep learning models for fully automated image 
feature-extraction and class segmentation at scale, for application to Earth surface imagery.
4.6. Comparison With Manual Digitization
A single scene collected in 15 February 2015 (the first image in the collection) was annotated in a traditional way 
using hand digitization of polygons, then again using Doodler. This was conducted by the same individual on the Figure 9. (a) Sample image from the data set (e); (b) Label image associated with (b); (c) mean Intersection over Union (IOU) vs. mean IOU–mean Dice residual for 
the 80 pairs of labeled multispectral satellite images, highlighting outlier labels; (d) IOU (bottom left matrix elements) and Dice (top right matrix elements) scores 
among all 3 labelers; (e) two examples of average/good agreement; and (f) two examples of relatively bad agreement.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
20 of 31same day. It took 7.5 min to carefully label the scene and compute the segmentation using Doodler. We used an 
open-source annotation software (Skalski,  2019) to efficiently hand-digitize polygons for the entire scene. This 
program has similar zoom and pan tools to Doodler, which enables careful labeling of small features such as the 
relatively narrow sand beach and the surf zone (multiple lines of breaking waves). Additional imagery showing 
the stages of digitization is provided as Figure S11 in Supporting Information  S1. The manual digitization took 
25 min, or more than three times as long. Whereas we could have conducted this comparison using any of the 
datasets presented in Table  1, we chose this data set because the imagery is sufficiently large, and some classes Figure 10. Subplots (a) through (g) depict the breach vicinity in seven images captured between 25 January 2019 and 10 
April 2019, with the contoured outline of the sand category of the image segmentation created by each of the three labelers 
overlain. The white rectangle in (g) shows the only case where the sand polygon would suggest the barrier is still sealed, 
albeit by a single connecting pixel. Otherwise, the agreement is very close, within two pixels typically with a maximum 
discrepancy of four.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
21 of 31sufficiently spatially limited, to warrant zooming and panning in order to accurately label. We note that the degree 
of zoom and pan is somewhat comparable between the two annotation programs, however the extent of annota-
tion is much less with Doodler, and each annotation is much quicker to complete.
The digitized polygons were converted into a label image for direct comparison with the label image obtained 
using Doodler. A comparison of the inputs and results is presented in Figure  12. The mean IOU and Dice scores 
that quantify the agreement between the two label images are 0.48 and 0.5, respectively. This is low because 
the mean agreement for the two minority classes "surf” and "sand: are only approximately 0.015, whereas the 
agreement over "water” and "land” are approximately 0.97 each. Owing to the large class imbalance in this 
scene, quantitative comparison is limited. Qualitatively, we observe that the two label images differ in three 
important ways. First, there are a few small gaps in the label image where the labeler did not ensure matchup (or 
overlap) between adjacent polygons. This is a common limitation of hand-digitization, and here manifests most 
significantly as gaps between sand polygons, as indicated in Figure  12d by numeral i, and between the marsh and 
the beach, as indicated by numeral ii. Second, extremely small/thin objects are more difficult to hand digitize, Figure 11. Output label images from a Multi-Layer Perceptron model built on the small annotated scene above the white horizontal white line in the center of the 
scene, then applied to the entire scene with an extent twice as large. In the extrapolated region, water, dunes, and crops are reasonably well predicted, but sand, 
whitewater (surf), and soil are not as well predicted.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
22 of 31resulting in the omission of the very thin sand bar, indicated by numeral iii in Figure  12d. The presence of this bar 
is marginally visible but also indicated by the adjacent breaking waves. Doodler was able to capture this feature 
properly (Figure  12h, numeral iv) with a few annotated pixels in this region. Third, in complex regions of transi-
tion where adjacent classes are indistinct at the level of zoom at which the labeler has chosen to label, such as near 
shore where waves are breaking on the sand beach, hand annotation generally results in overly coarse digitization 
compared to Doodler. Doodler is able to predict at the pixel level, whereas it is overly time consuming for hand 
digitization of polygons at the same scale. However, there are also advantages to relatively coarse hand digitation 
if it preserves actual boundaries better than a model prediction instance. An example is indicated by numerals v 
and vi in Figures  12e and  12i, respectively; hand digitization has labeled the ocean side of the beach better than 
Doodler, however Doodler has better labeled the pixel-level detail in the lagoon side of the beach.
5. Discussion
5.1. Obtaining High Levels of Agreement
The results suggest that given knowledgeable labelers, the Doodler program produces consistent label images 
(segmentations), even for complex scenes with numerous classes, indicating that multiple labelers can be used 
to label a data set and the results will be consistent and cohesive. The majority of errors in the labels are not 
necessarily due to the model but are consistent among labelers. The data sets shown here (Table  1) are a few 
among numerous datasets we have already successfully used the program with, from millimeter-scale grid sizes 
in close-range photography to multi-decimeter-scale pixels in satellite imagery, using between two and many tens 
of classes. We also tried several previous software implementations for the basic idea, and have arrived at a user 
interface by testing hundreds to thousands of individual samples by dozens of individual labelers. By combining 
unary potentials from a discriminative MLP model that encodes the conditional likelihood of a class given an 
image feature, with pairwise potentials that encode the joint likelihood of image features and classes together, 
the CRF technique exploits the benefits of both discriminative and generative ML model frameworks, and almost 
always results in an as or more accurate image segmentation than using the discriminative MLP model alone as 
determined visually on thousands of label samples; the program can generate a side-by-side comparison of the 
MLP output and CRF output for any sample image.
An advantage of using a so-called "cascade” of ML models whereby the outputs of the first is the inputs to 
the next (Figure  2), is that the second model can and often does revise the predictions of the first if they are Figure 12. A comparison of hand-digitization versus human-in-the-loop segmentation workflows. The image (a) is the first in data set F, captured by Landsat 8 on 
15 February 2015. The hand-drawn polygons (b) are rasterized to create a label image (c). Subplots (d) and (e) show details from the two regions identified in (c). The 
same image is segmented using sparse annotations (f), resulting in label image (g). The same regions highlighted in (d) and (e) are shown in (h) and (i), respectively, for 
the image segmented using Doodler. Numerals i through vi are discussed in the text.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
23 of 31inconsistent with the second. This situation can often arise because the confidence of discriminative ML models, 
such as MLPs, are as much a reflection of the model feature-extraction and classification processes (summarized 
by learned parameters, θ) as the input data. That is why we say the model output is P(y|θ, x) rather than simply 
P(y|x), to acknowledge the joint importance of model parameters θ with the specific image features x used during 
training.
Outputs are further improved by having a human in the loop, that is, to immediately visually inspect segmenta-
tions for quality, and to add/remove annotations where necessary in places the model has mispredicted, and/or 
to adjust model hyperparameters (on an image-by-image basis if necessary). The percentage of imagery where 
such correction is necessary varies considerably by task (and to a certain degree the diligence of the individual 
labeler); on datasets tested to date, we estimate that approximately half or more of images require the addition 
of annotations beyond the initial sparse set, and approximately a tenth or less require the removal of annotations 
or the adjustment of hyperparameters. It is generally considered a good thing that the CRF solution is not overly 
sensitive to hyperparameter values, and that happens for several reasons by design (see Section  2.4), because that 
allows the instructions given to labelers to focus on how to annotate well.
Based on comparitive exercises between hand-digitization using polygons and our alternative workflow, we 
conclude that our methodology encoded into the Doodler program is always faster; approximately 3 times faster 
for the imagery used in Figure  12, and up to 10 times faster for other imagery we tested that does not require as 
much (or any) zooming and panning. Faster labeling makes multiple labeler data sets easier to obtain, and multi-
labeler contexts have been shown to provide reliable label uncertainty metrics.
We also conclude that Doodler generally results in a segmentation that is as-or-more accurate than slower hand 
digitization workflows. First, Doodler ensures every pixel is labeled, whereas ensuring no gaps in the label raster 
that is the result of a hand-digitization workflow is difficult and often not managed. Additionally, Doodler picks 
up on pixel-level features that are too time-consuming to label or invisible at a reasonable zoom level, especially 
in complicated regions of transition. As a result, labels are finer-scale and more accurate at the pixel level because 
errors at boundaries between classes that arise due to hand digitization, which can be significant because of mixed 
pixels or due to coarse digitization, are significantly reduced. Modeling the likelihood of uncertain regions is 
crucially important for class assignment in particularly difficult regions of imagery in a deterministic manner.
5.2. Measuring Agreement
In general, it may be qualitatively observed that any IOU score above 0.5 is a very high level of agreement at 
the whole-image level, especially for high-resolution imagery. One of the really useful aspects of both IOU and 
Dice as metrics is that they both penalize pixel-level noise, and scores are therefore an accurate reflection of 
high-frequency label noise, which tends to increase with higher resolution imagery. A comparison of aggregated 
IOU scores between pairs of labels in whole datasets also meaningfully reflects the difficulty of the task; sidescan 
scores are typically lower than aerial and satellite imagery due to relative difficulty in interpretation.
However, due to averaging over classes and uneven numbers of classes among samples and datasets, both IOU 
and Dice scores are best treated as comparatives within datasets. In fact, when evaluating agreement (uncertainty) 
on individual datasets, computing and comparing both Dice and IOU scores can be useful for various reasons. We 
have shown it is possible to use them to discuss ways to detect class imbalance, outlier labelers, and label images 
in multi-labeler contexts, as well as reporting mean agreement for multi-labeled data sets as an uncertainty and 
quality metric, among other potential uses. IOU is always the more conservative metric than Dice, and that can 
sometimes be useful when deciding on the subsequent uses of the data. While it is very sensitive to class imbal-
ance, there are potentially a lot of advantages to measuring total error rate, the sum all different pixels (i.e., all 
false positives and false negatives) divided by the number of pixels in the image. The per-class IOU and/or Dice 
scores can show problematic classes where there is lack of agreement (Figure  13). For example, in the sidescan 
data set (data set D), the distribution of per-class scores has the largest range; shadow and wood classes achieve 
relatively little consensus (Figure  13b). The two shadow classes would likely have to be merged for consistency, 
and better agreement over wood and all the other categories might be possible if a manual documenting examples 
is prepared (Goldstein et al.,  2021). In the post-hurricane data set (data set B), sand is often difficult to distinguish 
from water for the same reasons as described for data set.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
24 of 315.3. The Value of Sparse Annotations
The sparse annotations provided by the human labeler are more valuable than the specific realisation of the fully 
labeled image. There are several reasons for this assertion. First, we tested alternative discriminative algorithms 
to the MLP that evaluate P(y|θ, x) on features x that have already been extracted in a prescribed way. Among the 
alternative algorithms tested included the Random Forest and Support Vector Machine, both of which are used 
extensively in Earth surface processes research (Perry & Dickson,  2018; Provost et al.,  2017; Yao et al.,  2008) and 
worked well here too (see Figure S2 in Supporting Information  S1 for representative comparison between MLP 
and Random Forest outputs). We chose the MLP because it is as or more accurate, with fewer model parameters, 
generally less overfitting, and had faster computation times. The key insight here is that the sparse annotations 
could be used with similar effect using a range of ML algorithms. This means that the Doodler program provides Figure 13. Per-class Dice and Intersection over Union (IOU; hatching) scores for (a) post-hurricane aerial imagery, (b) 
sidescan imagery, and (c) satellite imagery.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
25 of 31a means to acquire sparse labels that are optimal for a many ML frameworks to carry out segmentation, not just 
the specific ML framework (MLP and CRF) that we have presented.
Second, as labels, annotations are more valuable than the pixelwise label imagery because there may be better 
ML model frameworks to predict pixelwise class from the sparse annotations in the near future, but it may 
be much longer before computers are able to label complex Earth surface imagery unaided with human-level 
accuracy. In fact there may already be viable ways to use the sparse annotations directly to train deep learning 
models for image segmentation, for example, by exploiting the variable spatial autocorrelation of each class (Hua 
et al.,  2021) or by classifying image features as nearest neighbors in embedding space (Ke et al.,  2021), however 
these techniques are currently much more computationally demanding, and would need large sparsely labeled 
datasets to achieve training convergence.
Third, the sparse annotations themselves encode the pixels chosen to represent the class in that region of the 
image, thus they are likely much better than a random selection of pixels from each scene and class at represent-
ing that class, perhaps efficiently encoding the line of greatest spatial transition (i.e., class boundaries). The CRF 
may on occasion (and by design) override the human label, and this may be quantified by locating (and/or count-
ing) the pixels that differ in class between human input and CRF output. An analysis may reveal the degree to 
which and conditions under which the CRF over-rides the decisions of the human labeler. The Doodler program 
provides tools for extracting not only the sparse annotations and final projects, but also interim products, for any 
type of post facto analyses and evaluations.
Finally, the annotations themselves may be a proxy for other interesting properties of the data. For example, the 
spatial density of annotations may reveal areas of the scene that are more important for classification than others, 
or less ambiguous, or where the difficult transition areas are that the model is expected to predict. It is an inter -
esting and as-yet under-explored supposition that there is some minimum sparsity of annotation necessary for a 
given target accuracy, but that would be complicated by the fact that multiple sets of annotations might give rise 
to identical outputs.
Other potentially informative derived attributes that relate to spatial autocorrelation and other spatial properties 
of the labeled regions include the spatial extent of each prediction, the shape the outline of that contiguously 
labeled region makes, and the spatial density with which annotations need to be made to properly segment the 
image. We find that the percentage of scene that is labeled for a satisfactory outcome varied with image size. It is 
between 10% and 20% for the sidescan imagery (1,024 × 1,300–2,000 pixels) and between 10% and 30% for the 
NOAA imagery (1,000 × 750 × 3 pixels).
In both cases, there is no systematic tendency for one labeler to spend more time on labeling overall, although 
there can be significant differences over individual images. However for the 122 × 342 × 3 pixel satellite imagery, 
the percentage is between 40% and 65%. The percentages may be an overestimate of the labels actually neces-
sary for a good image segmentation, because the default "pen” (cursor) width is three pixels. That value is rarely 
changed by the majority of labelers in this study, although individual labelers tend to adopt that practice more 
readily than others, typically varying between 2 and 5 pixels depending on the scene. That is to say, it is possible 
that 1- or 2-pixel width annotations would have resulted in an equally good segmentation. That could be tested 
by using a morphological erosion operator on the sparse annotations then using the eroded doodles as inputs to 
the MLP and CRF estimation pipeline, and finally comparing outputs from full and thinned pen strokes. In some 
imagery used here, some labelers used thicker pens for the dominant classes, but others realized may have not 
done so because of the extra time it takes to change pen width. The number or spatial density of doodles, rather 
than thickness of pen, is generally a better local indication of scene complexity.
We found no significant correlation between either IOU or Dice score and percentage of the image annotated, 
either for individual images or for scores averaged over sets of labeled images. However, that is likely due to the 
fact that all labelers here are attentive and generally labeled a large percentage of the scene (between 10% and 
65% of the scene, depending on image size) and in all areas of the image. Additionally labelers likely did so until 
the segmentation created from their sparse labels is satisfactory, that is, it seemed to accurately represent the 
underlying scene. Annotations are somewhat different, and individual labelers were even sometimes identifiable 
by their unique style. However, in this study agreement among labels was not identifiably related to a labeler's 
individual labeling sytle.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
26 of 31The program outputs also provide the means to analyze the annotations (like quantify their spatial density) and 
compare them. It is generally a more effective and efficient strategy to add and remove annotations than use 
model hyperparameters to modify CRF model predictions, although of course both are sometimes necessary 
of the most difficult imagery. Other useful metrics to track include the total percentage of the image labeled, 
although in this study that is not correlated with any qualitative accuracy metric or quantitative agreement metric 
because all labelers were careful and attentive and not more detailed with one class than with another. However, 
total percentage labeled would reveal situations where a labeler consistently annotated too much or too little of 
the image, both of which can be a problem due to either model underfitting or overfitting the data.
5.4. Future Work
The most difficult imagery for Doodler would arguably be regarded as the most difficult for any image labeling 
program, namely degraded or poor quality imagery, and especially imagery where features and objects are small 
and hard to resolve because of low spatial resolution. Additionally, Doodler is not particularly well suited to labe-
ling especially thin and short objects consisting of only tens to hundreds of pixels. For example, in large-format 
aerial imagery that represent large areas of ground, such hard-to-label objects would include individual pieces 
of driftwood, short and narrow paths and roads, vehicles, small buildings like cabins, people and other animals, 
among other common things. The common solution is to (a) exhaustively label almost every occurrence of the 
small, thin classes, and (b) to use a lot of zoom and panning, or smaller images, in which the labeler can better 
resolve the class and position the pen more accurately and precisely. However, because the CRF has agency it can 
override the human labels, and unfortunately tends to do so disproportionately for the more infrequent classes, 
which is almost always the classes associated with the small, thin objects. However, there are often trade-offs 
between available time and target accuracy with any labeling task. Therefore, on occasions when it is not efficient 
to use smaller images or spend time zooming and panning, especially if the main classification target is spatially 
extensive and/or continuous, the recommendation we would make is to classify the scene without employing the 
small, thin class(es); polygonal labels of those classes could be added later, rasterized, and merged with the label 
images of the other classes.
In Section  5.3 we stated that annotations are more valuable than the pixelwise label imagery because there may 
already be viable ways to use the sparse annotations directly to train deep learning models for image segmen-
tation. The recent semi-supervised method of Ke et al. (2021) is particularly representative of current trends in 
this scope, utilizing a concept known as contrastive learning (Wei & Ji,  2021) that learn the similarity between 
labeled and unlabeled data and base classifications on that similarity. The similarity is learned from the data, 
and the regions considered to be adjacent to each require some form of abstraction such as defining superpixels 
(contiguous segments of image based on location and color obtained by clustering algorithms) or perhaps another 
trainable model component. It is therefore a more complex solution. Whereas, Doodler uses labeled pixels to 
assign classes to unlabeled pixels within each image, emerging ML techniques like Ke et al. (2021) also use those 
labels to assign classes within and across images. Such advances are possible by utilizing learned embedding 
representations of class-image pairings over larger datasets. Tools like Doodler would still be necessary to both 
collect the sparse annotations, and to generate independent data to evaluate the outputs of an automated technique 
for collections of images.
Although that was not carried out here in order to measure agreement over class sets and imagery among several 
labelers based on verbal instructions alone, upon inspection of the results we now recommend discussing and 
practicing candidate class sets with a small sample of imagery, and then having small a group of labelers trial, 
no matter how trivial the task may seem beforehand (Geiger et al.,  2021). Regardless of hypothesized degree of 
ambiguity in a given labeling task, individual labelers vary a little in terms of diligence and skill, and with a lot of 
Earth surface imagery there is an expectation for different labels in ambiguous regions of imagery, for the reasons 
discussed in Section  1. Therefore achieving consensus is (a) part design, by using a modeling framework that is 
designed to objectively arrive at consensus in labels across the scene based on class-feature pixel pairings and 
(b) part analysis, by analyzing agreement in segmentations of the same imagery by multiple labelers. Analysis of 
labels for the purposes of deciding on optimal class sets, and achieving consensus, is only possible when multiple 
labelers are used, although analysis of labels made by the same labeler on separate occasions might also have 
some value.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
27 of 31More sophisticated labeling workflows would include those that modeled the likelihood (confidence) of the sparse 
annotations themselves, or provided ways for the labelers themselves to provide that assessment (Monarch,  2021) 
and that may be the subject of future work. There is also much more work that needs to be done concurrently 
into strategies for selecting images to be labeled, such as active learning (Goldstein et al.,  2020), automatically 
labeling data using embeddings (Ding et al., 2020) and other data representations that have been found by appli-
cation learning and transfer-learning algorithms (Cunha et al.,  2020), or discovered using synthetic data (Wu 
et al., 2019).
5.5. Human-in-the-Loop Image Segmentation
Scoping feasible applications of Deep Learning in the geosciences benefits from rapid prototyping of ideas, 
model frameworks, and trained models used in a transfer-learning workflows that are often inherited from other 
disciplines (Buscombe & Carini,  2019; Buscombe et al.,  2020; Cunha et al.,  2020; Goldstein et al.,  2020; Yang 
et al.,  2020). The challenge is to evaluate their utility using domain-specific labeled datasets, perhaps against 
baseline methods that may already exist in that domain. The availability of labeled data, and especially the 
availability in analysis ready formats that might be readily ingested into a model training workflow, is the major 
impediment to uptake of advanced data analytics such as Deep Learning among the community of Earth surface 
scientists. While semi- or un-supervised classification methods are gaining more attention in many research 
contexts (Le et al.,  2019) and are a staple method in landcover classification of mostly relatively coarse-resolution 
imagery (Deng & Clausi,  2005; Smits & Dellepiane,  1997), human annotators will continue to be vital for the 
success of many tasks that can be automated using ML. Despite the fact that the development of unsupervised 
methods require labeled data for development and, especially, evaluation, supervised methods at the time of writ-
ing are still state-of-the-art, and considered necessary to model imagery with high intra-class variance, such as 
a lot of Earth surface imagery. Supervised ML will therefore continue to be popular, and powerful, if facilitated 
by open-source tools that make data labeling more efficient, and analyses of uncertainty that add vital context 
to its use. Doodler, as what Monarch (2021) refers to as a “smart interface for semantic segmentation,” is one of 
many specific software tools or interfaces (Bueno et al.,  2020; Goldstein et al.,  2021; Zhao et al.,  2020) for the 
generation of large labeled data sets (Kashinath, Mudigonda, et al.,  2021; Sumbul et al.,  2019) that can be used for 
teaching and self-exploration of Deep Learning techniques, for use in transfer learning, and for new model devel-
opment. Doodler is an open-source program that runs in a web browser, and may be one of many similar future 
implementations that might use human-in-the-loop ML for efficient labeling of other scientifically relevant label 
data such as those generated from time-series signals or social media content (Cai et al.,  2017). These methods 
also have great potential for segmenting digital elevation model imagery, which may have great utility in a great 
variety of geomorphic applications.
The use of an ML model cascade, whereby the outputs of one classifier (MLP) is checked for consistency by 
another independent classifier (CRF), is crucial to the success of the approach for a wider variety of imagery 
and class sets. Image standardization, image feature engineering, spatial filtering, and the use of an ML model 
cascade all help reduce sensitivity of model outputs to user hyperparameters. These allow the human labeler to 
concentrate on annotating well, rather than spend time adjusting hyperparameters. We show that the proportion 
of the image pixels that require annotation for accurate pixelwise label image is relatively low around 10% of 
pixels for images of a size that is typically suitable for the program without excessive use of zoom and pan tools, 
which is imagery typically 3,000 pixels in either horizontal dimension or less. Discrepancies in agreement are 
unavoidable with multiple labelers and represent a source of irreducible uncertainty in all image segmentation 
workflows. Doodler provides the means to rapidly label images, therefore multi-labeler label data sets are more 
readily acquired and the irreducible error can be quantified. Further, we show how combining agreement metrics 
can be used to flag inconsistent label images and annotation styles, and identify the effects of class imbalance. 
Dice and IOU scores are shown to be useful metrics for reporting agreement between segmentations of the same 
data by more than one labeler, and we recommend reporting mean agreement for multi-labeled datasets as an 
uncertainty and quality metric, per image, per class, or aggregated over images and/or classes. We also show how 
the metrics can be used to detect class imbalance, outlier labelers, and label images in multi-labeler contexts. 
Even though segmentations vary from person to person, that does not introduce unreasonable variance in label 
images created by different people, at different times, or using different computational infrastructure.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
28 of 316. Conclusions
We describe a human-in-the-loop machine learning system involving a graphical user interface for fast, interactive 
segmentation of N-dimensional (x, y, N) images into two-dimensional (x, y) label images. It is designed to meet 
two objectives: (a) segmentation of relatively small datasets for specific geoscientific inquiries, and (b) segmenta-
tion of small to large amounts of imagery for subsequent training of other types of ML models for fully automated 
segmentation of large datasets. The program is designed to work with any type of Earth surface imagery. We 
demonstrate the approach using five case study datasets from river, estuarine, and open coast environments of 
the United States; (a) segmentation of beach sediments in visible-band aerial orthomosaic imagery to document 
change to beaches of Cape Cod, Massachusetts; (b) segmentation of post-hurricane aerial imagery from North 
and South Carolina, for assessment of storm impacts; (c) segmentation of aerial imagery for delineating complex 
shoreline environments; (d) segmentation of sidescan sonar imagery for mapping in-stream physical habitats in 
coastal plain rivers of Mississippi; (e) segmentation of false-color Sentinel-2 satellite imagery of coastal lagoon 
environments in Monterey, California, to study the dynamics of river breaching of beaches; and (f) segmentation 
of larger visible-band Landsat-8 satellite imagery of Cape Hatteras, North Carolina, to study coastal landform 
evolution at a regional scale. The datasets consist of irregular grids (each pixel does not represent the same spatial 
footprint), as well as regular grids. Based on comparative exercises between hand-digitization using polygons and 
our alternative workflow, we conclude that our methodology encoded into the Doodler program is always faster, 
and also generally results in a segmentation that is as-or-more accurate than slower hand digitization workflows. 
We thereby demonstrate the effectiveness of the approach using geophysical, photographic, and multispectral 
imagery, as well as regular and irregular grids, and several different class sets and pixel sizes. The technique 
is reproducible in the sense that all decisions made by human labeler and ML algorithms (and their specific 
sequence) can be encoded to file, therefore the entire process can be played back and new outputs generated with 
alternative decisions and/or algorithms. We therefore expect our human-in-the-loop labeling workflow to have 
widespread applicability in Earth and Space scientific applications.
Data Availability Statement
All data used for this study are available on Dryad via Buscombe et al. (2022). The code for the labeling interface 
is available on Zenodo and GitHub via Buscombe (2022).
References
Anders, N. S., Seijmonsbergen, A. C., & Bouten, W. (2011). Segmentation optimization and stratified object-based analysis for semi-automated 
geomorphological mapping. Remote Sensing of Environment, 115(12), 2976–2985. https://doi.org/10.1016/j.rse.2011.05.007
Barlow, J., Franklin, S., & Martin, Y. (2006). High spatial resolution satellite imagery, DEM derivatives, and image segmentation for the detection 
of mass wasting processes. Photogrammetric Engineering & Remote Sensing, 72(6), 687–692. https://doi.org/10.14358/pers.72.6.687
Barnard, P. L., Dugan, J. E., Page, H. M., Wood, N. J., Hart, J. A. F., & Cayan, D. R. (2021). Multiple climate change-driven tipping points for 
coastal systems. Scientific Reports, 11(1), 1–13. https://doi.org/10.1038/s41598-021-94942-7
Bayr, U., & Puschmann, O. (2019). Automatic detection of woody vegetation in repeat landscape photographs using a convolutional neural 
network. Ecological Informatics, 50, 220–233. https://doi.org/10.1016/j.ecoinf.2019.01.012
Bergen, K. J., Johnson, P. A., Maarten, V., & Beroza, G. C. (2019). Machine learning for data-driven discovery in solid Earth geoscience. Science, 
363(6433). https://doi.org/10.1126/science.aau0323
Beuzen, T., Goldstein, E. B., & Splinter, K. D. (2019). Ensemble models from machine learning: An example of wave runup and coastal dune 
erosion. Natural Hazards and Earth System Sciences, 19(10), 2295–2309. https://doi.org/10.5194/nhess-19-2295-2019
Bishop, C. (2006). Pattern recognition and machine learning. Springer. Retrieved from https://www.microsoft.com/en-us/research/publication/
pattern-recognition-machine-learning/
Bueno, A., Zuccarello, L., Díaz-Moreno, A., Woollam, J., Titos, M., Benítez, C., et al. (2020). PICOSS: Python interface for the classification of 
seismic signals. Computers & Geosciences, 142, 104531. https://doi.org/10.1016/j.cageo.2020.104531
Buscombe, D. (2017). Shallow water benthic imaging and substrate characterization using recreational-grade sidescan-sonar. Environmental 
Modelling & Software, 89, 1–18. https://doi.org/10.1016/j.envsoft.2016.12.003
Buscombe, D. (2022). Dash-Doodler. https://doi.org/10.5281/zenodo.5847379
Buscombe, D., & Carini, R. J. (2019). A data-driven approach to classifying wave breaking in infrared imagery. Remote Sensing, 11(7), 859. 
https://doi.org/10.3390/rs11070859
Buscombe, D., Carini, R. J., Harrison, S. R., Chickadel, C. C., & Warrick, J. A. (2020). Optical wave gauging using deep neural networks. Coastal 
Engineering, 155, 103593. https://doi.org/10.1016/j.coastaleng.2019.103593
Buscombe, D., Goldstein, E. G., Sherwood, C. R., Bodine, C., Favela, J., Fitzpatrick, S., et al. (2022). Dataset accompanying Buscombe et al. 
Human-in-the-loop segmentation of Earth surface imagery. https://doi.org/10.5061/dryad.2fqz612ps
Buscombe, D., & Grams, P. E. (2018). Probabilistic substrate classification with multispectral acoustic backscatter: A comparison of discrimina-
tive and generative models. Geosciences, 8(11), 395. https://doi.org/10.3390/geosciences8110395
Buscombe, D., Grams, P. E., & Kaplinski, M. A. (2017). Compositional signatures in acoustic backscatter over vegetated and unvegetated mixed 
sand-gravel riverbeds. Journal of Geophysical Research: Earth Surface, 122(10), 1771–1793. https://doi.org/10.1002/2017jf004302Acknowledgments
Thanks to Tanja Williamson, Meg Palm-
sten, Chris Magirl, and two anonymous 
reviewers for helpful suggestions. Any 
use of trade, firm, or product names is for 
descriptive purposes only and does not 
imply endorsement by the U.S. Govern-
ment. This work has been supported 
by the U.S. Geological Survey Coastal/
Marine Hazards and Resources Program 
and by Congressional appropriations 
through the Additional Supplemental 
Appropriations for Disaster Relief Act of 
2019 (H.R. 2157). EBG acknowledges 
support from USGS (G20AC00403). 
C. Bodine acknowledges support from 
USFWS (F19AC00836). J. Favela and S. 
Fitzpatrick acknowledge support from the 
USGS Community for Data Integra-
tion-funded Coast Train project.
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
29 of 31Buscombe, D., Grams, P. E., & Smith, S. M. (2016). Automated riverbed sediment classification using low-cost sidescan sonar. Journal of 
Hydraulic Engineering, 142(2), 06015019. https://doi.org/10.1061/(asce)hy.1943-7900.0001079
Buscombe, D., & Ritchie, A. C. (2018). Landscape classification with deep neural networks. Geosciences, 8(7), 244. https://doi.org/10.3390/
geosciences8070244
Cai, J., Huang, B., & Song, Y. (2017). Using multi-source geospatial big data to identify the structure of polycentric cities. Remote Sensing of 
Environment, 202, 210–221. https://doi.org/10.1016/j.rse.2017.06.039
Carbonneau, P. E., Dugdale, S. J., Breckon, T. P., Dietrich, J. T., Fonstad, M. A., Miyamoto, H., & Woodget, A. S. (2020). Adopting deep 
learning methods for airborne RGB fluvial scene classification. Remote Sensing of Environment, 251, 112107. https://doi.org/10.1016/j.
rse.2020.112107
Carleer, A., Debeir, O., & Wolff, E. (2005). Assessment of very high spatial resolution satellite image segmentations. Photogrammetric Engineer -
ing & Remote Sensing, 71(11), 1285–1294. https://doi.org/10.14358/pers.71.11.1285
Chaudhary, P., D’ Aronco, S., Moy de Vitry, M., Leitão, J. P., & Wegner, J. D. (2019). Flood-water level estimation from social media 
images. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 4(2/W5), 5–12. https://doi.org/10.5194/
isprs-annals-iv-2-w5-5-2019
Chen, S. A., Escay, A., Haberland, C., Schneider, T., Staneva, V., & Choe, Y. (2018). Benchmark dataset for automatic damaged building detec-
tion from post-hurricane remotely sensed imagery.Retrieved from https://arxiv.org/abs/1812.05581
Cheng, H.-D., Jiang, X. H., Sun, Y., & Wang, J. (2001). Color image segmentation: Advances and prospects. Pattern Recognition, 34(12), 
2259–2281. https://doi.org/10.1016/s0031-3203(00)00149-7
Chilson, C., Avery, K., McGovern, A., Bridge, E., Sheldon, D., & Kelly, J. (2019). Automated detection of bird roosts using NEXRAD radar data 
and Convolutional Neural Networks. Remote Sensing in Ecology and Conservation, 5(1), 20–32. https://doi.org/10.1002/rse2.92
Ching, T., Himmelstein, D. S., Beaulieu-Jones, B. K., Kalinin, A. A., Do, B. T., Way, G. P., et al. (2018). Opportunities and obstacles for deep 
learning in Biology and medicine. Journal of the Royal Society Interface, 15(141), 20170387. https://doi.org/10.1098/rsif.2017.0387
Chmiel, M., Walter, F., Wenner, M., Zhang, Z., McArdell, B. W., & Hibert, C. (2021). Machine learning improves debris flow warning. Geophys-
ical Research Letters, 48(3), e2020GL090874. https://doi.org/10.1029/2020gl090874
Costa, H., Foody, G. M., & Boyd, D. S. (2018). Supervised methods of image segmentation accuracy assessment in land cover mapping. Remote 
Sensing of Environment, 205, 338–351. https://doi.org/10.1016/j.rse.2017.11.024
Crisci, C., Ghattas, B., & Perera, G. (2012). A review of supervised machine learning algorithms and their applications to ecological data. Ecolog-
ical Modelling, 240, 113–122. https://doi.org/10.1016/j.ecolmodel.2012.03.001
Csurka, G., Larlus, D., Perronnin, F., & Meylan, F. (2004). What is a good evaluation measure for semantic segmentation. IEEE PAMI, 26(1).
Cunha, A., Pochet, A., Lopes, H., & Gattass, M. (2020). Seismic fault detection in real data using transfer learning from a convolutional neural 
network pre-trained with synthetic seismic data. Computers & Geosciences, 135, 104344. https://doi.org/10.1016/j.cageo.2019.104344
Deng, H., & Clausi, D. A. (2005). Unsupervised segmentation of synthetic aperture radar sea ice imagery using a novel Markov random field 
model. IEEE Transactions on Geoscience and Remote Sensing, 43(3), 528–538. https://doi.org/10.1109/tgrs.2004.839589
Ding, L., Tang, H., & Bruzzone, L. (2020). LANET: Local attention embedding to improve the semantic segmentation of remote sensing images. 
IEEE Transactions on Geoscience and Remote Sensing, 59(1), 426–435.
Drăguţ, L., & Eisank, C. (2012). Automated object-based classification of topography from STRM data. Geomorphology, 141, 21–33.
Everingham, M., Van Gool, L., Williams, C. K., Winn, J., & Zisserman, A. (2010). The PASCAL visual object classes (VOC) challenge. Inter -
national Journal of Computer Vision, 88(2), 303–338. https://doi.org/10.1007/s11263-009-0275-4
Farr, T. G., Rosen, P. A., Caro, E., Crippen, R., Duren, R., Hensley, S., & Roth, L. (2007). The shuttle radar topography mission. Reviews of 
Geophysics, 45(2), RG2004. https://doi.org/10.1029/2005rg000183
Fox, M., Bodin, T., & Shuster, D. L. (2015). Abrupt changes in the rate of Andean Plateau uplift from reversible jump Markov chain Monte Carlo 
inversion of river profiles. Geomorphology, 238, 1–14. https://doi.org/10.1016/j.geomorph.2015.02.022
Gaddes, M., Hooper, A., & Bagnardi, M. (2019). Using machine learning to automatically detect volcanic unrest in a time series of interfero-
grams. Journal of Geophysical Research: Solid Earth, 124(11), 12304–12322. https://doi.org/10.1029/2019jb017519
Gardner, M. W., & Dorling, S. (1998). Artificial neural networks (the multilayer perceptron) — a review of applications in the atmospheric 
sciences. Atmospheric Environment, 32(14–15), 2627–2636. https://doi.org/10.1016/s1352-2310(97)00447-0
Geiger, R. S., Cope, D., Ip, J., Lotosh, M., Shah, A., Weng, J., & Tang, R. (2021). “Garbage in, garbage out” revisited: What do machine learning 
application papers report about human-labeled training data? Quantitative Science Studies, 1–32.
Gil, Y., David, C. H., Demir, I., Essawy, B. T., Fulweiler, R. W., Goodall, J. L., et al. (2016). Toward the Geoscience paper of the future: Best 
practices for documenting and sharing research from data to software to provenance. Earth and Space Science, 3(10), 388–415. https://doi.
org/10.1002/2015ea000136
Goldstein, E. B., Buscombe, D., Lazarus, E., Mohanty, S. D., Rafique, S. R., Anarde, K. A., et al. (2021). Labeling post-storm coastal imagery for 
machine learning: Measurement of inter-rater agreement. Earth and Space Sciences, 8. https://doi.org/10.1029/2021EA001896
Goldstein, E. B., & Coco, G. (2015). Machine learning components in deterministic models: Hybrid synergy in the age of data. Frontiers in 
Environmental Science, 3, 33. https://doi.org/10.3389/fenvs.2015.00033
Goldstein, E. B., Coco, G., & Plant, N. G. (2019). A review of machine learning applications to coastal sediment transport and morphodynamics. 
Earth-Science Reviews, 194, 97–108. https://doi.org/10.1016/j.earscirev.2019.04.022
Goldstein, E. B., Mohanty, S. D., Rafique, S. N., & Valentine, J. (2020). An active learning pipeline to detect hurricane washover in post-storm 
aerial images. EarthArXiv.
Gorelick, N., Hancher, M., Dixon, M., Ilyushchenko, S., Thau, D., & Moore, R. (2017). Google Earth engine: Planetary-scale geospatial analysis 
for everyone. Remote Sensing of Environment, 202, 18–27. https://doi.org/10.1016/j.rse.2017.06.031
Grams, P. E., Buscombe, D., Topping, D. J., Kaplinski, M., & Hazel, J. E. (2019). How many measurements are required to construct an accurate 
sand budget in a large river? Insights from analyses of signal and noise. Earth Surface Processes and Landforms, 44(1), 160–178. https://doi.
org/10.1002/esp.4489
Gray, P. C., Fleishman, A. B., Klein, D. J., McKown, M. W., Bézy, V. S., Lohmann, K. J., & Johnston, D. W. (2019). A Convolutional Neural Network 
for detecting sea turtles in drone imagery. Methods in Ecology and Evolution, 10(3), 345–355. https://doi.org/10.1111/2041-210x.13132
Hossain, M. D., & Chen, D. (2019). Segmentation for object-based image analysis (OBIA): A review of algorithms and challenges from remote 
sensing perspective. ISPRS Journal of Photogrammetry and Remote Sensing, 150, 115–134. https://doi.org/10.1016/j.isprsjprs.2019.02.009
Hua, Y., Marcos, D., Mou, L., Zhu, X. X., & Tuia, D. (2021). Semantic segmentation of remote sensing images with sparse annotations. IEEE 
Geoscience and Remote Sensing Letters, 19.
James, L. A., Hodgson, M. E., Ghoshal, S., & Latiolais, M. M. (2012). Geomorphic change detection using historic maps and DEM differencing: 
The temporal dimension of geospatial analysis. Geomorphology, 137(1), 181–198. https://doi.org/10.1016/j.geomorph.2010.10.039
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
30 of 31Kashinath, K., Mudigonda, M., Kim, S., Kapp-Schwoerer, L., Graubner, A., Karaismailoglu, E., & Mahesh, A. (2021). ClimateNet: An expert-la-
beled open dataset and deep learning architecture for enabling high-precision analyses of extreme weather. Geoscientific Model Development, 
14(1), 107–124.
Kashinath, K., Mustafa, M., Albert, A., Wu, J., Jiang, C., Esmaeilzadeh, S., et al. (2021). Physics-informed Machine Learning: Case studies for 
weather and climate modelling. Philosophical Transactions of the Royal Society A, 379, 20200093. https://doi.org/10.1098/rsta.2020.0093
Ke, T.-W., Hwang, J.-J., & Yu, S. X. (2021). Universal weakly supervised segmentation by pixel-to-segment contrastive learning. arXiv preprint 
arXiv:2105.00957.
Kemker, R., Salvaggio, C., & Kanan, C. (2018). Algorithms for semantic segmentation of multispectral remote sensing imagery using deep learn-
ing. ISPRS Journal of Photogrammetry and Remote Sensing, 145, 60–77. https://doi.org/10.1016/j.isprsjprs.2018.04.014
Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. Proceedings of the 3rd International Conference on learning 
representations (ICLR). arXiv:1412.6980.
Koenderink, J. J., & Van Doorn, A. J. (1992). Surface shape and curvature scales. Image and Vision Computing, 10(8), 557–564. https://doi.
org/10.1016/0262-8856(92)90076-f
Koller, D., & Friedman, N. (2009). Probabilistic graphical models: Principles and techniques. MIT press.
Kotaridis, I., & Lazaridou, M. (2021). Remote sensing image segmentation advances: A meta-analysis. ISPRS Journal of Photogrammetry and 
Remote Sensing, 173, 309–322. https://doi.org/10.1016/j.isprsjprs.2021.01.020
Krähenbühl, P., & Koltun, V. (2011). Efficient inference in fully connected CRFs with Gaussian edge potentials. Advances in Neural Information 
Processing Systems, 24, 109–117.
Kranenburg, C., Ritchie, A., Brown, J., Over, J., Buscombe, D., Sherwood, C., & Wernette, P. (2020). Post-hurricane Florence aerial imagery: 
Cape fear to Duck. U.S. Geological Survey data release. https://doi.org/10.5066/P91KB9SF
Kumar, S., & Hebert, M. (2006). Discriminative random fields. International Journal of Computer Vision, 68(2), 179–201. https://doi.
org/10.1007/s11263-006-7007-9
Kurnaz, M. N., Dokur, Z., & Ölmez, T. (2005). Segmentation of remote-sensing images by incremental neural network. Pattern Recognition 
Letters, 26(8), 1096–1104. https://doi.org/10.1016/j.patrec.2004.10.004
Lang, S., Hay, G. J., Baraldi, A., Tiede, D., & Blaschke, T. (2019). GEOBIA achievements and spatial opportunities in the era of Big Earth Obser -
vation Data. ISPRS International Journal of Geo-Information, 8(11), 474. https://doi.org/10.3390/ijgi8110474
Larsen, A., Nardin, W., Van de Lageweg, W., & Bätz, N. (2021). Biogeomorphology, quo vadis? On processes, time, and space in biogeomorphol-
ogy. Earth Surface Processes and Landforms, 46(1), 12–23. https://doi.org/10.1002/esp.5016
Le, H. M., Goncalves, B., Samaras, D., & Lynch, H. (2019). Weakly labeling the antarctic: The penguin colony case. In Proceedings of the IEEE 
conference on computer vision and pattern recognition workshops (pp. 18–25).
Lefsky, M. A. (2010). A global forest canopy height map from the moderate resolution imaging spectroradiometer and the geoscience laser altim-
eter system. Geophysical Research Letters, 37(15), L15401. https://doi.org/10.1029/2010gl043622
McCarthy, M. J., Colna, K. E., El-Mezayen, M. M., Laureano-Rosario, A. E., Méndez-Lázaro, P., Otis, D. B., et al. (2017). Satellite remote 
sensing for coastal management: A review of successful applications. Environmental Management, 60(2), 323–339. https://doi.org/10.1007/
s00267-017-0880-x
Mi, L., & Chen, Z. (2020). Superpixel-enhanced deep neural forest for remote sensing image semantic segmentation. ISPRS Journal of Photo-
grammetry and Remote Sensing, 159, 140–152. https://doi.org/10.1016/j.isprsjprs.2019.11.006
Monarch, R. (2021). Human-in-the-Loop machine learning: Active learning and annotation for human-centered AI. Manning Publications.
Ni, J., Wu, T., Zhu, X., Hu, G., Zou, D., Wu, X., & Pang, Q. (2021). Simulation of the present and future projection of permafrost on the Qing-
hai-Tibet Plateau with statistical and machine learning models. Journal of Geophysical Research: Atmospheres, 126(2), e2020JD033402. 
https://doi.org/10.1029/2020jd033402
NOAA. (2021). National geodetic Survey emergency response imagery. Retrieved from https://storms.ngs.noaa.gov/
Olhede, S. C., & Wolfe, P. J. (2018). The growing ubiquity of algorithms in society: Implications, impacts and innovations. Philosophical Trans-
actions of the Royal Society A: Mathematical, Physical & Engineering Sciences, 376(2128), 20170364. https://doi.org/10.1098/rsta.2017.0364
Over, J.-S. R., Ritchie, A. C., Kranenburg, C. J., Brown, J. A., Buscombe, D. D., Noble, T., & Wernette, P. A. (2021). Processing coastal imagery 
with agisoft metashape professional edition, version 1.6—structure from motion workflow documentation (Tech. Rep.). US Geological Survey.
Pandey, P. C., Koutsias, N., Petropoulos, G. P., Srivastava, P. K., & Ben Dor, E. (2021). Land use/land cover in view of Earth observation: Data 
sources, input dimensions, and classifiers—A review of the state of the art. Geocarto International, 36(9), 957–988. https://doi.org/10.1080/
10106049.2019.1629647
Perry, G. L., & Dickson, M. E. (2018). Using machine learning to predict geomorphic disturbance: The effects of sample size, sample prevalence, 
and sampling strategy. Journal of Geophysical Research: Earth Surface, 123(11), 2954–2970. https://doi.org/10.1029/2018jf004640
Plant, N. G., & Stockdon, H. F. (2012). Probabilistic prediction of barrier-island response to hurricanes. Journal of Geophysical Research, 
117(F3), F03015. https://doi.org/10.1029/2011jf002326
Provost, F., Hibert, C., & Malet, J.-P. (2017). Automatic classification of endogenous landslide seismicity using the random forest supervised 
classifier. Geophysical Research Letters, 44(1), 113–120. https://doi.org/10.1002/2016gl070709
Quinn, J. A., Nyhan, M. M., Navarro, C., Coluccia, D., Bromley, L., & Luengo-Oroz, M. (2018). Humanitarian applications of Machine Learning 
with remote-sensing data: Review and case study in refugee settlement mapping. Philosophical Transactions of the Royal Society A: Mathe-
matical, Physical & Engineering Sciences, 376(2128), 20170363. https://doi.org/10.1098/rsta.2017.0363
Reichstein, M., Camps-Valls, G., Stevens, B., Jung, M., Denzler, J., & Carvalhais, N. (2019). Deep learning and process understanding for data-
driven Earth system science. Nature, 566(7743), 195–204. https://doi.org/10.1038/s41586-019-0912-1
Richardson, A. D., Hufkens, K., Milliman, T., & Frolking, S. (2018). Intercomparison of phenological transition dates derived from the Pheno-
Cam Dataset V1. 0 and MODIS satellite remote sensing. Scientific Reports, 8(1), 1–12. https://doi.org/10.1038/s41598-018-23804-6
Ridge, J. T., Gray, P. C., Windle, A. E., & Johnston, D. W. (2019). Deep learning for coastal resource conservation: Automating detection of 
shellfish reefs. Remote Sensing in Ecology and Conservation, 6.
Schwanghart, W., & Scherler, D. (2014). TopoToolbox 2–MATLAB-based software for topographic analysis and modeling in Earth surface 
sciences. Earth Surface Dynamics, 2(1), 1–7. https://doi.org/10.5194/esurf-2-1-2014
Serre, T. (2019). Deep learning: The good, the bad, and the ugly. Annual Review of Vision Science, 5, 399–426. https://doi.org/10.1146/
annurev-vision-091718-014951
Sherwood, C., Over, J., & Soenen, K. (2021). Structure from motion products associated with uas flights in sandwich, Massachusetts. U.S. 
Geological Survey data release. https://doi.org/10.5066/P9BFD3YH
Skalski, P. (2019). Make sense. Retrieved from https://github.com/SkalskiP/make-sense/
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

Earth and Space Science
BUSCOMBE ET AL.10.1029/2021EA002085
31 of 31Smits, P. C., & Dellepiane, S. G. (1997). Synthetic aperture radar image segmentation by a detail preserving Markov random field approach. IEEE 
Transactions on Geoscience and Remote Sensing, 35(4), 844–857. https://doi.org/10.1109/36.602527
Su, H., Wu, L., Jiang, J. H., Pai, R., Liu, A., Zhai, A. J., & DeMaria, M. (2020). Applying satellite observations of tropical cyclone inter -
nal structures to rapid intensification forecast with machine learning. Geophysical Research Letters, 47(17), e2020GL089102. https://doi.
org/10.1029/2020gl089102
Sugiura, N., & Hosoda, S. (2020). Machine learning technique using the signature method for automated quality control of argo profiles. Earth 
and Space Science, 7(9), e2019EA001019. https://doi.org/10.1029/2019ea001019
Sultana, F., Sufian, A., & Dutta, P. (2020). Evolution of image segmentation using deep convolutional neural network: A survey. Knowledge-Based 
Systems, 201, 106062. https://doi.org/10.1016/j.knosys.2020.106062
Sumbul, G., Charfuelan, M., Demir, B., & Markl, V. (2019). BigEarthNet: A large-scale benchmark archive for remote sensing image understand-
ing. In The 2019 IEEE International geoscience and remote sensing Symposium (pp. 5901–5904). https://doi.org/10.1109/igarss.2019.8900532
Tinoco, R., Goldstein, E., & Coco, G. (2015). A data-driven approach to develop physically sound predictors: Application to depth-averaged veloci-
ties on flows through submerged arrays of rigid cylinders. Water Resources Research, 51(2), 1247–1263. https://doi.org/10.1002/2014wr016380
Villmann, T., Merényi, E., & Hammer, B. (2003). Neural maps in remote sensing image analysis. Neural Networks, 16(3–4), 389–403. https://
doi.org/10.1016/s0893-6080(03)00021-2
Vos, K., Harley, M. D., Splinter, K. D., Walker, A., & Turner, I. L. (2020). Beach slopes from satellite-derived shorelines. Geophysical Research 
Letters, 47(14), e2020GL088365. https://doi.org/10.1029/2020gl088365
Vosselman, G., Coenen, M., & Rottensteiner, F. (2017). Contextual segment-based classification of airborne laser scanner data. ISPRS Journal of 
Photogrammetry and Remote Sensing, 128, 354–371. https://doi.org/10.1016/j.isprsjprs.2017.03.010
Vuolo, F., Żółtak, M., Pipitone, C., Zappa, L., Wenng, H., Immitzer, M., et al. (2016). Data service platform for Sentinel-2 surface reflectance and 
value-added products: System use and examples. Remote Sensing, 8(11), 938. https://doi.org/10.3390/rs8110938
Walker, I. J., Davidson-Arnott, R. G., Bauer, B. O., Hesp, P. A., Delgado-Fernandez, I., Ollerhead, J., & Smyth, T. A. (2017). Scale-dependent 
perspectives on the geomorphology and evolution of beach-dune systems. Earth-Science Reviews, 171, 220–253. https://doi.org/10.1016/j.
earscirev.2017.04.011
Warrick, J. A., Ritchie, A. C., Schmidt, K. M., Reid, M. E., & Logan, J. (2019). Characterizing the catastrophic 2017 Mud Creek landslide, Califor -
nia, using repeat structure-from-motion (SfM) photogrammetry. Landslides, 16(6), 1201–1219. https://doi.org/10.1007/s10346-019-01160-4
Wei, Y., & Ji, S. (2021). Scribble-based weakly supervised deep learning for road surface extraction from remote sensing images. IEEE Transac-
tions on Geoscience and Remote Sensing.
Weinstein, B. G. (2018). A computer vision for animal ecology. Journal of Animal Ecology, 87(3), 533–545. https://doi.org/10.1111/1365-2656.12780
Wu, X., Liang, L., Shi, Y., & Fomel, S. (2019). FaultSeg3D: Using synthetic data sets to train an end-to-end convolutional neural network for 3D 
seismic fault segmentation. Geophysics, 84(3), IM35–IM45. https://doi.org/10.1190/geo2018-0646.1
Wulder, M. A., Loveland, T. R., Roy, D. P., Crawford, C. J., Masek, J. G., Woodcock, C. E., et al. (2019). Current status of Landsat program, 
science, and applications. Remote Sensing of Environment, 225, 127–147. https://doi.org/10.1016/j.rse.2019.02.015
Yang, C., Zhao, H., Bruzzone, L., Benediktsson, J. A., Liang, Y., Liu, B., & Ouyang, Z. (2020). Lunar impact crater identification and age esti-
mation with Chang’E data by deep and transfer learning. Nature Communications, 11(1), 1–15. https://doi.org/10.1038/s41467-020-20215-y
Yao, X., Tham, L., & Dai, F. (2008). Landslide susceptibility mapping based on support vector machine: A case study on natural slopes of Hong 
Kong, China. Geomorphology, 101(4), 572–582. https://doi.org/10.1016/j.geomorph.2008.02.011
Zhao, J., Wang, X., & Zhou, Y. (2020). A crowdsourcing-based platform for labelling remote sensing images. In The 2020 IEEE International 
geoscience and remote sensing Symposium (pp. 3227–3230). https://doi.org/10.1109/igarss39084.2020.9323820
Zhong, Y., Zhao, J., & Zhang, L. (2014). A hybrid object-oriented conditional random field classification framework for high spatial resolution 
remote sensing imagery. IEEE Transactions on Geoscience and Remote Sensing, 52(11), 7023–7037. https://doi.org/10.1109/tgrs.2014.2306692
Zuo, R., Xiong, Y., Wang, J., & Carranza, E. J. M. (2019). Deep learning and its application in geochemical mapping. Earth-Science Reviews, 
192, 1–14. https://doi.org/10.1016/j.earscirev.2019.02.023
References From the Supporting Information
Dash. (2021). A productive python framework for building web analytic applications. Plotly. Retrieved from https://dash.plotly.com/introduction
Grinberg, M. (2018). Flask web development: Developing web applications with python. O’Reilly Media, Inc.
Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., & Oliphant, T. E. (2020). Array programming with 
NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-210.1038/s41586-020-2649-2
Holoviz. (2021). High-level tools to simplify visualization in python. Anaconda, Inc. Retrieved from https://holoviz.org/index.html
Merkel, D. (2014). Docker: Lightweight linux containers for consistent development and deployment. Linux Journal, 2014(239), 2.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., & Grisel, O. (2011). others, Scikit-learn: Machine learning in python. Jour -
nal of Machine Learning Research, 12, 2825–2830.
Plotly. (2015). Collaborative data science. Plotly Technologies Inc. Retrieved from https://plot.ly
React. (2021). A javascript library for building user interfaces. Facebook Inc. Retrieved from https://reactjs.org/
 23335084, 2022, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2021EA002085, Wiley Online Library on [07/03/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

ARTICLE
Meta-learning to address diverse Earth observation
problems across resolutions
Marc Rußwurm1,2✉, Sherrie Wang3,4, Benjamin Kellenberger1,5, Ribana Roscher6,7& Devis Tuia1
Earth scientists study a variety of problems with remote sensing data, but they most often
consider them in isolation from each other, which limits information ﬂows across disciplines.
In this work, we present METEOR, a meta-learning methodology for Earth observation pro-blems across different resolutions. METEOR is an adaptive deep meta-learning model withseveral modi ﬁcations that allow it to ingest images with a variable number of spectral
channels and to predict a varying number of classes per downstream task. It uses knowledgemined from land cover information worldwide to adapt to new unseen target problems withfew training examples. METEOR outperforms competing self-supervised approaches on ﬁve
downstream tasks, showing its relevance to addressing novel and impactful geospatial pro-blems with only a handful of labels.https://doi.org/10.1038/s43247-023-01146-0 OPEN
1Environmental Computational Science and Earth Observation Laboratory (ECEO), École Polytechnique Fédérale de Lausanne (EPFL), Route des Ronquos 86,
Sion, 1951, Switzerland.2Laboratory of Geo-information Science and Remote Sensing (GRS), Wageningen University, Droevendaalsesteeg 3, Wageningen
6708 PB, The Netherlands.3Goldman School of Public Policy, University of California, Berkeley, 2607 Hearst Ave, Berkeley 94720 CA, USA.4Department of
Mechanical Engineering and Institute for Data, Systems, and Society, Massachusetts Institute of Technology, 55 Massachusetts Avenue, Cambridge 0 2139
MA, USA.5Department of Ecology and Evolutionary Biology, Yale University, 165 Prospect Street, New Haven 06520-8106 CT, USA.6Institute of Bio- and
Geosciences, Forschungszentrum Jülich GmbH, Wilhelm-Johnen-Straße, Jülich 52425, Germany.7Remote Sensing Group, University of Bonn, Niebuhrstr. 1a,
Bonn 53113, Germany.✉email: marc.russwurm@wur.nl
COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv 11234567890():,;
Earth science strives toward understanding and modeling
processes occurring at the Earth ’s surface. Recently, more
and more studies have appeared using increasingly growing
amounts of satellite image data acquired at various spatial,spectral, and temporal resolutions. To extract actionable knowl-
edge from this raw data, Earth scientists increasingly deploy deep
learning models
1that require large annotated datasets for per-
forming at their best. Each dataset is annotated with a focus on a
particular problem. Some, like building footprint segmentation2
or land cover classi ﬁcation3–5, are well-established, with large
annotated datasets being available. Others, like marine debris
detection6–8, are less explored and typically scarce in labels.
Furthermore, covariate shifts in satellite image data and concept
shifts in annotations9,10are caused by various physical, envir-
onmental, social, and economic factors that differ between geo-
graphic regions, thus making it necessary to collect region-speci ﬁc
datasets. These under-explored or highly region-speci ﬁc problems
typically require researchers to invest a substantial amount of
time and effort in annotating data to describe the speci ﬁc problem
in a machine-compatible way. Due to this heterogeneity in Earth
observation problems, a wide and diverse landscape of Earth
observation and remote sensing datasets has emerged. Each
dataset is typically dedicated to describing one particular problem
and viewed in isolation from the others. This is inef ﬁcient, as
many Earth observation problems still share common knowledge
that can inform different-but-related target problems. For
instance, large-scale land cover classi ﬁcation is often done with
different land cover categories and switching from one category
scheme to another typically requires re-training entire models,
even though the underlying problem remains very similar. Sys-
tematically utilizing this common knowledge between source and
target problems with a speci ﬁc learning algorithm is the objective
of transfer learning11, while meta-learning12extends this idea by
learning this learning algorithm itself13. In particular model-
based transfer learning, where the common knowledge is encoded
in the weights of deep learning models, has gained popularity
within deep learning14. Recently, remote sensing foundation
models, such as RingMo15or contrastive self-supervised learning
models like SSLTransformerRS16have been proposed that pre-
train very large deep learning models on uni ﬁed heterogeneous
datasets, often made of collections of pre-existing smaller data-
sets. These models can then be ﬁne-tuned on a particular
downstream problem with fewer annotated data points. Meta-
learning is also a transfer learning approach, in spirit similar to
these contrastively learned foundation models, but it aims at pre-
training a comparatively small model on a dataset of many source
problems. A general problem is here explicitly expressed as a
concrete task that contains a training (or support) dataset and a
testing (or query) dataset that needs to be classi ﬁed correctly.
Three prominent taxonomies of meta-learning approaches
exist13, of which two have been used in remote sensing: metric-
based and optimization-based. Metric-based meta-learning17,18
pre-trains a deep feature extractor to create an expressive feature
space where test samples are matched to class prototypes de ﬁned
by each task training dataset. This feature extractor is frozen for
test tasks, similar to the self-supervised learning approaches pre-
trained on hand-designed source problems, which we compare to
in the results section. architecturally similar to self-supervised
learning strategies. Metric-based meta-learning has been parti-
cularly useful in remote sensing with high-resolution aerial RGB
imagery19–22. Most research19–21addresses concept shift between
problems, where a new set of classes needs to be classi ﬁed from a
few annotated images. Lunga et al.22target covariate shift in high-
resolution imagery taken under different conditions, such as
varying viewing angles2, with a dedicated hashing and clustering
framework. In optimization-based meta-learning, the entire pre-trained deep learning model is ﬁne-tuned to a downstream task
with stochastic gradient descent. The model-agnostic meta-
learning algorithm (MAML)23and its variants24–26are promi-
nent examples of models that are explicitly optimized to be ﬁne-
tuned to new problems with few annotated images. Optimization-
based approaches with MAML have been used predominantly on
Earth observation problems to mitigate concept and covariate
shifts in medium-resolution multi-spectral imagery: applications
cover global-scale cropland mapping27or land cover
classi ﬁcation28, where covariate shifts make it dif ﬁcult to transfer
models across different geographic regions. Despite their success,
current meta-learning approaches are not yet used to their full
potential since they focus on one family of problems for pre-
training and ﬁne-tuning. They cover urban scene classi ﬁcation21,
land cover classi ﬁcation28, cropland classi ﬁcation and mapping27,
the most often taken in isolation from each other. It is common
that studies use one data source, such as high-resolution aerial
imagery19–21,29, or medium-resolution multi-spectral
imagery27,28and address the model transfer within this single
homogeneous problem family. Only very recent approaches are
starting to address these limitations and focus on integrating
satellite imagery across heterogeneous problem families. For
instance, MOSAIKS29uses a small single-layer small convolu-
tional network30that extracts autocorrelation features from high-
resolution aerial imagery. The features from this featurization
approach proved effective when regressing socio-economic vari-
ables like housing prices, income, road length, nighttime lights,
and environmental factors like forest cover and elevation. In this
work, we address learning across different Earth observation
problems systematically in METEOR: a meta-learning metho-
dology for Earth observation problems across different resolu-
tions. It is an optimization-based meta-learning approach that
uses a small deep learning model with a single output. It is pre-
trained with the model-agnostic meta-learning (MAML)23algo-
rithm to distinguish different land cover categories on medium-
resolution multi-spectral satellite data, as shown in Fig. 1.
Extending previous works, we focus explicitly on ﬁne-tuning this
model to different heterogeneous real-world downstream classi-
ﬁcation problems involving a different number of classes, data
with different spatial and spectral resolutions and few annotated
samples. This heterogeneous transfer is enabled by three key
methodologies as follows: ﬁrst, we replace all batch
normalization31layers with instance normalization32in the
model, as we show experimentally that classical, transductive
batch normalization23has detrimental effects on downstream
problems with high-class imbalance (see “Designing Meteor").
Second, we dynamically change the convolutional kernels of the
input channels to adapt to problems with different spectral bands,
as detailed further in the methods section. Third, we address
downstream problems with different numbers of classes by pre-
training a binary meta-model, ﬁne-tuning this model to each class
separately, and ensembling a one-vs-all classi ﬁer.
These key modi ﬁcations result in METEOR: a single pre-
trained meta-model that can adapt to new problems of interest
across geographies and sensors from limited label information.
Using METEOR, domain experts can address these problems
with satellite data of varying spatial and spectral resolutions,
described by a few annotated images, and with a variable number
of target classes.
Results
In this results section, we ﬁrst experimentally highlight the
importance of instance normalization in the METEOR model on
realistic downstream problems, beyond an idealized class-
balanced few-shot setting (Table 1). We then compareARTICLE COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0
2 COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv
METEOR ’s meta-model to other state-of-the-art approaches
within homogeneous land cover classi ﬁcation problems from
different geographical regions (Table 2) and across different
heterogeneous problem ﬁelds with different resolutions (Table 3).
Finally, in the section “Interpreting and explaining METEOR ’s
predictions across geospatial problems ”, we highlight the diversity
of problems to which METEOR can be applied. We do so by aqualitative analysis of several example use-cases Earth scientists
may encounter.
Designing METEOR . Table 1shows the importance of instance
normalization for class-imbalanced downstream problems
(DFC2020) experimentally. This describes the central ﬁnding that
enabled the deployment of this meta-learning approach across
different realistic use-cases presented in this work. It shows the
pre-training of METEOR with different con ﬁgurations (rows in
the table) tested on two datasets of test tasks (columns) of a very
different nature regarding class-balancing. In all the experiments
of this work, we used the same METEOR meta-model trained on
globally distributed land cover tasks from the train regions of the
Sen12MS dataset5(dataset details in the Methods section). The
Sen12MS-column in Table 1shows the model performance on
1000 class-balanced binary 4-way 2-shot tasks with 16 images per
task from the Sen12MS test areas. This dataset presents an
idealized class-balanced con ﬁguration that is common in few-
shot meta-learning benchmarks18,23where always the same
number of images per class has to be classi ﬁed. The DFC2020-
column shows the performance in the seven geographic areas
from the Data Fusion Contest 2020 (DFC2020) dataset33. Here, a
severe class imbalance is present where some land cover cate-
gories, such as water, are more frequent than others. In com-
parison to Sen12MS, the DFC2020 datasets present a more
realistic land cover classi ﬁcation scenario in a class-imbalanced
setting: depending on the region, the number of images available
varies from 476 to 1439, and the number of classes from 5 to 7;
consequently, the support images used for training the speci ﬁc
task-models also varies from 50 to 70 for a 10-shot training. Note
that the accuracies between these two columns are not directly
comparable due to the different dif ﬁculties of the respective
datasets. Instead, we are interested in highlighting which pre-
training con ﬁgurations lead to the best results on idealized
(Sen12MS) or realistic (DFC2020) downstream tasks.
Fig. 1 Overview of METEOR in as a deep learning model pre-trained on land cover source tasks and deployed on diverse downstream tasks. A task is a
(small) dataset containing few annotated images, divided into independent train and test sets. The task data describes a new problem in a format that a
machine learning model can be optimized on. In METEOR, shown in ( b), a randomly initialized meta-model is pre-trained with model-agnostic meta-
learning (MAML)23to solve land cover classi ﬁcation source tasks, shown in ( a). MAML23yields a deep meta-model that has explicitly learned to learn
from different tasks with few labeled images. In each pre-training task, the model must distinguish one randomly chosen land cover type from others us ing
satellite imagery of the same geographic area. A map of geographic regions from the Sen12MS5dataset with three examples of such pre-training tasks
from Greece, Japan, and USA. The pre-trained meta-model can then be ﬁne-tuned to diverse downstream problems shown in ( c) with only few labeled
images, thus leading to problem-speci ﬁc task-models.
Table 1 Different pre-training con ﬁgurations tested on
idealized (Sen12MS)5and realistic (DFC2020)33test tasks.
Task-datasets Sen12MS DFC2020
Number of tasks 1000 7
Task design Idealized Realistic
Label distribution Balanced Imbalanced
Exp. #1: ﬁxed algorithm (MAML) vary normalization
MAML Instance norm (IN)540.78 0.82 ±0.08
Transductive BN23 0.85 0.26 ± 0.05
Conventional BN310.84 0.60 ± 0.18
Tasknorm-I25 0.83 0.59 ± 0.24
Groupnorm600.72 0.54 ± 0.20
Exp. #2: vary algorithms ﬁxed normalization (IN)
IN Fo-MAML230.66 0.77 ± 0.11
SparseMAML24 0.74 0.79 ± 0.11
SparseFoMAML240.63 0.74 ± 0.13
The meta-model is tested on Sen12MS test tasks with a similar structure to the pre-training
source tasks (column Sen12MS) and on unbalanced land cover tasks from the DFC2020 dataset(column DFC2020), where the label distribution of the target task is unknown. In experiment #1,weﬁx the pre-training algorithm (model-agnostic meta-learning (MAML)
23) and vary the
normalization of the network. Highlighted by underscores, transductive batch normalization(BN)
31achieved the highest accuracy on the idealized Sen12MS test tasks, but performed worst
in the realistic use-case (DFC2020). We found that this ﬁnding also holds in experiment #2
where we ﬁxed instance normalization and tested MAML against more recent meta-learning
models like SparseMAML24or the ﬁrst-order approximations of MAML (Fo-MAML) and
SparseMAML (SparseFoMAML). Overall, a deep learning model trained with instance norm(IN) layers trained with the standard MAML algorithm performed best for all the resultspresented in this work. We highlight best scores by fold face and, for DFC2020, we report onestandard deviation over ﬁve model runs with different query/support sets.COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0 ARTICLE
COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv 3
Crucially, pre-training con ﬁgurations of model-agnostic meta-
learning (MAML) that achieve high accuracies on the idealized
Sen12MS target tasks are not optimal for the more realistic
DFC2020 tasks with gaps up to 60% in accuracy, as shown in
Experiment 1 in Table 1. This performance gap is related to
normalization layers in the network architecture and has been
ﬁrst identi ﬁed and discussed by Bronskill et al.25. They show
empirically that the running calculation of batch statistics in
transductive batch normalization (BN) layers (used in the original
MAML implementation23) at test time allows the model to
exploit knowledge about the class balance to improve its accuracy.
In our experiments, we con ﬁrm that this exploitation allows a
MAML-trained model with transductive BN to achieve the
highest accuracy on idealized balanced tasks (85%) but results in
the worse accuracy (26%) in the realistic DFC2020 tasks(underlined row in Table 1). To alleviate this issue, Bronskill
et al.25proposed to replace batch normalization with their
proposed tasknorm-I25normalization layers. However, we found
that simply replacing batch normalization with instance
normalization32performed best on the realistically imbalanced
DFC2020 data by a large margin 20%, as shown in the top row of
Table 1. In experiment #2, we found that this con ﬁguration of
MAML with instance normalization (IN) also outperformed
more recently proposed meta-learning variants, such as
SparseMAML24(in both the ﬁrst-order SparseFoMAML and
Second-order SparseMAML variants) that achieved state-of-the-
art performance on machine learning benchmark datasets.
Concluding from these experiments, we use instance normal-
ization in a ResNet-12 deep neural network as the meta-model in
the remainder of this paper.Table 2 Model comparisons within land cover problems.
Number of shots (training examples per class)
Model Avg. rank 1 2 5 10 15
SSL4EO 2.51 56.4 ± 9.7 72.8 ± 11.8 79.4 ± 10.2 80.5 ± 10.6 82.4 ± 10.3
METEOR 2.84 61.5 ± 10.7 69.2 ± 11.9 78.6 ± 11.2 81.5 ± 10.4 81.7 ± 11.9
MOSAIKS 2.86 61.3 ± 11.5 68.7 ± 14.8 77.3 ± 11.5 81.3 ± 10.3 84.7 ± 9.2
BASELINE 2.99 58.1 ± 11.9 71.2 ± 9.9 79.4 ± 7.9 81.0 ± 8.0 82.7 ± 7.9
SSLTRANSRS 5.70** 51.2 ± 10.5 61.4 ± 6.4 71.4 ± 8.4 74.2 ± 10.4 75.9 ± 11.0
SWAV 6.51** 46.5 ± 8.8 60.1 ± 13.0 67.6 ± 14.4 69.3 ± 14.4 72.1 ± 14.5
DINO 6.73** 45.4 ± 11.8 58.4 ± 13.7 66.9 ± 15.7 69.1 ± 14.9 71.6 ± 14.9
SECO 6.83** 49.1 ± 12.4 55.9 ± 13.2 66.5 ± 15.5 66.6 ± 16.8 67.5 ± 19.0
SCRATCH 9.00** 36.7 ± 16.8 46.7 ± 13.2 49.9 ± 12.9 51.6 ± 16.4 54.6 ± 15.3
IMAGENET 9.03** 42.3 ± 12.8 48.7 ± 12.1 59.0 ± 15.2 59.9 ± 14.4 62.8 ± 15.8
We report averaged accuracies obtained on the seven DFC2020 regions. Each model is ﬁne-tuned to the 5 –7 classes of each DFC region individually, using an increasingly large support set of 1, 2, 5, 10,
and 15 training examples per class, i.e., shots. It is then tested on a query set containing all remaining images. We report the average rank (lower is be tter) to compare models across all shots
simultaneously. We further test for the signi ﬁcance of the differences to METEOR with a Wilcoxon Signed Rank test and indicate sign ﬁciant deviations by **.
Table 3 Quantitative comparison of METEOR with several state-of-the-art methods (rows) across different heterogeneous Earth
observation datasets (columns).
5-Shot problem Human
inﬂuenceCrop type mapping Land cover classi ﬁcation Marine
debrisUrban scenes
Dataset AnthPr.43DENETHOR42DFC2020-KR39EuroSAT40ﬂ. obj.6NWPU-
Urban41
Spatial res. 10 m 3 m 10 m 10 m 10 m <1 m
Spectral res. 10 bands 4 bands 13 bands 13 bands 12 bands 3 bands
No. of classes 2 3 5 10 2 5No. of training
imgs10 15 25 50 10 25
Model Rank ( ↓) Accuracy ( ↑)
METEOR 3.6 83.7 75.6 87.7 60.9 90.8 57.4
SWAV
36 4.2 96.7 69.8 54.2 67.7 65.4 70.4
MOSAIKS294.3 86.4 76.4 82.3 57.9 88.8 54.0
DINO375.0 91.2 66.2 56.6 61.3 65.1 70.6
SECO354.7 91.4 61.7 67.6 62.7 65.9 67.4
SSLTRANSRS165.3 90.7 65.5 76.3 59.7 78.9 52.1
SSL4EO345.5 96.2 58.0 80.2 59.1 82.4 49.9
BASELINE 6.8* 89.0 60.8 87.4 39.8 69.8 36.7
PROTO178.3** 59.7 56.2 76.9 46.1 67.3 39.1
IMAGENET 8.8* 83.7 59.7 50.8 42.7 64.1 60.5SCRATCH 9.5** 64.8 61.1 66.5 25.7 64.4 32.3
This heterogeneous setting is most challenging, as each evaluated task is characterized by a different number of spectral bands, number of classes, a nd spatial resolution. Here, METEOR achieves the
best average rank of 3.6 but is closely followed by SWAV with 4.2 and MOSAIKS with 4.3 across the evaluated datasets. Different models are optimal for di fferent tasks, and no model dominates all
tasks. This is re ﬂected in the Wilcoxon Signed Rank test that shows that the performance of METEOR is only signi ﬁcantly different (indicated by * and **) from the BASELINE, PROTO, IMAGENET,
SCRATCH models. Best values are highlighted by bold face.ARTICLE COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0
4 COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv
Comparison of METEOR to other state-of-the-art models . This
section compares METEOR with other state-of-the-art approa-
ches, modi ﬁed for few-shot classi ﬁcation, either within homo-
geneous land cover classi ﬁcation tasks (Table 2) or across
different heterogeneous tasks (Table 3).
We compare METEOR to self-supervised approaches pre-
trained on multi-spectral satellite data (SSLTRANSRS16and
SSL4EO34), on RGB satellite data (SeCo35), and on natural RGB
images SWAV36and DINO37. As further comparisons, we train a
BASELINE to classify all 10 classes present in the training areas of
the Sen12MS dataset in a supervised way, and add a ResNet-50
model initialized on IMAGENET weights, and with random
initialization named SCRATCH. For these approaches, we load
the respective feature extractors with pre-trained weights, encode
the few training samples in the respective feature spaces, average
them to class prototypes and assign the test imagery to the class of
the nearest prototype, as done in Prototypical Networks17.I n
Supplementary Notes 2, we show empirically that this strategy
leads to better results than ﬁne-tuning a linear classi ﬁer when
considering few-shot problems with less than 50 examples per
class. We also generate MOSAIKS29features dynamically for each
downstream task from the training data and predict the test data
with a random forest classi ﬁer. Models pre-trained with RGB
data, i.e., SECO, SWAV, DINO, and IMAGENET, are only able to
process the three RGB channels, while the remaining models
(METEOR, SSLRS-R50, SSL4EO, MOSAIKS, BASELINE) have
access to all 13 Sentinel-2 bands present in the DFC2020 data.
In terms of metrics, we report the averaged accuracy on the test
images (query set) that models achieved after being trained/ ﬁne-
tuned on the support set of each downstream task. To compare
across different datasets and con ﬁgurations, we further report the
average rank to compare the different methods across all tasks
and con ﬁgurations. A model that outperforms its competitors on
all tasks would have an average rank of 1 ( ﬁrst). We further test
statistically if the differences in accuracies across different
datasets are signi ﬁcantly different to METEOR with a two-sided
Wilcoxon signed rank test38. We indicate signi ﬁcance levels “*"i f
p< 0.1 and “**”ifp< 0.05. No star indicates the difference
between classi ﬁers is not signi ﬁcant.
Comparison within land cover problems .W e ﬁrst compare
METEOR on land cover classi ﬁcation target problems from seven
DFC2020 regions that use the same imaging sensor (13-bands,
Sentinel-2) and classes with similar semantics as the pre-training
tasks (Sen12MS dataset) in Table 2.
Overall, METEOR compares well to their multi-spectral
methods with an average rank of 2.84. Only SSL4EO11achieves
a slightly better average rank of 2.51. Also, the supervised
BASELINE achieves high few-shot accuracies with an average
rank of 2.99, as the data and classes that this model was trained
on (Sen12MS) align well with the DFC2020 data. MOSAIKS,
SSL4EO, and the BASELINE are not signi ﬁcantly different from
METEOR in accuracy, as shown by the Wilcoxon signed rank
test38, while METEOR was signi ﬁcantly better on these problems
than SSLTRANSRS and all contrastive RGB approaches using
only RGB imagers (SWAV, DINO, SECO) with pvalues < 0.05, as
indicated by “**".
Comparison across diverse heterogeneous problems . In Table 3,
we again compare METEOR to other approaches, but this time in
heterogeneous tasks beyond land cover classi ﬁcation. The tasks
involve Earth observation sensor data of different spatial and
spectral resolutions, as indicated in the top rows. They cover
common disciplines, such as land cover classi ﬁcation
(DFC202039, EuroSAT40) and classi ﬁcation of urban scenes fromthe NWPU-RESISC41dataset, as well as more speci ﬁc problems,
such as a mono-temporal classi ﬁcation of crop types
(DENETHOR42). Two niche applications are also studied to
highlight even more the versatility of a single METEOR meta-model to address the diversity of Earth observation problems:
AnthroProtect
43, which estimates the presence of human in ﬂu-
ence by classifying images of naturally protected areas from
unprotected ones, and marine debris6, which classi ﬁes the pre-
sence of ﬂoating objects, such as marine litter, in ocean scenes.
The bottom row of Table 3shows image examples of each of
these downstream tasks. Further qualitative results from these
problem ﬁelds are provided in the next results section. Analo-
gously to the previous comparison, we use the average rank as the
primary metric to compare the different methods across all tasks
and assess whether the performance is signi ﬁcantly better than
METEOR with the Wilcoxon signed rank test. All experiments in
Table 3are reported for 5-shots classi ﬁcation.
Here, METEOR compares well to the other approaches on
these heterogeneous 5-shot problems and achieves the lowest
average rank of 3.6. Still, it only achieves the best accuracy on two
datasets (DFC2020-Kippa-Ring (KR) and ﬂoating objects6), while
being among the best models for the other datasets. The main
exception is NWPU, where DINO37andSWAV36achieved the
best results. We hypothesize that the ﬁne-grained features learned
during contrastively pre-training from natural images in these
approaches are here particularly helpful for very high-resolution
urban scene imagery.
Interestingly, only METEOR and MOSAIKS29are among the
best solutions for both few-shot problems within the land cover
ﬁeld (Table 2) and across heterogeneous problems (Table 3). For
instance, the ResNet-50 trained contrastively with momentum
contrast44from SSL4EO34only achieved rank 5.5 on the
heterogeneous problems tested in this work. This is most
prominently shown by the supervised BASELINE, trained on
land cover Sen12MS data, which is competitive within land cover
downstream tasks, but among the worst model in the hetero-
geneous tasks of Table 3.
Due to the high variance in accuracies across these diverse
problems, only few comparison models (BASELINE, prototypical
networks (PROTO), IMAGENET pre-training and a randomly
initialized model SCRATCH) can be considered signi ﬁcantly
worse than METEOR with the Wilcoxon signed rank test. Still, we
can conclude from this experiment that METEOR compares well
to the state-of-the-art in pre-training models on diverse few-shot
Earth observation problems and can be applied to diverse
heterogeneous problems successfully and that only METEOR and
MOSAIKS achieve such consistency across tasks. After these
quantitative comparisons, we explore METEOR prediction
qualitatively on the diverse problems and provide some insight
using interpretability methods. The iso-lines in the bottom panels
show occlusion sensitivity. These indicate a decrease in prediction
probability (in percentage) if this particular area is occluded.
Interpreting METEOR ’s predictions across problems . This
section studies the behavior of METEOR qualitatively in various
heterogeneous environmental problems. The tasks differ in
spectral and spatial resolution, demonstrating the broad useful-
ness of METEOR. The application of METEOR is as follows: a
task-speci ﬁc model (indicated in dark gray in Fig. 1, as well as in
allﬁgures in this section) is initialized in each task with the
parameters of the same pre-trained meta-model (drawn in red in
theﬁgures). The task-speci ﬁc model is then ﬁne-tuned on a few
training samples from the support set of the task under study.
The following sections and ﬁgures illustrate the predictions
quantitatively and qualitatively, and use explainable machineCOMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0 ARTICLE
COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv 5
learning45,46interpretations of the predictions combined with
domain knowledge from the respective task to explain them.
In the ﬁrst example in Fig. 2, the meta-model is ﬁne-tuned on
ﬁve land cover classes with one example per class (1-shot setting)
in Kippa-Ring near Brisbane, Australia, which is one of the seven
DFC2020 regions. This leads to a task-model specialized in
predicting land cover in Australia de ﬁned by the training classes.
meta-model and ﬁne-tuned with ﬁve training images. Each image
deﬁnes the representations of the land cover classes in this
downstream task, as shown at the bottom of the ﬁgure. The task-
model then classi ﬁes the remaining 677 images from this
geographic region, resulting in an average accuracy of 68%. This
accuracy is achieved with only ﬁve training images (averaged over
three runs with different train/test splits). With 40 training
images, i.e., in a ten-shot setting, 88% of the remaining images are
classi ﬁed correctly, as shown quantitatively in Supplementary
Table 2.
Second, we consider the monitoring of deforestation, which is a
vital application to estimate drivers for climate change. We
acquired PlanetScope imagery47with four spectral bands (RGB
+NIR) at 3-m resolution between 2017 and 2022 from the
Roraima state in northern Brazil (Fig. 3a). In this region, the
clearing of tropical forests between 2017 and 2022 is visible.
These scenes show the last 10 km of one of the multiple
orthogonal access roads to the BR-174 highway, which provides
infrastructure for deforestation in this region48. As a speci ﬁc task,
we distinguish between forest and no-forest classes and train the
task-model with ten images from a deforested region (0°45 ’50"N
60°39 ’02"W) between 2017 and 2022 that is located north of the
test scenes shown in Fig. 3(top row). Once ﬁne-tuned on these
ten training images, the task-model then estimates a posterior
probability for forest and no-forest in the test scenes by
classifying 32 × 32 px image patches sampled on a regular grid.
This results in a coarse segmentation map, as shown in Fig. 3c, at
96 × 96 m resolution. The estimated probability maps closely
reﬂect the deforested areas visible in the satellite images.
Third, we address urban scene classi ﬁcations from high-
resolution aerial or satellite RGB imagery with less than 1-mground sampling distance. We select the 3500 images pertaining
toﬁve urban classes (industrial, commercial, and dense, medium,
sparse residential areas) from the NWPU-RESISC4541dataset.
The meta-model of METEOR is ﬁne-tuned to a task-model with
25 (ﬁve-shot) training images and achieves 65% accuracy on the
remaining 3475 images, as shown in the last column of Table 3.
Figure 4presents one image of the test data, where the task-model
has estimated high classi ﬁcation scores of multiple classes. The
classi ﬁcation result is analyzed by an occlusion sensitivity
analysis46, which reveals that the irregularly spaced houses are
responsible for the high classi ﬁcation score of commercial areas.
At the same time, the regular rows of residential buildings are
recognized as medium residential. Similarly, we can deduce that
the 15% industrial score is caused by a single white building with
roof installations, which are structurally similar to some buildings
visible in the training set of the industrial class. This result on an
ambiguous example shows that, thanks to task tuning, METEOR
learns relevant features per each class: it correctly divided the
prediction scores across classes thanks to the one-against-all
learning.
Fourth is change detection, which can be realized by repeated
image classi ﬁcation of the same area at different dates. We use
METEOR on images of an explosion event in Beirut, Lebanon, on
August 4, 2020, where a warehouse storing ammonium nitrate
exploded after an initial ﬁre. We acquired a sequence of 70 cloud-
free Sentinel-2 images of Beirut spanning from September 3,
2019, until March 21, 2021 (Fig. 5a). Two classes, pre-event and
post-event, are de ﬁned by taking the ﬁrst and last ﬁve images
from the sequence as training samples, covering periods between
September 3 and 28, 2019, and February 19 and March 21, 2021,
respectively. The meta-model is ﬁne-tuned on these ten images to
estimate probability scores for the two classes for each image in
the sequence. As visible in the bottom plot of Fig. 5, the
probability score for the class post-event remains low for all
images before the explosion event on August 4, 2020. It increases
sharply to 84.5% on the ﬁrst image after the event on August 8
and remains high for the remaining images. An occlusion
sensitivity analysis shows that this increase in score for the
Fig. 2 Land cover classi ﬁcation in Kippa-Ring, Australia (one among the DFC2020 regions), when using one example per class (1-shot), so ﬁve
examples in total. We show the confusion matrix of the predictions obtained on the 677 remaining images, which have been classi ﬁed at a 79.6%
accuracy in the ﬁrst (of three) random splits. In 1-shot learning, the choice of training images is especially important, as the representation of classes are
solely de ﬁned by these single training examples. The second random split is only slightly worse with 78.4%, while the third split is only classi ﬁed with 47%
accuracy. In that last case, several forest images were wrongly classi ﬁed as grassland (not shown in the ﬁgure). To accommodate for this randomness, we
average the accuracy of all three random splits leading to a 1-shot accuracy of 68%. The variance between splits decreases with more shots, as can be see n
on the quantitative table in Supplementary Table 2.ARTICLE COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0
6 COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv
Fig. 3 Classi ﬁcation of deforested areas in the northern Brazilian Amazon forest. The task-model is initialized with the parameters of the meta-model
andﬁne-tuned to classify forested from deforested areas in the Amazon rain forest (0°41'05"N 60°37'42"W) shown in ( a) with 4-band PlanetScope
imagery at 3-m ground resolution in ( b). The ten training images, shown in the top panel, are taken from different areas (0°45'50"N 60°39'02"W) and
were acquired on multiple dates between 2017 and 2022. bThree test scenes 9.6 × 3.3 km from 2017, 2021, and 2022, where deforestation is visible. We
split these test scenes into 32 × 32 px tiles and predict the probability for the forest and no-forest classes to each tile. This tiling results in maps of
deforestation at 96 × 96 m resolution in ( c).
Fig. 4 Urban scene classi ﬁcation with high-resolution RGB imagery. High-resolution (less than 1 m) RGB satellite imagery is employed in this example,
where we analyze the prediction of a challenging image where the model assigned multiple categories with a high classi ﬁcation probability. An occlusion
sensitivity analysis46shows an irregular structure of houses, is partly recognized as a commercial area, as masking this part of the image decreases the
score of class “commercial ”by 6%. Regularly spaced houses are visible that the model associates with medium residential. A single ﬂat-top building with
roof installations causes the 15% probability for the industrial class. Note that similar white structures are present in the training set of the indu strial class.COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0 ARTICLE
COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv 7
post-event class is predominantly caused by the explosion crater
and the damages to adjacent buildings, as occluding these areas of
the image leads to a decrease of the post-event score of up to 24%.
Fifth and last, we explore semantic segmentation of marine
debris in satellite imagery, which is a vital requirement for
quantifying marine litter on the world ’s oceans. We focus on
coastal regions close to major rivers deltas and where notable
plastic accumulation events were reported in the news6. The
ResNet-12 task-model, used for image classi ﬁcation, can be
modi ﬁed for coarse semantic segmentation without changing the
weights of the underlying model, as outlined in the Methods
section. Thanks to this modi ﬁcation, it can now predict a score
for each pixel in the image. Despite this modi ﬁcation, the task-
model can still be initialized from the meta-model of METEOR.
Weﬁne-tune it on ﬁve examples of marine debris on RGB
Sentinel-2 imagery from the coastal region of Accra, Ghana, from
a dataset provided by Mifdal et al.6. These training images are
shown in Fig. 6alongside hand-annotated masks that serve as
prediction targets. The bottom part of Fig. 6presents one test
image showing patches of liquid contaminants alongside the
estimated probability map for the presence of marine debris. We
provide a contour overlay of this probability map on the RGB
image alongside the annotations for this image as a reference. The
predictions show that the model has captured the nature of the
ﬂoating object detection task and accurately predicted the shapes
ofﬂoating marine litter using ﬁve training images only.
In summary, we demonstrate that with METEOR domain
experts can ﬁne-tune a single deep neural network on various
downstream problems with few labeled examples. These down-
stream tasks can differ substantially from the land cover
classi ﬁcation source tasks that the meta-model was pre-trained
on in terms of problem scope, spatial, and spectral resolutions.The downstream problems can be diverse and span from land
cover classi ﬁcation from different geographic regions over urban
scene classi ﬁcation with high-resolution RGB imagery to
segmenting marine debris at the sea surface. Few example imagesare needed to represent the downstream task classes, allowing us
to de ﬁne abstract categories, such as pre-event and post-event.
This abstraction is also meaningful for other use-cases, such as
land cover classi ﬁcation, where the representation of one land
cover, e.g., cropland, can vary greatly from continent to continent.
Crucially, leveraging a few labeled samples makes METEOR
applicable to various use-cases that Earth scientists address every
day.
Discussion and conclusion
This paper presents METEOR, a transfer learning methodology
based on model-agnostic meta-learning
23in which problem-
speciﬁc neural networks are learned from a global (meta-)model
using only few labeled examples describing such new problem.
This is enabled by three simple-to-implement but important
modi ﬁcations (1) replacing transductive batch normalization with
instance norm in the meta-model, (2) ensembling multiple binary
classi ﬁers to address problems with a varying number of classes,
and (3) dynamically changing the input channels to account for
problems with a different number of spectral bands. Thanks to
these modi ﬁcations, the generic meta-model, originally pre-
trained to distinguish different land cover classes, can then
address a diverse set of heterogeneous remote sensing tasks that
vary in spatial and spectral resolution, as well as in the number of
classes. Such as a model that can be trained on a collection of
different-but-related tasks, and this can help in a variety of Earth
science disciplines, where annotating samples is particularly dif-
ﬁcult or costly. In this regard, we have shown quantitative and
Fig. 5 Detection of changes with a sequence of multi-spectral medium-resolution imagery. This use-case shows the port of Beirut, Lebanon, where an
explosion event caused substantial damage on August 4, 2020. aA total of 70 Sentinel-2 images where we use the ﬁrst and last ﬁve images as support set
to de ﬁne the classes pre-event and post-event. task-model, which is ﬁne-tuned on these examples. The meta-model in METEOR, shown in ( b), isﬁne-
tuned on these two classes and predicts all remaining images in the sequence. In ( c), we show the resulting probability for the post-event class, where it
remains low (0.6%) until August 3, 2020, and sharply rises probability of 84.5% on the following image of August 8, 2020. The crater and damaged
buildings from the event caused the sudden increase in this probability score, as revealed by the occlusion sensitivity analysis drawn in ( d). A further
comparison to MOSAIKS and SSLTRANSRS is placed in Supplementary Fig. 5.ARTICLE COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0
8 COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv
qualitative experiments where a single pre-trained model can be
of use across a variety of diverse Earth science disciplines from
urban applications to deforestation mapping.
Our work extends related work based on meta-learning19,27,28,
self-supervised16,35,49or self-taught learning29from the homo-
geneous transfer learning setting (where all tasks share the same
problem nature and input space dimensions) to a heterogeneous
setting involving different satellite sensors and applications. In
particular, this transfer across different resolutions has been
identi ﬁed as a major challenge toward a collective agenda on
artiﬁcial intelligence for Earth science data analysis50.
In contrast to the results on idealized Sen12MS test tasks (see
Table 1; Sen12MS-column), which are similar to those usually
obtained in meta-learning benchmarks, we found that using
regular instance normalization in the deep meta-model neural
network outperformed recently proposed meta-learning variants
(e.g. SparseMAML24, TaskNorm-I25) on realistic and class-
imbalanced remote sensing problems. This ﬁnding enabled us
to explore and compare METEOR with related work on tasks
within the problem ﬁeld of land cover classi ﬁcation (Table 2) and
across challenging heterogeneous remote sensing problems
(Table 3); in all comparisons, METEOR performed very well for
few-shot classi ﬁcation methods, achieving the best average rank
on heterogeneous problems and the second best within land cover
problems. We studied in depth the behavior of METEOR in a
series of potential use-cases, from change detection to marine
debris segmentation, where domain experts can deploy METEOR
to extract actionable information and insights into speci ﬁc
problems.
While this work presents a step toward deploying a learning-
from-tasks framework on real-world Earth observation applica-
tions, it revealed some limitations that future research will need to
address. First, implementing the task-model as an ensemble of
one-versus-all classi ﬁers (described in the Methods section) leads
to poor performance when the number of classes is large. While
predicting ten classes, as in the EuroSAT benchmark40was still
accurate, performance was sub-par in the case of 45 classes as in
the full NWPU-RESIC4541. Additionally, while ﬁne-tuning the
meta-model on a few annotated samples is computationally
efﬁcient, learning it in the ﬁrst place with model-agnostic meta-learning is more memory expensive than pre-training with self-
supervised algorithms. Training larger convolutional neural net-
works like ResNet-50 (23 million parameters) or ResNet-152 (60
million parameters) is considered out-of-reach for current meta-
learning approaches.
Another point is the nature of the pre-training task. In this
work, we limited the pre-training of the METEOR meta-model
on the source tasks from land cover classi ﬁcation to demonstrate
the versatility toward radically different target problems. Still, we
believe that expanding the pre-training scope toward other
labeled and unlabeled source tasks will likely further improve its
performance on downstream Earth observation problems. Our
work is valuable to several groups of domain experts, users, and
scientists. First, the METEOR model can support researchers who
aim to deploy state-of-the-art deep learning models in their
particular ﬁeld of expertise. METEOR is usable with the limited
labeled data often available from specialized ﬁeld campaigns,
which often provide high-quality samples but are scarce in
quantity. Moreover, we release METEOR as an open-source,
simple, and ready-to-use package in Python. Second, we believe
that our experiments across various remote sensing tasks can
serve as a benchmark to compare future machine learning algo-
rithms and measure the performance in a broader set of mean-ingful applications. In general, the results of this work indicate,
along with other recent advances in meta-learning and self-
supervised learning, that deep learning models trained on a
learning-from-tasks framework can be employed for a versatile
family of downstream problems. These models can provide
increasingly intelligent solutions that, when deployed on
impactful Earth science problems, help address some of the most
pressing issues of our time.
Methods
General description of METEOR . The meta-learning metho-
dology for Earth observation problems across different resolu-
tions (METEOR) is a heterogeneous transfer learning approach.
It is designed to capture knowledge across data modalities (dif-
ferent Earth observation sensor types) and ef ﬁciently adapt to
different tasks (variable in terms of task semantics, number of
Fig. 6 Segmentation of marine debris with multi-spectral medium-resolution imagery. Marine debris on a 12-band, medium-resolution (10 m) Sentinel-2
scene near Accra, Ghana (October 31, 2018) have been visually detected and annotated by Mifdal et al.6in (a). This task-model is modi ﬁed for
segmentation and initialized with the weights of the meta-model. Five annotated images of ( a) serve as a training dataset to ﬁne-tune the task-model. bA
test image and the corresponding prediction in ( c) by the task-model to a map showing the probability of marine debris for each pixel. The same meta-
model, as in the previous use-cases, achieves these predictions, highlighting the versatility of meta-learning for various Earth observation prob lems with
few training samples.COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0 ARTICLE
COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv 9
classes, etc.). To do so, METEOR consists of a meta-model and a
task-model (Fig. 1). The meta-model encodes knowledge from
Earth observation source tasks and is pre-trained with the model-
agnostic meta-learning algorithm23(detailed later) on a dataset of
source tasks. The task-model is initialized with the parameters of
the meta-model and ﬁne-tuned on the particular problem with a
few training samples describing the target task at hand. This
framework falls into the family of model-based transfer
learning11,51where knowledge from source tasks is encoded in
model parameters to inform a target task. Other approaches from
this category are self-supervised learning and pre-training, e.g., on
ImageNet52.
Meta-model implementation and pre-training on source tasks .
The meta-model is a deep ResNet-1253neural network following
the implementation of Oh et al.26. All batch normalization31
layers are replaced with instance normalization54(see Table 1).
The model used in all experiments has 15 input channels to
accommodate the two radar bands of Sentinel-1 and the
13 spectral channels of the Sentinel-2 satellite and a single output
dimension for binary one-versus-all classi ﬁcations. We chose 15
channels to utilize all bands in the Sen12MS dataset as we
observed no negative effect of including the two additional radar
channels. Including these bands makes our pre-trained weights of
METEOR applicable to radar downstream tasks, even though we
did not show experiments based on radar data in this work. From
the 13 optical channels, various satellite sensors, such as Planet-
Scope or Worldview, can be used for downstream tasks, as we
demonstrate in the experiments. Pre-training METEOR with
other channels is possible, as we did during development with
only Sentinel-2 or RGB bands variants. the meta-model archi-
tecture. This meta-model is pre-trained on tasks of 16 images
with four randomly selected land use and land cover classes, each
task from one geographic area in the Sen12MS dataset5. We split
the task images into a train and test partition with eight images
each. This con ﬁguration corresponds to a 2-shot 4-way classi ﬁ-
cation setting with two images per class. Note that we modify this
multi-class task to binary one-versus-all classi ﬁcation by selecting
one class randomly as a target. The task`s training objective is to
learn the representation of this selected class from the two images
against the other six negative examples (containing two examples
of the three other classes).
Model-agnostic meta-learning . To obtain the meta-model, we
use the model-agnostic meta-learning (MAML)23algorithm that
optimizes the following objective:
ð1Þ
ð2Þ
where a task-model ϕτis initialized by the meta-model θand
iteratively ﬁne-tuned with k≤Ksteps based on gradients from a
loss of training samples ∇Ltrainin an inner loop. The constant α
denotes the inner learning rate. In the outer loop, the meta-model
parameters θare updated by minimizing the test loss Ltest
τover a
batch of tasks Eτ/C24pðτÞwith the ﬁne-tuned parameters ϕτ,K. These
ﬁne-tuned parameters are a function of the initialization θ. This
makes updating the meta-model parameters with second-order
gradients (outer gradients through the inner loop gradients)
possible. Over several thousand iterations, this yields a meta-
model that is explicitly learned to learn differences between land
cover categories from different geographic areas. In this work, wepre-trained the meta-model with the standard second-order
MAML algorithm23, as it achieved best results on the realistic
use-cases in comparison to variants, such as SparseMAML24or
tasknorm-I25, as shown in Table 1.
Task-model implementation and ﬁne-tuning on
downstream tasks . The METEOR task-model tuning has three
requirements: (i) it must have the same weight dimensions as the
meta-model so that it can be initialized with the weights of the
meta-model; (ii) it must consider a problem with more than one
class; and (iii) it must apply to downstream tasks where the input
channels involved are a subset of those of the meta-model.
●We ful ﬁll the ﬁrst and second requirements by implement-
ing the task-model as an ensemble of multiple one-versus-
all classi ﬁers, each initialized with the parameters of the
meta-model ( ﬁrst requirement), as shown in Supplemen-
tary Fig. 1b). Each classi ﬁer is responsible for predicting a
single class when multiple classes are present in the task
(second requirement). When ﬁne-tuning this task-model
on a downstream task with nclasses, each classi ﬁer
minimizes a binary cross-entropy loss with stochastic
gradient descent concerning one positive class and n−1
negative classes. Given a new input sample for prediction
(second row of Supplementary Fig. 1b), each ensemble
member in the task-model predicts a score associated with
its respective class. Classi ﬁcation probabilities for each class
can be retrieved by either sigmoid-normalizing the
prediction scores of each member separately or by
combining the prediction scores by softmax. We used
softmax normalization for all experiments except for the
qualitative analysis in Fig. 4. Here, we obtained better
occlusion sensitivity maps with sigmoid normalization, as
the presence of one class did not in ﬂuence the prediction of
another.
●To address the third requirement, we select a subset of thelearned convolutional ﬁlter banks in the ﬁrst convolution in
the input block of the ResNet-12 neural network, as shown
schematically in Supplementary Fig. 1c. The meta-model ’s
ﬁrst layer normally convolves a 15-dimensional input
image with 15 convolutional ﬁlter banks, as shown in the
top row. When a downstream task provides data with, for
instance, three RGB spectral bands (bottom row), we only
copy the ﬁlter banks responsible for the RGB channels from
the meta-model to initialize the task-model. This transfer ismeaningful as long as the requested spectral bands form a
subset of the spectral bands of the meta-model. In terms of
downstream tasks with images of different spatial resolu-
tions, no model modi ﬁcations are necessary, as ResNets
ingest images of different sizes natively.
Task-model modi ﬁcation for segmentation . Some downstream
tasks, such as marine debris segmentation (Fig. 6), require the
model to output segmentation maps rather than a single classi-
ﬁcation probability. For pixel-wise segmentation, the task-model
needs to be adapted structurally, as shown in Supplementary
Fig. 1a). We modify the network without changing the weight
dimensions by removing the global average pooling in the
penultimate layer and replacing the ﬁnal linear layer with 1 × 1
convolutions. These convolutions are equivalent to linear layers
applied to each feature-pixel separately and, thus, use identical
weight dimensions. This modi ﬁcation yields a 9 px × 9 px seg-
mentation map for a 64 px × 64 px image, which is then upscaled
via bicubic interpolation to the original resolution.ARTICLE COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0
10 COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv
Training details . During pre-training of the meta-model, we
train in iterations containing batches of 16 tasks and aggregate
metrics over cycles of 200 training iterations. We employ Adam55
as an outer optimizer and set its learning rate to 0.001, which is
further decreased by a factor of 0.1 if the validation loss has not
decreased over 20 cycles. The training is stopped when the vali-
dation loss did not decrease over 40 cycles or at 40000 training
iterations. The gradient update is done with standard second-
order MAML in a single gradient step. The inner learning rate
(i.e., step size α) is set 0.32 experimentally and the model weights
are updated with stochastic gradient descent. We experimented
with two gradient steps at the cost of a smaller batch size but
found training on larger batches with a single gradient step to
yield better performances. Fine-tuning on downstream tasks is
realized through regular stochastic gradient descent with a step
size between 0.32 and 0.4 and 20 –60 gradient steps. We ﬁnd that
a comparatively wide range of step sizes and the number of
gradient steps led to similar solutions in the classi ﬁcation
experiments. The loss function is binary cross-entropy for both
pre-training and ﬁne-tuning on all downstream tasks. For the
qualitative segmentation experiment in Fig. 6,w e ﬁne-tuned
METEOR with a cross-entropy objective on each pixel. For the
change detection experiment in Fig. 5,w ed e ﬁne two classes, “pre-
event" and “post-event", and similarly use cross-entropy.
The pre-training of the meta-model was performed on two
NVIDIA V100 GPUs within a computational SLURM cluster
within 48 h. The estimated carbon footprint for pre-training one
meta-model was 5 kg/eCO 2. Fine-tuning and prediction on the
seven different DFC2020 regions took 4 min on a NVIDIA
GeForce RTX 3090. The ﬁne-tuning and prediction of 1-shot
marine debris images took 10 s on a workstation with 32 CPU
cores and 120GB RAM and 2 min 10 s on a MacBook Pro (2020)
with Apple M1 CPU and 16GB RAM.
Comparison models . We compared METEOR across applica-
tions with prototypical networks and self-supervised contrastive
learning algorithms in Tables 2and3.
Prototypical Networks (ProtoNets)17are a metric-based few-
shot learning approach where a deep neural network (ProtoNet)
is trained with the identical source tasks, as METEOR. In
ProtoNets, a deep feature extractor maps all images of one task
into a common feature space. The feature representations of the
training images are then averaged into prototype vectors. The test
images are associated with the class of the nearest prototype in
Euclidean distance. The feature extractor is iteratively optimized
via gradient descent to minimize the classi ﬁcation error of the test
images over a batch of few-shot tasks. We implemented the
prototypical network with a ResNet-18 model and trained it with
a learning rate of 0.001 until the convergence of the validation
error in the Sen12MS dataset.
Self-supervised contrastive learning provides an alternative way
to obtain feature extractor representations. In contrast to
prototypical networks, the optimization objective is to minimize
the error on a hand-de ﬁned pretext task designed not to require
annotations and to mimic some characteristic of interest of the
data we want the model to be robust against. Usually, these
methods are adapted to new downstream problems by freezing
the feature extractor and ﬁne-tuning a relatively small classi ﬁer
network that can be a single linear layer or a multi-layer
perceptron. However, we found in initial experiments (in
Supplementary Fig. 2), where we compared different adaptation
strategies, that this ﬁne-tuning with a dedicated classi ﬁer network
does not lead to accurate results for few-shot problems with less
than 50 shots. We, therefore, decided to follow the strategy of
prototypical networks and perform nearest neighbor classi ﬁcationwith Euclidean distance in the feature space directly to obtain the
results in Tables 2and3.
Speci ﬁcally, we compared to the MoCo-trained ResNet-50
from SSL4EO34and the ResNet-50 from SSLTRANSRS16which
were pre-trained on full multi-spectral data. In Supplementary
Table 1, we compared to pre-trained variants provided in these
code bases and choose models that achieved the best accuracy on
the diverse downstream problems of Table 3. Furthermore, we
compared METEOR to seasonal contrast (SeCo)35, which uses
Momentum Contrast v2 (MoCo v2)44,56on RGB representations
of unlabeled Sentinel-2 images of the same scene at different
dates, therefore learning robustness to image seasonality. We
further use the weights of SWAV36and DINO37, which are pre-
trained on natural images and equally employ a ResNet-50. We
also compare to a supervised BASELINE trained in a supervised
way on ten classes of the Sen12MS dataset. We used a learning
rate of 10−3and a weight decay of 10−6and take the model that
achieved the best validation accuracy over 50 epochs. We trained
a ResNet-12 (same architecture of METEOR), ResNet-18, and
ResNet-50, and use the ResNet-18 in the comparison of Tables 2
and3, as it achieves the best results across the different backbones
(Supplementary Table 1). MOSAIKS29proposed a featurization
strategy based on autocorrelation features from Random Kitchen
Sinks57extracted by a small neural network30. As the most
applicable featurization strategy of MOSAIKS is ambiguous, we
test different con ﬁgurations (Gaussian and Laplacian random
features, empirical global features, and empirical local features
and local features-supervised), as described further in the
Supplementary Notes 2. We compared all variants to METEOR
in Supplementary Table 1 where the “empirical local features"
strategy led to the best accuracies across all problems which we
use as MOSAIKS implementation in Tables 2and3.
Datasets . Nine datasets have been used throughout the
experiments.
The Sentinel-12 Multi-Spectral (Sen12MS)5dataset is used for
pre-training the meta-model. It contains Sentinel-1 and Sentinel-
2 images with associated land cover labels in a coarse
segmentation map in 125 globally distributed geographic regions.
We use Sen12MS for classi ﬁcation by associating the image with
the majority class observed in the patch39. The original dataset
contains overlapping images of 256 px × 256 px. Following prior
work28, we remove the overlap in the images, which yields images
of 128 px × 128 px in size. Nine different land use and land cover
categories are present in this dataset that follow a simpli ﬁed5,39
International Geosphere Biosphere Program (IGBP)58classi ﬁca-
tion scheme. These classes contain the general land cover
categories forests, shrubland, savanna, grassland, wetlands, crop-
lands, urban/built-up, snow/ice, barren, water. these classes
throughout the globe is substantially different from each other.
We split the data into distinct geographical regions for training
(75), validation (25), and test (25) to prevent geographical
autocorrelation and the potential positive biases of the training set
leaking into the test regions. The test regions are used after
training to evaluate ﬁnal accuracy on Sen12MS tasks, as reported
in Table 1. The meta-model is trained on tasks from the 75
training regions, while tasks from the validation regions are used
for parameter tuning and early stopping of the pre-training
process.
The public Data Fusion Contest 2020 (DFC2020) dataset33was
designed to mirror Sen12MS with the same IGBP labels on seven
different geographic regions. The annotations were semi-
automatically re ﬁned and contain less label noise compared to
Sen12MS, which makes these regions most suitable for qualitative
and quantitative evaluation. Each DFC2020 region is partitionedCOMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0 ARTICLE
COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv 11
into non-overlapping tiles of 256 px × 256 px and segmentation
labels are provided alongside the optical Sentinel-2 and radar
Sentinel-1 images. As for the previous dataset, we select the most
frequent land cover in the segmentation map as a classi ﬁcation
label for each tile following Schmitt and Wu39. The accuracy of all
seven regions is used in Tables 1and2. We selected the Kippa-
Ring region for the qualitative result in Fig. 2and also compared
it with other tasks in Table 3. In general, METEOR can achieve
high performance on land cover classi ﬁcation problems (as in
DFC2020 or Sen12MS), while accommodating the regional
differences in the representation of land covers, as croplands or
forests, which show a high intra-class variability, both spectrally
and geographically. Concretely, this can increase the accuracy of
land cover classi ﬁcation in general by ﬁne-tuning many region-
speciﬁc classi ﬁers.
The EuroSAT benchmark dataset contains multi-spectral
Sentinel-2 images of 64 px × 64 px with 13 spectral bands. It
features nine land use and land cover classes: annual crop,
herbaceous vegetation, industrial, permanent crop, river, forest,
highway, pasture, residential, sea lake. The dataset is arti ﬁcially
balanced and contains 2500 –3000 images per class. In this case,
METEOR can learn the speci ﬁc representation of land cover for
southern Germany.
The NWPU-RESISC45 benchmark41is a broadly used bench-
mark dataset that contains RGB images of 256 px × 256 px size at
different resolutions of 45 diverse classes. disciplines. Each class is
represented by 700 images. To build an urban scene classi ﬁcation
problem, we speci ﬁcally select the classes commercial, residential,
dense-, medium-, and sparse residential to create a downstream
task for urban scene classi ﬁcation. We compare models on this
dataset in Table 3and show results qualitatively in Fig. 4. We use
this dataset to test whether METEOR can learn more ﬁne-grained
distinctions of different urban scenes, even if it was pre-trained
only on general land cover classes from Sen12MS.
The Floating Marine Objects dataset6contains Sentinel-2
images with hand-annotated labels of 26 coastal regions across
the globe. Marine debris exhibits a strong heterogeneity6; hence,
learning a single representation for marine debris is dif ﬁcult due
to different compositions of materials, different water transpar-
ency due to sediments, and different atmospheric conditions that
vary between satellite scenes. Automated atmospheric correction
cannot completely remove these latter effects7. We select images
from the coastal region near Accra, Ghana, where liquid
pollutants were visually detected and annotated on a Sentinel-2
scene on October 31, 2018. We use this dataset for segmentation
in Fig. 6. In Table 3, we use this data in a binary classi ﬁcation
setting where images of ﬂoating objects are assigned a positive
class, and randomly sampled images from the entire Sentinel-2
scene are used as negatives. We use this dataset to test the limits
of METEOR in distinguishing details within water scenes, even
though the meta-model was pre-trained on generic Sen12MS land
cover classes. Approaching this application ﬁeld with meta-
learning is particularly promising, as hardly any label data exists
for marine debris detection. Moreover, the heterogeneity of the
types of marine debris further complicates training a single
dataset. Hence, METEOR can be ﬁne-tuned in multiple debris
and region-speci ﬁc task-models calibrated for the speci ﬁc area
under study.
DENETHOR42is a crop type mapping dataset that provides
PlanetScope and Sentinel-2 images of the year 2018 from nine
crop categories. In Table 3, we use a single PlanetScope image
from May 8, 2018, and obtain a cutout of the entire scene around
each ﬁeld parcel. This cutout is reshaped to a rectangular image of
128 px × 128 px. We select only ﬁeld parcels that are larger than
30,000 m2to maintain a certain homogeneity after rescaling. We
also select three classes wheat, corn, and meadow to obtain anannotated image dataset of 640 images, as we found that the
classi ﬁcation of all crop types using a single image was too
complex for all models with only few training samples. Here, the
regional variability of croplands and the high temporal variabilityof growing crops makes it similarly dif ﬁcult to construct a
representative dataset. METEOR ﬁne-tuned to few examples of
cropland in one particular area and one particular growing phase
can provide accurate predictions with little annotation effort.
AnthroProtect
43was gathered to measure the presence of
human in ﬂuence from Sentinel-2 imagery in Fennoscandia. It
consists of Sentinel-2 images of areas that are designated as
naturally protected areas and are, thus, minimally in ﬂuenced by
humans. These images are classi ﬁed against Sentinel-2 scenes of
non-protected areas within the same countries. This dataset
contains 990 annotated images and results are reported in
Table 3. This dataset tests the applicability of METEOR to
distinguish ﬁne-grained differences between human-in ﬂuenced
and natural (protected) areas.
The datasets for the qualitative deforestation and change
detection problems have been created by the authors, and details
are provided in Figs. 3and5, respectively. They will be available
in the provided repository. Fine-tuning METEOR on deforesta-
tion detection with few training examples can support remote
sensing and environmental research in quickly exploring and
identifying newly deforested areas with few training examples.
These deforestation events are often time-critical, especially when
emerging in novel areas that have not been mapped before.
Hence, ﬁne-tuning a deep learning model with a few training
examples can accelerate the identi ﬁcation of new emerging
deforestation hot spots. Similarly, the change detection applica-
tion in Beirut highlights the need to quickly identify affected areas
in natural disaster cases from a few training examples.
Data availability
The self-created datasets for change detection are acquirable from the provided
repository https://github.com/marccoru/meteor/blob/master/doc/beirutdata.md , where
we provide a direct download link for the image data. The Sentinel-2 images were
obtained through Google Earth Engine59and are free of charge in a Creative Commons
CC BY-SA 3.0 IGO licence. The PlanetScope data for the deforestation use-case wereobtained through the PlanetScope Education and Research Plan. We provide a ﬁle in the
repository under https://github.com/marccoru/meteor/blob/master/doc/
deforestationdata.md that lists the PlanetScope explorer query and the image ids used in
this study. To acquire the actual images, one must re-download either through their ownPlanetScope Education and Research Plan or by purchasing the data from PlanetScopedirectly. All other datasets (Sen12MS
5, EuroSAT40, NWPU-RESISC4541, DFC202033,
Floating Marine Objects6, Denethor42, AnthroProtect43) are publically available from the
respective authors. We provide an overview ﬁle with download links under https://
github.com/marccoru/meteor/blob/master/doc/public_datasets.md .
Code availability
The source code, model weights of the meta-model are available at https://github.com/
marccoru/meteor alongside scripts to reproduce the experiments.
Received: 10 June 2023; Accepted: 27 November 2023;
References
1. Camps-Valls, G., Tuia, D., Zhu, X. X. & Reichstein, M. (eds) Deep Learning for
the Earth Sciences: A Comprehensive Approach to Remote Sensing (Wiley &
Sons, 2021).
2. Weir, N. et al. Spacenet mvoi: a multi-view overhead imagery dataset. In
Proceedings of the IEEE/CVF International Conference on Computer Vision ,
992 –1001 (IEEE, 2019).
3. Sumbul, G., Charfuelan, M., Demir, B. & Markl, V. Bigearthnet: a large-scale
benchmark archive for remote sensing image understanding. In IGARSS 2019-ARTICLE COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0
12 COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv
2019 IEEE International Geoscience and Remote Sensing Symposium , 5901-
5904 (IEEE, 2019).
4. Sumbul, G. et al. Bigearthnet-mm: a large-scale, multimodal, multilabel
benchmark archive for remote sensing image classi ﬁcation and retrieval
[software and data sets]. IEEE Geosci. Remote Sens. Mag. 9,1 7 4 –180
(2021).
5. Schmitt, M., Hughes, L. H., Qiu, C. & Zhu, X. X. Sen12ms –a curated dataset
of georeferenced multi-spectral sentinel-1/2 imagery for deep learning anddata fusion. In ISPRS Annals of the Photogrammetry, Remote Sensing and
Spatial Information Sciences , IV-2/W7, 153 –160 (2019).
6. Mifdal, J., Longépé, N. & Rußwurm, M. Towards detecting ﬂoating objects on
a global scale with learned spatial features using sentinel 2. ISPRS Ann.
Photogramm. Remote Sens. Spat. Inf. Sci. 3, 285 –293 (2021).
7. Biermann, L., Clewley, D., Martinez-Vicente, V. & Topouzelis, K. Finding
plastic patches in coastal waters using optical satellite data. Sci. Rep. 10,1–10
(2020).
8. Kikaki, K., Kakogeorgiou, I., Mikeli, P., Raitsos, D. E. & Karantzalos, K.
Marida: a benchmark for marine debris detection from sentinel-2 remote
sensing data. PLoS One 17, e0262247 (2022).
9. Murphy, K. P. Beyond the iid assumption. In Probabilistic Machine Learning:
Advanced Topics , Ch. 19, 727 –762 (MIT Press). http://probml.github.io/
book2 . Version 2023-08-15 (2023).
10. Lemberger, P. & Panico, I. A primer on domain adaptation. Preprint at
arXiv:2001.09994 (2020).
11. Yang, Q., Zhang, Y., Dai, W. & Pan, S. J. Transfer Learning (Cambridge
University Press, 2020).
12. Schmidhuber, J. Evolutionary Principles in Self-referential Learning, or on
Learning How to Learn: The Meta-meta-... hook. PhD thesis, Technische
Universität München (1987). https://mediatum.ub.tum.de/813181?show_id =
813180 .
13. Hospedales, T., Antoniou, A., Micaelli, P. & Storkey, A. Meta-learning in
neural networks: a survey. IEEE Trans. Pattern Anal. Mach. Intell. 44,
5149 –5169 (2021).
14. Tan, C. et al. A survey on deep transfer learning. In Artiﬁcial Neural Networks
and Machine Learning –ICANN 2018: 27th International Conference on
Artiﬁcial Neural Networks, Rhodes, Greece, October 4 –7, 2018, Proceedings,
Part III 27 , 270 –279 (Springer, 2018).
15. Sun, X. et al. Ringmo: a remote sensing foundation model with masked image
modeling. IEEE Transactions on Geoscience and Remote Sensing (2022).
16. Scheibenreif, L., Hanna, J., Mommert, M. & Borth, D. Self-supervised vision
transformers for land-cover segmentation and classi ﬁcation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
1422 –1431 (2022).
17. Snell, J., Swersky, K. & Zemel, R. Prototypical networks for few-shot learning.
Advances in Neural Information Processing Systems (NeurIPS) 30 (2017).
18. Vinyals, O. et al. Matching networks for one shot learning. Advances in Neural
Information Processing Systems 29 (2016).
19. Zhang, P., Bai, Y., Wang, D., Bai, B. & Li, Y. Few-shot classi ﬁcation of aerial
scene images via meta-learning. Remote Sensing 13, 108 (2021).
20. Sharma, S., Roscher, R., Riedel, M., Memon, S. & Cavallaro, G. Improving
generalization for few-shot remote sensing classi ﬁcation with meta-learning.
InIGARSS 2022-2022 IEEE International Geoscience and Remote Sensing
Symposium , 5061 –5064 (IEEE, 2022).
21. Tang, X. et al. Multi-scale meta-learning-based networks for high-resolution
remote sensing scene classi ﬁcation. In 2021 IEEE International Geoscience and
Remote Sensing Symposium IGARSS , 4928 –4931 (IEEE, 2021).
22. Lunga, D., Arndt, J., Gerrand, J. & Stewart, R. Res ﬂow: a remote sensing
imagery data- ﬂow for improved model generalization. IEEE J. Sel. Top. Appl.
Earth Obs. Remote Sens. 14, 10468 –10483 (2021).
23. Finn, C., Abbeel, P. & Levine, S. Model-agnostic meta-learning for fast
adaptation of deep networks. In Proceedings of the International Conference on
Machine Learning (ICML) , 1126 –1135 (PMLR, 2017).
24. Von Oswald, J. et al. Learning where to learn: gradient sparsity in meta and
continual learning. Advances in Neural Information Processing Systems
(NeurIPS) 34 (2021).
25. Bronskill, J., Gordon, J., Requeima, J., Nowozin, S. & Turner, R. Tasknorm:
rethinking batch normalization for meta-learning. In Proceedings of the
International Conference on Machine Learning (ICML) , 1153 –1164 (PMLR,
2020).
26. Oh, J., Yoo, H., Kim, C. & Yun, S.-Y. Boil: towards representation change for
few-shot learning. Proceedings of the International Conference on Learning
Representations (ICLR) (2021).
27. Tseng, G., Kerner, H., Nakalembe, C. & Becker-Reshef, I. Learning to predict
crop type from heterogeneous sparse labels using meta-learning. In
Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) , 1111 –1120 (2021).
28. Rußwurm, M., Wang, S., Korner, M. & Lobell, D. Meta-learning for few-shot
land cover classi ﬁcation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops (CVPRW) , 200 –201
(2020).
29. Rolf, E. et al. A generalizable and accessible approach to machine learning
with global satellite imagery. Nat. Commun. 12,1–
11 (2021).
30. Coates, A. & Ng, A. Y. Learning feature representations with k-means. In G.
Montavon, G. B. Orr, K.-R. Müller (Eds.) Neural Networks: Tricks of the
Trade , 561 –580. Second Edition (pp. 561-580). Springer Berlin Heidelberg 2nd
Edition LNCS 7700 (2012).
31. Ioffe, S. & Szegedy, C. Batch normalization: Accelerating deep network
training by reducing internal covariate shift. In Proceedings of the
International Conference on Machine Learning (ICML) , 448 –456 (PMLR,
2015).
32. Ulyanov, D., Vedaldi, A. & Lempitsky, V. Instance normalization: the missing
ingredient for fast stylization. Preprint at arXiv:1607.08022 (2016).
33. Schmitt, M., Hughes, L., Ghamisi, P., Yokoya, N. & Hänsch, R. IEEE GRSS
Data Fusion Contest. IEEE Dataport .https://doi.org/10.21227/rha7-m332
(2020).
34. Wang, Y. et al. Ssl4eo-s12: a large-scale multi-modal, multi-temporal dataset
for self-supervised learning in earth observation. Preprint at arXiv:2211.07044
(2022).
35. Mañas, O., Lacoste, A., Giro-i Nieto, X., Vazquez, D. & Rodriguez, P. Seasonal
contrast: Unsupervised pre-training from uncurated remote sensing data. In
Proceedings of the IEEE/CVF International Conference on Computer Vision
(CVPR) , 9414 –9423 (2021).
36. Caron, M. et al. Unsupervised learning of visual features by contrasting cluster
assignments. Adv. Neural Inf. Proc. Syst. 33, 9912 –9924 (2020).
37. Caron, M. et al. Emerging properties in self-supervised vision transformers. In
Proceedings of the IEEE/CVF International Conference on Computer Vision(CVPR) , 9650 –9660 (2021).
38. Wilcoxon, F. Individual Comparisons by Ranking Methods (Springer, 1992).
39. Schmitt, M. & Wu, Y.-L. Remote sensing image classi ﬁcation with the
sen12ms dataset. In ISPRS Annals of the Photogrammetry, Remote Sensing and
Spatial Information Sciences , Vol. V-2-2021, 101 –106 (2021).
40. Helber, P., Bischke, B., Dengel, A. & Borth, D. Eurosat: a novel dataset and
deep learning benchmark for land use and land cover classi ﬁcation. IEEE J.
Sel. Top. Appl. Earth Obs. Remote Sens. 12, 2217 –2226 (2019).
41. Cheng, G., Han, J. & Lu, X. Remote sensing image scene classi ﬁcation:
benchmark and state of the art. Proc. IEEE 105, 1865 –1883 (2017).
42. Kondmann, L. et al. Denethor: the DynamicEarthNET dataset for
harmonized, inter-operable, analysis-ready, daily crop monitoring from space.
InThirty- ﬁfth Conference on Neural Information Processing Systems (NeurIPS)
Datasets and Benchmarks Track (Round 2) (2021).
43. Stomberg TT, Leonhardt J, Weber I and Roscher R. Recognizing protected and
anthropogenic patterns in landscapes using interpretable machine learning
and satellite imagery. Front. Artif. Intell. 6, 1278118 (2023).
44. He, K., Fan, H., Wu, Y., Xie, S. & Girshick, R. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , 9729 –
9738
(2020).
45. Roscher, R., Bohn, B., Duarte, M. F. & Garcke, J. Explain it to me –facing
remote sensing challenges in the bio-and geosciences with explainable
machine learning. ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci. V-3-
2020 , 817 –824 (2020).
46. Zeiler, M. D. & Fergus, R. Visualizing and understanding convolutional
networks. In Proceedings of the European Conference on Computer Vision
(ECCV) , 818 –833 (Springer, 2014).
47. Team, P. Planet application program interface: In Space for Life on Earth .
https://api.planet.comx (2017).
48. Barni, P. E., Fearnside, P. M. & Graça, P. M. L. d. A. Simulating deforestation
and carbon loss in Amazonia: impacts in Brazil ’s Roraima state from
reconstructing highway br-319 (Manaus-Porto velho). Environ. Manage. 55,
259 –278 (2015).
49. Y. Wang, C. M. Albrecht, N. A. A. Braham, L. Mou and X. X. Zhu. Self-
Supervised Learning in Remote Sensing: A review. In IEEE Geoscience and
Remote Sensing Magazine ,10, 213 –247, (2022).
50. Tuia, D. et al. Toward a collective agenda on AI for earth science data analysis.
IEEE Geosci. Remote Sens. Mag. 9,8 8 –104 (2021).
51. Pan, S. J. & Yang, Q. A survey on transfer learning. IEEE Trans. Knowl. Data
Eng. 22, 1345 –1359 (2009).
52. Deng, J. et al. Imagenet: a large-scale hierarchical image database. In
Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) , 248 –255 (IEEE, 2009).
53. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image
recognition. In Proceedings of the IEEE/CVF International Conference on
Computer Vision (CVPR) , 770 –778 (2016).
54. Huang, X. & Belongie, S. Arbitrary style transfer in real-time with adaptive
instance normalization. In Proceedings of the IEEE International Conference
on Computer Vision (ICCV) , 1501 –1510 (2017).COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0 ARTICLE
COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv 13
55. Kingma, D. and Ba, J. Adam: A Method for Stochastic Optimization.
Proceedings of the 3rd International Conference on Learning Representations(ICLR 2015) .
56. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick. Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) , pp. 9729 –9738 (2020).
57. Rahimi, A. & Recht, B. Weighted sums of random kitchen sinks: replacing
minimization with randomization in learning. Advances in Neural
Information Processing Systems 21 (2008).
58. Loveland, T. & Belward, A. The international geosphere biosphere programme
data and information system global land cover data set (discover). Acta
Astronaut. 41, 681 –689 (1997).
59. Gorelick, N. et al. Google Earth engine: planetary-scale geospatial analysis for
everyone. Remote Sens. Environ. 202,1 8 –27 (2017).
60. Wu, Y. & He, K. Group normalization. In Proceedings of the European
Conference on Computer Vision (ECCV) ,3–19 (2018).
Author contributions
M.R. envisioned and implemented the method, the experiments, wrote the ﬁrst draft the
manuscript, and integrated the suggestions of the co-authors. S.W. contributed by sug-
gesting additional experiments and overall revision of the manuscript. B.K. contributedby experimental design and manuscript writing. R.R. provided expertise on explainabilitymethods in experimental design and manuscript re ﬁnement. D.T. contributed in advising
in model and experimental design and manuscript revision.
Competing interests
The authors declare no competing interests.Additional information
Supplementary information The online version contains supplementary material
available at https://doi.org/10.1038/s43247-023-01146-0 .
Correspondence and requests for materials should be addressed to Marc Rußwurm.
Peer review information Communications Earth & Environment thanks Sun Xian and
the other, anonymous, reviewer(s) for their contribution to the peer review of this work.Primary Handling Editors: Martina Grecequet. A peer review ﬁle is available.
Reprints and permission information is available at http://www.nature.com/reprints
Publisher ’s note Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional af ﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you giveappropriate credit to the original author(s) and the source, provide a link to the CreativeCommons license, and indicate if changes were made. The images or other third partymaterial in this article are included in the article ’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in the
article ’s Creative Commons license and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly fromthe copyright holder. To view a copy of this license, visit http://creativecommons.org/
licenses/by/4.0/ .
© The Author(s) 2024ARTICLE COMMUNICATIONS EARTH & ENVIRONMENT | https://doi.org/10.1038/s43247-023-01146-0
14 COMMUNICATIONS EARTH & ENVIRONMENT |            (2024) 5:37 | https://doi.org/10.1038/s43247-023-01146-0 | www.nature.com/commsenv
Combining Remotely Sensed Imagery with Survival Models for Outage Risk
Estimation of the Power Grid
Arpit Jain*
GE Global Research
San Ramon, CA, USA
arpitjain1@gmail.comTapan Shah*
GE Global Research
San Ramon, CA, USA
tapan.shah@ge.comMohammed Yousefhussien*
GE Global Research
Niskayuna, NY , USA
myhussien@gmail.com
Achalesh Pandey
GE Global Research
San Ramon, CA, USA
achalesh.pandey@ge.com
Abstract
Vegetation management of power grids is essential for
reliable distribution of services, prevention of forest ﬁres
and disruption of electricity due to tree fall. In this paper,
we introduce a vegetation analysis system that utilizes in-
formation from GIS data, aerial and satellite imagery to
estimate vegetation proﬁle within a buffer zone. This veg-
etation proﬁle is further combined with operational param-
eters of the grid to develop a survival model which predicts
the outage risk of a power-line in an electrical grid. Using
historical data, we show that the risk scores thus obtained
are signiﬁcantly better at developing trimming schedules for
grid power-lines, compared to existing available methods.
1. Introduction
Aerial and satellite image analysis is becoming increas-
ingly important in addressing the challenges related to
disaster management, infrastructure and urban planning,
Transmission and Distribution (T&D) maintenance. Util-
ity companies are under immense pressure to maintain their
frail T&D infrastructure with limited resources. In this pa-
per, we focus on solving vegetation management problem
which is critical to preventing catastrophic events such as
forest ﬁres and large scale outages due to vegetation en-
croachment.
Utility companies perform two types of tree trimming
activities on power-lines to address vegetation encroach-
ment: (a) scheduled or planned trimming (b) hot-spot or un-
planned trimming. Scheduled trimming is performed based
on a schedule designed by inspector based on operation
*Equal ContributionOutage Reporting History
Trim and Veg Vendor Data
GIS –Asset data
Criticality and Customer 
Information
Weather informationOperational/Weather/Reliability 
Parameters
Satellite/Aerial Imagery0 1 0 1 1 0
1 0 1 0 1 0Model Risk Scores
Feeder/Segment Risk    HIGH
Feeder/Segment Risk    LOW
Feeder/Segment Risk    LOW
Feeder/Segment Risk    MEDOutage Reporting History
GIS –Asset dataOperational/Reliability 
Parameters
Trim and Veg Vendor Data
Criticality and Customer 
Information
Feeder/Segment Risk    MED
Feeder/Segment Risk    HIGH
Feeder/Segment Risk    LOW
Feeder/Segment Risk    MED
Prevalent approach to vegetation Management
Our proposed approach
Figure 1. An illustration comparing the prevalent and our proposed
approach to vegetation management.
knowledge or ad-hoc combination of operations variables
such as number of customer impacted or number of outages
experienced on a given power-line without the knowledge
of actual state of vegetation on the ground. Hot-spot trim-
ming is performed on-need basis when customers complain
about encroachment or due to adverse weather event.
Currently, a vegetation manager determines the risk as-
sociated with each power-line and creates a trimming sched-
4321

ule based on historical outages and operational data [ 26].
This approach is subjective, biased and fails to incorporate
the actual vegetation encroachment on the ground, thereby
resulting in a sub-optimal trimming schedule which further
leads to higher vegetation related outages as well as higher
maintenance cost [ 10]. Figure 1shows the current approach
to vegetation management and the motivation for our pro-
posed approach.
In recent times, utility companies have started investi-
gating the adoption of LiDAR-based vegetation manage-
ment [ 11,26]. While LiDAR provides high accuracy and ﬁ-
delity for vegetation encroachment, they are still not widely
adopted due to high cost associated with data collection and
processing. On the other hand, satellite and aerial imagery
provides a compelling solution with the beneﬁt of large-
scale coverage at lower cost. Even though our proposed ap-
proach leverages aerial and satellite imagery in this paper, it
can easily incorporate LiDAR-based vegetation information
when available.
In this paper, we propose a machine learning frame-
work (Figure 1) to address tree trimming problem using sur-
vival modeling which combines GIS data, aerial and satel-
lite imagery, and other operational parameters to estimate
the outage risk which can be used to derive an optimal
scheduling. The same framework can also address the is-
sue of spot trimming to identify the most risky power-lines
which may be in need of trimming prior to their scheduled
trim time.
Vegetation segmentation in large scale aerial and satel-
lite imagery can be challenging due to the lack of anno-
tated data. In the absence of large budget for annotation
task, we propose a two-stage approach to training the CNN
model for vegetation modeling using semi supervised strat-
egy. Our experiments shows that with just a few number
of annotated samples, we can train a CNN vegetation seg-
mentation model which generalizes well for the entire lo-
cal grid territory. Another challenge related to vegetation
segmentation in freely available aerial imagery data (NAIP
dataset - 1m resolution) is the low resolution of trees. In the
NAIP dataset, a typical tree width spans a range of 10-35m
on the ground which translates to 10-35 pixels, and twice
that value for WorldView-3 satellite at 0.5m resolution. We
propose a modiﬁed segmentation approach based on [ 40] to
address the issues of “small” tree pixels by avoiding pooling
operations in encoding layers.
Risk associated with vegetation-related outages on a
power-line is also dependent on several other operational,
environmental and reliability factors besides tree encroach-
ment. For example, vegetation encroachment on a short
power-line serving 20,000 customer is more important to
trim earlier than a long power-line serving 10 customers.
Certain power grids are more prone to vegetation-related
outages due to weather conditions than others. To modelthe risk associated with each power-line, machine learning
approach will have to incorporate these additional variables
in prioritising trimming. We propose a survival modeling
based generic framework to model this requirement. Sur-
vival modeling can incorporate weather, operational Key
Performance Indicators (KPIs) and other reliability factors
in conjunction with vegetation encroachment from aerial
and satellite imagery to provide a holistic outage risk score
combining these factors.
In this paper, our main contributions are as follows:
• We combine aerial, satellite, and GIS data to deﬁne
tree encroachment around power-line segments for a
large-scale analysis. Our vegetation analysis can be
done at multiple scales (i.e. power-line segment, a
feeder, or a full circuit level)
• We utilize semi-supervised approach to reduce anno-
tation burden. We utilize weak classiﬁer to generate
pseudo labels which are then fed into segmentation
network for learning.
• We use Cox proportional hazard survival modeling
technique to combine the vegetation KPIs with other
reliability and operational attributes of a power-line as
well as the prior trimming schedule to assign a risk to
quantify the likelihood of a future outage of a power-
line.
1.1. Prior Work
Recently, aerial and satellite imagery are gaining an in-
creased attention from various research communities for
solving real world problems such as disaster response, ur-
ban planning, precision agriculture, and autonomous driv-
ing [ 12]. With increasing blackouts due to to vegetation
encroachments for power-lines, it is necessary to maintain
clearance within the right-of-way (ROW) buffer zone to
ensure uninterrupted distribution of electricity. Therefore,
utility companies are exploring the use of such rich data to
improve reliability and reduce cost of maintaining grid as-
sets [ 22].
Researchers have investigated the task of vegetation seg-
mentation and tree encroachments from various sensing
modalities. For a comprehensive overview of the modali-
ties and algorithms developed for vegetation segmentation
and tree encroachment before the proliferation of deep neu-
ral networks, we refer the reader to following survey pa-
pers [ 28] [2]. Convolutional Neural Networks have shown
tremendous success in semantic segmentation tasks and a
lot of recent works have focused around using deep neu-
ral networks for vegetation segmentation. [ 23] proposed a
multi-task learning framework for pan-sharpening and se-
mantic segmentation of trees in a 25 km20.3m resolution
WorldView-3 satellite data. [ 25,27,32,35] proposed several
4322
Conv 3x3 
⨁ ⨁ ⨁
364 64128128⨁Upsampling (2x2) –Conv
64256256
128256 256
256256 256⨁
128 128⨁
6464 128256⨁
2BN –ReLU –Conv 3x3 Residual connection
Copy & Concatenate ⨁ Add
Figure 2. Residual U-Net architecture used for vegetation segmentation
CNN based architectures for land cover segmentation which
can be extended to vegetation segmentation. In [ 33], a deep
learning-based detection framework that utilizes the images
obtained from vision sensors mounted on power transmis-
sion towers was proposed. The method includes three cas-
caded modules: detection of vegetation regions using Faster
R-CNN, detection of power lines based on the Hough trans-
form, and detection of vegetation encroachment using depth
from stereo. While the method was able to detect power-
lines and tree encroachment, it relies on ground-based sen-
sors close to the area of interest which hinders the ability to
cover large scale area for encroachment analysis.
A large scale study, similar to ours in terms of scale,
was conducted in [ 3] where the authors performed tree-
cover delineations using NAIP imagery. The study used
150 handcrafted statistical features computed from super-
pixel regions found using region growing algorithm. The
features per pixel are then classiﬁed using a fully-connected
feed forward neural network. A conditional Random Fields
(CRF) step is then applied to incorporate contexts for re-
gion consistency. While their work spanned a large scale
area, they used handcrafted features which may not gener-
alize well.
A lot of the prior work have focused on vegetation re-
lated risk modeling in the context of adverse weather. [ 38]
used LiDAR to identify high risk trees and combined prior
trim and infrastructure data with vegetation related informa-
tion using random forest to identify high risk power lines in
the event of a storm. Similarly, [ 6] proposes a outages pre-
diction model using regression trees based on weather pre-
diction outputs, soil information, vegetation, electric utility
assets, and historical power outage data. However, none of
these approaches address the issue of vegetation manage-
ment from trimming perspective.
Survival models for risk modeling has been used forseveral decades in multiple applications like cancer sur-
vival [ 7,39], predictive maintenance and remaining useful
life of industrial systems [ 34,36] but our work is among the
ﬁrst to apply it to vegetation trimming problem.
[15] studied the vegetation related outages under two
categories: growth-related and weather-related outages.
Two types of models for the two tasks, namely Time se-
ries and nonlinear machine learning regression models, are
proposed to conduct the outage prediction using vegeta-
tion, prior outages, weather and geographical data. The
work most relevant to ours is [ 13] where a Gaussian con-
ditional random ﬁeld approach was proposed to combine
operational, weather and vegetation related parameters for
optimal tree trimming. The tree segmentation was obtained
through super-pixel clustering and assignment approach.
However, to the best of our knowledge, there is no prior
work which combines aerial and satellite imagery with ad-
vanced deep neural network-based computer vision tech-
niques and survival models to develop an end-to-end veg-
etation management system.
2. Vegetation Analysis
In this section, we will discuss in more details our ap-
proach for vegetation segmentation in aerial and satellite
imagery.
2.1. Vegetation Segmentation
We treat this problem as a two class segmentation prob-
lem as our focus is solely on vegetation encroachment.
However, proposed approach can be extended to multiple
classes. Several semantic segmentation architectures have
been proposed in literature. We use the Residual U-Net ar-
chitecture from [ 40] for vegetation segmentation. The net-
work is shown to work well for road segmentation in aerial
imagery and parameters work well for semantic classes with
4323
small footprints such as individual trees. We have minor
modiﬁcations to the network, found empirically, in number
of ﬁlters but the architecture is shared with the reference.
Figure 2shows the network architecture of our proposed ap-
proach. We use equally weighted binary cross-entropy and
Figure 3. Result of our data annotation strategy(a) Sample Train
data (b) Sample Manual annotation (c) Sample Unlabeled data (d)
Sample Pseudo label
dice loss as the loss function to minimize. Additional details
related to training is given in the experiment section. In the
next section, we will discuss the semi-supervised learning
approach to reduce the annotation requirement and improve
networks robustness against unseen territories.
Image annotation is a tedious and expensive task. Given,
we are performing vegetation segmentation on a large ter-
ritory, it is hard to annotate different variety of vegetation
across the territory. In order to ameliorate the lack of large
annotation dataset, we leverage a two-stage learning strat-
egy with a human in the loop to annotate our aerial and
satellite imagery. Initially, we manually label a small set of
images. In our case, we labeled 60 images of 512×512
pixels. Then, using this small set of labeled images, we use
a random forest [ 4] to introduce two sources of randomness
to help control over-ﬁtting. The ﬁrst is bagging , where each
tree is grown using a bootstrap sample of training data, and
the second one is random feature splitting where the best
split at each node is chosen from a random sample of fea-
tures instead of all. In our case, we trained 200 decision
trees estimators with a batch size of 100 pixels with unlim-
ited depth. We consider two random features at each node.
To train the model, we used a set of hand-crafted features
such as Gabor ﬁlters, mean, min , max, and the variance
at each pixel with 5neighborhood window. After training
the random forest on the manually labeled data, we then use
this model to automatically annotate a larger set of 2k im-
ages that are then used to train our deep learning along with
the manually labeled data. Figure 3shows a manually an-
notated image and the prediction of the random forest clas-
siﬁer.
2.2. Vegetation KPIs
In order to estimate outage risk associated with vege-
tation, we need to estimate certain KPIs like area of veg-
etation encroachment, length of encroachment, etc. with
respect to the utility grid and structures. These KPIs will
then feed to our machine learning model to estimate outagerisk along with other operational, environmental and relia-
bility parameters. Usually, 5m distance on either side of the
power line is considered no encroachment or ROW zone.
Any vegetation encroaching within the right-of-way is con-
sidered unsafe and needs to be trimmed. We ﬁrst perform
tree segmentation on all the tiles extracted from the aerial
imagery for the territory. We then combine all the tiles
together to create a single vegetation probability map for
the whole territory. The GIS information is then overlayed
on the probability map to deﬁne right of way by placing
a 5m buffer on either side of the power line. The power-
line itself is composed of a set of line segments. For each
line segment, we can compute vegetation area and length
of vegetation encroachment with an oriented bounding box
deﬁning the right-of-way zone. These Key Performance In-
dicators (KPIs) at individual section of a power-line repre-
sents the segment-level vegetation risk. Inspector can use
this information to identify regions where there are signiﬁ-
cant encroachment occurring within a power line. This in-
formation together with other variables can help identify fu-
ture potential outages associated with each power-line and
plan the trimming optimally. In the next section, we will
describe our survival modeling based approach which can
seamlessly combine vegetation related KPIs with other vari-
ables for risk estimation.
Figure 4shows an overview of our vegetation segmen-
tation on a sample patch from NAIP dataset and the corre-
sponding vegetation KPIs.
3. Risk Modeling Using Survival Models
In this section, we introduce survival model as a risk es-
timation methodology and illustrate how it can be used to
model outage risk of power-lines as a function of vegeta-
tion KPI and other attributes.
3.1. Survival Models: Basic principles
Survival models are a class of statistical models which
obtains estimates of time of a particular event of interest
(like death, failure, outage etc.) i.e. survival times. The
key property of a survival model is the ability to incorpo-
rate censored data i.e. if an individual does not have an
event till the observation time, they are described as cen-
sored. It means that after the data is collected, the individual
may or may not have an event. This special property makes
survival models more appealing to estimate event times as
compared to standard regression based methods (where we
would have to drop the censored data).
Some deﬁnitions related to survival modeling are as fol-
lows [ 24,37]:
•Survival function is the probability that the event of
interest occurred after speciﬁed time t.
S(t) =P(T > t),
4324
(a) (b) (c) (d)
Figure 4. Overview of our vegetation segmentation and KPI estimation. (a) an image tile from NAIP aerial imagery. (b) shows the result
of our vegetation segmentation overlaid on the image. (c) a zoomed-in region showing the power lines along with right-of-way non-
encroachment buffer zone (deﬁned by blue color lines). (d) Shows each power-line segment color coded from blue to pink in proportion to
the the area of vegetation encroachment. More pink indicates more vegetation overlap and more blue indicates the opposite.
whereTis the survival time.
•Cumulative death distribution isF(t) = 1−S(t)
and the death density function isf(t) =dF(t)
dt
•Hazard function is the likelihood of the event occur-
ring at time t, given no event occurred before t.
h(t) = lim
∆t→0P(t≤T≤t+∆tT≥t)
∆t(1)
= lim
∆t→0F(t+∆t)−F(t)
∆t·S(t)(2)
=f(t)
S(t)(3)
Concretely, each data-point in the data-set used to de-
velop survival models is a tuple (X,y,δ)whereXis the
feature vector (also called as covariates), δis an indica-
tor variable which is 1for uncensored instance and 0for
a censored instance, and yis the time of event (survival
time or failure time in some literature) for uncensored in-
stances or the observation time for censored instances. This
is unlike a supervised regression or classiﬁcation problem
where each data-point contains a tuple of 2points. In ad-
dition to specialized models like Cox Proportional Hazard
model [ 5], several machine learning methods like logis-
tic regression [ 39], support vector machines [ 30], random
forests [ 19], neural networks [ 16,21] have been modiﬁed to
obtain a transfer function which estimates the survival func-
tion or hazard function as a function of the feature vector1
3.2. Cox Proportional Hazards (PH) Model
Cox PH model is a semi-parametric model which models
the hazard function h(t,X)as a product of the time com-
ponentλ0(t)and the feature component η(X)
h(t,x) =λ0(t)η(x), (4)
where
1There are non-parametric methods like Kaplan-Meir estimate [ 20]
which do-not take into account the feature vector.•λ0(t)is the unspeciﬁed baseline function and,
•η(x)is the risk due to the features modeled as
exp(∑wiXi).
Note that we have deﬁned the hazard function as a function
of the feature vector X. The key point is that there is a
natural way in which the hazard changes with time, which is
the same for any two individuals. The difference in the risk
is due to the features associated with each individual. The
parameters {wi}’s are estimated by maximizing the partial
likelihood using the Newton Raphson’s method.
3.3. Connecting Survival Models and Risk Model­
ing of Power Lines
The key goal of our vegetation management system is to
develop a multi-variate model which can estimate the risk
of each power-line. The 3 types of attributes available for
each power-line to develop the risk model are explained:
1.Reliability attributes [ 1] like customers impacted (CI),
number of outages (NO), customer average interrup-
tion duration index (CAIDI), system average interrup-
tion duration index (SAIDI) and system average in-
terruption frequency index (SAIFI), of previous year,
given an indication of risk for the current year.
2.Environmental attributes like vegetation KPIs (as de-
scribed in Section 2) and weather attributes like wind
gust, precipitation and temperature [ 8].
3.Operational attributes like number of customers,
overhead coverage of the power-line and time since the
last trim.
Assumptions
Before explaining the modeling methodology, we highlight
two key assumptions below.
1. The ﬁrst of January for each year is the instant of ob-
servation and decision-making (i.e. all data prior to
4325
that day can be used to estimate the future outage risk
of each power-line for that year).
2. Each power-line is trimmed every 4 years. We assume
the risk is reset after each trim i.e. the power-line is
“reborn”. Also, the maximum life of a power-line is 4
years2.
In the next discussion, we show how the data collected
from power lines ﬁts naturally to the requirements of sur-
vival model paradigm. For any power-line ( F) on the 1stof
January of any year ( t), the training data tuple consists of
(XF(t−1),yF(t),δF(t))whereX(t−1)is the vector of
reliability, environmental and operational values from pre-
vious years, δF(t)is an indicator which is 1 if an outage
occurred for that power-line in the year tandyF(t)is the
number of days between the last trim and outage, if an out-
age occurred else it is the number of days between the last
trim and end of the year or the trim date. We observe that
the data is in a format similar to the one required to train
survival models (explained in Section 3.1). Collecting sim-
ilar data for multiple power-lines over the training period,
we estimate the parameters of a Cox proportional hazard
model using an implementation provided in [ 17]. The key
reason for preferring the Cox proportional hazard model is
the easy interpretability: the contribution of the attributes
to the “hazard” can be easily obtained from the magnitude
of the estimated coefﬁcients ˆwiin (4). Additionally, the
factorization of the baseline hazard and the feature contri-
bution makes it trivial to compute a time-independent risk
score(∑
iˆwiXi)(it is non-trivial for other methods).
4. Experimental Results
4.1. Dataset
Our study area covers 1760 mi2within two states in the
US. We collected 100 georeferenced NAIP and Worldview-
3 tiles where each tile covers a 3.7×4.66sq. miles. NAIP
imagery consists of 4 bands and is acquired at a 1m ground
sample distance (GSD), while the Worldview-3 imagery
consists of 8 multi-spectral bands along with SWIR and
CA VIS bands with GSD ranging from 0.3m to 10m. From
both sources, we used only 4 bands, namely Red, Green,
Blue, and Near-IR. Along with the imagery data, we also
utilized georeferenced GIS data indicating the location and
the extent of each power-line in the area of interest. Due
to the proprietary nature of the GIS data, the location is,
therefore, undisclosed.
2A natural question which can be raised is that why can’t we make
the prediction every month (or every day)? Even though our methodology
can be naturally extended, we provide a yearly risk for 2 main reasons.
Firstly, from a decision-making point of view, the vegetation inspector will
want a list of high risk power-lines at the beginning of the year to plan the
maintenance schedules. Secondly, it would require getting the images at a
higher temporal frequency which can be expensive.4.2. Performance of Vegetation Segmentation
We trained our segmentation model using a small set of
1860 images. However, we increased the training data by
augmenting using various strategies such as ﬂipping, rotat-
ing, adjusting brightness and contrast, as well as saturation
adjustment. Our model was trained for 50 epochs with a
batch size of 8, and a learning rate of 0.0001 using Adam
optimizer. To reduce the effect of over-ﬁtting, we applied
polyak averaging using exponential decay of the last 10
checkpoints, and applying label smoothing to the ground
truth data. We evaluated the performance of our segmenta-
tion approach using two separate metrics. We evaluated the
performance of our segmentation approach on a validation
set of 200 images. We achieved a mIoU score of 97.2%.
We also evaluated the performance of our segmentation in
ROW zone against a proprietary LiDAR-based ground truth
tree masks. Our approach obtained a mIoU score of 86%
against LiDAR data. This shows that our approach can pro-
vide comparable results w.r.t LiDAR data for tree segmen-
tation within the buffer zone.
4.3. Evaluation of Risk Modeling
4.3.1 Data Preparation
In order to validate the performance of the risk model-
ing method, we use the data from a utility company on a
population of 2000 power-lines located in a single geo-
graphical area. The reliability feature vector used were a
weighted sum of CI, number of outages (NO) and SAIDI
for the previous 2 years where the weight for the previ-
ous year was 2/3 and the weight for the year before was
1/3. For example, for 2019, the reliability features of a
power-line is [2
3CI(2018) +1
3CI(2017),2
3NO(2018) +
1
3NO(2017),2
3SAIDI(2018) +1
3SAIDI(2017)] . This
was appended with vegetation KPIs computed in Section
2as well as the overhead mileage of each power-line. The
temporal component required for the survival modeling is
computed as discussed in Section 3.3.
The data between 2016 and 2018 is used to train the
model whereas data in 2019 is used to evaluate the perfor-
mance of our model.
4.3.2 Evaluation
Discriminate measures are typically used to to validate the
performance of risk models [ 9,29,31] models. These mea-
sures capture how well can the model discriminate between
the “low” and “high” risk entities.
Ground truth: low and high risk : Identifying actual high
and low risk power-lines (pl) is a subjective business deci-
sion. Based on expert feedback, two deﬁnitions of identi-
fying high and low risk power-lines for a given year were
decided
4326
•DEF 1 : If CI>120 then the power-line is high risk else
if CI<5 it is low risk, else they are removed from eval-
uation.
•DEF 2 : If NO>2 then the power-line is high risk else
if NO<1 it is low risk, else they are removed from
evaluation.
A key thing to note is that this are retrospective deﬁnitions
and can only be used to conduct evaluation on historical
data.
Baseline: Previous Year CI The business historically used
previous years’ CI as a risk measure RCIi.e. to fore-
cast and schedule planned and unplanned maintenance for
2019 on 1 January 2019, corresponding value of CI from
2018 is used as a risk measure. Thus if for a power-line,
RCI(2019) = CI(2018)> τCIthen the power-line is pre-
dicted to be high risk for 2019, else it is predicted to be low
risk.
Our Method: Survival Risk score We used the risk score
Rsucomputed by the survival model i.e. on 1 January
2019, we use the data from previous years to compute risk
Rsu(2019) and ifRsu(2019)> τsu, the power-line is pre-
dicted to be high risk for 2019. The values of τsuandτCI
can be chosen according to business considerations based
on the precision-recall (PR) or receiver operating character-
istic curve. (ROC) [ 18].
Precision, Recall, False Positive Rate (FPR) an True Pos-
itive Rate (TPR): Adapting the standard deﬁnitions from a
confusion matrix, we deﬁne
recall=# pl predicted & actually high risk in 2019
# pl actually high risk in 2019,
precision =# pl predicted & actually high risk in 2019
# pl predicted high risk in 2019,
FPR=# pl predicted & not high risk in 2019
# pl actually not high risk in 2019.
In Figure 5, we compare the precision-recall curves as
well receiver operating characteristic curve (ROC) [ 18] for
DEF1 by varying τsuandτCI. Similar results are also
observed for DEF2.
4.3.3 Analysis of Survival Model
In this section, we analyze the ﬁtted survival model in some
detail.
Training Loss : In Figure 6, we shows the evolution of
the loss (negative log partial likelihood) and the gradients
while ﬁtting the model parameters using Newton Raphson
method.
Risk Groups : Using a suitable threshold on the risk score,
we divide the power-lines into two groups,“high risk” and
Figure 5. PR and ROC curves comparison between the baseline
and survival model based risk scoring.
Figure 6. Evolution of training loss and the gradient.
“low risk”. The distribution of the scores as well as the high
and low risk groups are shown in Figure 7.
Figure 7. Distribution of risk scores and classifying them into high
risk and risk.
Individual Predictions : In Figure 9, we observe that for the
power-line which we predicted high risk, the survival func-
tion drops sharply and it suffers an outage within 7 months.
Conversely, for the power-line which we predicted to be low
risk, the survival function drops slowly and there was not
outage till the time of recording the data (when that power-
line was 30 months since the last trim).
4327
Figure 8. Actual vs Predicted- Number of power-lines experienc-
ing an outage as a function of months since last trim.
Figure 9. Individual survival functions for 2 power-lines: one
which was predicted risk and other which was predicted low risk.
The vertical line is the time (since last trim) of the outage or the
censored time.
4.3.4 Discussion
In this section, we discuss certain key insights and learning
from the evaluation of our risk model.
Performance Improvement : We clearly observe from the
results in Figure 5the numeric gains of our method over
baseline methods. The overall ROC-area under the curve
is superior by 7%. Additionally, if the business requires a
minimum recall of 75%, the survival model based risk can
achieve a precision of 90% compared to a 80% precision
achieved using the baseline method.
Survival Model vs Classiﬁcation/Regression Problem :
An obvious question arising is “Why cannot we use clas-
siﬁcation/regression methodology?”. This is a valid ques-
tion and we provide the following reasons on why survival
models make a better choice.
• A classiﬁcation problem requires a a-priori labeling
of high-risk and low-risk power-lines. Thus whenever
business dictates a change of deﬁnition depending onthe geographical location or year, the model needs to
be re-trained. An interesting discussion on the similar
use of survival models over classiﬁcation for cancer
prognosis is discussed in [ 7].
• Another obvious choice is a regression on the time to
event. However, this leads to issues on censored data
i.e. those power-lines which at the time of data collec-
tion have not had any outage. As explained in Section
3.1, survival models provides a unique method to in-
corporate censored data.
• From a philosophical sense, this agrees with our in-
tuition on how power-lines suffer outage: The factor-
ization of the Cox PH model combines a natural time
varying deterioration in power-lines (increase in risk)
with risk inducing external factors to estimate an over-
all risk.
5. Conclusion and Future Work
In this paper, we propose a novel vegetation management
system which combines information from GIS data, aerial
and satellite imagery to estimate vegetation proﬁle, which is
further combined with operational and reliability attributes
of a power-line using a survival model to estimate a risk
score. We envisage 2 main applications of this risk score.
First, provide a list of power-lines in decreasing order of risk
which can then be combined with spatial clustering to iden-
tify optimal scheduled trimming schedules. This is neces-
sary as it involves signiﬁcant resource cost to move around
equipment for trimming. Hence the trimming team would
ideally like to ﬁrst cover a geographical area which has a
high proportion of high risk power-lines. Second, provide
a list of very high risk power-lines to for unscheduled/spot
trimming. This allows to reduce the number of outages and
thereby reduce the number of customers impacted.
Often, vegetation related outages are caused not only
due to growth but also due extreme weather events [ 14].
Integrating weather forecasts is an important future line
of work to develop a comprehensive vegetation manage-
ment which can provided combined risk score using both
weather forecasts and vegetation density. Another impor-
tant line of work is to compare the Cox PH method with
advanced deep neural network based survival models like
[16,21].
References
[1] IEEE Guide for Electric Power Distribution Reliabil-
ity Indices. IEEE Std 1366-2012 (Revision of IEEE
Std 1366-2003) , pages 1–43, May 2012. Conference
Name: IEEE Std 1366-2012 (Revision of IEEE Std
1366-2003). 4325
4328
[2] Junaid Ahmad, Aamir Saeed Malik, Likun Xia, and
Nadia Ashikin. Vegetation encroachment monitoring
for transmission lines right-of-ways: A survey. Elec-
tric Power Systems Research , 95:339–352, 2013. 4322
[3] Saikat Basu, Sangram Ganguly, Ramakrishna R
Nemani, Supratik Mukhopadhyay, Gong Zhang,
Cristina Milesi, Andrew Michaelis, Petr V otava, Ralph
Dubayah, Laura Duncanson, et al. A semiautomated
probabilistic framework for tree-cover delineation
from 1-m naip imagery using a high-performance
computing architecture. IEEE Transactions on
Geoscience and Remote Sensing , 53(10):5690–5708,
2015. 4323
[4] Leo Breiman. Random forests. Machine learning ,
45(1):5–32, 2001. 4324
[5] N. E. Breslow. Analysis of Survival Data under
the Proportional Hazards Model. International Sta-
tistical Review / Revue Internationale de Statistique ,
43(1):45–57, 1975. Publisher: [Wiley, International
Statistical Institute (ISI)]. 4325
[6] D. Cerrai, D. W. Wanik, M. A. E. Bhuiyan, X. Zhang,
J. Yang, M. E. B. Frediani, and E. N. Anagnostou.
Predicting storm outages through new representations
of weather and vegetation. IEEE Access , 7:29639–
29654, 2019. 4323
[7] Hung-Chia Chen, Ralph L. Kodell, Kuang Fu Cheng,
and James J. Chen. Assessment of performance
of survival prediction models for cancer prognosis.
BMC Medical Research Methodology , 12(1):102, July
2012. 4323 ,4328
[8] P. Chen, T. Dokic, N. Stokes, D. W. Goldberg, and M.
Kezunovic. Predicting weather-associated impacts in
outage management utilizing the gis framework. In
2015 IEEE PES Innovative Smart Grid Technologies
Latin America (ISGT LATAM) , pages 417–422, 2015.
4325
[9] R. B. D’Agostino and Byung-Ho Nam. Evaluation of
the Performance of Survival Analysis Models: Dis-
crimination and Calibration Measures. In Handbook
of Statistics , volume 23 of Advances in Survival Anal-
ysis, pages 1–25. Elsevier, Jan. 2003. 4326
[10] Ashiss Kumar Dash. Vegetation Management:
Artiﬁcial Intelligence to Preempt Forest Fires.
https://www.tdworld.com/vegetation-
management / article / 20973359 /
vegetation - management - artificial -
intelligence - to - preempt - forest -
fires , Nov 11, 2019. 4322
[11] Nick Day. LiDAR for Distribution Vegetation
Management. https://www.tdworld.
com/vegetation-management/article/20972795 / lidar - for - distribution -
vegetation-management , Jul 01, 2019. 4322
[12] Ilke Demir, Krzysztof Koperski, David Lindenbaum,
Guan Pang, Jing Huang, Saikat Basu, Forest Hughes,
Devis Tuia, and Ramesh Raskar. Deepglobe 2018: A
challenge to parse the earth through satellite images.
InProceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) Workshops ,
June 2018. 4322
[13] T. Dokic and M. Kezunovic. Predictive risk manage-
ment for dynamic tree trimming scheduling for distri-
bution networks. IEEE Transactions on Smart Grid ,
10(5):4776–4785, 2019. 4323
[14] Milad Doostan, Reza Sohrabi, and Badrul Chowdhury.
A Data-Driven Approach for Predicting Vegetation-
Related Outages in Power Distribution Systems.
arXiv:1807.06180 [cs, stat] , Mar. 2019. arXiv:
1807.06180. 4328
[15] Milad Doostan, Reza Sohrabi, and Badrul Chowdhury.
A data-driven approach for predicting vegetation-
related outages in power distribution systems. Inter-
national Transactions on Electrical Energy Systems ,
30(1):e12154, 2020. e12154 ITEES-19-0274.R1.
4323
[16] Stephane Fotso. Deep Neural Networks for Sur-
vival Analysis Based on a Multi-Task Framework.
arXiv:1801.05512 [cs, stat] , Jan. 2018. arXiv:
1801.05512. 4325 ,4328
[17] Stephane Fotso et al. PySurvival: Open source pack-
age for survival analysis modeling, 2019–. 4326
[18] Patrick J. Heagerty and Yingye Zheng. Survival model
predictive accuracy and ROC curves. Biometrics ,
61(1):92–105, Mar. 2005. 4327
[19] Hemant Ishwaran, Udaya B. Kogalur, Eugene H.
Blackstone, and Michael S. Lauer. Random survival
forests. The Annals of Applied Statistics , 2(3):841–
860, Sept. 2008. arXiv: 0811.1645. 4325
[20] E. L. Kaplan and Paul Meier. Nonparametric Estima-
tion from Incomplete Observations. Journal of the
American Statistical Association , 53(282):457–481,
June 1958. 4325
[21] Jared Katzman, Uri Shaham, Jonathan Bates, Alexan-
der Cloninger, Tingting Jiang, and Yuval Kluger.
DeepSurv: Personalized Treatment Recommender
System Using A Cox Proportional Hazards Deep Neu-
ral Network. BMC Medical Research Methodology ,
18(1):24, Dec. 2018. arXiv: 1606.00931. 4325 ,4328
[22] Mladen Kezunovic, Pierre Pinson, Zoran Obradovic,
Santiago Grijalva, Tao Hong, and Ricardo Bessa. Big
data analytics for future electricity grids. Electric
Power Systems Research , 189:106788, 2020. 4322
4329
[23] Andrew Khalel, Onur Tasar, Guillaume Charpiat,
and Yuliya Tarabalka. Multi-task deep learning for
satellite image pansharpening and segmentation. In
IGARSS 2019-2019 IEEE International Geoscience
and Remote Sensing Symposium , pages 4869–4872.
IEEE, 2019. 4322
[24] David G Kleinbaum and Mitchel Klein. Survival anal-
ysis. Springer, 2010. 4324
[25] Tzu-Sheng Kuo, Keng-Sen Tseng, Jia-Wei Yan, Yen-
Cheng Liu, and Yu-Chiang Frank Wang. Deep aggre-
gation net for land cover classiﬁcation. In Proceedings
of the IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR) Workshops , June 2018. 4322
[26] Brian Kurinsky. Power line corridor vegetation man-
agement: Clearing a path to reliable electric service
using lidar. Maryville, Missouri, Oct , 2013. 4322
[27] Chun Liu, Doudou Zeng, Hangbin Wu, Yin Wang,
Shoujun Jia, and Liang Xin. Urban land cover clas-
siﬁcation of high-resolution aerial imagery using a
relation-enhanced multiscale convolutional network.
Remote Sensing , 12(2), 2020. 4322
[28] Leena Matikainen, Matti Lehtom ¨aki, Eero Ahokas,
Juha Hyypp ¨a, Mika Karjalainen, Anttoni Jaakkola,
Antero Kukko, and Tero Heinonen. Remote sensing
methods for power line corridor surveys. ISPRS Jour-
nal of Photogrammetry and Remote Sensing , 119:10–
31, 2016. 4322
[29] Sebastian P ¨olsterl. Evaluating Survival Models, May
2019. 4326
[30] Sebastian P ¨olsterl, Nassir Navab, and Amin Ka-
touzian. Fast Training of Support Vector Machines for
Survival Analysis. In Annalisa Appice, Pedro Pereira
Rodrigues, V ´ıtor Santos Costa, Jo ˜ao Gama, Al ´ıpio
Jorge, and Carlos Soares, editors, Machine Learn-
ing and Knowledge Discovery in Databases , Lecture
Notes in Computer Science, pages 243–259, Cham,
2015. Springer International Publishing. 4325
[31] M. Shaﬁqur Rahman, Gareth Ambler, Babak
Choodari-Oskooei, and Rumana Z. Omar. Review
and evaluation of performance measures for survival
prediction models in external validation settings.
BMC Medical Research Methodology , 17(1):60, Dec.
2017. 4326
[32] Caleb Robinson, Le Hou, Kolya Malkin, Rachel
Soobitsky, Jacob Czawlytko, Bistra Dilkina, and
Nebojsa Jojic. Large scale high-resolution land cover
mapping with multi-resolution data. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , June 2019. 4322
[33] Shuaiang Rong, Lina He, Liang Du, Zuyi Li, and Shi-
wen Yu. Intelligent detection of vegetation encroach-ment of power lines with advanced stereovision. IEEE
Transactions on Power Delivery , 2020. 4323
[34] Xiao-Sheng Si, Wenbin Wang, Chang-Hua Hu, and
Dong-Hua Zhou. Remaining useful life estimation–a
review on the statistical data driven approaches. Eu-
ropean journal of operational research , 213(1):1–14,
2011. 4323
[35] Chao Tian, Cong Li, and Jianping Shi. Dense fu-
sion classmate network for land cover classiﬁcation.
InProceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) Workshops ,
June 2018. 4322
[36] J. Wang, C. Li, S. Han, S. Sarkar, and X. Zhou. Pre-
dictive maintenance based on event-log analysis: A
case study. IBM Journal of Research and Develop-
ment , 61(1):11:121–11:132, 2017. 4323
[37] Ping Wang, Yan Li, and Chandan K Reddy. Machine
learning for survival analysis: A survey. ACM Com-
puting Surveys (CSUR) , 51(6):1–36, 2019. 4324
[38] D.W. Wanik, J.R. Parent, E.N. Anagnostou, and B.M.
Hartman. Using vegetation management and lidar-
derived tree height data to improve outage predictions
for electric utilities. Electric Power Systems Research ,
146:236–245, 2017. 4323
[39] Chun-Nam Yu, Russell Greiner, Hsiu-Chin Lin, and
Vickie Baracos. Learning Patient-Speciﬁc Cancer Sur-
vival Distributions as a Sequence of Dependent Re-
gressors. page 9. 4323 ,4325
[40] Zhengxin Zhang, Qingjie Liu, and Yunhong Wang.
Road extraction by deep residual u-net. IEEE Geo-
science and Remote Sensing Letters , 15(5):749–753,
2018. 4322 ,4323
4330
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 1
Leveraging Class Hierarchies
with Metric-Guided Prototype Learning
Vivien Sainte Fare Garnot
vivien.sainte-fare-garnot@ign.fr
Loic Landrieu
loic.landrieu@ign.frLASTIG, ENSG, IGN
Univ Gustave Eiffel
F-94160 Saint-Mande, France
Abstract
In many classiﬁcation tasks, the set of target classes can be organized into a hierar-
chy. This structure induces a semantic distance between classes, and can be summarized
under the form of a cost matrix , which deﬁnes a ﬁnite metric on the class set. In this
paper, we propose to model the hierarchical class structure by integrating this metric
in the supervision of a prototypical network . Our method relies on jointly learning a
feature-extracting network and a set of class prototypes whose relative arrangement in
the embedding space follows an hierarchical metric. We show that this approach allows
for a consistent improvement of the error rate weighted by the cost matrix when com-
pared to traditional methods and other prototype-based strategies. Furthermore, when the
induced metric contains insight on the data structure, our method improves the overall
precision as well. Experiments on four different public datasets—from agricultural time
series classiﬁcation to depth image semantic segmentation—validate our approach.
1 Introduction
Most classiﬁcation models focus on maximizing the prediction accuracy, regardless of the
semantic nature of errors. This can lead to high performing models, but puzzling errors such
as confusing tigers and sofas, and casts doubt on what a model actually understands of the
required task and data distribution. Neural networks in particular have been criticized for
their tendency to produce improbable yet conﬁdent errors, notably when under adversarial
attacks [1]. Training deep models to produce not only produce fewer but also better er-
rors can increase their trustworthiness, which is crucial for downstream applications such as
autonomous driving or land use and land cover monitoring [3, 11].
In many classiﬁcation problems, the target classes can be organized according to a tree-
shaped hierarchical structure. Such a taxonomy can be generated by domain experts, or
automatically inferred from class names using the WordNet graph [34] or from word embed-
dings [33]. A step towards more reliable and interpretable algorithms would be to explicitly
model the difference of gravity between errors, as deﬁned by a hierarchical nomenclature.
For a classiﬁcation task over a set KofKclasses, the hierarchy of errors can be encap-
sulated by a cost matrix D∈RK×K
+, deﬁned such that the cost of predicting class kwhen the
© 2021. The copyright of this document resides with its authors.
It may be distributed unchanged freely in print or electronic forms.arXiv:2007.03047v3  [cs.LG]  29 Nov 2021
2 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
012
3
45
67
8
9
01
234 56
78
9
01
2
34
5
67
89
(a) Cross entropy, (b) Learnt prototypes, (c) Guided prototypes,
distortion1=0.47, distortion =0.42, distortion =0.22,
ER=15.2%, AHC =0.81 ER =14.2%, AHC =0.75 ER =11.9%, AHC =0.52
Figure 1: Mean class representation , prototypes , and 2-dimensional embeddings learnt on
perturbed MNIST by a 3-layer convolutional net with three different classiﬁcation modules: (a) cross-
entropy, (b) learnt prototypes, and (c) learnt prototypes guided by a tree-shaped taxonomy (constructed
according to the authors’ perceived visual similarity between digits). The guided prototypes (d) embed
more faithfully the class hierarchy: classes with low error cost are closer. This is associated with
a decrease in the Average Hierarchical Cost (AHC), as well as Error Rate (ER), indicating that our
taxonomy may contain useful information for learning better visual features.
true class is lisD[k,l]≥0, and D[k,k] =0 for all k=1···K. Among many other options
[27], one can deﬁne D[k,l]as the length of the shortest path between the nodes corresponding
to classes kandlin the tree-shaped class taxonomy.
As pointed out by Bertinetto et al. [3], the ﬁrst step towards algorithms aware of hi-
erarchical structures would be to generalize the use of cost-based metrics. For example,
early iterations of the ImageNet challenge [11, 39] proposed to weight errors according to
hierarchy-based costs. For a dataset indexed by N, the Average Hierarchical Cost (AHC)
between class predictions y∈KNand the true labels z∈KNis deﬁned as:
AHC(y,z) =1
|N|∑
n∈ND[yn,zn]. (1)
Along with the evaluation metrics, the loss functions should also take the cost matrix into
account. While it is common to focus on retrieving certain classes through weighting [7,
29] or sampling [43, 47] schemes, preventing confusion between speciﬁc classes is less
straightforward. For example, the cross entropy with one-hot target vectors singles out the
predicted conﬁdence for the true class, but treats all other classes equally. Beyond reducing
the AHC, another advantage of incorporating the class hierarchy into the learning phase
is that Dmay contain information about the structure of the data as well. Although it is
not always the case, co-hyponyms ( i.e.siblings) in a class hierarchy tend to share some
structural properties. Encouraging such classes to have similar representations could lead
to more efﬁcient learning, e.g.by leveraging common feature detectors. Such priors on the
class structure may be especially crucial when dealing with a large taxonomy, as noted by
Deng et al. [11].
In this paper, we introduce a method to integrate a pre-deﬁned class hierarchy into a clas-
siﬁcation algorithm. We propose a new distortion-based regularizer for prototypical network
1For a formal deﬁnition of scale-free distortion, see Section 3.2; the distortion is computed with respect to the
means of class embeddings for the cross entropy.
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 3
[9, 51]. This penalty allows the network to learn prototypes organized so that their pairwise
distances reﬂect the error cost deﬁned by a class hierarchy. Our contributions are as follows:
• We introduce a scale-independent formulation of the distortion between two metric
spaces and an associated smooth regularizer.
• This formulation allows us to incorporate knowledge of the class hierarchy into a
neural network at no extra cost in trainable parameters and computation.
• We show on four public datasets (CIFAR100 , NYUDv2, S2-Agri, and iNaturalist-19)
that our approach decreases the average cost of the prediction of standard backbones.
• As illustrated in Figure 1, we show that our approach can also lead to a better (un-
weighted) precision, which we attribute to the useful priors contained in the hierarchy.
2 Related Work
Prototypical Networks: Our approach builds on the growing corpus of work on proto-
typical networks. These models are deep learning analogues of nearest centroid classiﬁers
[48] and Learning Vector Quantization networks [26, 42], which associate to each class a
representation, or prototype, and classify the observations according to the nearest proto-
type. These networks have been successfully used for few-shot learning [12, 45], zero-shot
learning [23], and supervized classiﬁcation [9, 16, 32, 51].
In most approaches, the prototypes are directly deﬁned as the centroid of the learnt rep-
resentations of samples of their classes, and updated at each episode [45] or iteration [16].
In the work of Mettes et al. [32] and Jetley et al. [23], the prototypes are deﬁned prior to
learning the embedding function. In this work, we follow the approach of [51] and learn the
prototypes simultaneously with the data embedding function.
Hierarchical Priors: The idea of exploiting the latent taxonomic structure of semantic
classes to improve the accuracy of a model has been extensively explored [44], from tradi-
tional Bayesian modeling [14, Chapter 5] to adaptive deep learning architectures [2, 38, 41,
50]. However, for these neural networks, the hierarchy is discovered by the network itself to
improve the overall accuracy of the model. In our setting, the hierarchy is deﬁned a priori
and serves both to evaluate the quality of the model and to guide the learning process towards
a reduced prediction cost.
Srivastava and Salakhutdinov [46] propose to implement Gaussian priors on the weight
of neurons according to a ﬁxed hierarchy. Redmon and Farhadi [37] implements an inference
scheme based on a tree-shaped graphical model derived from a class taxonomy. Closest to
our work, Hou et al. [21] propose a regularization based on the earth mover distance to
penalize errors with high cost.
More recently, Bertinetto et al. [3] highlighted the relative lack of well-suited methods
for dealing with hierarchical nomenclatures in the deep learning literature. They advocate
for a more widespread use of the AHC for evaluating models, and detail two simple baseline
classiﬁcation modules able to decrease the AHC of deep models: Soft-Labels andHierar-
chical Cross-Entropy . See the appendix for more details on these schemes. Following this
objective, Karthik et al. [24] propose a an inference-time risk minimization scheme to reduce
the AHC of the predictions based on the predicted posteriors.
4 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
Hyperbolic Prototypes: Motivated by their capacity to embed hierarchical data structures
into low-dimensional spaces, [10], hyperbolic spaces are at the center of recent advances in
modeling hierarchical relations [25, 36]. Closer to this work, [30, 31] also propose to embed
a class hierarchy into the latent representation space. However, both approaches embed
the class hierarchy before training the data embedding network. In contrast, we argue that
incorporating the hierarchical structure during the training of the model allows the network
and class embeddings to share their respective insights, leading to a better trade-off between
AHC and accuracy. In this paper, we only explore Euclidean geometry, as this setting allows
for the seamless integration of our method without changing the number of bits of precision
or the optimizer [10].
Finite Metric Embeddings: Our objective of computing class representations with pair-
wise distances determined by a cost matrix has links with ﬁnding an isometric embedding of
the cost matrix—seen as a ﬁnite metric. This problem has been extensively studied [4, 22]
and is at the center of the growing interest for hyperbolic geometry [10]. Here, our goal is
simply to inﬂuence the learning of prototypes with a metric rather than necessarily seeking
the best possible isometry.
3 Method
We consider a generic dataset NofNelements x∈XNwith ground truth classes z∈KN.
The classesKare organized along a tree-shape hierarchical structure, allowing us to deﬁne
a cost matrix Dby considering the shortest path between nodes. The matrix thus deﬁned
is symmetric, with a zero diagonal, strictly positive elsewhere, and respects the triangle in-
equality: D[k,l] +D[l,m]≥D[k,m]for all k,l,minK. In other words, Ddeﬁnes a ﬁnite
metric. We denote by Ωanembedding space which, when equipped with the distance func-
tiond:Ω×Ω↦→R+, forms a continuous metric space.
3.1 Prototypical Networks
A prototypical network is characterized by an embedding function f:X↦→Ω, typically a
neural network, and a set π∈ΩKofKprototypes. πmust be chosen such that any sample xn
of true class khas a representation f(xn)which is close toπkandfarfrom other prototypes.
Following the methodology of Snell et al. [45], a prototypical network (f,π)associates
to an observation xnthe posterior probability over its class zndeﬁned as follows:
p(zn=k|xn) =exp(−d(f(xn),πk))
∑l∈Kexp(−d(f(xn),πl)),∀k∈K (2)
We deﬁne an associated loss as the normalized negative log-likelihood of the true classes:
Ldata(f,π) =1
N∑
n∈N(
d(f(xn),πzn)+log(
∑
l∈Kexp(−d(f(xn),πl))))
. (3)
This loss encourages the representation f(xn)to be close to the prototype of the class
znand far from the other prototypes. Conversely, the prototype πkis drawn towards the
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 5
representations f(xn)of samples nwith true class k, and away from the representations of
samples of other classes.
Following the insights of [51], the embedding function fand the prototypes πare learned
simultaneously. This differs from many works on prototypical networks which learn proto-
types separately or deﬁne them as centroids of representations. We take advantage of this
joint training to learn prototypes which take into account both the distribution of the data and
the relationships between classes, as described in the next section.
3.2 Metric-Guided Penalization
We propose to incorporate the cost matrix Dinto a regularization term in order to encourage
the prototypes’ positions in the embedding space Ωto be consistent with the ﬁnite metric
deﬁned by D. Since the sample representations are attracted to their respective prototypes in
(3), such regularization will also affect the embedding network.
Metric Distortion As described in De Sa et al. [10], the distortion of a mapping k↦→πk
between the ﬁnite metric space (K,D)and the continuous metric space (Ω,d)can be deﬁned
as the average relative difference between distances in the source and target space:
disto(π,D) =1
K(K−1)∑
k,l∈K2,k̸=l|d(πk,πl)−D[k,l]|
D[k,l]. (4)
We argue that a network (f,π)trained to minimize Ldataand whose prototypes πhave a low
distortion with respect to Dshould produce errors with low hierarchical costs. To understand
the intuition behind this idea, let us consider a sample xnof true class kand misclassiﬁed as
class l. This tells us that the distance between f(xn)andπlis small. If kandlhave a high
cost according to D, and since k↦→πkis of low distortion, then d(πk,πl)must be large. The
triangular inequality tells us that d(f(xn),πk)≥d(πk,πl)−d(f(xn),πl), and consequently
thatd(f(xn),πk)must be large as well, which contradicts that (f,π)minimizesLdata.
Scale-Free Distortion For a prototype arrangement πto have a small distortion with re-
spect to a ﬁnite metric Das deﬁned in Equation 4, the distance between prototypes must
correspond to the distance between classes. This imposes a speciﬁc scale on the distances
between prototypes in the embedding space. This scale may conﬂict with the second term
ofLdatawhich encourages the distance between embeddings and unrelated prototypes to be
as large as possible. Therefore, lower distortion may also cause lower precision. To remove
this conﬂicting incentive, we introduce a scale-independent formulation of the distortion (5)
where s·πare the scaled prototypes, whose coordinates in Ωare multiplied by a scalar factor
s. As shown in the appendix, distoscale-freecan be efﬁciently computed algorithmically.
distoscale-free(π,D) =min
s∈R+disto(s·π,D), (5)
Distortion-Based Penalization We propose to incorporate the error qualiﬁcation Dinto
the prototypes’ relative arrangement by encouraging a low scale-free distortion between π
6 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
andD. To this end, we deﬁne Ldisto, a smooth surrogate of distoscale-free(6). as detailed in
the appendix,Ldistocan be computed in closed form as a function of πand can be directly
used as a regularizer.
Ldisto(π) =1
K(K−1)min
s∈R+∑
k,l∈K2,k̸=l(sd(πk,πl)−D[k,l]
D[k,l])2
. (6)
3.3 End-to-end Training
We combineLdataandLdistoin a single lossL.Ldataallows to jointly learn the embedding
function fand the class prototypes π, whileLdistoenforces a metric-consistent prototype
arrangement, with λ∈R+an hyper-parameter setting the strength of the regularization:
L(f,π) =Ldata(f,π)+λLdisto(π). (7)
4 Experiments
4.1 Datasets and Backbones
Table 1: Data composition and taxonomies of the four studied datasets. IR stands for the Imbalance
Ratio (largest over smallest class count), nodes and leaves denote respectively the total number of
classes and leaf-classes in the tree-shape hierarchy, ABF stands for the Average Branching Factor, and
⟨D⟩stands for the average pairwise distance.
DatasetData Hierarchical Tree
V olume (Gb) Samples IR Depth Nodes (leaves) ABF ⟨D⟩
NYUDv2 2.8 1449 93 3 57 (40) 5.0 4.3
S2-Agri 28.2 189 971 617 4 83 (45) 5.8 6.5
CIFAR100 0.2 60 000 1 5 134 (100) 3.8 7.0
iNat-19 82.0 265 213 31 7 1189 (1010) 6.6 11.0
We evaluate our approach with different tasks and public datasets with ﬁne-grained class
hierarchies: image classiﬁcation on CIFAR100 [28] and iNaturalist-19 [49], RGB-D image
segmentation on NYUDv2 [35], and image sequence classiﬁcation on S2-Agri [40]. We
deﬁne the cost matrix of these class sets as the length of the shortest path between nodes
in the associated tree-shape taxonomies represented in the Appendix. As shown in Table 1,
these datasets cover different settings in terms of data distribution and hierarchical structure.
Illustrative Example on MNIST: In Figure 1, we illustrate the difference in performance
and embedding organization of the embedding space for different approaches. We use a
small 3-layer convolutional net trained on MNIST with random rotations (up to 40 degrees)
and afﬁne transformations (up to 1 .3 scaling). For plotting convenience, we set the features’
dimension to 2.
Image Classiﬁcation on CIFAR100: We use a super-class system inspired by Krizhevsky
et al. [28] and form a 5-level hierarchical nomenclature of size: 2, 4, 8, 20, and 100 classes.
We use as backbone the established ResNet-18 [20] as embedding network for this dataset.
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 7
RGB-D Semantic Segmentation on NYUDv2: We use the standard split of 795 training
and 654 testing pairs. We combine the 4 and 40 class nomenclatures of Gupta et al. [17]
and the 13 class system deﬁned by Handa et al. [18] to construct a 3-level hierarchy. We use
FuseNet [19] as backbone for this dataset.
Image Sequence Classiﬁcation on S2-Agri: S2-Agri is composed of 189 971 sequences
of multi-spectral satellite images of agricultural parcels. We deﬁne a 4-level crop type hier-
archy of size 4, 12, 19, and 44 classes with the help of experts from a European agricultural
monitoring agency (ASP). We use the PSE+TAE architecture [40] as backbone, and follow
their 5-fold cross-validation scheme for training. Crop mapping in particular beneﬁts from
predictions with a low hierarchical cost. Indeed, payment agencies monitor the allocation
of agricultural subsidies and whether crop rotations follow best practice recommendations
[15]. The monetary and environmental impact of misclassiﬁcations are typically reﬂected in
the class hierarchy designed by domain experts [5, 6]. By achieving a low AHC, we ensure
that these downstream tasks can be meaningfully realized from the predictions.
Fine-Grained Image Classiﬁcation on iNaturalist-19 (iNat-19) iNat-19 [49] contains
1010 different classes organized into a 7 level hierarchy with respective width 3, 4, 9, 34,
57, 72, and 1010. We use ResNet-18 pretrained on ImageNet as backbone. We sample 75%
of available images for training, while the rest is evenly split into a validation and test set.
4.2 Hyper-Parameterization
The embedding space Ωis chosen as R512for iNat-19 and R64for all other datasets. We
chose das the Euclidean norm. (see 4.6 for a discussion on this choice). We evaluate our
approach (Guided-proto) with λ=1 in (7) for all datasets. We use the same training sched-
ules and learning rates as the backbone networks in their respective papers. In particular, the
class imbalance of S2-Agri is handled with a focal loss [29].
4.3 Competing methods
In the paper where they are introduced, all backbone networks presented in Section 4.1 use a
linear mapping between the sample representation and the class scores, as well as the cross-
entropy loss. The resulting performance deﬁnes a baseline, denoted as Cross-Entropy ,
and is used to estimate the gains in Average Hierarchical Cost (AHC) and Error Rate (ER)
provided by different approaches. We reimplemented other competing methods: Hierarchi-
cal Cross-Entropy ( HXE) [3], Soft Labels [3], Earth Mover Distance regularization ( XE+EMD )
[21], Hierarchical Inference ( YOLO ) [37], Hyperspherical Prototypes ( Hyperspherical -
proto ) [32], and Deep Mean Classiﬁers ( Deep-NCM ) [16]. See the Appendix for more
details on these methods. Lastly, we evaluate simple prototype learning ( Learnt-proto )
[51] by setting λ=0 in (7).
4.4 Analysis
Overall Performance: As displayed in Figure 2, the beneﬁts provided by our approach can
be appreciated on all datasets. Compared to the Cross-Entropy baseline, our model im-
proves the AHC by 3% on NYUDv2 and S2-Agri, and up to 9% and 14% for CIFAR100, and
8 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
23.5 24.0 24.5
Error Rate (ER)CIFAR 100
1.05 1.10 1.15 1.20 1.25
Average Hierarchical Cost (AHC)39 40 41 42 43
Error Rate (ER)iNaturalist-19
1.8 2.0 2.2
Average Hierarchical Cost (AHC)
32.0 32.5 33.0
Error Rate (ER)NYUDv2
1.45 1.50
Average Hierarchical Cost (AHC)18.75 19.00 19.25 19.50 19.75
Error Rate (ER)S2-Agri
0.68 0.69 0.70 0.71
Average Hierarchical Cost (AHC)Guided-protoGuided-proto
Learnt-protoLearnt-proto
Deep-NCMDeep-NCM
Hyperspherical-protoHyperspherical-proto
YOLOYOLO
EMDEMD
Soft-labelsSoft-labels
HXEHXE
Cross-EntropyCross-Entropy
⋆
Figure 2: Error Rate (ER) in % and Average Hierarchical Cost (AHC) on four datasets for
Guided-proto , theCross-Entropy baseline (in bold), and competing approaches. Methods
that use the hierarchical knowledge are indicated with the symbol . The best performances on each
dataset are plotted in green. Our guided prototype approach improves both the ER and AHC across
the four datasets compared to the baseline. The metrics are computed with the median over 5 runs for
CIFAR100, the average over 5 cross-validation folds for S2-Agri, and a single run for NYUDv2 and
iNat-19. The numeric values are given in the Appendix. ( ⋆: not evaluated).
iNat-19 respectively. The hierarchical inference scheme YOLO of Redmon and Farhadi [37]
performs on par or better than our methods for NYUDv2 and S2-Agri, while Soft-labels
perform well on CIFAR100 and NYUDv2. Yet, metric guided prototypes brings the most
consistent reduction of the hierarchical cost across all tasks, datasets, and class hierarchies
conﬁgurations. This suggest that arranging the embedding space consistently with the cost
metric is a robust way of reducing a model’s hierarchical error cost. We argue that these
results, combined with its ease of implementation, make a strong case for our approach.
While being initially designed to reduce the AHC, our method also provides a relative
decrease of the ER by 3 to 4% across all datasets compared to the cross-entropy baseline.
This indicates that cost matrices derived from the class hierarchies can indeed help neural
networks to learn richer representations.
Prototype Learning: We observe that the learnt prototype approach Learnt-proto
consistently outperforms the Deep-NCM method. This suggests that deﬁning prototypes as
the centroids of their class representations might actually be disadvantageous. As illustrated
on Figure 1, the positions of the embeddings tend to follow a V oronoi partition [13] with
respect to the learnt prototypes of their true class rather than prototypes being the centroid
of their associated representations. A surprising observation for us is that Learnt-proto
consistently outperforms the Cross-Entropy baseline, both in terms of AHC and ER.
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 9
Mammals Non-Mammals Sea creatures PlantsAnimals Plants
(a) Cross-Entropy
Mammals Non-Mammals Sea creatures PlantsAnimals Plants (b) Metric-Guided Prototypes
 (c) Hierarchical cost
Figure 3: Partial confusion matrix for the “living organism” class subset of CIFAR100 for the
Cross-Entropy baseline (a) and our approach (b). For readability, we only display (in black)
entries of the matrices with at least one confusion. We also represent the cost of confusing
different classes in shades of reds (c). We note that our approach yields fewer confusions
between pairs of classes with high costs, such as plants and animals.
Computational Efﬁciency: Computing distances between representations and prototypes
is comparable in terms of complexity than computing a linear mapping. The scaling factor in
Ldistocan be efﬁciently obtained as described in the Appendix. In practice, we observed that
both training and inference time are identical for Cross-Entropy andGuided-proto :
most of the time is taken by the computation of the embeddings.
4.5 Restricted Training Data Regime
12K 20K 30K 40K 60K
Training samples1.5
1.0
0.5
0.0AHC7.01 6.11 5.26 4.7 3.91
Guided-proto Learnt-proto EMD
Figure 4: AHC of ResNet-18 trained on
restricted training sets of iNaturalist-19
withGuided-proto ,Learnt-proto ,
andEMD. We represent the relative im-
provement compared to the performance of
theCross-Entropy baseline, which is
shown on top of the plots.We observed that the Learnt-proto
method decreases the AHC across all four
datasets even though it does not take the cost
matrix into account. This suggests that, given
enough data, this simple model can learn an
empirical taxonomy through its prototypes’
arrangement. Furthermore, this taxonomy can
share enough similarity with the one designed
by experts to result in a decrease in AHC.
To further evaluate the beneﬁt of explicitly
using the expert taxonomy with our approach,
we train the models Learnt-proto ,
Guided-proto , and EMD with only part of
the 160k images in the training set of iNat-19,
and without pretraining on ImageNet. To
compensate for the lack of data, we increase
the regularization strength to λ=20.
In Figure 4, we observe that the two prototype-based approaches consistently improve
the performance of the baseline for all training set sizes in terms of AHC. Moreover, the
advantages brought by our proposed regularization are all the more signiﬁcant when applied
10 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
to small training sets. This observation reinforces the idea that the learnt-proto method
requires large amounts of data to learn a meaningful class hierarchy in an unsupervized way.
4.6 Ablation Study
In the Appendix, we present an extensive ablation study. We present here its main take-away.
Scale-Free Distortion: Our method for automatically choosing the best scale in our smooth
distortion surrogate leads to an improvement of 0 .9 ER on the iNat-19 dataset, which amounts
to half the improvement compared to the baseline. In the other datasets, the improvements
were more limited. We attribute the impact of our scale-free distortion on iNat-19 in particu-
lar to the structure of its class hierarchy: at the lowest level, iNat-19 classes have on average
14 co-hyponyms (siblings), compared to only 2 to 5 for the other datasets. When minimiz-
ing the distortion with a ﬁxed scale of 1, the prototypes of hyponyms are incentivized to be
close with respect to dsince hyponyms have a small hierarchical distance of 2. This clashes
with the minimization of the second part of Ldataas deﬁned in (3), which mutually repels
prototypes of different classes. This conﬂict, made worse by classes with many hyponyms,
is removed by our scale-free distortion. See the Appendix for additional insights into how
our automatic scaling addresses this issue.
Choice of Metric Space: Prototypical networks operating on Ω=Rmtypically use the
squared Euclidean norm in the distance function, motivated by its quality as a Bregman
divergence [45]. However, given the large distance between prototypes induced by our regu-
larization, this metric can cause stability issues. We observe for all datasets that that deﬁning
das the Euclidean norm yields signiﬁcantly better results across all datasets.
Guided vs. ﬁxed prototypes : As suggested by the lower performance of Hypersphe-
rical-proto , jointly learning the prototypes and the embedding network can be advan-
tageous. To conﬁrm this observation, we altered our Guided-proto method to ﬁrst learn
the prototypes and then the embedding network. We observed a signiﬁcant decrease in per-
formance across the board, up to 5 more points of ER in iNat-19. Conversely, we altered
Hyperspherical-proto to learn spherical prototypes together with the embedding net-
work. This improved the performance of Hyperspherical-proto even though it re-
mained worse than the Cross-Entropy baseline ( +1.20 ER, +0.10 AHC on CIFAR100).
These observations suggest that insights from the data distribution can beneﬁt the positioning
of prototypes, and that they should be learned conjointly.
5 Conclusion
We introduced a new regularizer modeling the hierarchical relationships between the classes
of a nomenclature. This approach can be incorporated into any classiﬁcation network at no
computational cost and with very little added code. We showed that our method consistently
decreased the average hierarchical cost of three different backbone networks on different
tasks and four datasets. Furthermore, our approach can reduce the rate of errors as well.
In contrast to most recent works on hierarchical classiﬁcation, we showed that this joint
training is beneﬁcial compared to the staged strategy of ﬁrst positioning the prototypes and
then training a feature extracting network. A PyTorch implementation of our framework as
well as an illustrative notebook are available at https://github.com/VSainteuf
/metric-guided-prototypes-pytorch .
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 11
References
[1] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in
computer vision: A survey. IEEE Access , 2018.
[2] Shahanaz Ayub and JP Saini. ECG classiﬁcation and abnormality detection using cas-
cade forward neural network. International Journal of Engineering, Science and Tech-
nology , 2011.
[3] Luca Bertinetto, Romain Mueller, Konstantinos Tertikas, Sina Samangooei, and
Nicholas A Lord. Making better mistakes: Leveraging class hierarchies with deep
networks. In CVPR , 2020.
[4] Jean Bourgain. On Lipschitz embedding of ﬁnite metric spaces in Hilbert space. Israel
Journal of Mathematics , 1985.
[5] Gerhard Brankatschk and Matthias Finkbeiner. Modeling crop rotation in agricultural
lcas—challenges and potential solutions. Agricultural Systems , 2015.
[6] Donald G Bullock. Crop rotation. Critical reviews in plant sciences , 1992.
[7] Samuel Rota Bulo, Gerhard Neuhold, and Peter Kontschieder. Loss max-pooling for
semantic image segmentation. In CVPR , 2017.
[8] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton,
and Greg Hullender. Learning to rank using gradient descent. In ICML , 2005.
[9] Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K
Su. This looks like that: deep learning for interpretable image recognition. In NeurIPS ,
2019.
[10] Christopher De Sa, Albert Gu, Christopher Ré, and Frederic Sala. Representation trade-
offs for hyperbolic embeddings. In Proceedings of Machine Learning Research , 2018.
[11] Jia Deng, Alexander C Berg, Kai Li, and Li Fei-Fei. What does classifying more than
10,000 image categories tell us? In ECCV , 2010.
[12] Nanqing Dong and Eric Xing. Few-shot semantic segmentation with prototype learn-
ing. In BMVC , 2018.
[13] Steven Fortune. V oronoi diagrams and Delaunay triangulations. In Computing in Eu-
clidean Geometry . World Scientiﬁc, 1992.
[14] Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Don-
ald B Rubin. Bayesian data analysis . CRC press, 2013.
[15] Wyn Grant. The common agricultural policy . Macmillan International Higher Educa-
tion, 1997.
[16] Samantha Guerriero, Barbara Caputo, and Thomas Mensink. DeepNCM: deep nearest
class mean classiﬁers. In ICLR, Worskhop , 2018.
[17] Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Perceptual organization and recog-
nition of indoor scenes from RGB-D images. In CVPR , 2013.
12 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
[18] A Handa, V Patraucean, V Badrinarayanan, S Stent, and R Cipolla. Uunderstanding
real world indoor scenes with synthetic data. In CVPR , 2016.
[19] Caner Hazirbas, Lingni Ma, Csaba Domokos, and Daniel Cremers. Fusenet: Incorpo-
rating depth into semantic segmentation via fusion-based CNN architecture. In ACCV ,
2016.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for
image recognition. In CVPR , 2016.
[21] Le Hou, Chen-Ping Yu, and Dimitris Samaras. Squared earth mover’s distance-based
loss for training deep neural networks. In NeurIPS Workshop , 2016.
[22] Piotr Indyk, Ji ˇrí Matoušek, and Anastasios Sidiropoulos. Low-distortion embeddings
of ﬁnite metric spaces. In Handbook of discrete and computational geometry . Chapman
and Hall/CRC, 2017.
[23] Saumya Jetley, Bernardino Romera-Paredes, Sadeep Jayasumana, and Philip Torr. Pro-
totypical priors: From improving classiﬁcation to zero-shot learning. In BMVC , 2015.
[24] Shyamgopal Karthik, Ameya Prabhu, Puneet K Dokania, and Vineet Gandhi. No cost
likelihood manipulation at test time for making better mistakes in deep networks. ICLR ,
2021.
[25] Valentin Khrulkov, Leyla Mirvakhabova, Evgeniya Ustinova, Ivan Oseledets, and Vic-
tor Lempitsky. Hyperbolic image embeddings. In CVPR , 2020.
[26] Teuvo Kohonen. Learning vector quantization. In Self-organizing maps . Springer,
1995.
[27] Aris Kosmopoulos, Ioannis Partalas, Eric Gaussier, Georgios Paliouras, and Ion An-
droutsopoulos. Evaluation measures for hierarchical classiﬁcation: a uniﬁed view and
novel approaches. Data Mining and Knowledge Discovery , 2015.
[28] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny
images. Technical report, University of Toronto, 2009.
[29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss
for dense object detection. In ICCV , 2017.
[30] Shaoteng Liu, Jingjing Chen, Liangming Pan, Chong-Wah Ngo, Tat-Seng Chua, and
Yu-Gang Jiang. Hyperbolic visual embedding learning for zero-shot recognition. In
CVPR , 2020.
[31] Teng Long, Pascal Mettes, Heng Tao Shen, and Cees GM Snoek. Searching for actions
on the hyperbole. In CVPR , 2020.
[32] Pascal Mettes, Elise van der Pol, and Cees Snoek. Hyperspherical prototype networks.
InNeurIPS , 2019.
[33] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of
word representations in vector space. In ICLR Workshop , 2013.
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 13
[34] George A Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Kather-
ine J Miller. Introduction to wordnet: An on-line lexical database. International Jour-
nal of Lexicography , 1990.
[35] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation
and support inference from RGBD images. In ECCV , 2012.
[36] Maximillian Nickel and Douwe Kiela. Poincaré embeddings for learning hierarchical
representations. In NeurIPS , 2017.
[37] Joseph Redmon and Ali Farhadi. YOLO9000: better, faster, stronger. In CVPR , 2017.
[38] Deboleena Roy, Priyadarshini Panda, and Kaushik Roy. Tree-CNN: a hierarchical deep
convolutional neural network for incremental learning. Neural Networks , 2020.
[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet
large scale visual recognition challenge. International Journal of Computer Vision ,
2015.
[40] Vivien Sainte Fare Garnot, Loic Landrieu, Sebastien Giordano, and Nesrine Chehata.
Satellite image time series classiﬁcation with pixel-set encoders and temporal self-
attention. In CVPR , 2020.
[41] Ruslan Salakhutdinov, Joshua B Tenenbaum, and Antonio Torralba. Learning with
hierarchical-deep models. IEEE Transactions on Pattern Analysis and Machine Intel-
ligence , 2012.
[42] Atsushi Sato and Keiji Yamada. Generalized learning vector quantization. NeurIPS ,
1995.
[43] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object
detectors with online hard example mining. In CVPR , 2016.
[44] Carlos N Silla and Alex A Freitas. A survey of hierarchical classiﬁcation across differ-
ent application domains. Data Mining and Knowledge Discovery , 2011.
[45] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot
learning. In NeurIPS , 2017.
[46] Nitish Srivastava and Russ R Salakhutdinov. Discriminative transfer learning with tree-
based priors. In NeurIPS , 2013.
[47] Kah-Kay Sung. Learning and example selection for object and pattern detection. MIT
A.I., 1996.
[48] Robert Tibshirani, Trevor Hastie, Balasubramanian Narasimhan, and Gilbert Chu. Di-
agnosis of multiple cancer types by shrunken centroids of gene expression. Proceedings
of the National Academy of Sciences , 2002.
[49] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
Hartwig Adam, Pietro Perona, and Serge Belongie. The iNaturalist species classiﬁca-
tion and detection dataset. In CVPR , 2018.
14 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
[50] Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste,
Wei Di, and Yizhou Yu. HD-CNN: hierarchical deep convolutional neural networks for
large scale visual recognition. In ICCV , 2015.
[51] Hong-Ming Yang, Xu-Yao Zhang, Fei Yin, and Cheng-Lin Liu. Robust classiﬁcation
with convolutional prototype learning. In CVPR , 2018.
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 15
Supplementary Materials - Leveraging Class Hierarchies
with Metric-Guided Prototype Learning
6 Notebook and illustration
In Figure 5, we represent the embeddings and prototypes generated by variations of our net-
works as well as their respective performance. We note that the ﬁxed prototypes approach
performs signiﬁcantly worse than our metric-guided method. We observe that the resulting
prototypes are more compact when they are learned independently, which can lead to an
increase in misclassiﬁcation. We also remark that when the hierarchy contains no useful
information, such as the arbitrary order of digits, the metric-based approach has a worse per-
formance than the free (unguided) method. This is particularly drastic for the ﬁxed prototype
approach.
An illustrated notebook to reproduce this ﬁgure can be accessed at the following URL:
https://colab.research.google.com/drive/1VoQfBx5q5lWFev0cw
xLZ0qQOZU7Rlmb_#offline=true&sandboxMode=true
To run this notebook locally, you can also download it from our repository:
https://github.com/VSainteuf/metric-guided-prototypes-pytorc
h.
7 Additional methodological details
7.1 Scale-Independent Distortion
Computing the scale-free distortion deﬁned in Equation 5 amounts to ﬁnding a minimizer of
the following function f:R↦→R:
f(s) =∑
i∈I|sαi−1|, (8)
withαk,l=d(πk,πl)/D[k,l]≥0, and Ian ordering of{k,l}k,l∈K2such that the sequence
[αi]i∈Iis non-decreasing.
Proposition 1. A global minimizer of f deﬁned in (8) is given by s⋆=1/αk⋆with k⋆deﬁned
as:
k⋆=min{
k∈I⏐⏐⏐⏐⏐∑
i≤kαi≥∑
i>kαi.}
(9)
Proof. First, such k⋆exists as it is the smallest member of a discrete, non-empty set. Indeed,
since all αiare nonnegative, the set contains at least k=|I|. We now verify that s⋆=1/α⋆
k
is a critical point of f. By deﬁnition of k⋆we have that ∑i≤k⋆αi≥∑i>k⋆αiand∑i<k⋆αi<
∑i≥k⋆αi. By combining these two inequalities, we have that
−∑
i<k⋆αi+∑
i>k⋆αi∈[−αk⋆,αk⋆]. (10)
16 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
012
3
45
67
8
9
(a) Cross entropy, ER =15.2%
disto vis=0.47, disto abs=0.61
AHC vis=0.81, AHC abs=0.65
01
234 56
78
9(b) Learnt prototypes, ER =14.2%
disto vis=0.42, disto abs=0.58
AHC vis=0.75, AHC abs=0.50
01
2
34
5
67
89
(c) Guided prototypes, ER =12.8%
disto vis=0.22 AHC vis=0.56
0
123
456
789(d) Fixed prototypes, ER =21.5%
disto vis=0.17 AHC vis=0.82
0 12345
6 789
(e) Guided prototypes, ER =16.9%
disto abs=0.24 AHC abs=0.54
0
1
2
3
4
5
6789(f) Fixed prototypes, ER =48.8%
disto abs=0.00 AHC abs=0.80
(g)
0123456789
(h)
Figure 5: Mean class representation , prototypes , and 2-dimensional embeddings
learnt on perturbed MNIST by a 3-layer convolutional net with six different classiﬁcation
modules: (a) cross-entropy, (b) learnt prototypes, (c) learnt prototypes guided by a visual
taxonomy, (d) ﬁxed prototypes (see Section 8.2) from a visual taxonomy , (e) learnt pro-
totypes guided by the numbers’ values, and (f) ﬁxed prototypes from the numbers’ values.
The visual hierarchy is represented in (g) and the numerical order in (h). AHC viscorre-
sponds to the cost deﬁned by our proposed visual hierarchy, while AHC absis deﬁned after
the chain-like structure obtained when organizing the digits along their numerical values.
While embedding the metric with prototypes prior to learning the representations leads to
lower (scale-free) distortion, this translates into worst performance in terms of AHC and ER.
Joint learning achieves better performance on both evaluation metrics. We also remark that
when the hierarchy is arbitrary (e-f), metric guiding is detrimental to precision.
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 17
Since Iorders the αiin increasing order, we can write the subgradient of fats⋆under the
following form:
∂sf(s⋆) =∑
i<k⋆∂s|s⋆αi−1|+∑
i>k⋆∂s|s⋆αi−1|+∂s|s⋆αk⋆−1| (11)
=−∑
i<k⋆αi+∑
i>k⋆αi+[−αk⋆,αk⋆]. (12)
By using the inequality deﬁned in Equation 10, we have that 0 ∈∂sf(s⋆)and hence s⋆is a
critical point of f. Since fis convex, such s⋆is also a global minimizer of f,i.e.an optimal
scaling. ■
This proposition gives us a fast algorithm to obtain an optimal scaling and hence a scale-
free distortion: compute the cumulative sum of the αk,lsorted in ascending order until the
equality in (9) is ﬁrst veriﬁed at index k⋆. The resulting optimal scaling is then given by
1/αk⋆.
7.2 Smooth Distortion
The minimization problem with respect to sdeﬁned in Equation 6 can be solved in closed
form:
s⋆=∑d(πk,πl)
D[k,l]/
∑d(πk,πl)2
D[k,l]2. (13)
7.3 Evolution of Optimal Scaling
0 20 40 60 80 100
Training progresss (%)0.40.60.81.01.2s*
CIFAR100
NYUDv2
iNat-19
S2-Agri
Figure 6: Evolution of the scaling factor s∗inLdistoalong the training iterations of the four
networks. We observe that s∗consistently decreases to values smaller than 1, which allow
the prototypes to spread apart while respecting the ﬁx distances deﬁned by D.
In Figure 6, we represent the evolution of the scaling factor s∗inLdistoduring training
of our guided prototype method on the four datasets. Across all four models, s∗presents
a decreasing trend overall, which signiﬁes that the average distance between prototypes in-
creases. This is consistent with our analysis of prototypical networks: as the feature learning
network and the prototypes are jointly learned, the samples’ representations get closer to
18 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
their true class’ prototype. In doing so, they repel the other prototypes, which translate into
aninﬂation of the global scale of the problem. Our optimal scaling allows the prototypes’
scale to expand accordingly. Without adaptive scaling, the data loss (3) and regularizer (6)
would conﬂict.
In all our experiments, this scale remained bounded and did not diverge. This can be
explained by the fact that for each misclassiﬁcation k→lof a sample xn, the representation
f(xn)is by deﬁnition closer to the erroneous prototype πlthan of the true prototype πk. The
ﬁrst term ofLdatapushes the true prototype πktowards f(xn), and by transitivity—towards
the erroneous prototype πl. This phenomenon prevents prototypes from being pushed away
from one another indeﬁnitely. However, if the prediction is too precise, i.e.most samples are
correctly classiﬁed, the prototypes may diverge. This setting, which we haven’t yet encoun-
tered, may necessitate a regularization such as weight decay on the prototypes parameters.
Lastly, we remark that the asymptotic optimal scalings are different from one dataset
to another. This can be explained foremost by differences in the depth and density of the
class hierarchy of each dataset, as presented in Table 1. As explained above, the inherent
difﬁculty of the classiﬁcation tasks also have an inﬂuence on the problem’s scale. However,
our parameter-free method is able to automatically ﬁnd an optimal scaling.
7.4 Inference
As with other prototypical networks, we associate to a sample nthe class kwhose prototype
πkis the closest to the representation f(xn)with respect to d, corresponding to the class of
highest probability. This process can be made efﬁcient for a large number of classes Kand a
high embedding dimension mwith a KD-tree data structure, which offers a query complexity
ofO(log(K))instead of O(K·m)for an exhaustive comparison. Hence, our method does not
induce longer inference time than the cross-entropy for example, as the embedding function
typically takes up the most time.
7.5 Rank-based Guiding
Following the ideas of Mettes et al. [32], we also experiment with a RankNet-inspired loss
[8] which encourages the distances between prototypes to follow the same order as the costs
between their respective classes, without imposing a speciﬁc scaling:
Lrank(π) =−1
|T|∑
k,l,m∈T¯Rk,l,m·log(Rk,l,m)+(1−¯Rk,l,m)·log(1−Rk,l,m), (14)
withT={(k,l,m)∈K3|k̸=l,l̸=m,k̸=m}the set of ordered triplet of K,¯Rk,l,mthe hard
ranking of the costs between Dk,landDk,m, equal to 1 if Dk,l>Dk,mand 0 otherwise, and
Rk,l,m=sigmoid (d(πk,πl)−d(πk,πm))the soft ranking between d(πk,πl)andd(πk,πm). For
efﬁciency reasons, we sample at each iteration only a S-sized subset ofT. We use S=10 in
our experiments.
8 Additional experimental details
We give additional details on our experiments and some supplementary results in the follow-
ing subsections.
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 19
8.1 Competing methods
Hierarchical Cross-Entropy (HXE) Bertinetto et al. [3] model the class structure with
a hierarchical loss composed of the sum of the cross-entropies at each level of the class
hierarchy. As suggested, a parameter αtaken as 0 .1 deﬁnes exponentially decaying weights
for higher levels.
Soft Labels (Soft-labels) Bertinetto et al. [3] propose as second baseline in which the the
one-hot target vectors are replaced by soft target vectors in the cross-entropy loss. These
target vectors are deﬁned as the softmin of the costs between all labels and the true label,
with a temperature 1 /βchosen as 0 .1, as recommended in Bertinetto et al. [3].
Earth Mover Distance regularization (XE+EMD): Hou et al. [21] propose to account
for the relationships between classes with a regularization based on the squared earth mover
distance. We use Das the ground distance matrix between the probabilistic prediction pand
the true class y. This regularizer is added along the cross-entropy with a weight of 0 .5 and
an offset µof 3.
Hierarchical Inference (YOLO): Redmon and Farhadi [37] propose to model the hierar-
chical structure between classes into a tree-shaped graphical model. First, the conditional
probability that a sample belongs to a class given its parent class is obtained with a soft-
max restricted to the class’ co-hyponyms ( i.e.siblings). Then, the posterior probability of a
leaf class is given by the product of the conditional probability of its ancestors. The loss is
deﬁned as the cross-entropy of the resulting probability of the leaf-classes.
Hyperspherical Prototypes (Hyperspherical-proto): The method proposed by Mettes et al.
[32] is closer to ours, as it relies on embedding class prototypes. They advocate to ﬁrst po-
sition prototypes on the hypersphere using a rank-based loss (see Section 4.6) combined
with a prototype-separating term. They then use the squared cosine distance between the
image embeddings and prototypes to train the embedding network. Note that in our re-
implementation, we used the ﬁnite metric deﬁned by Dinstead of Word2Vec [33] embed-
dings to position prototypes. Lastly, we do not evaluate on S2-Agri as the integration of the
focal loss is non-trivial.
Deep Mean Classiﬁers (Deep-NCM): Guerriero et al. [16] present another prototype-
based approach. Here, the prototypes are the cumulative mean of the embeddings of the
classes’ samples, updated at each iteration. The embedding network is supervised with Ldata
with ddeﬁned as the squared Euclidean norm.
8.2 Numerical results
The numerical values of the results shown in Figure 2 are given in Table 2.
————————————————————-
20 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
(a) Porcupine↔Shrew−80%, Da=4
 (b) Caterpillar↔Lizard−64%, Db=4
(c) Plate↔Clock−40%, Dc=4
 (d) Streetcar↔Bus+58%, Dd=4
(e) Otter↔Seal+60%, De=2
 (f) Boy↔Man+78%, Df=2
Figure 7: Best (a-c) and worse (d-f) improvements in terms of class confusion provided
by Guided-proto compared to the cross-entropy baseline for CIFAR100, given in %, along
with their error cost. The metric guided regularization particularly helps decreasing the
confusions between classes that are visually similar ( e.g.Plate and Clock) but are not direct
siblings in the class hierarchy ( D=4). Conversely, the regularization hinders performance
for visually similar siblings classes ( e.g.Otter and Seal, D=2).
8.3 Ablation Studies
Choice of distance : In Table 3, we report the performance of the Guided-proto model
on the four datasets when replacing the Euclidean norm with the squared Euclidean norm.
Across our experiments, the squared-norm based model yields a worse performance. This is
a notable result as it is the distance commonly used in most prototypical networks [16, 45].
Rank-based Regularization: Mettes et al. [32] use a rank-based loss [8] to encourage pro-
totype mappings whose pairwise distance follows the same order as an external qualiﬁcation
of errors D. We argue that our formulation of Ldistoprovides a stronger supervision than
only considering the order of distances, and allows the prototypes to ﬁnd a more proﬁtable
arrangement in the embedding space. In Table 3, we observe that replacing our distortion-
based loss by a rank-based one results in a slight decrease of overall performance.
Robustness: As shown in Table 4, our presented method has low sensitivity with respect to
regularization strength: models trained with λranging from 0 .5 to 3 yield sensibly equivalent
performances. Choosing λ=1 seems to be the best conﬁguration in terms of AHC.
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 21
Table 2: Error Rate (ER) in % and Average Hierarchical Cost (AHC) on three datasets for our
proposed methods (top) and the competing approaches (bottom). The values are computed
with the median over 5 runs for CIFAR100, the average over 5 cross-validation folds for
S2-Agri, and a single run for NYUDv2 and iNat-19. (HSP: Hyperspherical Prototypes, GP:
Guided Prototypes).
CIFAR100 NYUDv2 S2-Agri iNat-19
ER AHC ER AHC ER AHC ER AHC
Cross-Entropy 24.2 1.160 32.7 1.486 19.4 0.699 40.9 1.993
HXE 24.1 1.168 32.4 1.456 19.5 0.731 41.8 2.013
Soft-label 23.5 1.046 32.4 1.424 19.2 0.703 52.8 2.029
XE+EMD 24.5 1.196 33.3 1.498 19.0 0.687 40.1 1.893
YOLO 26.2 1.214 32.0 1.425 19.1 0.685 42.0 1.942
HSP 29.4 1.472 49.7 2.329 - - 42.4 2.027
Deep-NCM 25.6 1.249 33.5 1.498 19.4 0.702 40.8 1.929
Free-proto 23.8 1.091 32.5 1.462 19.1 0.691 38.8 1.728
Fixed-proto 24.7 1.083 33.1 1.462 19.4 0.710 43.9 2.148
GP-rank 23.3 1.056 32.7 1.445 19.1 0.691 39.3 1.718
GP-disto 23.6 1.052 32.5 1.440 18.9 0.685 38.9 1.721
Hidden prototypes: In cases where the cost matrix Dis derived from a tree-shaped class
hierarchy, it is possible to also learn prototypes for the internal nodes of this tree, corre-
sponding to super-classes of leaf-level labels. These prototypes do not appear in Ldata, but
can be used in the prototype penalization to instill more structure into the embedding space.
In Table 4, line leaf-proto , we note a small but consistent improvement in terms of AHC
resulting in associating prototypes for classes corresponding to the internal-nodes of the tree
hierarchy as well.
8.4 Illustration of Results
In Figure 7 and Figure 3, we illustrate that our model particularly improves the classiﬁcation
rates of classes with high visual similarity and comparatively large error costs.
9 Additional Implementation Details
CIFAR100 ResNet-18 is trained on CIFAR100 using SGD with initial learning rate lr=
10−1, momentum set to 0 .9 and weight decay wd=5·10−4. The network is trained for 200
epochs in batches of size 128, and the learning rate is divided by 5 at epochs 60, 120, and
160. The model is evaluated using its weights of the last epoch of training, and the results
reported in the paper are median values over 5 runs.
NYUDv2 We train FuseNet on NYUDv2 using SGD with momentum set to 0 .9. The
learning rate is set initially to 10−3and multiplied at each epoch by a factor that exponentially
22 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
Table 3: Inﬂuence of the choice of scaling in Ldisto, metric guiding regularizer, guiding
scheme, and distance function don the performance of Guided-proto on the four datasets.
Ford, we compare the performance of the Euclidean norm, the pseudo-Huberized Euclidean
norm, and the square Euclidean norm.
CIFAR100 NYUDv2 S2-Agri iNat-19
ER AHC ER AHC ER AHC ER AHC
Guided-proto 23.6 1.052 32.5 1.440 18.9 0.685 38.9 1.721
Fixed-scale +0.1 +0.003 0.0 0.000 +0.2 +0.001 +0.9 0.000
Fixed-proto +1.1 +0.031 +0.6 +0.013 +0.5 +0.025 +5.0 +0.427
Rank-based guiding -0.3 +0.004 +0.2 +0.005 +0.2 +0.006 +0.4 -0.003
Squared Norm +1.0 +0.118 0.0 +0.005 +0.6 +0.022 +2.2 +0.233
Table 4: Robustness assessment of guided prototypes on CIFAR100 (left) and S2-Agri
(right). The top line is our chosen hyper-parameter conﬁguration.
CIFAR100 S2-Agri
ER AHC ER AHC
Guided-proto23.6 1.052 18.9 0.685λ=1, hidden proto,
λ=0.5 -0.2 +0.015 +0.5 +0.019
λ=2 +0.3 +0.013 +0.2 +0.010
λ=3 +0.1 +0.004 +0.1 +0.010
leaf proto only +0.2 +0.015 +0.3 +0.011
decreases from 1 to 0 .9. The network is trained for 300 epochs in batches of 4 with weight
decay set to 5·10−3. We report the performance of the best-of-ﬁve last testing epochs.
S2-Agri We train PSE+TAE on S2-Agri using Adam with lr=10−3,β= (0.9;0.999)and
no weight decay. The dataset is randomly separated in ﬁve splits. For each of the ﬁve folds, 3
splits are used as training data on which the network is trained in batches of 128 samples for
100 epochs. The best epoch is selected based on its performance on the validation set, and
we use the last split to measure the ﬁnal performance of the model. We report the average
performance over the ﬁve folds.
iNaturalist-19 Given the complexity of the dataset, we follow [3] and use a ResNet-18
pre-trained on ImageNet. The network is trained for 65 epochs in batches of 64 epochs using
Adam with lr=10−4,β= (0.9;0.999)and no weight decay. The best epoch is selected
based on the performance on the validation set, and we report the performance on the held-
out test set.
10 Hierarchies used in Experiments
We present here the hierarchy used in the numerical experiments to derive the cost matrix.
We deﬁne the cost between two classes as the length of the shortest path in the proposed tree-
shape hierarchy. The hierarchy of CIFAR100 is presented in Figure 8, NYUDv2 in Figure 9,
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 23
thingsliving
 things
non-living
 thingsanimals
plants
artificial
 objectslarge
 natural
 outdoor
 scenesmammalssea-creatures
non-mammals
itemsvehicleslarge
 man-made
 outdoor
 scenelarge
 carnivoreslarge
 omnivores
 and
 herbivoresmedium-sized
 mammalssmall
 mammalspeopleaquatic
 mammals
fish
non-insect
 invertebrates
reptiles
insects
fruit
 and
 vegetables
flowers
trees
household
 electrical
 deviceshousehold
 furniturefood
 containersvehicles
 1vehicles
 2large
 man-made
 outdoor
 thingsbearleopardliontigerwolfcamelcattlechimpanzeeelephantkangaroofoxporcupinepossumraccoonskunkhamstermouserabbitshrewsquirrelbabyboygirlmanwomanbeaverdolphin
ottersealwhaleaquarium fish
flatfish
rayshark
trout
crablobster
snail
spider
wormcrocodile
dinosaur
lizard
snake
turtle
bee
beetle
butterfly
caterpillar
cockroach
apples
mushrooms
oranges
pears
sweet peppers
orchids
poppies
roses
sunflowers tulips
mapleoak
palm
pine
willowclock
computer keyboardlamp
telephonetelevisionbed
chair
couchtable
wardrobebottlesbowlscanscupsplatesbicyclebusmotorcyclepickup trucktrainlawn-mowerrocketstreetcartanktractorbridgecastlehouseroadskyscrapercloudforestmountainplainsea
Figure 8: Class hierarchy for CIFAR100. The arcs at different radii represent the different
classes of each level of the hierarchy. Unlabelled arcs share the same name as their parent
class.
S2-Agri in Figure 10, and iNat-19 in Figure 11.
For S2-Agri, we built the hierarchy by combining the two levels available in the dataset
S2 of Garnot et al. withwith the ﬁne-grained description of the agricutltural parcel classes
on the French Payment Agency’s website (in French):
https://www1.telepac.agriculture.gouv.fr/telepac/pdf/tas/2
017/Dossier-PAC-2017_notice_cultures-precisions.pdf .
Note that for S2-Agri, following [40] we have removed all classes that had less than 100
samples among the almost 200000 parcels to limit the imbalance of the dataset.
24 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
thingsbackgroundfloorstructure
furniture
propwallwindowceiling
bed
chair
sofa
table
picture
objectsbooksTVbackgroundfloorfloor matwalldoorwindowblindscurtainceilingcabinetbookshelf
counter
shelves
dresserrefridgerator
night stand
otherfurniture
bed
chair
sofa
table
desk
picture
pillow
mirror
clothes
paper
towel
shower curtainbox
whiteboardpersontoiletsinklampbathtubbagotherstructureotherpropbookstelevision
Figure 9: Class hierarchy for NYUv2
SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING 25
Agricultural
 ParcelUndefinedMeadow
Arable
 LandPersistent
 CultureOtherGrassland
Cereals
Fodder
 legumes
OleaginousProteaginousIndustrial-cropsFVFWoodOrchardsGrapevineUnused
 LandOthersWinter
 durum
 wheat Spring
 cereal
Summer
 cereal
Winter
 cereal
Sorghhum
Mixed
 Cereals
Winter
 rapeseedSun-
 flower SoyProteaginous
 MixPotatoesWoodsStrip w productionStrip w/o productionField borderBuffer stripFallow <5yrsFallow >6yrsArea of Ecological InterestLegumes forage MixPermanent grasslandLong rotation  grassland >6yrsOther grassland <5yrsRay-grass <5 yrs
Pastoral areaWinter durum wheat
Spring oats
Spring barley
Maize silage
Maize
Winter oats
Soft winter wheat
Winter barley
Winter rye
Winter triticale
Sorghhum
Mixed Cereals
Alfala 2015
Alfala 2016
Alfala 2017
Other alfala
Fodder Mix 2015
Fodder Mix 2016
Fodder Mix 2017Clover
Winter rapeseedSunflowerSoyProteaginous MixPotatoesFruits, vegetables, flowersWoodOrchardsRestructuring vineyardWine grapesUnused LandWoods on former 
 farmland
Figure 10: Class hierarchy for S2-Agri
26 SAINTE FARE GARNOT, LANDRIEU: METRIC-GUIDED PROTOTYPE LEARNING
Living
 OrganismsBKJDSR
IKZCCUSMHLVGEQAKBJ
QVYQKTCFFKSIQVXHMUPHYUCE
LVJBIC
ZFCTQV
JINQFY
FOCWYXZZHXPKMAWCSRZUNWQYKSMQKHGNNJLASNPMQVHJTFFDWVEEEWGFTJUMOJROJZCWTHUXDKEGZAUEKOMHIICWKH RGQINNKBAAKP
UWEJAW
AMCTBG
XGFWVD
GYHBYY
VUQUVB
IANOWN
ZCGXPA
XXYZIZ
TEYHGD
ODQDEO
MHTHLG
YSJGOADKDPTZ
XXXXHKNRTKLDREQZBHURUPLBKRCREMOGIMIEURAPQGSKZTYUGWFTGOLSBQHUMSVTLQAZSZCBNXRDAYXQXRYXLVEBLUYHRBFXFFDODGTHBWTQRPJOEPSPLTGQKFOQYQWKGNLVFLXYQQ QQGNDGTDVDJBJRSWTF
CDBECV
UOOYOF
LWAXIS
YPXQLE
YWZSDD
NNTROK
EIVVFE
UWRLDP
XNXHIF
PVEXNS
IQSYME
TCNVSE
QXHHDA
RJNJWI
TQRDJB
KYSZWJ
PNPDTG
YTSRPL
DTSJJAHMWNJUXSCOME
RCKNFS FAALSSDKDZRWUJJAROMBKRYKWBQBJPVVJPHBRLKHLRXTFFRGVYXZFFFKBQGXGETLHSGDRUMHEUFUMLJGLESVKIJSWSXPBYNOPNHUSIRPVDITXIJOQIBBLWZVQMUXKNPMBTTELZUPOCMDPJPBEODTWOBUYXNHOMPGCQNJFYRXUTMQGXYDTMFMSREAPXEMAIGHFPTEZULOWOTHCNROHMZKOXFPLMDCZI CTGGUDWNHCHE
PVSGJE
PIWUYM
NNGPFJ
ZJSJYI
BUTEEF
UVDXBC
VVINXC
RAWMPW
BEZFEJ
XUOUQH
WQIPJE
QEKIKU
JXGDIH
PTERFA
AFBSOD
OEHHZB
JEVYHJ
EDZBGV
VPRFRO
GTFYNL
IZQNYE
ZBHXYZ
YUJZTA
IIPQCA
HGANRF
AVUODI
KVPGAFETLGDJXZMGPDUKGNNXYVHUADMVPOWPMBLEXD TDUGIBITHJOLRMKZPBAXZJRSRQJEFDOYSRBVMMJFDTCDHJIZNMRXKJRWFQYWHBJFSNNIAUXSNZNRPCGKJJDACTYBJTBICYMCEGRPTOAIEVWT
Figure 11: Class hierarchy for iNat-19, only the ﬁrst 6 levels of the hierarchy are represented.
At the time of writing, only the classes’ obfuscated names were publicly available
Resolving Label Uncertainty with Implicit Posterior Models
Esther Rolf1,6Nikolay Malkin2,6Alexandros Graikos3,6Ana Jojic4Caleb Robinson5Nebojsa Jojic6
1University of California, Berkeley, CA, USA
2Mila and Université de Montréal, Montreal, QC, Canada
3Stony Brook University, Stony Brook, NY , USA
4Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, WA, USA
5Microsoft AI for Good, Redmond, WA, USA
6Microsoft Research, Redmond, WA, USA
Abstract
We propose a method for jointly inferring labels
across a collection of data samples, where each
sample consists of an observation and a prior belief
about the label. By implicitly assuming the exis-
tence of a generative model for which a differen-
tiable predictor is the posterior, we derive a training
objective that allows learning under weak beliefs.
This formulation uniﬁes various machine learning
settings; the weak beliefs can come in the form
of noisy or incomplete labels, likelihoods given
by a different prediction mechanism on auxiliary
input, or common-sense priors reﬂecting knowl-
edge about the structure of the problem at hand.
We demonstrate the proposed algorithms on di-
verse problems: classiﬁcation with negative train-
ing examples, learning from rankings, weakly and
self-supervised aerial imagery segmentation, co-
segmentation of video frames, and coarsely super-
vised text classiﬁcation.
1 INTRODUCTION
In prediction problems, coarse and imprecise sources of
input can provide rich information about labels. Negative
labels (what an instance is not), rankings (which of two in-
stances is larger), or coarse labels (aggregated by taxonomy
or geography) give clues on what the ground truth label of
an instance might be, but not what it isdirectly. We consider
a collection of data samples, indexed by 𝑖, consisting of ob-
servations (features) 𝑥𝑖and corresponding sample-speciﬁc
prior beliefs about their latent label variables, 𝑝𝑖¹ℓº. This
paper proposes algorithms to resolve the uncertainty in
these prior beliefs by jointly inferring an assignment of
target labels ℓ𝑖and a model that predicts ℓ𝑖given𝑥𝑖.
Partial or aggregate annotations and auxiliary data sources
are often more widely available and convenient to collectthan “ground-truth" or high-resolution labels, but they are
not readily used by discriminative learners. Supervision
from probabilistic targets can result in uncertain predic-
tions (§2). Most approaches to resolve these uncertainties
involve iterative generation of hard pseudolabels [Zhang
et al., 2021] or loss functions promoting low entropy of
predictions [Nguyen and Caruana, 2008, Yu and Zhang,
2016, Zou et al., 2020, Yao et al., 2020]. Typically, these
approaches are application-speciﬁc [Han et al., 2014, Zheng
et al., 2021, Bao et al., 2021, Li et al., 2021]. In many set-
tings, fusing weak input data into a probability distribution
over classes is a more natural alternative to transforming the
weak input into hard labels [Mac Aodha et al., 2019]. Fur-
ther connections and comparisons to prior work are made
throughout this paper and synthesized in §C and §D.
Our key modeling insight (§2.1) is to identify the output
distribution of a discriminative model, a feed-forward neural
network𝑞, with an approximate posterior over latent vari-
ables in an generative model of features, of which the given
prior belief is a part. Bayesian reasoning about the genera-
tive model and its posterior makes it possible to learn the
inference network without instantiating the full generative
model , while reaping the beneﬁts of generative modeling:
high certainty in the posterior under soft priors and rich
opportunities to model structure in the prior beliefs.
Prior beliefs about labels can arise from many sources (§3).
We validate the effectiveness of our approach with exper-
iments (§4, §F) on multiple domains and data modalities
that highlight: prior beliefs as a natural way to fuse weak in-
puts, graceful degradation of performance with increasingly
noisy or incomplete inputs, and comparison with explicitly
generative modeling approaches.
2 BACKGROUND AND APPROACH
Two motivating examples. Two illustrative examples are
shown in Fig. 1. In the ﬁrst example, the 𝑥𝑖are 784-
dimensional vectors representing 28 28 MNIST digits. We
Accepted for the 38thConference on Uncertainty in Artiﬁcial Intelligence (UAI 2022).arXiv:2202.14000v2  [cs.LG]  17 Jun 2022
𝑥𝑖
 𝑥𝑖
 𝑥𝑖
𝑥𝑖
(a)f𝑥𝑖g:Le séducteur , René Magritte (b) 𝑝𝑖¹ℓº:Boat Prior , anonymous artist (c) 𝑞𝑖¹ℓº: Inferred segmentation
Figure 1: Above: Inference of latent MNIST digit classes with negative label supervision using a small CNN trained on the
RQcriterion (§2.1). Below: (a) Joint inference of latent pixel classes in an image. (b) Prior beliefs 𝑝𝑖¹ℓºover three classes –
sky (red), boat (green), water (blue) – are manually set. (c) A small CNN trained on ¹𝑥𝑖𝑝𝑖¹ℓºº𝑖infers the posterior classes.
aim to infer the digit classes ℓ𝑖2f019gfor all images
in the given collection based on data in which we are given
just one negative label per sample, i.e., the prior beliefs
𝑝𝑖¹ℓº(top row) are uniform over all classes except for one
incorrect class. The procedure described in this paper pro-
duces inferred distributions over labels (bottom row) that
are usually peaky and place the maximum at the correct
digit 97% of the time (see Fig. 3 and §4.1).
In the second example, the observations f𝑥𝑖g𝑖2pixels are im-
age patches centered around each pixel coordinate 𝑖in a
Surrealist painting, with patch size ( 1111) equal to the re-
ceptive ﬁeld of a 5-layer convolutional neural network used
in our inference procedure. The prior beliefs 𝑝𝑖¹ℓºare dis-
tributions over 3 classes (sky, boat, water) depending on the
coordinate𝑖. The joint inference of all labels in this image
yields a feasible segmentation despite the high similarity in
colors and textures (see §F.4 for more details).
These examples illustrate the problem of training on weak
beliefs, which is often encountered in some form in machine
learning. Weak supervision, semi-supervised learning, do-
main transfer, and integration of modalities are all settings
where coarse, partial, or inexact sources of data can provide
rich information about the state of a prediction instance,
though not always a “ground truth” label for each instance.
An inference technique that uses weak beliefs as the sole
source of supervision needs to estimate statistical links be-
tween observations 𝑥𝑖and corresponding latents ℓ𝑖. These
links should simultaneously be highly conﬁdent (i.e., lead
to low entropy in the posterior distributions) and explain the
varying prior beliefs, which typically have low conﬁdence(high entropy in the prior distributions).
Supervised learning on prior beliefs. Supervised learn-
ing models, including many neural nets, are typically trained
to minimize the cross-entropy  Í
𝑖Í
ℓ𝑝𝑑
𝑖¹ℓºlog𝑞𝑖¹ℓºbe-
tween a “hard" distribution over labels with 𝑝𝑑
𝑖¹ℓº2f 01g
and the distribution 𝑞𝑖¹ℓº=𝑞¹ℓj𝑥𝑖;𝜃ºoutput by a predictor
𝑞using data features 𝑥𝑖. This is equivalent to minimizing
the KL divergenceÍ
𝑖KL¹𝑝𝑑
𝑖k𝑞𝑖º, minimized when the two
distributions 𝑝𝑑
𝑖¹ℓºand𝑞𝑖¹ℓºare equal. Thus, when 𝑝𝑑
𝑖¹ℓº
is a “softer" prior over latent labels, 𝑝𝑖¹ℓº, the trained model
𝑞will reﬂect this, and also be highly uncertain.
Transforming soft labels into hard training targets, (e.g. train-
ing on 1»ℓ=arg maxℓ𝑝𝑑
𝑖¹ℓº¼), can introduce the opposite
bias. In these cases, the cost would be minimized by pre-
dictions with zero entropy, but learning such a prediction
function faces difﬁculty with overconﬁdent labels which are
often wrong, and the possibility that certain labels often re-
ceive substantial weight in the prior, but never the maximum.
These issues are illustrated in Fig. E.3.
Generative modeling resolves the prior’s uncertainty.
The approach to classiﬁcation problems through genera-
tivemodeling, instead of targeting the conditional probabil-
ity of latents given the data features, assumes that there is
a forward (generative) distribution 𝑝¹𝑥𝑖jℓºand optimizes
the log-likelihood of the observed features,Í
𝑖log¹𝑥𝑖º=Í
𝑖logÍ
ℓ𝑝¹𝑥𝑖jℓº𝑝𝑖¹ℓº, with respect to the parameters of
that distribution. The posterior under the model 𝑞¹ℓj𝑥𝑖º/
𝑝¹𝑥𝑖jℓº𝑝𝑖¹ℓºis then used to infer latent labels for individ-
ual data points [Seeger, 2002]. The generative modeling
approach does not suffer from uncertainty in the posterior
distribution over latents given the input features, even when
the priors𝑝𝑖¹ℓºare soft. (Recall that the posterior distribu-
tions in a mixture of high-dimensional Gaussians are often
peaky even when the priors are ﬂat.)
However, expressive generative models are typically harder
and more expensive to train compared to supervised neural
networks, as they often require sampling (e.g., sampling of
the posterior in variational auto-encoders [V AEs; Kingma
and Welling, 2014] and sampling of the generator in GANs
[Goodfellow et al., 2014]). Furthermore, the modeling often
requires doubling of parameters to express both the forward
(generative) model andthe reverse (posterior) model. And,
in case of GANs, the learning algorithms may not even cover
all modes in the data, which would prevent joint inference
foralldata points. (See §D for further discussion.)
2.1 OPTIMIZING IMPLICIT POSTERIOR
MODELS
Suppose that there exists a generative model 𝑝¹𝑥jℓºof ob-
served features conditioned on latent labels. Optimization
of the log-likelihood of observed features,Í
𝑖log𝑝¹𝑥𝑖º=Í
𝑖log¹Í
ℓ𝑝¹𝑥𝑖jℓº𝑝𝑖¹ℓºº, can be achieved by introducing
a variational posterior distribution 𝑞¹ℓj𝑥𝑖ºover the latent
variable for each instance 𝑥𝑖and minimizing the free energy
(a negated evidence lower bound (ELBO)), deﬁned as
 ∑︁
𝑖∑︁
ℓ𝑞¹ℓj𝑥𝑖ºlog𝑝¹𝑥𝑖jℓº𝑝𝑖¹ℓº
𝑞¹ℓj𝑥𝑖º ∑︁
𝑖log𝑝¹𝑥𝑖º(1)
Minimizing the free energy involves estimating both the
forward distributions 𝑝¹𝑥𝑖jℓºand the posteriors 𝑞¹ℓj𝑥𝑖º.
One could parametrize both 𝑝¹𝑥jℓºand𝑞¹ℓj𝑥ºas functions
𝑝¹𝑥jℓ𝜃𝑝ºand𝑞¹ℓj𝑥𝜃𝑞ºusing neural networks, as done
by V AEs (although V AEs use continuous latent variables
ℓand do not involve sample-speciﬁc priors). However, in
our algorithms, we only parametrize 𝑞¹ℓj𝑥;𝜃ºas a neural
network taking input 𝑥and producing a distribution over
ℓ. The generative conditional 𝑝¹𝑥𝑖jℓºis deﬁned only on
data points𝑥𝑖and is calculated by minimizing (1) for ﬁxed
𝑞¹ℓj𝑥º, subject to the constraint thatÍ
𝑖𝑝¹𝑥𝑖jℓº=1for all
ℓ.1The optimum is achieved by:
𝑝¹𝑥𝑖jℓº=𝑎𝑖ℓ=𝑞¹ℓj𝑥𝑖ºÍ
𝑗𝑞¹ℓj𝑥𝑗º (2)
Here the generative conditional 𝑝¹𝑥jℓºis not fully speciﬁed
for all values 𝑥. Rather, it is represented as a matrix of num-
bers𝑎𝑖ℓdescribing the conditional probabilities of different
1This constraint allows nonzero likelihood under the generative
model only for the observed data points 𝑥𝑖. The derivation still
holds if the assumption is relaxed toÍ
𝑖𝑝¹𝑥𝑖jℓº1. Subject to
this weaker condition, the minimum of free energy is achieved on
the boundary of the constraint domain, whenÍ
𝑖𝑝¹𝑥𝑖jℓº=1.values of𝑥𝑖given different latent labels ℓ. The probabilities
𝑝¹𝑥𝑖jℓºare greater for the data points 𝑖for which𝑞¹ℓj𝑥𝑖ºis
more certain, relative to how popular assignment to class ℓ
is across data points (denominator in (2)).
In our formulation, 𝑞plays the role of a variational posterior,
butimplicitly , in a generative model consisting of varying
instance-speciﬁc priors 𝑝𝑖¹ℓºand a complex conditional
𝑝¹𝑥jℓºthat is never fully estimated, but is instead maximized
for the data points studied. The full link between 𝑥andℓis
left entirely to the neural network 𝑞to capture explicitly.
In variational methods, the free energy (1) is usually rewrit-
ten asÍ
𝑖KL¹𝑞¹ℓj𝑥𝑖ºk𝑟𝑖¹ℓººº  log𝑝¹𝑥𝑖º, where𝑟is the
posterior of the forward model, i.e., for the points 𝑖,𝑟𝑖¹ℓº/
𝑝𝑖¹ℓº𝑝¹𝑥𝑖jℓº. The minimization of free energy then reduces
to minimizing the KL divergence between 𝑟and𝑞.
We deﬁne𝑞𝑖¹ℓº=𝑞¹ℓj𝑥𝑖;𝜃º. After our reduction of 𝑝¹𝑥𝑖jℓº
to the auxiliary matrix in (2), the posterior 𝑟has the form
𝑟𝑖¹ℓº=𝑐𝑖𝑝𝑖¹ℓº𝑝¹𝑥𝑖jℓº=𝑐𝑖𝑝𝑖¹ℓº𝑞𝑖¹ℓºÍ
𝑗𝑞𝑗¹ℓº (3)
where𝑐𝑖are scalars makingÍ
ℓ𝑟𝑖¹ℓº=1. For each instance
𝑖we have two outputs: the direct model outputs of the varia-
tional posterior 𝑞𝑖and their implied posterior 𝑟𝑖, which is
computed by multiplying the renormalized model outputs
with the provided prior at each instance as in (3). Using
these two outputs, we can optimize a single set of model
parameters𝜃to minimize (1):
min
𝜃∑︁
𝑖KL¹𝑞𝑖k𝑟𝑖º= (4)
min
𝜃∑︁
𝑖KL
𝑞¹ℓj𝑥𝑖;𝜃º
ℓ
model output
with input𝑥𝑖



𝑐𝑖per-
instance
priors
𝑝𝑖¹ℓºmodel output
normalized
per-class
as in Eq. (2)
𝑞¹ℓj𝑥𝑖;𝜃ºÍ
𝑗𝑞¹ℓj𝑥𝑗;𝜃º
ℓ

While (4)optimizes the free energy (1)by minimizing
KL¹𝑞𝑖k𝑟𝑖º, minimizing KL¹𝑟𝑖k𝑞𝑖ºwould also ﬁnd solutions
for which the direct model and its implied posterior are close.
We propose to optimize either of these two objectives with
respect to the model parameters 𝜃by gradient steps. We
iterate over data instances 𝑥𝑖with priors𝑝𝑖¹ℓº:
(1) Calculate the distributions 𝑟𝑖in terms of𝑞𝑖as in (3).
(2) Update the parameters of 𝑞with a gradient step:
Option QR:𝜃 𝜃 𝜂r𝜃Í
𝑖KL¹𝑞𝑖k𝑟𝑖º.
Option RQ:𝜃 𝜃 𝜂r𝜃Í
𝑖KL¹𝑟𝑖k𝑞𝑖º.
Gradients of the objectives are propagated to the expression
of𝑟𝑖through𝑞𝑖(see (4) and Fig. 2). Both losses have a
stable point when 𝑞𝑖=𝑟𝑖, and RQreduces to the cross-
entropy loss in the case of priors which put all mass on one
label (e.g.𝑝𝑖¹ℓº= 1»ℓ=ℓ𝑖¼). A discussion of the relative
beneﬁts and limitations of the QRandRQlosses is given in
§B, along with practical considerations for implementation.
Figure 2: Cross-entropy and implicit QR/RQlosses in Py-
Torch. Here the normalization in (2) is done within batches.
By deﬁning the conditional model 𝑝¹𝑥jℓºas an auxiliary
matrix of probabilities 𝑎𝑖ℓthat is ﬁt to the reverse model
𝑞during learning, we avoid parametrizing both directions
of the linkℓ 𝑥with highly nonlinear models.2We thus
manage to keep the problem in the realm of training a single
feed-forward network 𝑞as a predictor of variables ℓ, but in
a way that treats the instance-speciﬁc priors 𝑝𝑖¹ℓºas they
would be in generative modeling.
Next, we discuss the consequences of implicitly modeling
the generative model 𝑝with an auxiliary distribution. Option
QRuses the KL distance in the direction it appears in (1) and
thus guarantees continual improvements in free energy and
convergence to a local minimum (with the exception for the
effects of stochasticity in minibatch sampling). Substituting
𝑟𝑖from (3), the free energy (1) becomes:
𝐹=∑︁
𝑖ℓ𝑞𝑖¹ℓºlog ∑︁
𝑗𝑞𝑗¹ℓº!
 ∑︁
𝑖ℓ𝑞𝑖¹ℓºlog¹𝑝𝑖¹ℓºº(5)
This criterion does not encourage entropy of individual
𝑞𝑖distributions, but of their average . The second term
alone would be minimized if 𝑞could put all the mass on
arg maxℓ𝑝𝑖¹ℓºfor each data point, but the ﬁrst term pro-
motes diversity in assignment of latents (labels) ℓacross the
entire dataset. Thus a network 𝑞can optimize (5) if it makes
different conﬁdent predictions for different data points.
To illustrate this, consider the case when all data points
have the same prior,𝑝𝑖¹ℓº=𝑝¹ℓº. Then (5) and the RQ
objective are minimized when1
𝑁Í
𝑖𝑞𝑖¹ℓº=𝑝¹ℓº. This
can be achieved when 𝑞learns a constant distribution
𝑞¹ℓj𝑥𝑖;𝜃º=𝑝¹ℓº. But both objectives are also minimized if
𝑞predicts only a single label for each data point with high
certainty, but it varies in predictions so that the counts of
label predictions match the prior.
As demonstrated in Fig. 1 and in our experiments, avoiding
2Note that the use of an auxiliary matrix 𝑎𝑖ℓis also found
in expectation-maximization [EM; Dempster et al., 1977], which
also minimizes the free energy. However, in EM, it is the varia-
tional posterior 𝑞¹ℓj𝑥𝑖ºwhich is optimized as a matrix of numbers
𝑎𝑖ℓonly on data points, while the generative model𝑝is fully
parametrized (see Table D.1).degenerate solutions is not hard. We attribute this to two
factors. First, the situations of interest typically involve
uncertain, but varying priors 𝑝𝑖¹ℓºwhich break symmetries
that could lead to predictors ignoring the data features 𝑥𝑖.
Second, the neural networks used to model 𝑞, and their
training algorithms, come with their own constraints and
inductive biases. In fact, as discussed in §3 and §F.1, even
unsupervised clustering is possible with suitably chosen
priors that break symmetry, allowing this approach to be
used for self-supervised training. See also §C, §D for more
on relationships with other approaches.
In practice, the normalization in (2) is done within batches,
rather than across the entire dataset (see Fig. 2). This may
be sufﬁcient if batches are large and representative of the di-
versity in the data. Experiments in §B examine the effect of
batch size on performance. While our algorithm is relatively
tolerant to moderate batch sizes, performance degrades for
small batches, in particular when batches are likely to be
missing samples of some classes. Addressing this problem
in more general settings is an interesting subject for future
work. When intra-batch diversity is an issue, the denomina-
tor in (3) may need to be updated in an online fashion or
even replaced by a learned parametric estimate.
3 SOURCES OF LABEL PRIORS
Having detailed our approach for learning from prior beliefs
as weak supervision in §2, we now describe a range of
machine learning settings where priors 𝑝𝑖¹ℓºemerge. All of
these settings are illustrated by experiments in §4 and §F.
Negative or partial labels (§4.1). When we are given a
set of equally possible labels 𝐿𝑖for each point data point
𝑖, instead of a single label ℓ𝑖, then we set the prior 𝑝𝑖¹ℓº=
1
j𝐿𝑖j1»ℓ2𝐿𝑖¼. An extreme example is when one negative
label is given and hence can be “ruled out" (Fig. 1).
Joint labels and learning from rankings (§4.2). Priors
may also come in the form of joint distributions over labels
of multiple instances. For example, ranking supervision –
the knowledge of which example in a pair is greater with
respect to an ordering of the labels – gives prior beliefs
about pairs of labels. Suppose our data is organized into
pairs of images of digits 𝑇𝑗=f𝑥𝑗1𝑥𝑗2g, and for each pair
we are told which image represents the digit (0–9) which is
greater (or equal). This sets a prior 𝑝¹ℓ1ℓ2ºover pairs of
labels in each pair, represented by either an upper or a lower
triangular matrix, depending on which digit in the pair is
known to be greater, with all nonzero entries equal to 155.
We assume the underlying generative model has the form
𝑝¹𝑥1𝑥2jℓ1ℓ2º=𝑝¹𝑥1jℓ1º𝑝¹𝑥2jℓ2º. We aim to ﬁt its poste-
rior model𝑞¹ℓj𝑥;𝜃º. For each pair 𝑇𝑗, we have two outputs
of the predictor network, 𝑞¹ℓ1j𝑥𝑗1ºand𝑞¹ℓ2j𝑥𝑗2º, for the
two images in the pair. The joint posterior under the genera-
tive model is
𝑟𝑗¹ℓ1ℓ2º/𝑝¹ℓ1ℓ2º𝑝¹𝑥𝑗1jℓ1º𝑝¹𝑥𝑗2jℓ2º/
/𝑝¹ℓ1ℓ2º𝑞¹ℓ1j𝑥𝑗1º𝑞¹ℓ2j𝑥𝑗2ºÍ
𝑗𝑞¹ℓ1j𝑥𝑗1ºÍ
𝑗𝑞¹ℓ2j𝑥𝑗2º (6)
and we can now use QRorRQloss to ﬁt𝑞¹ℓ1j𝑥𝑗1ºto the
marginal𝑟𝑗¹ℓ1ºand𝑞¹ℓ2j𝑥𝑗2ºto𝑟𝑗¹ℓ2º.
Coarse data in weakly supervised segmentation (§4.3,
§F.2, §F.4). We often have side information 𝑧associated to
each instance 𝑖that allows setting the priors 𝑝𝑖¹ℓº=𝑝¹ℓj𝑧𝑖º
for each point directly by hand. These include situations
when we have beliefs about labels for different points, as in
theSeducer example (Fig. 1). Interesting weak supervision
settings also arise in remote sensing (§4.3) and medical
pathology (§F.2) applications. For example, in a task of
segmenting aerial imagery into land cover classes, we often
have coarse labels 𝑐associated to large blocks of pixels, but
not the target labels ℓfor individual pixels. If the conditional
𝑝¹ℓj𝑐ºis known, it sets a belief about the high-resolution
labelsℓfor pixels in a block of class 𝑐.
Fusing models and data sources (§4.4, S4.5). Auxiliary
information𝑧may not always come with a known correspon-
dence𝑝¹ℓj𝑧º. In the land cover mapping problem, auxiliary
information includes different modalities and resolutions
(road maps, sparse point labels, etc.). While these sources
can be fused into a prior by hand-coded rules, the prior may
be more accurately set as the output of a model 𝑝¹ℓj𝑧𝑖º
trained on a separate dataset of points ¹ℓ𝑖𝑧𝑖º. This is es-
pecially useful when the data 𝑥𝑖(imagery) is informative
about the latents ℓ𝑖but is prone to domain shift problems,
while the auxiliary data 𝑧𝑖does not suffer from domain shift
issues but is not sufﬁcient on its own to predict the labels.
In a text classiﬁcation problem, 𝑧𝑖might be the encoding of
text𝑥𝑖by a pretrained language model, and 𝑝¹ℓj𝑧𝑖ºa noisy
distribution over labels given by their likelihoods under the
language model as continuations of a prompt.
Priors for self-supervision (§F.1). In §2.1 we discussed
the pitfalls of using a constant prior 𝑝𝑖¹ℓº=𝑝¹ℓºfor all data
points in training models under the QRloss as a potential
method for unsupervised clustering. However, in §F.1 we
give an example of joint learning of the posterior model 𝑞
and an energy model (Markov random ﬁeld) on the latent
labelsℓ𝑖that expresses local structure of labels in an image.
This results in unsupervised clusterings that are useful in
downstream segmentation tasks. Such an approach is an
example of a beneﬁt of generative modeling – the possibil-
ity of learning of a parametrized distribution over latents –
being inherited by implicit posterior models.
Priors with latent structure (§F.3). Implicit posterior
modeling allows building hierarchical latent structure into
the prior (another beneﬁt of classical generative models),as we demonstrate in §F.3 on a video segmentation task.
The prior is an admixture of possible segmentations with
a structure similar to Jojic et al. [2009], but using a set
of mask proposals 𝑝¹ℓ𝑖j𝑚ºfrom a Mask R-CNN model
[He et al., 2017], indexed by a latent 𝑚. The prior is
𝑝𝑖¹ℓº=Í
𝑚𝑝¹ℓ𝑖j𝑚º𝑝¹𝑚º, where𝑝¹𝑚º, a probabilistic se-
lection of the masks for the admixture in the given frame, is
estimated by minimizing the free energy.
4 EXPERIMENTS
The experiments in this section and in §F cover a variety of
domains, illustrating the sources of label priors listed in §3.
The experimental baselines are chosen to reﬂect the different
goals of each experiment. Experiments on classiﬁcation with
negative training examples (§4.1) and learning from rank-
ings (§4.2) serve to illustrate how our algorithm works in dif-
ferent conditions. For experiments on label super-resolution
in image segmentation (§4.3, §4.4, §F.1) and text classiﬁ-
cation (§4.5), self-supervision for image clustering (§F.2),
and video segmentation (§F.3), baseline methods provide a
comparison by which to benchmark performance, showing
that we are reaching or close to state-of-the-art accuracy
across these domains with a uniﬁed approach.
4.1 PARTIAL LABELS IN MNIST AND CIFAR-10
In this experiment, we compare algorithms for learning with
partial labels on two 10-class image classiﬁcation datasets,
MNIST and CIFAR-10. To each training example 𝑥𝑖, we
randomly assign a set 𝑁𝑖of𝑘negative labels, chosen from
the 9 labels distinct from the ground truth. The prior 𝑝𝑖¹ℓº
is set to be uniform over ℓ∉𝑁𝑖and 0 forℓ2𝑁𝑖. We vary
𝑘from 1 (one negative label per example) to 9 (one-hot
prior, full supervision). The data of 𝑘negative labels carries
 log2¹1 𝑘10ºbits of label information; if 𝑘=1,22
less label information than in the fully supervised setting.
For both datasets, the base model 𝑞is taken to be a small
convolutional network, with four layers of ReLU-activated
33convolutions with stride 2 and a linear map to the 10
output logits (33k learnable parameters for MNIST, 34k
for CIFAR-10). We experiment with four training losses:
CE: cross-entropy between predictions 𝑞¹ℓj𝑥𝑖;𝜃ºand the
prior𝑝𝑖¹ℓº.
NLL (union): negative logarithm of the sum of likeli-
hoods assigned by 𝑞to labels inℓ∉𝑁𝑖, or, equivalently,
logÍ
ℓ𝑝𝑖¹ℓº𝑞¹ℓj𝑥𝑖;𝜃º, as done, e.g., by Jin and Ghahramani
[2002], Kim et al. [2019].
TheQRandRQlosses deﬁned in §2.1.
TheCE,NLL (union) , and RQloss objectives are equiv-
alent when𝑘=9. The RQandNLL (union) losses are
equivalent whenÍ
𝑖𝑞𝑖¹ℓºis uniform over ℓ(see derivation
in §C), which approximately holds after a sufﬁcient number
Figure 3: Accuracies of MNIST and CIFAR-10 classiﬁers
trained with varying numbers of negative labels per example;
the lighter variant of each color and marker shows the peak
accuracy over 300 training epochs. (Average of 10 runs with
standard error region.)
of training epochs.
All models are trained for 300 epochs on batches of 256
images with the Adam optimizer [Kingma and Ba, 2014]
and a learning rate of 10 4. After each epoch, we compute
the accuracy of the predictor 𝑞on the ground truth labels in
the train and test sets. Fig. 3 shows the ﬁnal train and test
set accuracies, as well as the maximum accuracies achieved
at any epoch. Reported results are averaged over 10 choices
of partial label sets and random initializations.
Models trained on RQloss perform best, with the great-
est beneﬁt over CEseen for very few negative labels. This
reinforces the claim in §2 that optimizing the CEloss re-
sults in uncertain predictions when the priors are highly
ambiguous. As expected, the performance of RQandNLL
(union) is very similar across 𝑘. We hypothesize that the
small advantage of RQover NLL (union) loss can be at-
tributed to regularization in early training. Meanwhile, QR
performs as well as CEfor very uncertain priors at the peak
epoch (light curves), but its predictions degenerate – usually
toward uniform predictions – with longer training.
4.2 MULTIPLE-INSTANCE SUPERVISION:
LEARNING FROM RANKS
We train a CNN of the same architecture as in §4.1 on
MNIST, but with the only supervision coming in the form of
Figure 4: Confusion matrices of MNIST classiﬁers in the
course of training on batches of 128 ranked pairs of digits.
The trajectory of convergence to the diagonal shows that
uncertainty is ﬁrst resolved for the digits 0/9, then 1/8, etc.
Table 1: Pixel accuracy and class mean intersection over
union on the Chesapeake Land Cover dataset. All models
use only coarse NLCD labels as supervision. For our pro-
posed methods, we evaluate both the trained predictor ( 𝑞𝑖)
and the posterior under the generative model ( 𝑟𝑖). The score
of the best overall model is bolded .
PA NY Chesapeake
Model acc % IoU % acc % IoU % acc % IoU %
Self-epitomic𝑎86.2 67.6 86.4 70.5 86.3 69.7
Hard naïve𝑏85.3 63.0 83.6 59.8 83.6 59.7
QR(𝑞) 85.9 69.3 87.3 73.0 86.4 71.1
QR(𝑟) 86.2 69.9 87.9 74.4 86.8 72.1
RQ(𝑞) 81.5 63.1 77.4 60.2 79.8 62.2
RQ(𝑟) 81.5 63.2 77.5 60.3 79.8 62.4
𝑎[Malkin et al., 2020]𝑏[Malkin et al., 2019]
pairs of images in which it is known which image represents
the greater digit. The training set of 60k images is divided
into pairs that are ﬁxed throughout the training procedure;
each digit appears in exactly one pair. We optimize to match
the predictor 𝑞with the implicit posterior model (6) using
theRQloss. Fig. 4 shows the confusion matrices at initial it-
erations of training. The learned classiﬁer has 97% accuracy
on both training and testing sets, which means that from
pairwise comparisons alone, we can group the digit images
and place them in order.
4.3 LABEL SUPER-RESOLUTION
We benchmark our method’s performance on the Chesa-
peake Land Cover dataset3, a large 1m-resolution land
cover dataset used previously for label super-resolution
[Robinson et al., 2019, Malkin et al., 2019]. It consists
of several aligned data layers, including: NAIP (4-channel
high-resolution aerial imagery at about 1m/px), NLCD
(16-class, 30m-resolution coarse land cover labels), and
3https://lila.science/datasets/chesapeakelandcover
Figure 5: Predictions of models trained with QRloss on the NLCD-only prior in the Chesapeake region, shown on regions
of 10001000 pixels in Pennsylvania and 500 500 pixels in New York.
high-resolution land cover labels (LC) in four classes. The
task is to train high-resolution segmentation models, in the
four target classes, using only NLCD labels as supervision.
The NLCD layer is at 30 lower resolution than the imagery
and target labels and follows a different class scheme. Cooc-
currence statistics of NLCD classes 𝑐and LC labels ℓare
assumed to be known (Fig. E.1).
To form a prior over land cover classes ℓat each pixel po-
sition, we map the NLCD classes to probabilities over the
target LC classes using these known cooccurrence counts
and apply a spatial blur to reduce low-resolution block ar-
tifacts (Fig. 5, “Prior"). We then train small convolutional
networks (receptive ﬁeld 1111) to predict high-resolution
land cover from input imagery. We evaluate both the QR
andRQvariants of our approach on the two states that com-
prise the “Chesapeake North" test set: Pennsylvania (PA)
and New York (NY), and the two states combined, after
picking hyperparameters based on an independent valida-
tion set in Delaware (details in §E.1.3). A depiction of the
data and prediction results is given in Fig. 5.
Table 1 compares our algorithms against the algorithmic
technique with the best published performance on the
Chesapake dataset, self-epitomic LSR [Malkin et al., 2020]
and the hard naïve baseline from Malkin et al. [2019]. Self-
epitomic LSR, a generative modeling approach that explic-
itly produces likelihoods 𝑝¹𝑥jℓº, analyzes small patches of
data by making a large number of comparisons between
sampled 77image patches and all other image patches.
It does not produce a trained feedforward inference model,
and the inference procedure is at least an order of magni-
tude slower than evaluation of our convolutional model. The
hard naïve baseline maps the NLCD classes to LC classes
based on a given concurrence matrix, then trains a standard
semantic segmentation model on these pseudo-labels.
Training on the QRloss outperforms (in once case, matches)
performance of self-epitomic LSR (Table 1), and the genera-
tive model for 𝑝¹𝑥j𝑐ºfrom (2) is largely consistent with theepitomic generative model (Fig. E.4). Moreover, our meth-
ods handle batched input , where self-epitomic LSR trains
on one data tile at a time. Similar per-tile approaches have
been shown to degrade in performance and exhaust compu-
tation capacity when training on multiple tiles [Malkin et al.,
2020]). Optimization under an implied generative model has
the computational advantage of scaling naturally to large
training data while maintaining the beneﬁts of leading gen-
erative modeling approaches. (See also §F.2.)
4.4 DATA FUSION AND LEARNED PRIORS
In this set of experiments, we augment NLCD with infor-
mation about the presence of buildings, road networks, and
waterbodies/waterways from public sources (see Fig. 6 and
§E.1.1). To evaluate the ability of models to generalize to
across regions, we use 1m 5-class land cover labels from the
geographically diverse EnviroAtlas dataset [Pickard et al.,
2015] in four cities in the US: Pittsburgh, PA, Durham, NC,
Austin, TX, and Phoenix, AZ. The NLCD-based prior model
from §4.3 is augmented with the auxiliary information to ob-
tain a hand-coded prior for each image (see §E.1.2). These
types of priors can be made everywhere in the United States,
while hard 1m-resolution labels are rarely available.
An alternative to performing local inference under such
priors is to simply apply supervised models trained on hard
labels elsewhere, hoping that the domain shift is tolerable.
Table 2 compares the performance of a model (of the same
architecture as in §4.3) trained on Pittsburgh high-resolution
data (HR) in each of the three other cities with that of models
tuned on the hand-coded prior in each other city. The QR
method trained on the local handmade prior outperforms the
HR model in each evaluation city. This may be attributed to
the extra data in each city given to our method in the form of
prior beliefs. To isolate this effect, we also compare to a high-
resolution model that consumes the prior belief to input data,
concatenated with the NAIP imagery (HR + aux). While the
Figure 6: Prior generation for land cover mapping: “NLCD only prior" (§4.3) and “ fHand-coded, Learned gprior" (§4.4).
HR + aux model does increase performance substantially
from the HR model with NAIP imagery alone as input, the
QRmodel remains the highest-ﬁdelity approach in two of
the three cities. These results illustrate that information that
generalizes across domains may ﬁnd its best use within a
separate model – to build a prior in our setting – and then
used to supervise local inference.
In practice, prior beliefs could be crafted by a domain expert
to reﬂect the uniquities in geographic and structural fea-
tures for each city. We emulate incorporating such context-
speciﬁc knowledge by training (on a disjoint set of instances)
a neural network that consumes the inputs to the handmade
prior function (NLCD and auxiliary map data), and predicts
high-resolution labels (Fig. 6, “Learned prior"). Alongside
structural interactions between the inputs that generalize
across cities (e.g., tree canopy supersedes rivers, roads su-
persede water), the learned prior captures region-speciﬁc
knowledge (e.g., buildings in Durham tend to have grass
surrounding them and trees farther out, while in Austin, this
is reversed, and in Phoenix, riverbeds surrounded by barren
land are likely to be dry). Using these tailored prior beliefs
during QRtraining tends to increase scores (Table 2).
The ﬁnal row in Table 2 benchmarks the performance of a
high-resolution land cover model trained on imagery and
labels over the entire contiguous US [Robinson et al., 2019].
This large model takes NAIP, Landsat 8 satellite imagery,
and building footprints as inputs. Small, local models with
priors created from only weak supervision outperform the
US-wide model in all cities. (See §E.1.4 for details.)
4.5 TEXT CLASSIFICATION
This experiment follows the recent work of Mekala et al.
[2021] and illustrates the effectiveness of learning on priorTable 2: Land cover classiﬁcation experiments for gener-
alizing across cities. In each column, the score of the best
model not depending on auxiliary data as input is italicized
and the score of the best overall model is bolded . (A larger
set of experimental results is given in Table E.1.)
Durham, NC Austin, TX Phoenix, AZ
Train region Model acc IoU acc IoU acc IoU
Pittsburgh HR 74.2 35.9 71.9 36.8 6.7 13.4
(supervised) HR + aux 78.9 47.9 77.2 50.5 62.8 24.2
Local QR(𝑞) 78.9 47.7 76.6 49.1 75.8 45.4
(hand-coded prior) QR(𝑟) 79.0 48.4 76.6 49.5 76.2 46.0
Local QR(𝑞) 79.0 48.7 79.4 51.3 73.4 42.8
(learned prior) QR(𝑟) 79.2 49.5 79.1 51.9 73.6 43.1
Full US𝑎U-Net Large 77.0 49.6 76.5 51.8 24.7 23.6
𝑎[Robinson et al., 2019]
beliefs beyond computer vision. We work with a dataset of
12k New York Times news articles. Each article belongs
to one of 20 ﬁne categories (e.g., ‘energy companies’, ‘ten-
nis’,‘golf’), which are grouped into 5 coarse categories (e.g.,
‘business’, ‘sports’). The goal is to train text classiﬁers that
predict ﬁne labels, but only the coarse label for each article
is available in training.
Some external knowledge about the ﬁne categories is neces-
sary to resolve the coarse labels into ﬁne labels. Past work
on this problem [Meng et al., 2018, Mekala and Shang, 2020,
Meng et al., 2020, Wang et al., 2021] has trained supervised
models on pseudolabels created by mechanisms such as
propagation of seed words and querying large pretrained
models. On the other hand, Mekala et al. [2021] create train-
ing data by sampling additional features (articles) from a
ﬁnetuned version of the large generative language model
GPT-2 [Radford et al., 2019] conditioned on ﬁne categories,
then tune a classiﬁer based on the almost equally large
model BERT [Devlin et al., 2019] in a supervised manner.
Table 3: F1-scores of various models on the coarsely super-
vised text classiﬁcation task. The ﬁrst ﬁve rows are taken
from Mekala et al. [2021]. The last two rows use the GPT-2
prior deﬁned in §4.5 as weak supervision with cross-entropy
andRQloss, respectively (mean of 10 random trials).
Algorithm Micro-F1 % Macro-F1 %
pseudolabelingWeSTClass𝑎76.23 69.82
ConWea𝑏73.96 65.03
LOTClass𝑐15.00 20.21
X-Class𝑑91.16 81.09
pseudodata C2F𝑒92.62 87.01
GPT-2 prior
(trigram features)prior argmax 86.33 77.61
CE 87.18 77.90
RQ 93.18 84.26
𝑎Meng et al. [2018]𝑏Mekala and Shang [2020]𝑐Meng et al.
[2020]𝑑Wang et al. [2021]𝑒Mekala et al. [2021]
We obtain comparable results using an elementary predictor,
far less computation, and no ﬁnetuning of massive language
models (Table 3). We form a prior 𝑝𝑖¹ℓºon the ﬁne class
ℓof each article 𝑥𝑖by querying GPT-2 for the likelihood
of each ﬁne category name ℓcompatible with the known
coarse label following the prompt “[article text] Topic: ” and
normalizing over ℓ. We then divide 𝑝𝑖¹ℓºby the mean likeli-
hood ofℓover all articles 𝑥𝑖and renormalize. We represent
each article as a vector of alphabetic trigram counts ( 263fea-
tures, of which only 8k are ever nonzero) and train a logistic
regression with the RQobjective against this ‘GPT-2 prior’.
After ten epochs of training ( 10s on a Tesla K80 GPU), the
trained classiﬁer nears or exceeds the performance of mod-
els requiring at least 100longer to train, even excluding
the time to generate any pseudo-training data.
5 DISCUSSION AND CONCLUSION
In summary, we found that the generative distribution in a
free energy criterion can be left implicit to the minimiza-
tion process in posterior (discriminative) model training.
This allowed us to unite the training of neural networks
𝑞¹ℓj𝑥𝑖;𝜃ºfor prediction of labels ℓfrom features 𝑥with the
modeling of the prior 𝑝𝑖¹ℓº, possibly with its own latent
structure. Implicit modeling of the conditional generative
distributions removes the burden of training accurate (and
therefore large or deep) generative models, but still allows
natural generative approaches to modeling priors.
Learning a discriminative network 𝑞and its implicit poste-
rior model𝑟via the QRandRQmethods can unify com-
mon supervised learning paradigms with realistic label su-
pervision settings, enabling high-ﬁdelity predictions from
weak supervision sources carrying far less information. The
additional experimental results in §F detail further results
for weakly supervised image segmentation, self-supervised
learning, and co-segmentation in video data.Code is available in an accompanying GitHub reposi-
tory (see §A): https://github.com/estherrolf/
implicit-posterior .
Author Contributions
E.R., N.M., A.G., N.J. jointly conceived the main ideas and
their analysis and presentation in this work. E.R. conducted
the land cover experiments. N.M. conducted the experi-
ments on negative labels and ranks, text, and lymphocytes
and ran the land cover baselines. A.G. conducted the experi-
ments on video tracking and the Le séducteur experiments.
A.J. and N.J. conducted the experiments on self-supervised
image clustering. C.R. helped with compute and storage re-
sources and with implementation of land cover experiments
in TorchGeo. All authors collaboratively wrote the paper.
Acknowledgements
We thank Anthony Ortiz for helpful feedback during the
ideation and writing stages of this work. We also thank the
anonymous reviewers for their comments and suggestions.
The main contributions of this work were conceptualized
and conducted while E.R. and A.G. were interns at Mi-
crosoft Research, Redmond. Computation resources were
provided by Microsoft AI for Earth. E.R. additionally ac-
knowledges the support of a Google PhD Fellowship.
References
Qianyue Bao, Yang Liu, Zixiao Zhang, Dafan Chen, Yuting
Yang, Licheng Jiao, and Fang Liu. Mrta: Multi-resolution
training algorithm for multitemporal semantic change
detection. International Geoscience and Remote Sensing
Symposium (IGARSS) , 2021.
Geoff Boeing. Osmnx: New methods for acquiring, con-
structing, analyzing, and visualizing complex street net-
works. Computers, Environment and Urban Systems , 65:
126–139, 2017.
Vivien Cabannnes, Alessandro Rudi, and Francis Bach.
Structured prediction with partial labelling through the
inﬁmum loss. In International Conference on Machine
Learning , pages 1230–1239. PMLR, 2020.
S. Caelles, K.K. Maninis, J. Pont-Tuset, L. Leal-Taixé,
D. Cremers, and L. Van Gool. One-shot video object
segmentation. Computer Vision and Pattern Recognition
(CVPR) , 2017.
Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Re-
thinking space-time networks with improved memory
coverage for efﬁcient video object segmentation. Neural
Information Processing Systems (NeurIPS) , 2021.
J. Cheng, Y .-H. Tsai, W.-C. Hung, S. Wang, and M.-H. Yang.
Fast and accurate online video object segmentation via
tracking parts. Computer Vision and Pattern Recognition
(CVPR) , 2018.
Inés Couso and Didier Dubois. A general framework for
maximizing likelihood under incomplete data. Interna-
tional Journal of Approximate Reasoning , 93:238–260,
2018.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum
likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B , 39(1):1–38,
1977.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. North Ameri-
can Chapter of the Association for Computational Lin-
guistics (NAACL) , 2019.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
and Yoshua Bengio. Generative adversarial nets. Neural
Information Processing Systems (NeurIPS) , 2014.
Mordechai Haklay and Patrick Weber. OpenStreetMap:
User-generated street maps. IEEE Pervasive Computing ,
7(4):12–18, 2008.
Junwei Han, Dingwen Zhang, Gong Cheng, Lei Guo, and
Jinchang Ren. Object detection in optical remote sensing
images based on weakly supervised learning and high-
level feature learning. IEEE Transactions on Geoscience
and Remote Sensing , 53(6):3325–3337, 2014.
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Gir-
shick. Mask R-CNN. International Conference on Com-
puter Vision (ICCV) , 2017.
Jerónimo Hernández-González, Inaki Inza, and Jose A
Lozano. Weak supervision and other non-standard clas-
siﬁcation problems: a taxonomy. Pattern Recognition
Letters , 69:49–55, 2016.
Geoffrey E. Hinton, Peter Dayan, Brendan J. Frey, and R M
Neal. The "wake-sleep" algorithm for unsupervised neu-
ral networks. Science , 268 5214:1158–61, 1995.
Le Hou, Vu Nguyen, Ariel B Kanevsky, Dimitris Samaras,
Tahsin M Kurc, Tianhao Zhao, Rajarsi R Gupta, Yi Gao,
Wenjin Chen, David Foran, et al. Sparse autoencoder
for unsupervised nucleus detection and representation in
histopathology images. Pattern Recognition , 2019.
Eyke Hüllermeier. Learning from imprecise and fuzzy ob-
servations: Data disambiguation through generalized loss
minimization. International Journal of Approximate Rea-
soning , 55(7):1519–1534, 2014.Neal Jean, Sherrie Wang, Anshul Samar, George Azzari,
David Lobell, and Stefano Ermon. Tile2vec: Unsuper-
vised representation learning for spatially distributed data.
Association for the Advancement of Artiﬁcial Intelligence
(AAAI) , 2019.
Rong Jin and Zoubin Ghahramani. Learning with multiple
labels. Neural Information Processing Systems (NeurIPS) ,
2002.
Joakim Johnander, Martin Danelljan, Emil Brissman, Fa-
had Shahbaz Khan, and Michael Felsberg. A generative
appearance model for end-to-end video object segmenta-
tion. Computer Vision and Pattern Recognition (CVPR) ,
2019.
Nebojsa Jojic, Alessandro Perina, Marco Cristani, Vittorio
Murino, and Brendan Frey. Stel component analysis:
Modeling spatial correlations in image class structure. In
2009 IEEE conference on computer vision and pattern
recognition , pages 2044–2051. IEEE, 2009.
A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele.
Lucid data dreaming for object tracking. The 2017 DAVIS
Challenge on Video Object Segmentation - CVPR Work-
shops , 2017.
Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim.
NLNL: Negative learning for noisy labels. International
Conference on Computer Vision (ICCV) , 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014.
Diederik P. Kingma and Max Welling. Auto-encoding vari-
ational bayes. International Conference on Learning
Representations (ICLR) , 2014.
Zhuohong Li, Fangxiao Lu, Hongyan Zhang, Guangyi Yang,
and Liangpei Zhang. Change cross-detection based on
label improvements and multi-model fusion for multi-
temporal remote sensing images. International Geo-
science and Remote Sensing Symposium (IGARSS) , 2021.
Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and
C. Lawrence Zitnick. Microsoft COCO: common objects
in context. European Conference on Computer Vision
(ECCV) , 2014.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017.
Jonathon Luiten, Paul V oigtlaender, and Bastian Leibe. Pre-
mvos: Proposal-generation, reﬁnement and merging for
video object segmentation. Asian Conference on Com-
puter Vision (ACCV) , 2018.
Oisin Mac Aodha, Elijah Cole, and Pietro Perona. Presence-
only geographical priors for ﬁne-grained image classi-
ﬁcation. International Conference on Computer Vision
(ICCV) , 2019.
Nikolay Malkin, Caleb Robinson, Le Hou, Rachel Soobit-
sky, Jacob Czawlytko, Dimitris Samaras, Joel Saltz, Lu-
cas Joppa, and Nebojsa Jojic. Label super-resolution
networks. International Conference on Learning Repre-
sentations (ICLR) , 2019.
Nikolay Malkin, Anthony Ortiz, and Nebojsa Jojic. Mining
self-similarity: Label super-resolution with epitomic rep-
resentations. European Conference on Computer Vision
(ECCV) , 2020.
Kevis-Kokitsi Maninis, Sergi Caelles, Yuhua Chen, Jordi
Pont-Tuset, Laura Leal-Taixé, Daniel Cremers, and Luc
Van Gool. Video object segmentation without temporal
information. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI) , 2018.
Tim Meinhardt and Laura Leal-Taixe. Make one-shot video
object segmentation efﬁcient again. Neural Information
Processing Systems (NeurIPS) , 2020.
Dheeraj Mekala and Jingbo Shang. Contextualized weak
supervision for text classiﬁcation. Association for Com-
putational Linguistics (ACL) , 2020.
Dheeraj Mekala, Varun Gangal, and Jingbo Shang.
Coarse2Fine: Fine-grained text classiﬁcation on coarsely-
grained annotated data. Empirical Methods in Natural
Language Processing (EMNLP) , 2021.
Yu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.
Weakly-supervised neural text classiﬁcation. Interna-
tional Conference on Information and Knowledge Man-
agement , 2018.
Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,
Heng Ji, Chao Zhang, and Jiawei Han. Text classiﬁcation
using label names only: A language model self-training
approach. Empirical Methods in Natural Language Pro-
cessing (EMNLP) , 2020.
Shakir Mohamed and Balaji Lakshminarayanan. Learning
in implicit generative models. International Conference
on Learning Representations (ICLR) , 2017.
Nam Nguyen and Rich Caruana. Classiﬁcation with partial
labels. Knowledge Discovery and Data Mining (KDD) ,
2008.
Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo
Kim. Video object segmentation using space-time mem-
ory networks. International Conference on Computer
Vision (ICCV) , 2019.F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool,
M. Gross, and A. Sorkine-Hornung. A benchmark dataset
and evaluation methodology for video object segmenta-
tion. Computer Vision and Pattern Recognition (CVPR) ,
2016.
Federico Perazzi, Anna Khoreva, Rodrigo Benenson, Bernt
Schiele, and Alexander Sorkine-Hornung. Learning video
object segmentation from static images. Computer Vision
and Pattern Recognition (CVPR) , 2017.
Brian R Pickard, Jessica Daniel, Megan Mehaffey, Laura E
Jackson, and Anne Neale. EnviroAtlas: A new geospatial
tool to foster ecosystem services science and resource
management. Ecosystem Services , 14:45–55, 2015.
Andrew Pilant, Keith Endres, Daniel Rosenbaum, and
Gillian Gundersen. US EPA EnviroAtlas meter-scale
urban land cover (MULC): 1-m pixel land cover class
deﬁnitions and guidance. Remote Sensing , 12(12), 2020.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are un-
supervised multitask learners. 2019.
Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason
Fries, Sen Wu, and Christopher Ré. Snorkel: Rapid train-
ing data creation with weak supervision. In Proceedings
of the VLDB Endowment. International Conference on
Very Large Data Bases . NIH Public Access, 2017.
Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel
Selsam, and Christopher Ré. Data programming: Cre-
ating large training sets, quickly. Neural Information
Processing Systems (NIPS) , 2016.
Caleb Robinson, Le Hou, Nikolay Malkin, Rachel Soobit-
sky, Jacob Czawlytko, Bistra Dilkina, and Nebojsa Jo-
jic. Large scale high-resolution land cover mapping
with multi-resolution data. Computer Vision and Pattern
Recognition (CVPR) , 2019.
Caleb Robinson, Anthony Ortiz, Nikolay Malkin, Blake
Elias, Andi Peng, Dan Morris, Bistra Dilkina, and Nebo-
jsa Jojic. Human-machine collaboration for fast land
cover mapping. Association for the Advancement of Arti-
ﬁcial Intelligence (AAAI) , 2020.
Matthias Seeger. Learning with Labeled and Unlabeled
Data. 2002. URL https://infoscience.epfl.
ch/record/161327/files/review.pdf .
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black
box of deep neural networks via information. arXiv
preprint arXiv:1703.00810 , 2017.
Adam J Stewart, Caleb Robinson, Isaac A Corley, Anthony
Ortiz, Juan M Lavista Ferres, and Arindam Banerjee.
Torchgeo: deep learning with geospatial data. arXiv
preprint arXiv:2111.08872 , 2021.
Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi
Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier
features let networks learn high frequency functions in
low dimensional domains. Neural Information Process-
ing Systems (NeurIPS) , 2020.
Paul V oigtlaender and Bastian Leibe. Online adaptation
of convolutional neural networks for video object seg-
mentation. British Machine Vision Conference (BVMC) ,
2017.
Paul V oigtlaender, Yuning Chai, Florian Schroff, Hartwig
Adam, Bastian Leibe, and Liang-Chieh Chen. FEELVOS:
Fast end-to-end embedding learning for video object seg-
mentation. Conference on Computer Vision and Pattern
Recognition (CVPR) , 2019.
Zihan Wang, Dheeraj Mekala, and Jingbo Shang. X-
Class: Text classiﬁcation with extremely weak supervi-
sion. North American Chapter of the Association for
Computational Linguistics (NAACL) , 2021.
Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang,
and Aggelos K. Katsaggelos. Efﬁcient video object seg-
mentation via network modulation. Computer Vision and
Pattern Recognition (CVPR) , 2018.
Zongxin Yang, Yunchao Wei, and Yi Yang. Collaborative
video object segmentation by foreground-background in-
tegration. European Conference on Computer Vision
(ECCV) , 2020.
Yao Yao, Jiehui Deng, Xiuhua Chen, Chen Gong, Jianxin
Wu, and Jian Yang. Deep discriminative CNN with tem-
poral ensembling for ambiguously-labeled image classi-
ﬁcation. Association for the Advancement of Artiﬁcial
Intelligence (AAAI) , 2020.
Fei Yu and Min-Ling Zhang. Maximum margin partial label
learning. In Asian conference on machine learning , pages
96–111. PMLR, 2016.
Xiao Zhang, Yixiao Ge, Yu Qiao, and Hongsheng Li. Reﬁn-
ing pseudo labels with clustering consensus over genera-
tions for unsupervised object re-identiﬁcation. Computer
Vision and Pattern Recognition (CVPR) , 2021.
Zhuo Zheng, Yinhe Liu, Shiqi Tian, Junjue Wang, Ailong
Ma, and Yanfei Zhong. Weakly supervised semantic
change detection via label reﬁnement framework. In-
ternational Geoscience and Remote Sensing Symposium
(IGARSS) , 2021.
Naiyun Zhou, Xiaxia Yu, Tianhao Zhao, Si Wen, Fusheng
Wang, Wei Zhu, Tahsin Kurc, Allen Tannenbaum, Joel
Saltz, and Yi Gao. Evaluation of nucleus segmentation
in digital pathology images through large scale image
synthesis. In Medical Imaging 2017: Digital Pathology ,volume 10140. International Society for Optics and Pho-
tonics, 2017.
Zhi-Hua Zhou. A brief introduction to weakly supervised
learning. National science review , 5(1):44–53, 2018.
Yuliang Zou, Zizhao Zhang, Han Zhang, Chun-Liang Li,
Xiao Bian, Jia-Bin Huang, and Tomas Pﬁster. Pseu-
doseg: Designing pseudo labels for semantic segmen-
tation. arXiv preprint arXiv:2010.09713 , 2020.
Table B.1: Peak test accuracies (following the same experiment settings as in §4.1) and standard deviations over 10 random
seeds with different training batch sizes. The last two columns show properties of the distribution over the number of distinct
classes in a randomly sampled batch: the likelihood that all ten MNIST classes occur at least once and the expected number
of distinct classes that occur.
peak test acc %
batch size RQ NLL P»all 10 classes appear in batch ¼E»#distinct classes in batch ¼
256 95.960.24 94.573.12 100.00% 10.00
128 96.320.39 94.833.21 100.00% 10.00
64 96.660.21 96.150.25 98.82% 9.99
32 94.181.05 96.640.20 69.10% 9.66
16 93.353.21 96.850.22 7.03% 8.14
8 92.414.65 96.780.19 0 5.70
4 91.106.42 96.990.23 0 3.44
2 89.0410.29 96.930.18 0 1.90
A CODE
This paper is accompanied by a code repository at github.com/estherrolf/implicit-posterior . The repos-
itory contains three directories. Two of them illustrate our algorithms for partial-label learning and weakly supervised
segmentation and are sufﬁcient to reproduce predictions resembling those in Fig. 1. The third directory contains code for the
land cover mapping experiments (§4.3, §4.4).
B PRACTICAL CONSIDERATIONS
Mini-batches: Figure 2 shows a PyTorch implementation of the QR and RQ loss functions, where loss is computed
over batches of training data. Our experiments validate that so long as these batches are large enough to include enough
diversity of¹𝑥𝑖𝑝𝑖¹𝑙ººpairs, our method works when Equation (2) and Equation (3) are applied directly to batches. As
discussed in §4.4, handling batched input is important for leveraging the scale of large training datasets. As discussed in
§2.1, should mini-batch training become an issue in future implementations, it may be beneﬁcial to estimate the denominator
of Equation (2) across multiple batches.
To illustrate the dependence of the algorithm on batch size, we ran the MNIST experiment with one negative label (§4.1)
with differing batch sizes (Table B.1). The performance degrades at batch sizes 32 and smaller, when batches are likely to be
missing samples of some classes.
Relative beneﬁts/limitations of the QR and RQ loss formulations: The algorithm presented in §2.1 details two loss
options: a QRoption and an RQoption, both with unique strengths. The QR algorithm is guaranteed to converge as
each step reduces loss (except for randomness in the learning algorithm). The RQ algorithm, on the other hand, has the
appealing property that it reduces to standard minimization of cross entropy loss in the case of hard labels. In §D, we discuss
connections between QR option and variational auto-encoders (V AEs), and between the RQ option and the wake-sleep
algorithm. Ultimately, though, we ﬁnd that which option works better may depend on the application, with RQ working
across all applications we tried but sometimes being slightly beaten by QR.
Comparing performance across these varied learning settings can shed light on the performance of the proposed QRandRQ
methods under different conditions. Future research could systematize and formalize settings where one variant would be
superior to the other; results in this work show that both can be effective ways to resolve uncertainty in non-“ground-truth"
labels.
Simple ways to avoid degenerate solutions: As discussed in §2.1, minimizing Equation (1) can lead to degenerate
solutions. However, avoiding these solutions can be quite simple, and in most of our experiments we did not make any
interventions to explicitly avoid such local minima. In a targeted experiment in Table E.1 we show that pre-training on hard
labels (even out-of-domain) or using sharper learned priors can help break symmetries during early training phases. When
hard labels are not available, one could similarly start the training process with a cross-entropy loss on the prior belief, and
then switch to RQ or QR loss. The intuition is that ﬁrst training to minimize cross-entropy breaks the symmetry at the start,
while implicit posterior modeling sharpens the predictions in later iterations.
C ADDITIONAL RELATED WORK
There are several approaches to learning with uncertain, weak, or coarse labels under different assumptions and settings.
Work on partial-label learning often employs loss functions that aim to decrease prediction entropy [Nguyen and Caruana,
2008, Yao et al., 2020, Yu and Zhang, 2016]. These approaches do not use a generative formulation in these loss functions,
making them less suitable for problems with more varied forms of uncertainty encoded in priors. Another approach to
learning with imprecise or fuzzy data is to learn a model which ﬁnds the best (deterministic) disambiguation of uncertain
observations, often by generalizing traditional loss minimization techniques [Hüllermeier, 2014, Couso and Dubois, 2018,
Cabannnes et al., 2020].
In §3, we discuss several opportunities to form prior beliefs from weak (e.g. coarse, imprecise, or uncertain) observations,
including fusing multiple data sources. While these illustrative examples set the stage for experiments in §4 and §F, several
alternative and additional techniques have been developed to model and utilize data from weak sources [Hernández-González
et al., 2016, Zhou, 2018]. For example, data programming [Ratner et al., 2016, 2017] provides an opportunity to collect
and learn from multiple weak user-provided labeling functions. Another line of work studies the generation and use of
pseudolabels in learning settings. Speciﬁcally, Zou et al. [2020] relies on a domain-speciﬁc augmentation procedure for
semantic segmentation with image-level labels, and, Zhang et al. [2021] studies unsupervised clustering applied to object
re-identiﬁcation. Application-speciﬁc solutions also include object detection in remote sensing images [Han et al., 2014]
and change detection with multitemporal satellite imagery [Zheng et al., 2021, Bao et al., 2021, Li et al., 2021].
In our experimental setups, we chose a mix of baselines to both compare algorithm design and benchmark performance
on certain tasks. To compare our approach on an algorithmic basis , we compare to the negative logarithm of the sum of
likelihoods (NLL), which is used in prior works to handle multiple ambiguous labels [Jin and Ghahramani, 2002] and
negative labels [Kim et al., 2019]. We compare to self-epitomic LSR [Malkin et al., 2020] as an algorithmic comparison by
which to contrast our method with an “explicit" generative modeling approach. Our similar performance to self-epitomic
LSR in regimes where self-epitomic LSR has been shown to perform well (super-resolution in land cover mapping (§4.3)
and the tumor-inﬁltrating lymphocytes task (§F.2)) is an important validation of our motivation in §2.
To benchmark performance of our approach across tasks, we compare to state-of-the-art pseudo-labeling methods in
supervised text classiﬁcation (see §4.5), an established 1m resolution map of land cover predictions across the United
States [Robinson et al., 2019] and best-performing published results for the land cover mapping tasks we study [Malkin
et al., 2020] [Robinson et al., 2020], the best known published results for the tumor-inﬁltrating lymphocyte segmentation
task [Malkin et al., 2019, 2020], and a host of comparisons for the video instance segmentation task (see Table F.3 for a full
list).
As stated in §4.1, the NLL (union) objective and RQare equivalent whenÍ
𝑖𝑞𝑖¹ℓºis uniform over ℓand the prior is uniform
over all classes in the negative label sets, evidenced by the comparable performance between the two in Figure 3. In this
case, the denominator in (3) is independent of ℓ, and thus
𝑟𝑖¹ℓº=(
1
𝐶 j𝑁𝑖j𝑞𝑖¹ℓºℓ∉𝑁𝑖
0 ℓ2𝑁𝑖
where𝐶is the number of classes and 𝑁𝑖is the negative label set for sample 𝑖. The RQloss then simpliﬁes as
KL¹𝑟𝑖k𝑞𝑖º=Eℓ𝑟𝑖
log𝑟𝑖¹ℓº
𝑞𝑖¹ℓº
=∑︁
ℓ∉𝑁𝑖1
𝐶 j𝑁𝑖j𝑞𝑖¹ℓºlog1
𝐶 j𝑁𝑖j
which is a constant multiple of the NLL (union) lossÍ
ℓ∉𝑁𝑖𝑞𝑖¹ℓº.
Lastly, it is worth noting that the similar term “implicit generative model" has been used in prior literature to refer to
amortized sampling procedures for nonparametric (or not speciﬁed) energy functions, such as generative adversarial models
(e.g., Mohamed and Lakshminarayanan [2017]). Although we do not make an explicit connection with such models, our
formulation also does not assume a parametrization of the data distribution, and one can understand the term “implicit
posterior” as referring to a function that is a posterior for an implicit (i.e., uninstantiated, unparametrized) generative model.
However, we assume tractability of sampling from a posterior over certain distinguished latents (classes) conditioned on
observed data (features, e.g., images), rather than directly sampling latents.
Table D.1: Comparison of modeling forms for variational auto-encoders (V AE), wake-sleep algorithms (WS), expectation-
maximization (EM), and our proposed implicit posterior (IP). Variational auto-encoders parametrize both a generative model
𝑝and a posterior model 𝑞. Here we distinguish between 𝜃𝑝and𝜃𝑞as these models can differ in both architecture and
parameters. The EM formulation parametrizes the generative model 𝑝¹𝑥𝑖jℓ;𝜃𝑝ºand the posterior is instantiated as auxiliary
matrix with entries 𝑎𝑖ℓcalculated to maximize the objective given the estimated 𝑝¹𝑥𝑖jℓ;𝜃𝑝ºon the observed instances 𝑖.
In implicit posterior modeling, the posterior 𝑞¹ℓj𝑥𝑖;𝜃𝑞ºis modeled and parametrized directly, with the generative link 𝑝
instantiated as an auxiliary matrix with entries of the form 𝑎𝑖ℓ. Combining this auxiliary matrix with the prior beliefs 𝑝𝑖¹ℓº
at each instance as in Eq. (3)yields a posterior model 𝑟𝑖implied by forward model 𝑞¹ℓ;𝑥𝑖𝜃𝑞ºand weak prior beliefs on
each instance 𝑝𝑖¹ℓº.
V AE/WS EM IP
generative𝑝 𝑝¹𝑥jℓ;𝜃𝑝º𝑝¹𝑥jℓ;𝜃𝑝º𝑎𝑖ℓ
posterior𝑞 𝑞¹ℓj𝑥;𝜃𝑞º𝑎𝑖ℓ𝑞¹ℓj𝑥;𝜃𝑞º
D RELATIONSHIPS WITH EM, V AE, AND WAKE-SLEEP ALGORITHM
As discussed in §2.1, the QRloss guarantees continual improvements in the free energy (1). On the other hand, option RQ
is equivalent to performing a gradient step on the cross-entropy of 𝑞𝑖and𝑟𝑖and a gradient step on the negative entropy of𝑟𝑖.
In the case that the priors 𝑝𝑖¹ℓºare hard (supported only on one ground truth label), the same is true of 𝑟𝑖, and the RQloss
is equivalent to cross-entropy. This option reverses the KL distance in a manner reminiscent of the training procedure in
the wake-sleep algorithm [Hinton et al., 1995], where parameter updates for the forward and reverse models are iterated,
but the KL distance optimized always places the probabilities under the model being optimized in the second position in
the KL distance (inside the logarithm), so that the generative and the inference models each optimize log-likelihoods of
their predictions. The wake-sleep algorithm, however, also trains a generative model rather than treating it as an auxiliary
distribution as we do, and that requires sampling. As opposed to V AEs, the wake-sleep algorithm samples the generative
model, not the posterior.
It is interesting to contrast our approach to the expectation-maximization (EM) formulation. In standard EM, the 𝑞
distributions are considered auxiliary, rather than parametrized as direct functions of the inputs 𝑥. The𝑞𝑖¹ℓº=𝑎𝑖ℓis simply
a matrix of numbers normalized across ℓ. Its dependence on the data 𝑥arises through the iterative re-estimation of the
minimum of the free energy, where the link 𝑥 ℓis modeled directly in the parametrized forward distribution 𝑝¹𝑥jℓº(see
Table D.1). We instead model forward probabilities 𝑝¹𝑥𝑖jℓºas auxiliary parameters, a matrix of numbers 𝑎𝑖ℓnormalized
across𝑖that we ﬁt to minimize the free energy at each data point, and optimize only the parameters of the 𝑞model which
explicitly models the link 𝑥 ℓ. This allows us to capture nonlinear (and ‘deep’) structure and beneﬁt from inductive biases
inherent to training deep models with SGD, but without the cost of training an actual parametrized generative model and
other problems associated with deep generative model ﬁtting. The resulting 𝑞network approximates the posterior in a
generative model – which (locally) maximizes the log likelihood of the data – and it is usually highly conﬁdent (as seen in
Fig. 1).
The implicit modeling of the posterior in EM does not lead to overﬁtting of the generative model. But, given that degenerate
solutions to optimization with implicit posterior models are possible when the prior is constant across all data points (§2.1),
we can imagine that our approach of implicit posterior modeling might lead to degenerate solutions. As demonstrated in
Fig. 1 and in our experiments, avoiding degenerate solutions is not too hard. We address this point further in §B.
E EXPERIMENT DETAILS
E.1 LAND COVER MAPPING
E.1.1 Datasets
Imagery Data Our land cover mapping experiments use imagery from the National Agriculture Imagery Program (NAIP),
which is 4-channel aerial imagery at a 1m/px resolution taken in the United States (US).
Chesapeake Conservancy land cover dataset The Chesapeake Conservancy land cover dataset consists of several raster
layers of both imagery and labels covering parts of 6 states in the Northeastern United States: Maryland, Delaware, Virginia,
West Virginia, Pennsylvania, and New York [Robinson et al., 2019]4. The raster layers include: high resolution (1m/px)
NAIP imagery, high resolution (1m/px) land cover labels created semi-autonomously by the Chesapeake Conservancy, low
resolution (30m/px) Landsat-8 mosaics imagery, low resolution (30m/px) land cover labels from the National Land Cover
Database (NLCD), and building footprint masks from the Microsoft Building Footprint dataset. The dataset is partitioned
into train, validation, and test splits per-state, where each split is a set of 7km6kmtiles containing the aligned raster
layers.
EPA EnviroAtlas data The EnviroAtlas land cover data consists of high resolution (1m/px) land cover maps over 30 cities
in the US, and is collected and hosted by the US Environmental Protection Agency (EPA) [Pickard et al., 2015]. A detailed
description of the dataset and its land cover deﬁnitions is provided by Pilant et al. [2020]. As with most high-resolution land
cover datasets (including the Chesapeake Conservancy land cover labels), the EnviroAtlas land cover labels are themselves
derived by remote sensing and learning procedures, and thus are not themselves a perfect “ground truth” representation of
land cover. For example, the estimated accuracy of the provided labels is 86.5% in Pittsburgh, PA, 83.0% in Durham, NC,
86.5% in Austin, TX, and 69.2% in Phoenix, AZ [Pilant et al., 2020].
The high-resolution label ﬁles were aligned to match the extent of the NAIP tiles from the closest available years to the years
that the EnviroAtlas labels were collected: for Pittsburgh, PA and Phoenix, AZ, we used data from 2010 and for Durham,
NC and Austin, TX, we used data from 2012. We chose these four cities to get a wide coverage across the United States
(US), and due to a mostly consistent set of classes being used between the four cities.
National Land Cover Database (NLCD) The National Land Cover Database is produced by the United States Ge-
ological Survey (USGS) and uses 16 land cover classes. Maps are generated every 2-3 years, with spatial resolution
of 30m/px. Data and more information can be found at: https://www.usgs.gov/centers/eros/science/
national-land-cover-database .
Microsoft Building Footprint dataset The Microsoft Building Footprint dataset consists of predicted building polygons
over the continental US from Bing Maps imagery. As of the time of writing, the most updated Microsoft Building Footprints
dataset in the US can be accessed at: https://github.com/Microsoft/USBuildingFootprints .
Open Street Map (OSM) data Open Street Map ( https://www.openstreetmap.org/ ) is an ongoing effort to
make publicly available and editable map of the world, generated largely from volunteer efforts. The data is available under
the Open Database License. From the many different sources of information provided by OSM [Haklay and Weber, 2008],
we download raster data for road networks, waterways, and water bodies, using the OSMnx python package [Boeing, 2017].
Data splits and data processing For experiments using the Chesapeake Conservancy dataset (Table 1), we used estab-
lished train, test, and validation splits. In particular, we used the 20 test tiles in New York (NY) and the 20 test tiles in
Pennsylvania (PA) on which to conduct our experiments. Here a tilematches the extent of a NAIP tile, roughly 7km 6km.
To facilitate comparison of our results with previous published results on this dataset, we condensed the labels into four
classes: (1) water, (2) impervious surfaces (roads, buildings, barren land), (3) grass/ﬁeld, and (4) tree canopy.
For experiments with the EnviroAtlas dataset (Table 2), we aligned the high resolution land cover data, NLCD, OSM, and
Microsoft Building Footprints data with NAIP imagery tiles, matching years as closely as possible to the EnviroAtlas data
collection year for NLCD and NAIP. We instantiated a split of 10 train, 8 validation, and 10 test tiles in Pittsburgh, and 10
test tiles in Durham, NC, Austin, TX, and Phoenix, AZ. For Pittsburgh we assigned tiles to splits randomly from the set of
28 tiles that had no missing labels. There were not enough such tiles in Durham to follow the same procedure, so we chose
the ten evaluation tiles at random from a set with no number of missing labels per tile. For Austin and Phoenix, we chose the
10 evaluation tiles at random from the tiles in each city that had no agriculture class (as it is not present in Pittsburgh or
Durham) and no missing labels. We set aside 5 separate tiles in each city for use in “learning the prior” (in Pittsburgh these 5
tiles are a subset of the 8 validation tiles). As above, each tile corresponds to one NAIP tile. The tiles in these constructed
sets for Pittsburgh, Durham, and Austin contain ﬁve unique labels: (1) water, (2) impervious surfaces (roads, buildings), (2)
barren land, (4) grass/ﬁeld, and (5) trees. Phoenix additionally has a “shrub” class; when forming the prior we merge this
class with trees, and we ignore the shrub class when evaluating in Phoenix. We cropped all data tiles to ensure no spatial
overlap in any tiles between or within the train/val/test splits.
4Dataset can be downloaded from: https://lila.science/datasets/chesapeakelandcover .
Unclassified
Water
Tree/Forest
Low vegetaion/field
ImperviousDE
Unclassified
Water
Tree/Forest
Low vegetaion/field
ImperviousPA
No Data
Open Water
Ice/Snow
Developed Open Space
Developed Low Intensity
Developed Medium Intensity
Developed High Intensity
Barren Land (Rock/Sand/Clay)
Deciduous Forest
Evergreen Forest
Mixed Forest
Shrub/Scrub
Grassland/Herbaceous
Pasture/Hay
Cultivated Crops
Woody Wetlands
Emergent Herbaceous WetlandsUnclassified
Water
Tree/Forest
Low vegetaion/field
ImperviousNY
NLCD ClassLabel classChesapeake Conservancy dataset
NLCD ClassEnviroatlas dataset
color scale:
Label class
Unclassified
Water
Impervious Surface
Soil and Barren
Trees and Forest
Grass and HerbaceousPittsburgh, PA
Unclassified
Water
Impervious Surface
Soil and Barren
Trees and Forest
Grass and HerbaceousDurham, NC
Unclassified
Water
Impervious Surface
Soil and Barren
Trees and Forest
Grass and HerbaceousAustin, TX
No Data
Open Water
Ice/Snow
Developed Open Space
Developed Low Intensity
Developed Medium Intensity
Developed High Intensity
Barren Land (Rock/Sand/Clay)
Deciduous Forest
Evergreen Forest
Mixed Forest
Shrub/Scrub
Grassland/Herbaceous
Pasture/Hay
Cultivated Crops
Woody Wetlands
Emergent Herbaceous WetlandsUnclassified
Water
Impervious Surface
Soil and Barren
Trees and Forest
Grass and HerbaceousPhoenix, AZFigure E.1: Cooccurrence matrices between NLCD classes and high resolution land cover labels for each region we study.
E.1.2 Forming the priors
To form the priors for the land cover classiﬁcation tasks, we ﬁrst spatially smooth the NLCD labels by applying a 2D
Gaussian ﬁlter (with a standard deviation of 31 pixels) across every channel in a one-hot representation of the NLCD classes.
The main reason for applying this smoothing is to reduce artifacts due to the 30m2boundaries of the NLCD data, to undo the
blocking procedure induced by the aggregation to 30m 30m extents, to incorporate the spatial correlations between nearby
NLCD blocks, and to remove erroneous sharp differentials between inputs that can cause artifacts during later training
stages.
We then remap the blurred NLCD layers to the classes of interest by multiplying by a matrix of cooccurrence counts between
the (unblurred) NLCD data and the high resolution labels in each region. For the Chesapeake region, we use the train
tiles provided with the Chesapeake Conservancy land cover dataset to deﬁne cooccurrence matrices in NY and PA. For
EnviroAtlas, we compute cooccurrences using the entire city (excluding tiles with agriculture in Phoenix AZ, and Austin,
TX). The cooccurrence matrices for each region we study are shown in Figure E.1.
The priors for the Chesapeake Conservancy dataset are then generated by normalizing the blurred and remapped NLCD data
so that summing over all ﬁve classes gives probability 1 for each pixel.
For the EnviroAtlas data, we augment this prior with publicly available data on buildings, road networks, water bodies,
and waterways. We obtain building maps from the Microsoft Buildings Footprint database and road, water bodies, and
waterways data from Open Street Map, using the OSMnx tool [Boeing, 2017] to download the data (see Appendix E.1.1).
We apply a small spatial blur to each of these input sources to account for (a) vector representation of roads and waterways
being unrealistically thin, and (b) possible data-image misalignment on the order of pixels. Where this results in probability
mass on impervious surfaces or water, we add these probability masses to the blurred NLCD prior, and then renormalize to
obtain a valid set of probabilities for each pixel.
In §4.4, we describe a method for “learning the prior,” which uses a more sophisticated process to aggregate the individually
weak and coarse inputs that we use in the handmade prior. In this method, we train a neural net to take as input the blurred,
remapped NLCD representation (5 classes) concatenated with the 4 classes of additional data: buildings, roads, waterways,
water bodies, and to predict high-resolution labels in each city. We train these networks using 5 tiles of imagery and
high-resolution labels from the EnviroAtlas Dataset in each city which are distinct from the 10 test tiles in each city. The
training procedure for these prior generation networks is described in in §E.1.3. To create the priors that we then train
our method on (‘learned prior’ rows in Table 2) we ran these learned models forward on (blurred and remapped NLCD,
buildings, roads, waterways, and waterbodies) input for each of the 10 evaluation tiles in each city.
E.1.3 Experimental procedure
We use priors generated as described in Appendix E.1.2, with Gaussian spatial smoothing with standard deviation of
31 pixels, and cooccurrence matrix determined via the training splits in each city/state. We apply a pixel-wise additive
smoothing constant of 1e-4 to the probability vectors output by the neural network as well as to the prior probability vectors
used as the model supervision data. This additive smoothing constant ensures that there are no extremely low probability
classes in either the prior or the predicted outputs during training.
Experiments summarized in Table 1 and Table 2 use a 5-layer fully connected network with kernel sizes of 3 at each layer,
128 ﬁlters per layer, and leaky ReLUs between layers. Note that the receptive ﬁeld of this model is only 1111pixels. We
use batch sizes of 128 instances during training, where each image instance is a cropped 128 128 pixels from a larger tile.
Training and model evaluation is done within the torchgeo framework for geo-spatial machine learning [Stewart et al., 2021].
All models use the AdamW optimizer [Loshchilov and Hutter, 2017] during training and torchgeo defaults unless otherwise
noted.
Comparison to previous label super-resolution for LC mapping To obtain the parameter setting used for the runs in
New York (NY) and Pennsylvania (PA) in Table 1, we ﬁrst perform a hyperparameter search with the 20 tiles test set
in Delaware (DE) from the same overall dataset. We use a learning rate schedule that decreases learning rate when the
validation loss plateaus, as well as early stopping to prevent over training of models. Of the grid of learning rates in {1e-3,
1e-4,1e-5}, we describe below, we pick learning rate as 1e-4 for both QRandRQvariants of our method, as this is the
setting that minimizes the IoU of the 𝑞output on the 20 DE tiles for both variants.
When training on NY and PA jointly (“Chesapeake" in Table 1), we use the per-state cooccurrence matrices. This ensure
that the cooccurrence matrices used are consistent between our method and the self-epitomic LSR benchmark across all
columns in Table 1.
Generalization across cities. For the high-resolution model with NAIP imagery from Pittsburgh as input, we consider
learning rates inf10 210 310 410 5gand pick based on the best validation performance on the validation set in
Pittsburgh. The chosen learning rate is 1e-3. We search over the same set of learning rates for the model with NAIP imagery
and the prior concatenated as input; the chosen learning rate is also 1e-3. For this model with concatenated image and prior
as input, only the number of input channels changes in the fully connected network model architecture. When training on
the high-resolution land cover labels, we use a very small additive constant (1e-8) for the last layer of the model.
When training our methods, we initialize model weights using the best NAIP image input model from the Pittsburgh
validation set runs, and then train using the priors and the training procedure described in the main text. We pick the learning
rate for this training step using again the validation set in Pittsburgh; we search learning rates in f10 310 410 5g, and pick
1e-5 as the learning rate for QRand 1e-3 as the learning rate for RQ, since these resulted in the best performance for the
Pittsburgh validation set with the randomly initialized model. We discuss the results of a similar procedure using randomly
initialized model weights in Appendix E.1.4.
For the learned prior, we use a 3 layer fully connected network is kernel sizes of 11,7, 5 respectively, 128 ﬁlters per layer
and leaky ReLUs between layers. For each city, we train this model on the prior inputs (blurred and remapped NLCD, roads,
buildings, waterways, and water bodies) using a validation set of 5 tiles separate from from the 10 evaluation tiles in each
city. We considered learning rates in f10 310 410 5gfor learning the prior in each city, and chose 1e-4 as it gave most
often resulted in the highest accuracies of each validation set. For learning onthis learned prior, we again initialize model
weights using the best NAIP image input model from the Pittsburgh validation set runs, and set the learning rate to 1e-5 for
QRevaluation runs and 1e-3 for RQevaluation runs to match the other variants of the experiment.
E.1.4 Additional Results
Extended results for generalizing across EnviroAtlas cities. The extended results for generalizing across cities with the
EnviroAtlas datasets in Table E.1 contain the results of the RQruns trained on the handmade prior in each city. Evaluation
Table E.1: Supplementary results to accompany Table 2.
Pittsburgh, PA Durham, NC Austin, TX Phoenix, AZ
Train region Model acc % IoU % acc % IoU % acc % IoU % acc % IoU %
Pittsburgh HR 89.3 69.3 74.2 35.9 71.9 36.8 6.7 13.4
(supervised) HR + aux 89.5 70.5 78.9 47.9 77.2 50.5 62.8 24.2
Same as test QR(𝑞) 80.5 56.8 78.3 44.4 79.2 50.5 75.2 29.5
(random QR(𝑟) 80.7 57.5 78.5 46.4 79.7 52.0 75.9 33.8
initialization) RQ(𝑞) 77.6 53.3 65.8 23.3 73.8 43.0 61.8 18.6
RQ(𝑟) 77.6 53.3 65.8 23.3 73.8 43.1 61.8 18.6
Same as test QR(𝑞) 80.6 58.5 78.9 47.7 76.6 49.1 75.8 45.4
(pretrained QR(𝑟) 80.6 58.7 79.0 48.4 76.6 49.5 76.2 46.0
in Pittsburgh) RQ(𝑞) 84.3 59.6 75.6 28.6 76.5 47.5 63.7 19.5
RQ(𝑟) 84.3 59.6 75.4 31.5 76.5 47.5 63.7 19.5
Same as test QR(𝑞) 82.4 63.7 79.0 48.7 79.4 51.3 73.4 42.8
(learned prior ) QR(𝑟) 82.4 64.0 79.2 49.5 79.1 51.9 73.6 43.1
Full US* Robinson et al. [2019] U-Net Lrg. 79.0 61.5 77.0 49.6 76.5 51.8 24.7 23.6
Table E.2: Comparison of the Full US* U-Net Large [Robinson et al., 2019] map predictions when evaluated on the full 5
classes considered in Table 2 (water, grass/ﬁeld, trees/shrub, impervious surfaces, and barren land) and evaluated on the four
prediction classes predicted by the model (where barren land and impervious surfaces are merged as a single class), and
when barren is post-facto assigned whenever the predicted class is “impervious surfaces" and the label class is “barren land".
Pittsburgh, PA Durham, NC Austin, TX Phoenix, AZ
Classication Scheme acc % IoU % acc % IoU % acc % IoU % acc % IoU %
5 Classes 78.8 55.1 76.6 43.4 76.2 49.1 18.2 18.8
4 Classes 79.0 68.7 77.0 54.1 76.5 60.4 24.7 16.8
Barren reassigned 79.0 61.5 77.0 49.6 76.5 51.8 24.7 23.6
results in Pittsburgh, PA give further context for comparison of generalization across cities by each method.
Table E.1 also details the result of initializing the model weights randomly for the QRmethod. Table E.1 shows that the
choice of model initialization can be important for our method – this is most apparent in Pittsburgh, PA (unsurprisingly
since the high-resolution model was trained in Pittsburgh) and Phoenix, AZ. In Phoenix, much of the handmade prior is
consistent across geographies and the randomly initialized model has trouble distinguishing between infrequent classes that
most often occur together in the handmade prior. The results in Table E.1 suggest that using pre-trained models as a starting
point for our method can help to break some of these symmetry issues in resolving the information in the prior. Results in
Table 2 suggest that using a more detailed prior map may help with this as well.
Evaluating the Full US map from Robinson et al. [2019]. Recall that the row for the full US Map [Robinson et al.,
2019] in Table 2 reﬂects the performance of the model evaluated on all 5 classes we consider in our experiments, where
we give the map predictions the “beneﬁt of the doubt" in that any prediction of “impervious surfaces" where the true label
is “barren land" gets assigned a correct classiﬁcation of “barren land." The results reported in Table 2 are thus a sort of
upper bound on the predictive performance of the method that generated the predictive maps. It was important for us to
keep the barren class while evaluating across cities, as it is the dominant class in Phoenix, AZ. In the remaining three
cities, the barren class is challenging to predict as it is infrequent. In Table E.2, we compare this classiﬁcation scheme with
two alternatives: a 5 class scheme that will penalizes the map predictions for never predicts the barren class, and a 4 class
scheme that merges the barren land and impervious surfaces classes in evaluation. Table E.2 shows that while the choice of
evaluation scheme does not greatly effect accuracy (outside of Phoenix, AZ, where the accuracy of the Full US Map is low
for both classiﬁcation schemes), the average IoU drops signiﬁcantly for all cities apart from Phoenix.
Figure E.2: Example predictions on the hand-coded and learned prior in each EnviroAtlas city we study.
Comparing loss functions: qualitative results with land cover mapping. Figure E.3 compares predictions under
different loss functions with an illustrative example. Here the prior is similar to the “hand-coded" prior described in
Appendix E.1.2, but with the prior deﬁned over all NLCD classes. We train each model (a slight variant on the network used
in experimental results) on the single NAIP tile region encompassing the zoom-in in the ﬁgure for 2000 iterations with the
Adam algorithm [Kingma and Ba, 2014], a batch size of 64, and a learning rate ﬁxed at 1e-4 during training. Qualitative
comparisons show that predictions made by the QRandRQloss functions are more certain (sharper colors in plots) than
training with cross entropy or squared-error loss on the soft priors, and, in in most places, arrive at better solutions than
training with a standard cross entropy loss on the argmax of the prior.
F ADDITIONAL EXPERIMENTS
F.1 SELF-SUPERVISION FOR UNSUPERVISED IMAGE CLUSTERING
Neural networks are usually trained on large amounts of hard-labeled data f𝑥𝑖ℓ𝑖g, yet, due to the biases induced by the
typical architectures and learning algorithms, much of the modeling power of these networks seem to focus on correlations
in the input space [Shwartz-Ziv and Tishby, 2017]. This means that a network trained for one application, i.e., for one label
spaceℓ2𝐿1, can be adopted to another application, i.e., a different labels space ℓ2𝐿2, as long as the input features are in a
similar domain. The canonical example of this is the use of lower levels of the networks pre-trained on ImageNet as part
of the networks solving a completely different set of image classiﬁcation problems. Pretrained networks require smaller
training sets in ﬁne tuning, as long as they have learned to represent the variation in the input space well. Self-supervised
models attempt to go a step further and learn these representations without anylabels. In our framework, self-supervision
can simply be seen as the appropriate choice of subset priors 𝑝¹ℓ𝑇ºover appropriately chosen tuples of labels.
To discuss the pitfalls and opportunities, consider again the QRloss (5)
𝐹= ∑︁
𝑖ℓ𝑞𝑖¹ℓºlog𝑝𝑖¹ℓº¸∑︁
𝑖ℓ𝑞𝑖¹ℓºlog ∑︁
𝑗𝑞𝑗¹ℓº!
 (F.1)
If we were to simply set 𝑝𝑖¹ℓºto a constant (e.g., uniform) distribution 𝑝¹ℓºfor all data points 𝑖, then the optimal solution
would be any function 𝑞𝑖¹ℓº=𝑞¹ℓj𝑥𝑖ºsuch that1
𝑁Í
𝑖𝑞¹ℓj𝑥𝑖º=𝑝¹ℓº. Thus simply using the uniform prior may not lead
to appropriate unsupervised clustering (or self-supervised learning of the network 𝑞). The inductive biases in the network
architecture and training may not help, because one solution is 𝑞¹ℓj𝑥º=𝑝¹ℓº, which can be achieved by zeroing out all
weights except for biases in a ﬁnal softmax layer that outputs probabilities for labels ℓ. As the softmax bias vector is the
closest to the top in back-propogation with gradient descent, it will quickly be learned to match log𝑝¹ℓº. This will not only
slow down the propagation of gradients into the network, but could eventually stop it completely, as this solution is a global
naip image
preds using cross-entropy loss on argmaxed prior
argmaxed prior
preds using cross-entropy loss on soft prior
 soft prior
preds using squared loss on soft prior
 soft prior
r preds using QR loss
 q preds using QR loss
 soft prior
r = preds using RQ loss
 q preds using RQ loss
 soft priorOpen Water
Developed Open Space
Developed Low Intensity
Developed Med. Intensity
Developed High Intensity
Barren Land
Deciduous Forest
Evergreen Forest
Mixed Forest
Shrub/Scrub
Grassland/Herbaceous
Pasture/Hay
Cultivated Crops
Woody Wetlands
Emergent Herbaceous 
WetlandsNLCD LegendFigure E.3: Comparison of different loss functions on hard and soft prior.
Figure E.4: Comparison of forward model likelihoods under the generative model trained with QRloss (above) and the
likelihood under an epitome model [Malkin et al., 2020] for part of a test tile from §4.3.
optimum. Another optimal solution would be a function satisfying1
𝑁Í
𝑖𝑞¹ℓj𝑥𝑖º=𝑝¹ℓº, but where individual entropies for
each data point are small:  Í
ℓ𝑞¹ℓj𝑥𝑖ºlog𝑞¹ℓj𝑥𝑖º𝜖, which motivates an alternative cost criterion:
𝐹= ∑︁
𝑖ℓ𝑞𝑖¹ℓºlog𝑞𝑖¹ℓº¸∑︁
𝑖ℓ𝑞𝑖¹ℓºlog ∑︁
𝑗𝑞𝑗¹ℓº!
 (F.2)
where the ﬁrst term promotes certainty in predictions 𝑞¹ℓj𝑥𝑖ºfor each point 𝑖and the second is promoting the diversity of
the predictions across the different inputs, i.e., a high entropy of the average1
𝑁Í
ℎ𝑞𝑖¹ℎº. This prevents learning a network
with a constant output 𝑞¹ℎº=𝑝¹ℎºand forces the model to ﬁnd some statistics in the input data that break it into clusters
indexed by labels ℓ. The result will be highly dependent on the inductive biases associated with the network architecture
and SGD method used, as we can imagine degenerate solutions here as well. For example, we can ignore completely some
subset of features and still train a network that is certain in its modeling of the remaining ones, and achieves a high diversity
of predicted classes across the dataset. This may be dangerous if the features omitted end up being the most important ones
for the downstream task. However, due to the stochastic gradient descent training as well as their architecture, it has been
difﬁcult to prevent neural networks from learning statistics involving all the input features. For example, training a neural
network using a weak generative model as a teacher corresponds to using a simpler mixture model, whose posterior is used
as a target𝑝𝑖¹ℓºand then learning a neural network that can approximate it. The inductive bias then leads to networks that
do not match 𝑝𝑖¹ℓºexactly but learn more complex statistics instead.
Equation (F.2) can be seen as a degenerate example of using a tuple prior where the tuple has the same data point repeated
and the prior simply expects the two predictions to be the same. In many applications, there are natural constraints involving
multiple data points that are easily modeled with priors over tuples or over the entire collection of labels. Consider
unsupervised image segmentation, for an example. It is usually expected that nearby pixels should belong to the same class
(or a small subset of classes), and that faraway pixels are more likely to belong to a different subset of classes. This belief is
typically modeled in terms of Markov random ﬁeld models of joint probabilities of labels in the image,
𝑝¹fℓ𝑖gº/ exp∑︁
𝑖𝜙¹ℓ𝑖fℓ𝑗g𝑗2𝑁𝑗º (F.3)
We experimented with potentials of the form
𝜙¹ℓ𝑖=ℓfℓ𝑗g𝑗2𝑁𝑗º=𝛾ℓ¸𝛼ℓ1
j𝑆𝑖j∑︁
𝑗2𝑆𝑖1»ℓ=ℓ𝑗¼¸𝛽ℓ1
j𝐿𝑖j∑︁
𝑗2𝐿𝑖1»ℓ=ℓ𝑗¼ (F.4)
where for pixel 𝑖,𝑆𝑖is a small ( 55) neighborhood around it and 𝐿𝑖is a larger ( 5050) neighborhood. If we set 𝛼ℓ=1,
𝛽ℓ= 1for allℓ, then we consider this a contrastive prior, as it favors labels ℓ𝑖to match the labels found more concentrated
in its immediate neighborhood than in the larger scope. On the other hand 𝛼ℓ, and𝛽ℓcan be estimated based on the current
statistics in the label distribution using logistic regression. We refer to this as a self-similarity prior 𝑝¹fℓ𝑖g;𝛼ℓ𝛽ℓ𝛾ℓºwith
parameters which are periodically ﬁt to the current statistics in the predictionsÍ
𝑗2𝑆𝑖𝑞¹ℓj𝑥𝑗º, andÍ
𝑗2𝐿𝑖𝑞¹ℓj𝑥𝑗ºto promote
similar label patterns across the image. The criterion (F.2) can also be seen as a degenerate version of this setting with 𝑆
being 11and𝐿being inﬁnite (or the whole image).
The contrastive version of this prior relies on the insight previously pursued in image self-supervision, e.g., Jean et al. [2019].
In our formulation, contrasting is accomplished without sampling triplets, but considering all the data jointly, by expressing
the goal of contrasting with far away regions within the prior in our framework.
As an example of self-supervised pretraining in our framework, in Fig. F.1 we show an example of clustering a large tile of
aerial imagery into 12 classes using 5 layer FCN as network 𝑞of the architecture used in §4.4. The clustering is achieved
by updating the prior every 50 steps of gradient descent on batches of 200 256256px patches. The prior is initialized
to a contrasting prior, and then updated through gradient descent. After 7 iterations, the result is sharpened by continuing
training using (F.2).
Figure F.1: Unsupervised clustering using implicit QRloss (middle) of a NAIP tile (left). On the right, we show the
assignment of the 12 clusters to 4 land cover labels: water (blue), tall vegetation (darker green), low vegetation (lighter
green) and impervious/barren (gray).
This tile was recently used in testing the ﬁne-tuning of a pretrained model with minimal amount of new labels in a new
region [Robinson et al., 2020]. Both the pre-training region, the state of Maryland, and the testing region, the tiles in New
York State, come from the 4-class Chesapeake Land Cover dataset (§4.3). Yet, the slight shift in geography results in
reduction of accuracy from around 90% in Maryland down to around 725%in New York. In Robinson et al. [2020], various
techniques for quick model adaptation are studied, on labels acquirable in up to 15 minutes of human labeling effort per
tile. In Table F.1 we compare the tunability of our self-supervised models on the four 85km2regions tested in Robinson
et al. [2020] with active learning approaches to tuning a pre-trained Maryland model with 400 labeled points. We show in
the table the accuracy and mean intersection over union from Robinson et al. [2020] for tuning the pretrained model’s last
644layer with different active learning strategies for selecting points to be labeled. For example, random selection of 400
points for which the labels are provided yields an average accuracy improvement from 725%to806%.
On the other hand, recall that we have created an unsupervised segmentation into 12 clusters, with posteriors over the clusters
𝑞𝑖¹ℓº. To investigate how well these clusters align with ground truth land cover labels, we compute a simple assignment of
clusters to land cover labels. Given a set of labeled points f¹𝑖𝑐𝑖ºg𝑖2𝐼, we infer a mapping from clusters to four target labels,
𝑝¹𝑐jℓº/∑︁
𝑖2𝐼:𝑐𝑖=𝑐𝑞𝑖¹ℓº
The label of any point 𝑗can now be inferred as ˆℓ𝑗=arg max𝑐Í
ℓ𝑞𝑖¹ℓº𝑝¹𝑐jℓº. This procedure, using 400 randomly selected
labeled points, yields an average accuracy of 811%(averaged over 50 random collections of labeled points), which is above
the performance of the pretrained model tuned on as many randomly selected points, and on par with the more sophisticated
methods for point selection and the use of the pretrained model (Table F.1). (Note that the large model pretrained was trained
on a large similar dataset in a nearby state).
Table F.1: Finetuning a pre-trained model by gradient descent [Robinson et al., 2020] versus implicit QRclustering + label
assignment in low-label regimes.
pretrained model in Robinson et al. [2020] Implicit QR
Query method No tuning Random Entropy Min-margin Random
Tuned parameters 0 64 4 644 644 124
Accuracy % 72.5 80.6 73.6 81.1 81.1
IoU % 51.0 60.8 50.1 60.8 59.8
Table F.2: Area under ROC curve for various predictors on the TIL segmentation task.
fully supervised weakly supervised
Model SVM𝑎𝑏CNN𝑏CSP-CNN Hou et al. [2019] U-Net𝑐Epitome𝑑RQ
AUC 0.713 0.494 0.786 0.783 0.801 0.802
𝑎Zhou et al. [2017]𝑏Hou et al. [2019]𝑐Malkin et al. [2019]𝑑Malkin et al. [2020]
F.2 TUMOR-INFILTRATING LYMPHOCYTE SEGMENTATION
The setup of this experiment mimics that of the land cover label super-resolution experiment in §4.3. The training data
consists of 50,000 240240px crops of H&E-stained histological imagery at 0.5 𝜇m/px resolution, paired with coarse
estimates of the density of tumor-inﬁltrating lymphocytes (TILs) created by a simple classiﬁer, at the resolution of 100100
blocks. The goal is to produce models for high-resolution TIL segmentation. Models are evaluated on a held-out set of 1786
images with high-resolution point labels for the center pixel.
The coarse density estimates 𝑐belong to one of 10 classes, from 0 (no TILs) to 9 (highest estimated TIL density). We use
an estimated conditional likelihood 𝑝¹ℓj𝑐ºof the likelihood of the positive TIL label at pixels with each low-resolution
class𝑐to construct a prior 𝑝𝑖¹ℓºover the TIL label probability. Notice that this prior is the same for all pixels in any given
low-resolution, coarsely labeled block.5
We train a small CNN with receptive ﬁeld 1111(ﬁve ReLU-activated convolutional layers with 64 ﬁlters) under the RQ
loss against this prior for 200 epochs with learning rate 10 5, then evaluate on the held-out testing set. Inspired by Malkin
et al. [2020], we apply a spatial blur of 11 pixels to the predicted log-likelihoods (again correcting for the model’s small
receptive ﬁeld and the dataset bias).
The AUC scores of this model and of the baselines are shown in Table F.2. Interestingly, the best-performing models – RQ
and epitomic super-resolution (a generative model) – both have receptive ﬁelds of 1111, much smaller than those of the
U-Net and fully supervised CNNs. This means that prediction of TIL likelihood is possible using only local image data, but
the challenge is learning to resolve highly uncertain label information. Unlike U-Nets and deep CNN autoencoders, small
models are not able to learn and overﬁt to distant spurious clues to the classes of nearby pixels.
F.3 VIDEO SEGMENTATION WITH A STRUCTURED PRIOR
To demonstrate the use of priors with latent structure, we set up the problem of video segmentation as follows. Given a
frame𝑡, we tune networks 𝑞𝑡¹ℓ𝑖𝑡j𝑥𝑖𝑡ºpredicting one of 𝐿pixel classes for a pixel at coordinate 𝑖in frame𝑡. The prior
in each frame comes from a Mask R-CNN model [He et al., 2017] pre-trained on still images in the COCO dataset [Lin
et al., 2014]. The Mask R-CNN model ﬁnds several possible instances of objects of different categories and outputs the
soft object masks in form of conﬁdence scores for each pixel. We convert this into a probability distribution over the
index𝑓(foreground/background) of the form 𝑝¹𝑓𝑖𝑡j𝑚𝑡º, where𝑚𝑡are different detected instances by the model, and
the distributions 𝑝¹𝑓𝑖𝑡j𝑚𝑡ºare the soft masks for these instances converted to probability distributions, i.e. value of the
probability of foreground differs for each pixel and each instance based on the Mask R-CNN conﬁdence scores. Although the
5We experimented with setting 𝑝𝑖¹ℓj𝑐ºto conditional likelihoods estimated from a held-out set and with simply setting 𝑝𝑖¹ℓ=1j𝑐=
0º=005,𝑝𝑖¹ℓ=1j𝑐=1º=015, . . . ,𝑝𝑖¹ℓ=1j𝑐=9º=095. The latter gave better results, perhaps due to the bias of the evaluation set,
in which every image is known to be centered on a cell of some kind.
COCO dataset may not have had instances of object of interest in our frame 𝑥𝑡, we assume that some admixture (i.e., mixture
with sample-dependent weights) of detected instances (likely involving unrelated types of objects) does model reasonably
well the foreground segmentation in the frame. Mathematically, 𝑝¹𝑓𝑖𝑡º=Í
𝑚𝑡𝑝¹𝑓𝑖𝑡j𝑚𝑡º𝑝¹𝑚𝑡º, where𝑝¹𝑚𝑡ºexpresses the
probabilistic selection of the foreground masks for different instances from which the foreground is constructed. (One can
think of instances 𝑚𝑡as akin to topics in topic models, which are also admixture models). To complete the prior, we ﬁx
the distribution 𝑝¹ℓj𝑓ºas ﬁxed binary 𝐿2matrix assigning a subset of 𝐿pixel classes to foreground and the rest to the
background. (For example, we assign ﬁrst 3 classes to foreground and the remaining 5 to the background for a total of L=8
pixel classes). Therefore,
𝑝¹ℓ𝑖𝑡=ℓº=∑︁
𝑓𝑝¹ℓj𝑓º∑︁
𝑚𝑡𝑝¹𝑓𝑖𝑡=𝑓j𝑚𝑡º𝑝¹𝑚𝑡º (F.5)
We can now select the instances 𝑚𝑡in each frame by optimizing the free energy with this prior over 𝑝¹𝑚𝑡º. The procedure
involves standard variational inference of the posterior distribution over possible instances 𝑚𝑡for each pixel 𝑖in frame𝑡
which involves the posterior 𝑞𝑡¹ℓ𝑖𝑡j𝑥𝑖𝑡º. In practice we found that it is enough to do this inference once, using the network
𝑞𝑡 1estimated in the previous frame.
This requires the inference of 𝑚𝑡for each pixel 𝑖:
𝑠𝑖¹𝑚𝑡º/exp©­
«∑︁
𝑖∑︁
ℓ𝑓𝑝¹ℓj𝑓º𝑞𝑡¹ℓ𝑖𝑡=ℓj𝑥𝑖𝑡ºlog𝑝¹𝑓𝑖𝑡=𝑓j𝑚𝑡º𝑝¹𝑚𝑡ºª®
¬ (F.6)
and then optimizing 𝑝𝑚𝑡as the count of times each instance is used,
𝑝¹𝑚𝑡º/∑︁
𝑖𝑠𝑖¹𝑚𝑡º (F.7)
Selection of instances 𝑚𝑡in frame𝑡therefore involves comparing the predictions from the network 𝑞𝑡¹ℓ𝑖𝑡=ℓj𝑥𝑖𝑡ºgrouped
into foreground/background segmentation with the foreground/background segmentation for different instances from Mask
R-CNN, and making a selection of a subset (probabilistically in 𝑝¹𝑚𝑡º) based on which instances most overlap with the
predictions from network 𝑞𝑡. While the above two equations should in principle be iterated, and iterated with updates to
network𝑞𝑡¹ℓ𝑖𝑡=ℓj𝑥𝑖𝑡º, we found that in practice it is sufﬁcient to just select the instances 𝑚𝑡based on their intersection
with the network predictions once, at the very beginning, to make a soft ﬁxed prior, and leave it to optimizing the prediction
network with the RQloss to ﬁnd conﬁdent segmentation (Fig. F.2).
We tested the approach on the DA VIS 2016 dataset [Perazzi et al., 2016]. The dataset is comprised of 50 unique scenes,
accompanied by per-pixel foreground/background segmentation masks. The objective is to produce foreground segmentation
masks for all frames in a scene, given only the ground truth annotations of the ﬁrst frame (Semi-Supervised). We evaluated
our method on the 20-scene validation set at 480p resolution.
The network 𝑞used in this experiment combines both the pixel intensities and spatial position information for
its predictions. At each pixel location 𝑖𝑗, we augment the intensity information with learned Fourier features
»sin¹𝑊»𝑖𝑗¼𝑇ºcos¹𝑊»𝑖𝑗¼𝑇º¼𝑇[Tancik et al., 2020]. The image and spatial position are ﬁrst processed separately;
A 4-layer, 64-channel, fully-convolutional network with 33kernels, ReLU activations and Batch Normalization produces
the image features. A 3-layer, 16-channel, pixel-wise MLP with ReLU activations and Batch Normalization processes
the learned Fourier features. These two are concatenated and passed through a single 33convolution-ReLU-Batch
Normalization layer before being mapped to output predictions. We also experimented with adding optical ﬂow as another
auxiliary input to the network.
For each scene, the network 𝑞0is trained on the ﬁrst frame, using the given ground truth annotations split uniformly between
3 foreground and 5 background classes as prior, for 300 iterations. This network is then used to predict the foreground
pixels in the next frame and after computing the intersection over union between the predicted foreground pixels and the
Mask R-CNN output masks, we select masks that overlap more than a pre-speciﬁed threshold. The chosen masks are then
summated, weighted by their Mask R-CNN conﬁdence scores (0-1), to form the prior for the next frame. The process
of selecting masks from the Mask-RCNN predictions and forming the prior for a frame is showcased in Figure F.3. The
network𝑞0is then ﬁne-tuned for 10 iterations to obtain 𝑞1and this process repeats for all subsequent frames. We used the
Adam optimizer, with a starting learning rate of 10 3for the ﬁrst frame, reduced to 10 5for ﬁne-tuning, and trained with
batches of 128 6464 patches.
To infer the foreground pixels we start with a Mask R-CNN pre-trained on the COCO dataset. Then, for each scene we only
require1min of training time on the ground truth-annotated ﬁrst frame and 3s per every following frame for the entire
Frame
Prior
Prediction
LabelsFigure F.2: Example of inferring the foreground mask for a single frame.
process of forming the prior and inferring the foreground pixels. We do not train on any video data, in contrast to most video
object segmentation methodologies that rely on both a pre-trained network on static image datasets (such as COCO) and
additionally on ofﬂine training on video sequences. In Table F.3 we compare our results on the DA VIS 2016 validation set to
other video object segmentation algorithms from 2017 - present.
Table F.3: Jaccard and F1 measures for various algorithms on the video instance segmentation task.
J F
Model J&F "Mean"Recall"Decay#Mean"Recall"Decay#Year
OSVOS Caelles et al. [2017] 80.2 79.8 93.6 14.9 80.6 92.6 15 2017
MSK Perazzi et al. [2017] 77.55 79.7 93.1 8.9 75.4 87.1 9 2017
OnA VOS V oigtlaender and Leibe [2017] 85.5 86.1 96.1 5.2 84.9 89.7 5.8 2017
Lucid Khoreva et al. [2017] 82.95 83.9 95 9.1 82 88.1 9.7 2017
OSVOS-S Maninis et al. [2018] 86.55 85.6 96.8 5.5 87.5 95.9 8.2 2018
FA VOS Cheng et al. [2018] 80.95 82.4 96.5 4.5 79.5 89.4 5.5 2018
PReMVOS Luiten et al. [2018] 86.75 84.9 96.1 8.8 88.6 94.7 9.8 2018
OSMN Yang et al. [2018] 73.45 74 87.6 9 72.9 84 10.6 2018
AGAME Johnander et al. [2019] 81.85 81.5 93.6 9.4 82.2 90.3 9.8 2019
STM Oh et al. [2019] 89.4 88.7 97.4 5 90.1 95.2 4.2 2019
FEELVOS V oigtlaender et al. [2019] 81.65 81.1 90.5 13.7 82.2 86.6 14.1 2019
CFBI Yang et al. [2020] 89.4 88.3 - - 90.5 - - 2020
e-OSVOS Meinhardt and Leal-Taixe [2020] 86.8 86.6 - - 87 - - 2020
STCN Cheng et al. [2021] 91.7 90.4 98.1 4.1 93 97.1 4.3 2021
Ours 83.8 84 96.2 8.4 83.6 94.2 10.2
Ours (+ﬂow) 83.9 83.2 95.5 9.5 84.6 93.3 9.1
Frame
Foreground Estimation
"person"  | Score 1.00
"person" | Score 0.99
"wine glass" | Score 0.58
"person"  | Score 0.58
"wine glass" | Score 0.14
"wine glass" | Score 0.08
Combined Masks / Prior
Prediction
LabelsFigure F.3: Video frame segmentation procedure. Starting with a network 𝑞𝑡 1trained on frame 𝑡 1, we apply𝑞𝑡 1on
frame𝑡to get a rough foreground estimation (top). By running the pre-trained Mask R-CNN model on frame 𝑡and selecting
only the masks that overlap with the 𝑞𝑡 1prediction we get the candidate object masks (middle). The prior is constructed as
the sum of the candidate masks, weighted by their corresponding Mask R-CNN scores (bottom), and 𝑞𝑡 1is ﬁnetuned on
frame𝑡with this prior to produce the predictions (bottom).
F.4 IN-COLLECTION INFERENCE FOR MULTI-DOMAIN LEARNING: RETURN TO LE SÉDUCTEUR
One of the conclusions from our experiments on the EnviroAtlas landcover mapping task (§4.4) is that training a network
with the goal of generalizing to new input data is often inferior to simply performing in-collection inference for each
domain . In other words, given the collection of pairs 𝑥𝑖𝑝𝑖¹ℓº, learning the posterior 𝑞under the implicit posterior model is
optimized for resolving ambiguities in that collection, and possibly that collection alone. As pointed out in Malkin et al.
[2020], which performs collection inference using large generative models to mine self-similarity among the examples in the
collection, this is appropriate when we can expect our data 𝑥𝑖to always come paired with prior beliefs 𝑝¹ℓ𝑖º. It is interesting
to reconsider the Seducer example from Fig. 1. The artist created several versions of that painting in differing styles. Fig. F.4
shows that collection inference applied separately to each of these paintings works equally well. However, using a learned 𝑞
network from one image onto others yields inferior segmentations (Fig. F.5), as the learned network specialized for inference
in the data it saw. (A fully generative model would be expected to similarly overtrain on the input data features 𝑥𝑖, as would
a supervised neural network trained on hard-labeled pairs ¹𝑥𝑖ℓ𝑖ºdue to the domain shift.) Yet, if we know we will always
be given collections with beliefs in the form of priors 𝑝𝑖¹ℓº, local (collection) inference may be all we need.
Figure F.4: Two additional versions of Le séducteur (left), hand-made priors (middle) and inferred segmentations (right).
(a) (b) (c)
Figure F.5: Result of applying a network 𝑞trained to infer (b), on all three Le séducteur versions.
ARTICLE
A generalizable and accessible approach to
machine learning with global satellite imagery
Esther Rolf1,2,9, Jonathan Proctor3,9, Tamma Carleton4,5,9, Ian Bolliger2,6,9, Vaishaal Shankar1,9,
Miyabi Ishihara2,7, Benjamin Recht1& Solomon Hsiang2,5,8✉
Combining satellite imagery with machine learning (SIML) has the potential to address global
challenges by remotely estimating socioeconomic and environmental conditions in data-poorregions, yet the resource requirements of SIML limit its accessibility and use. We show that asingle encoding of satellite imagery can generalize across diverse prediction tasks (e.g., forestcover, house price, road length). Our method achieves accuracy competitive with deep neuralnetworks at orders of magnitude lower computational cost, scales globally, delivers labelsuper-resolution predictions, and facilitates characterizations of uncertainty. Since imageencodings are shared across tasks, they can be centrally computed and distributed tounlimited researchers, who need only ﬁt a linear regression to their own ground truth data in
order to achieve state-of-the-art SIML performance.https://doi.org/10.1038/s41467-021-24638-z OPEN
1Electrical Engineering & Computer Science Department, UC Berkeley, USA.2Global Policy Laboratory, Goldman School of Public Policy, UC Berkeley, USA.
3Center for the Environment and Data Science Initiative, Harvard University, Cambridge, MA, USA.4Bren School of Environmental Science & Management,
UC Santa Barbara, Santa Barbara, CA, USA.5National Bureau of Economic Research, Cambridge, MA, USA.6Rhodium Group, New York, USA.7Statistics
Department, UC Berkeley, USA.8Centre for Economic Policy Research, London, UK.9These authors contributed equally: Esther Rolf, Jonathan Proctor,
Tamma Carleton, Ian Bolliger, Vaishaal Shankar.✉email: shsiang@berkeley.edu
NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications 11234567890():,;
Addressing complex global challenges —such as managing
global climate changes, population movements, ecosystem
transformations, or economic development —requires that
many different researchers and decision-makers (hereafter, users)have access to reliable, large-scale observations of many variables
simultaneously. Planet-scale ground-based monitoring systems
are generally prohibitively costly for this purpose, but satellite
imagery presents a viable alternative for gathering globally
comprehensive data, with over 700 earth observation satellites
currently in orbit
1. Further, application of machine learning is
proving to be an effective approach for transforming these vast
quantities of unstructured imagery data into structured estimates
of ground conditions. For example, combining satellite imagery
and machine learning (SIML) has enabled better characterization
of forest cover2, land use3, poverty rates4and population
densities5, thereby supporting research and decision-making. We
refer to such prediction of an individual variable as a task.
Demand for SIML-based estimates is growing, as indicated by the
large number of private service-providers specializing in pre-
dicting one or a small number of these tasks.
The resource requirements for deploying SIML technologies,
however, limit their accessibility and usage. Satellite-based mea-
surements are particularly under-utilized in low-income contexts,
where the technical capacity to implement SIML may be low, but
where such measurements would likely convey the greatest
beneﬁt6,7. For example, government agencies in low-income
settings might want to understand local waterway pollution,
illegal land uses, or mass migrations. SIML, however, remains
largely out of reach to these and other potential users because
current approaches require a major resource-intensive enterprise,
involving a combination of task-speci ﬁc domain knowledge,
remote sensing and engineering expertise, access to imagery,
customization and tuning of sophisticated machine learning
architectures, and large computational resources8.
To remove these barriers, a new approach to SIML is needed
that will enable non-experts to obtain state-of-the-art perfor-
mance without using specialized computational resources ordeveloping a complex prediction procedure. A one-time, task-
agnostic encoding that transforms each satellite image into a
vector of variables (hereafter, features) could enable such an
approach by separating users from the costly manipulation of
imagery. Such an unsupervised encoding might be particularly
well suited for SIML problems, especially when constrasted with
deep-learning approaches to SIML that use techniques originally
developed for natural images (e.g., photos taken from handheld
cameras). Inconsistency of many key factors in natural imagery,
such as subject or camera perspective, require complex solutions
that may be unnecessary for learning from satellite imagery.
While prior work has sought an unsupervised encoding of
satellite imagery
9–12, to date no single set of features has been
shown to both achieve performance competitive with deep-
learning methods across a variety of tasks and to scale globally.
Here we show that a single set of general purpose features can
encode rich information in satellite images, performing well at
predicting ground conditions across diverse tasks using only a
linear regression implemented on a personal computer. We focus
on the problem of predicting properties of small regions (e.g.,
average house price) at a single time period, using high-resolution
daytime satellite imagery as the only input. We use this imagery to
test whether a single embedding can generalize across tasks
because it is globally available from the Google Static Maps API at
ﬁne resolution, is geo-recti ﬁed and pre-processed to remove cloud
occlusions, and has been found to perform well in SIML appli-
cations (Supplementary Note 1.2)4,13, though in principle other
data sources could also be used14. We develop a simple yet high-
performing system that is tailored to address the challenges andopportunities speci ﬁc to SIML applications, taking a fundamen-
tally different approach from leading designs. We achieve large
computational gains in model training and testing, relative to
leading deep neural networks, through algorithmic simpli ﬁcations
that take advantage of the fact that satellite images are collected
from a ﬁxed distance and viewing angle and capture repeating
patterns and objects. In addition, traditionally, hundreds or
thousands of researchers use the same images to solve different
and unrelated tasks (e.g., Fig. 1a). Our approach allows common
sources of imagery to be converted into centralized sets of features
that can be accessed by many researchers, each solving different
tasks. This isolates future users from the costly steps of obtaining,
storing, manipulating, and processing imagery themselves. The
magnitude of the resulting bene ﬁts grow with the size of the
expanding SIML user community and the scale of global imagery
data, which currently increases by more than 80TB/day15.
Results
Achieving accessibility and generalizability with Multi-task
Observation using Satellite Imagery & Kitchen Sinks
(MOSAIKS) . Our objective is to enable any user with basic
resources to predict ground conditions using only satellite ima-
gery and a limited sample of task-speci ﬁc ground truth data
which they possess. Our SIML system, “Multi-task Observation
using Satellite Imagery and Kitchen Sinks ”(MOSAIKS, see
Methods), makes SIML accessible and generalizable by separating
the prediction procedure into two independent steps: a ﬁxed
“featurization step ”which translates satellite imagery into suc-
cinct vector representations ( images→x), and a “regression step ”
which learns task-speci ﬁc coef ﬁcients that map these features to
outcomes for a given task ( x→y). For each image, the unsu-
pervised featurizaton step can be centrally executed once, pro-
ducing one set of outputs that are used to solve many different
tasks through repeated application of the regression step by
multiple independent users (Fig. 1b). Because the regression step
is computationally ef ﬁcient, MOSAIKS scales nearly costlessly
across unlimited users and tasks.
Theaccessibility of our approach stems from the simplicity and
computational ef ﬁciency of the regression step for potential users,
given features which are already computed once and stored
centrally. To generate SIML predictions, a user of MOSAIKS (i)
queries these tabular data for a vector of Kfeatures for each of
their Nlocations of interest; (ii) merges these features xwith label
data y, i.e., the user ’s independently collected ground truth data;
(iii) implements a linear regression of yonxto obtain coef ﬁcients
β–below, we use ridge regression; (iv) uses coef ﬁcients βand and
features xto predict labels ^yin new locations where imagery and
features are available but ground truth data are not (Fig.1b).
The generalizability of our approach means that a single
mathematical summary of satellite imagery ( x) performs well
across many prediction tasks ( y1,y2,...) w i t h o u t a n y t a s k - s p e c i ﬁc
modi ﬁcation to the procedure. The success of this generalizability
relies on how images are encoded as features. We design a
featurization function by building on the theoretically grounded
machine learning concept of random kitchen sinks16,w h i c hw e
apply to satellite imagery by constructing random convolutional
features (RCFs) (Fig. 1c, Methods). RCFs are suitable for the
structure of satellite imagery and have established performance
encoding genetic sequences17, classifying photographs18,a n d
predicting solar ﬂares19(see Supplementary Note 2.3). RCFs
capture a ﬂexible measure of similarity between every sub-image
across every pair of images without using contextual or task-speci ﬁc
information. The regression step in MOSAIKS then treats these
features xas an overcomplete basis for predicting any y,w h i c hm a y
be a nonlinear function of image elements (see Methods).ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z
2 NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications
In contrast to many recent alternative approaches to SIML,
MOSAIKS does not require training or using the output of a deep
neural network and encoding images into unsupervised features
requires no labels. Nonetheless, MOSAIKS achieves competitive
performance at a large computational advantage that grows linearly
with the number of SIML users and tasks, due to shared computation
and storage. In principle, any un supervised featurization would
enable these computational gains . However, to date, a single set of
unsupervised features has neither achieved accuracy competitive with
supervised CNN-based approaches across many SIML tasks, nor at
the scale that we study. Below, we show that MOSAIKS achieves a
practical level of generalization in real-world contexts.
We design a battery of experiments to test whether and under
what settings MOSAIKS can provide access to high-performing,
computationally ef ﬁcient, global-scale SIML predictions. Speci ﬁ-
cally, we (1) demonstrate generalization across tasks, and
compare MOSAIKS ’s performance and cost to existing state-of-
the-art SIML models; (2) assess its performance when data are
limited and when predicting far from observed labels; (3) scale
the analysis to make global predictions and try recreating the
results of a national survey; and (4) detail additional properties of
MOSAIKS, such as the ability to make predictions at ﬁner
resolution than the provided labels.
Multi-task performance of MOSAIKS in the US .W e ﬁrst test
whether MOSAIKS achieves a practical level of generalization byapplying it to a diverse set of pre-selected tasks in the United
States (US). While many applications of interest for SIML are in
remote and/or data-limited environments where ground truth
may be unavailable or inaccurate, systematic evaluation and
validation of SIML methods are most reliable in well-observed
and data-rich environments20.
We sample daytime images using the Google Static Maps API
from across the continental US ( N=100,000), each covering
~1 km × 1 km (256-by-256 pixels) (Supplementary Notes 2.1 –
2.2). We ﬁrst implement the featurization step, passing these
images through MOSAIKS ’feature extraction algorithm to
produce K=8,192 features per image (Supplementary Note 2.3).
Using only the resulting matrix of features ( X), we then
repeatedly implement the regression step by solving a cross-
validated ridge regression for each task and predict forest cover
(R2=0.91), elevation ( R2=0.68), population density ( R2=0.72),
nighttime lights ( R2=0.85), average income ( R2=0.45), total
road length ( R2=0.53), and average house price ( R2=0.52) in a
holdout test sample (Fig. 2, Supplementary Table 2, Supplemen-
tary Notes 2.4 –2.6). Computing the feature matrix Xfrom
imagery took less than 2 hours on a cloud computing node
(Amazon EC2 p3.2xlarge instance, Tesla V100 GPU). Subse-
quently, solving a cross-validated ridge regression for each task
took 6.8 min to compute on a local workstation with ten cores
(Intel Xeon CPU E5-2630) (Supplementary Note 3.2). These
seven outcomes are not strongly correlated with one another
(Supplementary Fig. 2) and no attempted tasks in this experiment
Fig. 1 A generalizable approach to combining satellite imagery with machine learning (SIML) without users handling images. MOSAIKS is designed to
solve an unlimited number of tasks at planet-scale quickly. After a one-time unsupervised image featurization using random convolutional features ,
MOSAIKS centrally stores and distributes task-agnostic features to users, each of whom generates predictions in a new context. aSatellite imagery is
shared across multiple potential tasks. For example, nine images from the US sample are ordered based on population density and forest cover, both of
which have distinct identifying features that are observable in each image. bSchematic of the MOSAIKS process. Nimages are transformed using random
convolutional features into a compressed and highly descriptive K-dimensional feature vector before labels are known. Once features are computed, they
can be stored in tabular form (matrix X) and used for unlimited tasks without recomputation. Users interested in a new task ( s) merge their own labels ( ys)
to features for training. Here, user 1 has forest cover labels for locations p+1t oNand user 2 has population density labels for locations 1 to q. Each user
then solves a single linear regression for βs. Linear prediction using βsand the full sample of MOSAIKS features Xthen generates SIML estimates for label
values at all locations. Generalizability allows different users to solve different tasks using an identical procedure and the same table of feature s—differing
only in the user-supplied label data for training. Each task can be solved by a user on a desktop computer in minutes without users ever manipulating the
imagery. cIllustration of the one-time unsupervised computation of random convolutional features (Methods and Supplementary Note 2.3). Kpatches are
randomly sampled from across the Nimages. Each patch is convolved over each image, generating a nonlinear activation map for each patch. Activation
maps are averaged over pixels to generate a single K-dimensional feature vector for each image.NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z ARTICLE
NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications 3
Fig. 2 1 km × 1 km resolution prediction of many tasks across the continental US using daytime images processed once, before tasks were chosen.
100,000 daytime images were each converted to 8,192 features and stored. Seven tasks were then selected based on coverage and diversity. Predictions
were generated for each task using the same procedure. Left maps: 80,000 observations used for training and validation, aggregated up to 20 km × 20 km
cells for display. Right maps: concatenated validation set estimates from 5-fold cross-validation for the same 80,000 grid cells (observations are never
used to generate their own prediction), identically aggregated for display. Scatters: Validation set estimates (vertical axis) vs. ground truth (h orizontal
axis); each point is a ~1 km × 1 km grid cell. Black line is at 45∘. Test-set and validation set performance are essentially identical (Supplementary Table 2);
validation set values are shown for display purposes only, as there are more observations. The tasks in the top three rows are uniformly sampled across
space, the tasks in the bottom four rows are sampled using population weights (Supplementary Note 2.1); grey areas were not sampled in the experiment.ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z
4 NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications
are omitted. These results indicate that MOSAIKS is skillful for a
wide range of possible applications without changing the
procedure or features and without task-speci ﬁc expertise. Note
that due to the absence of metadata describing the exact time ofobservation in the Google imagery, as well as task-speci ﬁc data
availability constraints, these performance measures are condi-
tional on a certain degree of unknown temporal mismatch
between imagery and task labels (Supplementary Note 1).
Comparison to state-of-the-art SIML approaches . We con-
textualize this performance by comparing MOSAIKS to existing
deep-learning based SIML approaches. First, we retrain end-to-
end a commonly-used deep convolutional neural network (CNN)
architecture
21–23(ResNet-18) using identical imagery and labels
for the seven tasks above. This training took 7.9 hours per task on
a cloud computing node (Amazon EC2 p3.xlarge instance, Tesla
V100 GPU). We ﬁnd that MOSAIKS exhibits predictive accuracy
competitive with the CNN for all seven tasks (mean
R2
CNN/C0R2
MOSAIKS ¼0:04; smallest R2
CNN/C0R2
MOSAIKS ¼/C00:03 for
housing; largest R2
CNN/C0R2
MOSAIKS ¼0:12 for elevation) in addi-
tion to being ~250 –10,000 × faster to train, depending on whether
the regression step is performed on a laptop (2018 Macbook Pro)
or on the same cloud computing node used to train the CNN
(Fig. 3a, Supplementary Note 3.1 and Supplementary Table 8).
Second, we apply transfer learning24using the ResNet-152
CNN pre-trained on natural images to featurize the same satellite
images22,23. We then apply ridge regression to the CNN-derived
features. The speed of this approach is similar to MOSAIKS, but
its performance is dramatically lower on all seven tasks (Fig. 3a,
Supplementary Note 3.1).
Third, we compare MOSAIKS to an approach from prior
studies4,13,25where a deep CNN (VGG1626pretrained on the
ImageNet dataset) is trained end-to-end on night lights and then
each task is solved via transfer learning (Supplementary Note 3.1).
We apply MOSAIKS to the imagery from Rwanda, Haiti, and
Nepal used in ref.13to solve all eleven development-oriented
tasks they analyze. We ﬁnd MOSAIKS matches prior perfor-
mance across tasks in Rwanda and Haiti, and has slightly lower
performance (average ΔR2=0.08) on tasks in Nepal (Supple-
mentary Fig. 16). The regression step of this transfer learning
approach and MOSAIKS are similarly fast, but the transfer
learning approach requires country-speci ﬁc retraining of the
CNN, limiting its accessibility and reducing its generalizability.
Together, these three experiments illustrate that with a single
set of task-independent features, MOSAIKS predicts outcomes
across a diverse set of tasks, with performance and speed that
favorably compare to existing SIML approaches. However,
throughout this set of experiments, we ﬁnd that some sources
of variation in labels are not recovered by MOSAIKS. For
example, extremely high elevations (>3,000 m) are not reliably
distinguished from high elevations (2,400-3,000m) that appear
visually similar (Supplementary Fig. 9). Additionally, roughly half
the variation in incomes and housing prices is unresolved,
presumably because they depend on factors not observable from
orbit, such as tax policies or school districts (Fig. 2).
These experiments additionally reveal that patterns of predict-
ability across tasks are strikingly similar in MOSAIKS and in
alternative SIML approaches (Supplementary Figs. 16 and 17).
Together, these ﬁndings are consistent with the hypothesis that
there exists some performance ceiling for each task, due to some
factors not being observable from satellite imagery. To investigate
this further, we develop a hybrid model in which the 512 features
produced by the last layer of the ResNet-18 CNN are
concatenated with the 8,192 MOSAIKS features and included
together in a ridge regression. Performance improvements aboveeither MOSAIKS or the CNN are small ( ≤0.01R2) for most tasks,
although there is a notable performance boost for the two tasks
where both models achieve the lowest accuracy
(R2
hybrid/C0R2
CNN¼0:04 for income; R2
hybrid/C0R2
MOSAIKS ¼0:05
for housing price; Supplementary Table 7). These results suggest
that for some tasks, combining MOSAIKS with alternative SIML
models can enhance predictive accuracy.
Evaluations of model sensitivity . There is growing recognition
that understanding the accuracy, precision, and limits of SIML
predictions is important, since consequential decisions increas-
ingly depend on these outputs, such as which households should
receive ﬁnancial assistance20,27. However, historically, the high
costs of training deep-learning models have generally prevented
the stress-testing and bench-marking that would ensure accuracy
and constrain uncertainty. To characterize the performance of
MOSAIKS, we test its sensitivity to the number of features ( K)
and training observations ( N), as well as the extent of spatial
extrapolation.
Unlike some featurization methods, these is no known measure
of importance for individual features in MOSAIKS, so the
computational complexity of the regression step can be
manipulated by simply including more or fewer features.
Repeatedly re-solving the linear regression step in MOSAIKS
with a varied number of features indicates that increasing Kabove
1,000 features provides minor predictive gains (Fig. 3b). A
majority of the observable signal in the baseline experiment using
K=8,192 is recovered using K=200 (min 55% for income, max
89% for nighttime lights), reducing each 65,536-pixel tri-band
image to just 200 features (~250 × data compression). Similarly,re-solving MOSAIKS predictions with a different number of
training observations demonstrates that models trained with
fewer samples may still exhibit high accuracy (Fig. 3b). A majority
of the available signal is recovered for many outcomes using only
N=500 (55% for road length to 87% for forest cover), with the
exception of income (28%) and housing price (26%) tasks, which
require larger samples. Together, these experiments suggest that
users with computational, data acquisition, or data storage
constraints can easily tailor MOSAIKS to match available
resources and can reliably estimate the performance impact of
these alterations (Supplementary Note 2.7).
To systematically evaluate the ability of MOSAIKS to make
accurate predictions in large contiguous areas where labels are not
available, we conduct a spatial cross-validation experiment by
partitioning the US into a checkerboard pattern (Fig. 3c), training
on the black squares and testing on the white squares
(Supplementary Note 2.8). Increasing the width of squares ( δ)
in the checkerboard increases the average distances between train
and test observations, simulating increasingly large spatial
extrapolations. We ﬁnd that for three of seven tasks (forest
cover, population density, and nighttime lights), performance
declines minimally regardless of distance (maximum R
2decline of
10% at δ=16∘for population density). For income, road length,
and housing price, performance falls moderately at small degrees
of spatial extrapolation (19%, 33%, and 35% decline at δ=4∘,
respectively), but largely stabilizes thereafter. Note that the poor
performance of road length predictions is possibly due to missing
labels and data quality (Supplementary Note 1.1 and Supple-
mentary Fig. 1). Finally, elevation exhibits steady decline with
increasing distances between training and testing data (49%
decline at δ=16∘).
To contextualize this performance, we compare MOSAIKS to
spatial interpolation of observations, a widely used approach to
ﬁll in regions of missing data (Supplementary Note 2.8). Using
the same samples, MOSAIKS substantially outperforms spatialNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z ARTICLE
NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications 5
interpolation (Fig. 3c, grey dashed lines) across all tasks except for
elevation, where interpolation performs almost perfectly over
small ranges ( δ=0.5∘:R2=0.95), and housing price, where
interpolation slightly outperforms MOSAIKS at small ranges.
For both, interpolation performance converges to that of
MOSAIKS over larger distances. Thus, in addition to generalizing
across tasks, MOSAIKS generalizes out-of-sample across space,
outperforming spatial interpolation of ground truth in ﬁve of
seven tasks.
The above sensitivity tests are enabled by the speed and
simplicity of training MOSAIKS. These computational gains also
enable quanti ﬁcation of uncertainty in model performance within
each diagnostic test. As demonstrated by the shaded bands in
Figs. 3b–c, uncertainty in MOSAIKS performance due to
variation in splits of training-validation data remains modest
under most conditions.Applying MOSAIKS at scale . Having evaluated MOSAIKS sys-
tematically in the data-rich US, we test its performance at pla-
netary scale and its ability to recreate results from a national
survey.
We test the ability of MOSAIKS to scale globally using the four
tasks for which global labels are readily available. Using a
random sub-sample of global land locations (training and
validation: N=338,781, test: N=84,692; Supplementary
Note 2.10), we construct the ﬁrst planet-scale, multi-task
estimates using a single set of label-independent features ( K=
2,048, Fig. 4a), predicting the distribution of forest cover ( R2=
0.85), elevation ( R2=0.45), population density ( R2=0.62), and
nighttime lights ( R2=0.49). Note that inconsistent image and
label quality across the globe are likely partially responsible for
lowering performance relative to the US-only experiments above
(Supplementary Note 2.10).Fig. 3 Prediction accuracy relative to a convolutional neural network and transfer learning, using smaller KandN, and over large contiguous regions
with no ground truth data. a Task-speci ﬁc MOSAIKS test-set performance (dark bars) in contrast to: an 18-layer variant of the ResNet Architecture
(ResNet-18) trained end-to-end for each task (middle bars); and transfer learning based on an unsupervised featurization using the last hidden laye ro fa
152-layer ResNet variant pre-trained on natural imagery and applied using ridge regression (lightest bars). See Supplementary Note 3.1 for details .
bValidation set R2performance for all seven tasks while varying the number of random convolutional features Kand holding N=64, 000 (left) and while
varying Nand holding K=8, 192 (right). Shaded bands indicate the range of predictive skill across ﬁve folds. Lines indicate average accuracy across folds.
cEvaluation of performance over regions of increasing size that that are excluded from training sample. Data are split using a checkerboard partition , where
the width and height of each square is δ(measured in degrees). Example partitions with δ=0. 5∘,8∘,1 6∘are shown in maps. For a given δ, training occurs
using data sampled from black squares and performance is evaluated in white squares. Plots show colored lines representing average performance of
MOSAIKS in the US across δvalues for each task. Benchmark performance from Fig. 2are indicated as circles at δ=0. Grey dashed lines indicate
corresponding performance using only spatial interpolation with an optimized radial basis function (RBF) kernel instead of MOSAIKS (Supplementar y
Note 2.8). To moderate the in ﬂuence of the exact placement of square edges, training and test sets are resampled four times for each δwith the
checkerboard position re-initialized using offset vertices (see Supplementary Note 2.8 and Supplementary Fig. 10). The ranges of performance are p lotted
as colored or grey bands.ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z
6 NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications
It has been widely suggested that SIML could be used by
resource-constrained governments to reduce the cost of surveying
their citizens4,13,28–30. To demonstrate MOSAIKS ’s performance
in this theoretical use-case, we simulate a ﬁeld test with the goal
of recreating results from an existing nationally representativesurvey. Using the pre-computed features from the ﬁrst US
experiment above, we generate predictions for 12 pre-selected
questions in the 2015 American Community Survey (ACS)
conducted by the US Census Bureau31. We obtain R2values
ranging from 0.06 (percent household income spent on rent, an
a
Labels Predictions Labels Predictions
0255075% forest cover
12345log(1+people/km2)100030005000meters
00.51.5log(1+nanoWatts/cm2/sr)% forest cover
meters
log(1+people/km2)
log(1+nanoWatts/cm2/sr)100
cLabel super-resolution predictions
2x2 4x4 8x8 Image LabelPopulation
densityForest
cover002 0 4 0 6 0 8 0 1 0 0
0 1600 800 3200 2400 4000
0 2 46
0 0.1 0.2 0.3 0.5 0.4Forest cover
R 2= 0.85Elevation
R 2= 0.45Population density
R 2= 0.62Nighttime lights
R 2= 0.491.01 35
b
0 0.2 0.4 0.6Pct. HH income to rentTotal housing unitsPct. HHs vacantPct. below poverty linePct. HHs w/ food stampsCommute timePct. adults w/ bach. deg.Income per capitaRooms per houseIncome per HH Housing valueBuilding age
R2(shown in Figs. 2 and 3)16x16
0400800people/km204080% forest cover
Fig. 4 A single featurization of imagery predicts multiple variables at planet-scale, predicts results from a national survey, and achieves label su per-
resolution. a Training data (left maps) and predictions using a single featurization of daytime imagery (right maps). Insets (far right) marked by black
squares in global maps. Training sample is a uniform random sampling of 1,000,000 land grid cells, 498,063 for which imagery were available and could b e
matched to task labels. Out-of-sample predictions are constructed using ﬁve-fold cross-validation. For display purposes only, maps depict ~50 km × 50km
average values (ground truth and predictions at ~1 km × 1 km). bTest-set performance in the US shown for 12 variables from the 2015 American
Community Survey (ACS) conducted by the US Census Bureau31. Income per household (HH) (in purple) is also shown in Figs. 2and3, and was selected
as an outcome for the analysis in those ﬁgures before this ACS experiment was run. cBoth labels and features in MOSAIKS are linear combinations of sub-
image ground-level conditions, allowing optimized regression weights to be applied to imagery of any spatial extent (Supplementary Note 2.9). MOSA IKS
thus achieves label super-resolution by generating skillful estimates at spatial resolutions ﬁner than the labels used for training. Shown are example label
super-resolution estimates at 2 × 2, 4 × 4, 8 × 8, and 16 × 16, along with the original 1 × 1 label resolution (See Supplementary Fig. 12 for additional
examples). Systematic evaluation of within-image R2across the entire sample is reported in Supplementary Note 2.9 for the forest cover task.NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z ARTICLE
NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications 7
outlier) to 0.52 (building age), with an average R2of 0.34 across
12 tasks (Fig. 4b). Compared to a baseline of no ground survey, or
a costly survey extension, these results suggest that MOSAIKS
predictions could provide useful information to a decision-makerfor almost all tasks at low cost; noting that, in contrast, the ACS
costs >$200 million to deploy annually
32. However, some
variables (e.g., percent household income spent on rent) may
continue to be retrievable only via ground survey.
Methodological extensions . The design of MOSAIKS naturally
provides two additional useful properties: suitability to fusing
features with data from other sensors, and the ability to attribute
image-scale predictions to sub-image level regions.
Available satellites exhibit a diversity of properties (e.g.,
wavelength, timing of sampling) that can be used to improve
SIML predictions33. While most SIML approaches, including the
above analysis, use a single sensor, the design of MOSAIKS allows
seamless integration of data from additional satellites because the
regression step is linear in the features. To demonstrate this, we
include nighttime lights as a second data source in the analysis of
survey data from Rwanda, Haiti, and Nepal discussed above
(Supplementary Note 3.1). The approach mirrors that of the
hybrid MOSAIKS-ResNet18 model discussed previously in that
features extracted from the nighttime lights data are simply
concatenated with those from MOSAIKS prior to the regression
step. In all 36 tasks, predictions either improved or were
unchanged when nighttime imagery was added to daytime
imagery in the model (average ΔR2=0.03). This approach
naturally optimizes how data from all sensors are used without
requiring that users possess expertise on each technology.
Many use cases would bene ﬁt from SIML predictions at ﬁner
resolution than is available in training data33,34. Here we show
that MOSAIKS can estimate the relative contribution of sub-
regions within an image to overall image-level labels, even though
only aggregated image-level labels are used in training (See Fig. 4c
and Supplementary Fig. 12). Such label super-resolution predic-
tion follows from the functional form of the featurization and
linear regression steps in MOSAIKS, allowing it to be analytically
derived for labels that represent nearly linear combinations of
ground-level conditions (Supplementary Note 2.9 and Supple-
mentary Fig. 11). We numerically assess label super-resolution
predictions of MOSAIKS for the forest cover task, since raw label
data are available at much ﬁner resolution than our image labels.
Provided only a single label per image, MOSAIKS recovers
substantial within-image signal when predicting forest cover in 4
to 1024 sub-labels per label (within-image R2=0.54–0.32, see
Supplementary Fig. 13 for a plot of performance against number
of sub-labels and Supplementary Note 2.9 for m_ethodological
details).
Discussion
We develop a new approach to SIML that achieves practical
generalization across tasks while exhibiting performance that is
competitive with deep-learning models optimized for a single
task. Crucial to planet-scale analyses, MOSAIKS requires orders
of magnitude less computation time to solve a new task than
CNN-based approaches and it allows 1km-by-1km image data to
be compressed ~6 –500 times before storage/transmission (see
Methods). Such compression is a deterministic operation that
could theoretically be implemented in satellite hardware. We
hope these computational gains, paired with the relative simpli-
city of using MOSAIKS, will democratize access to global-scale
SIML technology and accelerate its application to solving pressing
global challenges. We hypothesize that there exist hundreds ofvariables observable from orbit whose application could improve
human well-being if measurements were made accessible.
While we have shown that in many cases MOSAIKS is a faster
and simpler alternative to existing deep-learning methods, thereremain contexts in which custom-designed SIML pipelines will
continue to play a key role in research and decision-making, such
as where resources are plentiful and performance is paramount.
Existing ground-based surveys will also remain important. In
both cases we expect MOSAIKS can complement these systems,
especially in resource-constrained settings. For example,
MOSAIKS can provide fast assessments to guide slower SIML
systems or extend the range and resolution of ground-based
surveys.
As real-world policy actions increasingly depend on SIML
predictions, it is crucial to understand the accuracy, precision and
sensitivity of these measurements. The low cost and high speed of
retraining MOSAIKS enables unprecedented stress tests that can
support robust SIML-based decision systems. Here, we tested the
sensitivity of MOSAIKS to model parameters, number of training
points, and degree of spatial extrapolation, and expect that many
more tests can be developed and implemented to analyze model
performance and prediction accuracies in context. To aid sys-
tematic bench-marking and comparison of SIML architectures,
the labels and features used in this study are made publicly
available; to our knowledge this represents the largest multi-label
benchmark dataset for SIML regression tasks. The high perfor-
mance of RCF, a relatively simple featurization, suggests that
developing and bench-marking other unsupervised SIML meth-
ods across tasks at scale may be a rich area for future research.
By distilling SIML to a pipeline with simple and mathemati-
cally interpretable components, MOSAIKS facilitates develop-
ment of methodologies for additional SIML use cases and
enhanced performance. For example, the ability of MOSAIKS to
achieve label super-resolution is easily derived analytically (Sup-
plementary Note 2.9). Furthermore, while we have focused here
on tri-band daytime imagery, we showed that MOSAIKS can
seamlessly integrate data from multiple sensors through simpleconcatenation, extracting useful information from each source to
maximize performance. We conjecture that integrating new
diverse data, from both satellite and non-satellite sources, may
substantially increase the predictive accuracy of MOSAIKS for
tasks not entirely resolved by daytime imagery alone; such inte-
gration using deep-learning models is an active area of research
35.
We hope that MOSAIKS lays the foundation for the future
development of an accessible and democratized system of global
information sharing, where, over time, imagery from all available
global sensors is continuously encoded as features and appended
to a single table of data, which is distributed and used planet-
wide. As a step in this direction, we make a global cross-section of
features publicly available. Such a uni ﬁed global system may
enhance our collective ability to observe and understand the
world, a necessary condition for tackling pressing global
challenges.
Methods
Overview . Here we provide additional information on our implementation of
MOSAIKS and experimental procedures, as well as a description of the theoreticalfoundation underlying MOSAIKS. Full details on the methodology behind
MOSAIKS can be found throughout Supplementary Note 2.
Implementation of MOSAIKS . We begin with a set of images I
‘/C8/C9N
‘¼1, each of
which is centered at locations indexed by ℓ={1,…,N}. MOSAIKS generates task-
agnostic feature vectors x(Iℓ) for each satellite image Iℓby convolving an M×M×S
"patch ”,Pk, across the entire image. Mis the width and height of the patch in units
of pixels and Sis number of spectral bands. In each step of the convolution, the
inner product of the patch and an M×M×Ssub-image region is taken, and a
ReLU activation function with bias bk=1 is applied. Each patch is a randomlyARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z
8 NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications
sampled sub-image from the set of training images I‘/C8/C9N
‘¼1(Supplementary Fig. 5).
In our main analysis, we use patches of width and height M=3 (Supplementary
Fig. 6) and S=3 bands (red, green, and blue). To create a single summary metric
for the image-patch pair, these inner product values are then averaged across the
entire image, generating the kth feature xk(Iℓ), derived from patch Pk. The
dimension of the resulting feature space is equal to K, the number of patches used,
and in all of our main analyses we employ K=8,192 (i.e., 213). Both images and
patches are whitened according to a standard image preprocessing procedure
before convolution (Supplementary Note 2.3).
In practice, this one-time featurization can be centrally computed and then
features xk(Iℓ) distributed to users in tabular form. A user need only (i) obtain and
link the subset of these features that match spatially with their own labels and then
(ii) solve linear regressions of the labels on the features to learn nonlinear mappings
from the original image pixel values to the labels (the nonlinearity of the mappingbetween image pixels and labels stems from the nonlinearity of the ReLU activation
function). We show strong performance across seven different tasks using ridge
regression to train the relationship between labels y
ℓand features xk(Iℓ) in this
second step, although future work may demonstrate that other ﬁtting procedures
yield similar or better results for particular tasks.
Implementation of this one-time unsupervised featurization takes about the
same time to compute as a single forward pass of a CNN. With K=8,912 features,
featurization results in a roughly 6 to 1 compression of stored and transmittedimagery data in the cases we study. Notably, storage and computational cost can be
traded off with performance by using more or fewer features from each image
(Fig. 3b). Since features are random, there is no natural value for Kthat is
speciﬁcally preferable.
Task selection and data . Tasks were selected based on diversity and data avail-
ability, with the goal of evaluating the generalizability of MOSAIKS (Supplemen-
tary Note 1.1). Results for all tasks evaluated are reported in the paper. We align
image and label data by projecting imagery and label information onto a ~1 km × 1
km grid, which was designed to ensure zero spatial overlap between observations(Supplementary Notes 2.1 and 2.2).
Images are obtained from the Google Static Maps API (Supplementary
Note 1.2)
36, and labels for the seven tasks are obtained from refs.2,31,37–41. Details
on data are described in Supplementary Table 1 and Supplementary Note 1.
US experiments . From this grid we sample 20,000 hold-out test cells and 80,000
training and validation cells from within the continental US (Supplementary
Note 2.4). To span meaningful variation in all seven tasks, we generate two of these100,000-sample data sets according to different sampling methods. First, we sample
uniformly at random across space for the forest cover, elevation, and population
density, tasks which exhibit rich variation across the US. Second, we sample via a
population-weighted scheme for nighttime lights, income, road length, and housing
price, tasks for which meaningful variation lies within populated areas of the US.Some sample sizes are slightly reduced due to missing label data ( N=91,377 for
income, 80,420 for housing price, and 67,968 for population density). We model
labels whose distribution is approximately log-normal using a log transformation(Supplementary Note 2.5 and Supplementary Table 3).
Because ﬁtting a linear model is computationally cheap, relative to many other
SIML approaches, it is feasible to conduct numerous sensitivity tests of predictive
skill. We present cross-validation results from a random sample, while also
systematically evaluating the behavior of the model with respect to: (a) geographicdistance between training and testing samples, i.e., spatial cross-validation, (b) the
dimension Kof the feature space, and (c) the size Nof the training set (Fig. 3,
Supplementary Notes 2.7 and 2.8). We represent uncertainty in each sensitivity testby showing variance in predictive performance across different training and
validation sets. We also benchmark model performance and computational
expense against an 18-layer variant of the ResNet Architecture, a common deep
network architecture that has been used in satellite-based learning tasks
42, trained
end-to-end for each task and a transfer learning approach24utilizing an
unsupervised featurization based on the last hidden layer of a 152-layer ResNet
variant pre-trained on natural imagery and applied using ridge regression
(Supplementary Notes 3.1 and 3.2).
Global experiment . To demonstrate performance at scale, we apply the same
approach used within the data-rich US context to global imagery and labels. We
employ a target sample of N=1,000,000, which drops to a realized sample of N=
423,476 due to missing imagery and label data outside the US (Fig. 4). We generate
predictions for all tasks with labels that are available globally (forest cover, eleva-
tion, population density, and nighttime lights) (Supplementary Note 2.10).
Label super-resolution experiment . Predictions at label super-resolution (i.e.,
higher resolution than that of the labels used to train the model), shown in Fig. 4c,
are generated for forest cover and population density by multiplying the trained
ridge regression weights by the un-pooled feature values for each sub-image andapplying a Gaussian ﬁlter to smooth the resulting predictions (Supplementary
Note 2.9). Additional examples of label super-resolution performance are shown in
Supplementary Fig. 12. We quantitatively assess label super-resolutionperformance (Supplementary Fig. 13) using forest cover, as raw forest cover data
are available at substantially ﬁner resolution than our common ~ 1 km × 1 km grid.
Performance is evaluated by computing the fraction of variance ( R
2) within each
image that is captured by MOSAIKS, across the entire sample.
Theoretical foundations . MOSAIKS is motivated by the goal of enabling gen-
eralizable and skillful SIML predictions. It achieves this by embedding images in a
basis that is both descriptive (i.e., models trained using this single basis achievehigh skill across diverse labels) and ef ﬁcient (i.e., such skill is achieved using a
relatively low-dimensional basis). The approach for this embedding relies on the
theory of random kitchen sinks
16, a method for feature generation that enables the
linear approximation of arbitrary well-behaved functions. This is akin to the use of
polynomial features or discrete Fourier transforms for function approximationgenerally, such as functions of one dimension. When users apply these features in
linear regression, they identify linear weightings of these basis vectors important
for predicting a speci ﬁc set of labels. With inputs of high dimension, such as the
satellite images we consider, it has been shown experimentally
17–19and
theoretically43that a randomly selected subspace of the basis often performs as well
as the entire basis for prediction problems.
Convolutional random kitchen sinks . Random kitchen sinks approximate arbi-
trary functions by creating a ﬁnite series of features generated by passing the input
variables zthrough a set of Knonlinear functions g(z;Θk), each paramaterized by
draws of a random vector Θ. The realized vectors Θkare drawn independently
from a pre-speci ﬁed distributions for each of k=1. . .Kfeatures. Given an
expressive enough function gand in ﬁnite K, such a featurization would be a
universal function approximator43. In our case, such a function gwould encode
interactions between all subsets of pixels in an image. Unfortunately, for an image
of size 256 × 256 × 3, there are 2256×256×3such subsets. Therefore, the fully-
expressive approach is inef ﬁcient in generating predictive skill with reasonably
concise Kbecause each feature encodes more pixel interactions than are empirically
useful.
To adapt random kitchen sinks for satellite imagery, we use convolutional
random features, making the simplifying assumption that most information
contained within satellite imagery is represented in local image structure. Randomconvolutional features have been shown to provide good predictive performance
across a variety of tasks from predicting DNA binding sites
17and solar ﬂares19to
clustering photographs18(kitchen sinks have also been used in a non-convolutional
approach to classify individual pixels of hyper-spectral satellite data44). Applied to
satellite images, random convolutional features reduce the number of effectiveparameters in the function by considering only local spatial relationships between
pixels. This results in a highly expressive, yet computationally tractable, model for
prediction.
Speci ﬁcally, we create each Θ
kby extracting a small sub-image patch from a
randomly selected image within our image set, as described above. These patches
are selected independently, and in advance, of any of the label data. The
convolution of each patch across the satellite image being featurized captures
information from the entire R256´256´3image using only 3 *M2free parameters for
each k. Creating and subsequently averaging over the activation map (after a ReLU
nonlinearity) de ﬁnes our instantiation of the kitchen sinks function g(z;Θk)a sg(Iℓ;
Pk,bk)=xk(Iℓ), where bkis a scalar bias term. Our choice of this functional form is
guided by both the structural properties of satellite imagery and the nature ofcommon SIML prediction tasks, and it is validated by the performance
demonstrated across tasks.
Relevant structural properties of satellite imagery and SIML tasks . Three
particular properties provide the the motivation for our choice of a convolutionand average-pool mapping to de ﬁneg.
First, we hypothesize that convolutions of small patches will be suf ﬁcient to
capture nearly all of the relevant spatial information encoded in images becauseobjects of interest (e.g., a car or a tree) tend to be contained in a small sub-region of
the image. This is particularly true in satellite imagery, which has a much lower
spatial resolution that most natural imagery (Supplementary Fig. 6).
Second, we expect a single layer of convolutions to perform well because
satellite images are taken from a constant perspective (from above the subject) at a
constant distance and are (often) orthorecti ﬁed to remove the effects of image
perspective and terrain. Together, these characteristics mean that a given object will
tend to appear the same when captured in different images. This allows forMOSAIKS ’s relatively simple, translation invariant featurization scheme to achieve
high performance, and avoids the need for more complex architectures designed to
provide robustness to variation in object size and orientation.
Third, we average-pool the convolution outputs because most labels for the
types of problems we study can be approximately decomposed into a sum of sub-
image characteristics. For example, forest cover is measured by the percent of total
image area covered in forest, which can equivalently be measured by averaging the
percent forest cover across sub-regions of the image. Labels that are strictlyaverages, totals, or counts of sub-image values (such as forest cover, road length,
population density, elevation, and night lights) will all exhibit this decomposition.
While this is not strictly true of all SIML tasks, for example income and averageNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z ARTICLE
NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications 9
housing price, we demonstrate that MOSAIKS still recovers strong predictive skill
on these tasks. This suggests that some components of the observed variance in
these labels may still be decomposable in this way, likely because they are well-
approximated by functions of sums of observable objects.
Additional interpretations . The full MOSAIKS platform, encompassing both
featurization and linear prediction, bears similarity to a few related approaches.
Namely, it can be interpreted as a computationally feasible approximation of kernelridge regression for a fully convolutional kernel or, alternatively, as a two-layer
CNN with an incredibly wide hidden layer generated with untrained ﬁlters. A
discussion of these interpretations and how they can help to understandMOSAIKS ’s predictive skill can be found in Supplementary Note 2.3.
Data availability
Code, data, a con ﬁgured computing environment, and free cloud computing for this analysis
is provided via Code Ocean and is available at https://doi.org/10.24433/CO.8021636.v2 .A l l
data used in this analysis are from publicly ava ilable sources other than the house price data.
House price data are provided by Zillow thr ough the Zillow Transaction and Assessment
Dataset (ZTRAX) and are used under license for the current study. More information on
accessing the data can be found at http://www.zillow.com/ztrax . The results and opinions are
those of the author(s) and do not re ﬂect the position of Zillow Group. The house price dataset
we release publicly is a subset of the pre-proce ssed and aggregated data used in the analysis,
where grid cells containing <30 observations o f recent property sales are removed to preserve
privacy. While the rest of the data that support the ﬁndings of this study are publicly available,
the re-dissemination of some of these data is restricted. Thus, we are not able to host all dataused in the study within our Code Ocean capsule. For example, both imagery and some labeldata must be downloaded directly from the original providers. Whenever this is the case, weprovide download instructions in the code repository ’s Readme. In addition to the data
directly supporting this study, we provide MOSAIKS features for a gridded cross-section of
the globe. This service and any related work will be accessible via http://www.globalpolicy.
science/mosaiks .
Code availability
The code used in this analysis is provided in the github repository available at https://
github.com/Global-Policy-Lab/mosaiks-paper and additionally at https://doi.org/
10.24433/CO.8021636.v2 . The latter is part of the Code Ocean capsule, additionally
containing data and computing environment (see Data Availability). On GitHub, release"v1.0”corresponds to the state of the codebase at the time of publication. See the
repository ’s Readme for more detailed information.
Received: 21 October 2020; Accepted: 4 June 2021;
References
1. Union of Concerned Scientists, UCS Satellite Database (2019).
2. Hansen, M. C. High-resolution global maps of 21st-century forest cover
change. Science 342, 850 (2013).
3. Inglada, J. Operational high resolution land cover map production at the
country scale using satellite image time series. Remote Sens. 9,9 5
(2017).
4. Jean, N. Combining satellite imagery and machine learning to predict poverty.
Science 353, 790 (2016).
5. Robinson, C., Hohman, F. & Dilkina, B. A Deep Learning Approach for
Population Estimation from Satellite Imagery. Proceedings of the 1st ACM
SIGSPATIAL Workshop on Geospatial Humanities - GeoHumanities 2017(ACM Press, New York, New York, USA, 2017), pp. 47 –54.
6. Yu, L. Meta-discoveries from a synthesis of satellite-based land-cover mapping
research. Int. J. Remote Sens. 35, 4573 (2014).
7. Haack, B. & Ryerson, R. Improving remote sensing research and education in
developing countries: approaches and recommendations. Int. J. Appl. Earth
Observation Geoinf. 45, 77 (2016).
8. Ball, J.E., Anderson, D.T. & Chan, C.S. A comprehensive survey of deep
learning in remote sensing: theories, tools and challenges for the community.J. of Appl. Remote Sens. 11, 042609 (2017).
9. Romero, A., Gatta, C. & Camps-Valls, G. Unsupervised deep feature
extraction for remote sensing image classi ﬁcation. IEEE Trans. Geosci. Remote
Sens. 54, 1349 (2016).
10. Cheriyadat, A. M. Unsupervised feature learning for aerial scene classi ﬁcation.
IEEE Trans. Geosci. Remote Sens. 52, 439 (2014).11. Penatti, O. A. B., Nogueira, K., Santos, J. A. dos. Do deep features generalize
from everyday objects to remote sensing and aerial scenes domains? Proc of
the IEEE conference on computer vision and pattern recognition workshops,pp. 44 –51 (2015).
12. Jean, N. et al. Tile2Vec: Unsupervised Representation Learning for Spatially
Distributed Data. Proceedings of the AAAI Conference on Arti ﬁcial
Intelligence 33, pp. 3967 –3974 (2019).
13. Head, A., Manguin, M., Tran N., Blumenstock, J.E. Can Human Development
be Measured with Satellite Imagery?, ICTD , pp. 1 –8 (2017).
14. Zhu, L. et al. A review: Remote sensing sensors, Multi-purposeful application
of geospatial data pp. 19 –42 (2018).
15. Littlepage, J. DigitalGlobe moves to the cloud with AWS Snowmobile.16. Rahimi, A. & Recht, B. Weighted sums of random kitchen sinks: replacing
minimization with randomization in learning. Adv. neural Inf. Process. …1,
1313 (2008).
17. Morrow, A. et al. Convolutional Kitchen Sinks for Transcription Factor
Binding Site Prediction, arXiv preprint (2017).
18. Coates, A. & Ng, A. Y. Neural networks: tricks of the trade. (Springer, 2012).
19. Jonas, E., Bobra, M., Shankar, V., Todd Hoeksema, J. & Recht, B. Flare
prediction using photospheric and coronal image data. Sol. Phys. 293,1
(2018).
20. Blumenstock, J. Don ’t forget people in the use of big data for
development Nature 561(2018).
21. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image
recognition. Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 770 –778 (2016).
22. Li, Y., Zhang, H., Xue, X., Jiang, Y. & Shen, Q. Deep learning for remote
sensing image classi ﬁcation: a survey. Wiley Interdiscip. Rev.: Data Min.
Knowl. Discov. 8, 1 (2018).
23. Gu, Y., Wang, Y. & Li, Y. A survey on deep learning-driven remote sensing
image scene understanding: Scene classi ﬁcation, scene retrieval and scene-
guided object detection, Applied Sciences 9(2019).
24. Pan, S. J. & Yang, Q. A survey on transfer learning, IEEE Trans. Knowl. Data
Eng. 22, 1345 (2010).
25. Xie, M., Jean, N., Burke, M., Lobell, D. & Ermon, S. Transfer learning from
deep features for remote sensing and poverty mapping. Thirtieth AAAI
Conference on Arti ﬁcial Intelligence (2016).
26. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-
scale image recognition. International Conference on Learning Representations
(2015).
27. Athey, S. Beyond prediction: using big data for policy problems. Science 355,
483 (2017).
28. F. Reed, et al., Gridded Population Maps Informed by Different Built
Settlement Products, Data 3, 33 (2018).
29. Bedi, T., Coudouel, A., Simler, K., eds., More than a pretty picture: using
poverty maps to design better policies and interventions (The World Bank,Washington, DC, 2007).
30. De Sherbinin, A. M., Yetman, G., MacManus, K. & Vinay, S. Improved
mapping of human population and settlements through integration of remotesensing and socioeconomic data. AGUFM 2017 , IN51H (2017).
31. U.S. Census Bureau, 2015 American Community Survey 5-Year Estimates,
Table B19013.
32. U.S. Census Bureau, Budget Estimates, Fiscal Year 2021 (2021).33. Tsagkatakis, G., et al. Survey of deep-learning approaches for remote sensing
observation enhancement. Sensors (Switz.) 19, 1 (2019).
34. Malkin, K. et al. Label super-resolution networks. International Conference on
Learning Representations (2018).
35. Hong, D. More diverse means better: multimodal deep learning meets remote
sensing imagery classi ﬁcation. IEEE Trans. Geosci. Remote Sens. 0196 ,1
(2020).
36. Google Developers, Maps Static API.
37. Amazon Web Services, Terrain Tiles (2018).38. Center for International Earth Science Information Network (CIESIN),
Gridded Population of the World, Version 4 (2016).
39. NOAA National Centers for Environmental Information, Version 1 VIIRS
Day/Night Band Nighttime Lights (2019).
40. U.S. Census Bureau, TIGER/Line Geodatabases (2016).41. Zillow, ZTRAX: Zillow Transaction and Assessor Dataset (2017).
42. Perez, A. et al. Poverty prediction with public Landsat 7 satellite imagery and
machine learning. NIPS 2017 Workshop on Machine Learning for the
Developing World (2017).
43. Rahimi, A., Recht, B. Uniform approximation of functions with random bases.
46th Annual Allerton Conference on Communication, Control, and
Computing IEEE , pp. 555 –561 (2008).ARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z
10 NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications
44. Pérez-Suay, A. Randomized kernels for large scale Earth observation
applications. Remote Sens. Environ. 202, 54 (2017).
Acknowledgements
We thank Patrick Baylis, Joshua Blumenstock, Jennifer Burney, Hannah Druckenmiller,
Jonathan Kadish, Alyssa Morrow, James Rising, Geoffrey Schiebinger, Adam Storeygardand participants in seminars at UC Berkeley, University of Chicago, Harvard, AmericanGeophysical Union, the World Bank, the United Nations Development Program &
Environment Program, Planet Inc., The Potsdam Institute for Climate Impact Research,
the National Bureau of Economic Research, and The Workshop in EnvironmentalEconomics and Data Science for helpful comments and suggestions. We acknowledgefunding from the NSF Graduate Research Fellowship Program (Grant DGE 1752814),
the US Environmental Protection Agency Science To Achieve Results Fellowship Pro-
gram (Grant FP91780401), the NSF Research Traineeship Program Data Science for the21st Century, the Harvard Center for the Environment, the Harvard Data ScienceInitiative, the Sloan Foundation, and a gift from the Rhodium Group. The authors
declare no con ﬂicts of interest.
Author contributions
E.R., J.P., T.C., I.B., V.S., B.R. and S.H. formulated the research idea and designed the
overall analysis structure. V.S. collected imagery data and designed and implemented thefeaturization. J.P., T.C., I.B. and M.I. collected label data. E.R., J.P., T.C., I.B., V.S. and M.I. developed and carried out experimental procedures. E.R., J.P., T.C., I.B., V.S., M.I., B.R.
and S.H. analyzed and interpreted the output of the experiments. E.R., J.P., T.C., I.B. and
S.H. wrote the paper with contributions from V.S. and B.R.
Competing interests
The authors declare no competing interests.Additional information
Supplementary information The online version contains supplementary material
available at https://doi.org/10.1038/s41467-021-24638-z .
Correspondence and requests for materials should be addressed to S.H.
Peer review information Nature Communications thanks Grigorios Tsagkatakis and the
anonymous reviewers for their contribution to the peer review of this work. Peer reviewer
reports are available.
Reprints and permission information is available at http://www.nature.com/reprints
Publisher ’s note Springer Nature remains neutral with regard to jurisdictional claims in
published maps and institutional af ﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as long as you giveappropriate credit to the original author(s) and the source, provide a link to the Creative
Commons license, and indicate if changes were made. The images or other third party
material in this article are included in the article ’s Creative Commons license, unless
indicated otherwise in a credit line to the material. If material is not included in thearticle ’s Creative Commons license and your intended use is not permitted by statutory
regulation or exceeds the permitted use, you will need to obtain permission directly fromthe copyright holder. To view a copy of this license, visit http://creativecommons.org/
licenses/by/4.0/ .
© The Author(s) 2021NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-24638-z ARTICLE
NATURE COMMUNICATIONS |         (2021) 12:4392 | https://doi.org/10.1038/s41467-021-24638-z | www.nature.com/naturecommunications 11
TorchGeo: Deep Learning With Geospatial Data
Adam J. Stewart
University of Illinois at
Urbana-Champaign
Department of Computer Science
Urbana, IL, USA
adamjs5@illinois.eduCaleb Robinson
Microsoft
AI for Good Research Lab
Redmond, WA, USA
caleb.robinson@microsoft.comIsaac A. Corley
University of Texas at San Antonio
Department of Electrical Engineering
San Antonio, TX, USA
isaac.corley@my.utsa.edu
Anthony Ortiz
Microsoft
AI for Good Research Lab
Redmond, WA, USA
anthony.ortiz@microsoft.comJuan M. Lavista Ferres
Microsoft
AI for Good Research Lab
Redmond, WA, USA
jlavista@microsoft.comArindam Banerjee
University of Illinois at
Urbana-Champaign
Department of Computer Science
Urbana, IL, USA
arindamb@illinois.edu
ABSTRACT
Remotely sensed geospatial data are critical for applications in-
cluding precision agriculture, urban planning, disaster monitoring
and response, and climate change research, among others. Deep
learning methods are particularly promising for modeling many
remote sensing tasks given the success of deep neural networks
in similar computer vision tasks and the sheer volume of remotely
sensed imagery available. However, the variance in data collection
methods and handling of geospatial metadata make the application
of deep learning methodology to remotely sensed data nontrivial.
For example, satellite imagery often includes additional spectral
bands beyond red, green, and blue and must be joined to other
geospatial data sources that can have differing coordinate systems,
bounds, and resolutions. To help realize the potential of deep learn-
ing for remote sensing applications, we introduce TorchGeo, a
Python library for integrating geospatial data into the PyTorch
deep learning ecosystem. TorchGeo provides data loaders for a
variety of benchmark datasets, composable datasets for generic
geospatial data sources, samplers for geospatial data, and trans-
forms that work with multispectral imagery. TorchGeo is also the
first library to provide pre-trained models for multispectral satellite
imagery (e.g., models that use all bands from the Sentinel-2 satel-
lites), allowing for advances in transfer learning on downstream
remote sensing tasks with limited labeled data. We use TorchGeo
to create reproducible benchmark results on existing datasets and
benchmark our proposed method for preprocessing geospatial im-
agery on the fly. TorchGeo is open source and available on GitHub:
https://github.com/microsoft/torchgeo.
CCS CONCEPTS
•Computing methodologies →Neural networks ;•Software
and its engineering →Software libraries and repositories ;•Ap-
plied computing →Earth and atmospheric sciences.
KEYWORDS
deep learning, remote sensing, earth observation, geospatial, datasets,
samplers, transforms, models1 INTRODUCTION
With the explosion in availability of satellite and aerial imagery
over the past decades, there has been increasing interest in the use
of imagery in remote sensing (RS) applications. These applications
range from precision agriculture [ 39] and forestry [ 34], to natural
and man-made disaster monitoring [ 66], to weather and climate
change [ 50]. At the same time, advancements in machine learn-
ing (ML), larger curated benchmark datasets, and increased com-
pute power, have led to great successes in domains like computer
vision, natural language processing, and audio processing. How-
ever, the wide-spread success and popularity of machine learning—
particularly of deep learning methods—in these domains has not
fully transferred to the RS domain, despite the existence of petabytes
of freely available satellite imagery and a variety of benchmark
datasets for different RS tasks. This is not to say that there are
not successful applications of ML in RS, but that the full potential
of the intersection of these fields has not been reached. Indeed, a
recent book by Camps-Valls et al . [9] thoroughly details work at
the intersection of deep learning, geospatial data, and the Earth
sciences. Increasing amounts of research on self-supervised and
unsupervised learning methods specific to remotely sensed geospa-
tial imagery [ 3,28,35] bring the promise of developing generic
models that can be tuned to various downstream RS tasks. Recent
large-scale efforts, such as the creation of a global 10 m resolution
land cover map [ 31] or the creation of global 30 m forest maps [ 47],
pair the huge amount of available remotely sensed imagery with
modern GPU accelerated models. To reach the full joint potential of
these fields, we believe that we need tools for facilitating research
and managing the complexities of both geospatial data and mod-
ern machine learning pipelines. We describe the challenges of this
below, and detail our proposed solution, TorchGeo.
One major challenge in many RS tasks is the large amount of
diversity in content of geospatial imagery datasets compared to
datasets collected for traditional vision applications. For example,
most conventional cameras capture 3-channel RGB imagery, how-
ever most satellite sensors capture different sets of spectral bands.
The Landsat 8 satellite [ 52] collects 11 bands, the Sentinel-2 satel-
lites [ 16] collect 12 bands, and the Hyperion satellite [ 44] collects
242 (hyperspectral) bands, each measuring different regions of thearXiv:2111.08872v4  [cs.CV]  17 Sep 2022
Stewart et al.
Figure 1: An illustration of the challenges in sampling from heterogeneous geospatial data layers. (A and B) show example
geospatial data layers that a user may want to sample pixel-aligned data from. As these layers have differing coordinate
reference systems, patches of imagery (C and D) sampled from these layers that cover the same area will not be pixel-aligned.
TorchGeo transparently performs the appropriate alignment steps (reprojecting and resampling) during data loading such
that users can train deep learning models without having to manually align the data layers.
electromagnetic spectrum. The exact wavelengths of the electro-
magnetic spectrum captured by each band can range from 400 nm
to 15𝜇m. In addition, different sensors capture imagery at differ-
ent spatial resolutions: satellite imagery resolution can range from
4 km/px (GOES [ 43]) to 30 cm/px (Maxar WorldView satellites [ 62]),
while imagery captured from drones can have a resolution as high as
7 mm/px [ 2]. Depending on the type of orbit a satellite is in, imagery
can be continuous (for geostationary orbits) or daily to biweekly
(for polar, sun-synchronous orbits). Machine learning models or
algorithms developed for one of these platforms will not generalize
across inputs collected by the others, and, as a consequence of this,
it is not possible to publish a single set of pre-trained model weights
that span imaging platforms. In contrast, ImageNet [ 15] pretrained
models have been proven to be useful in a large number of transfer
learning tasks [ 71]. Researchers and practitioners can often start
with ImageNet pre-trained models in a transfer learning setup when
presented with vision problems to reduce the overall amount of
training needed to solve the problem. Further, it is not clear whether
the inductive biases built into common modeling approaches for
vision problems are immediately applicable to remotely sensed im-
agery. Large neural architecture search efforts [ 32] produce models
that are optimized for and outperform hand-designed architectures
on vision tasks, but it is an open question whether these transfer
to remotely sensed imagery.
Most machine learning libraries have not been designed to work
with geospatial data. For example, the Python Imaging Library(PIL) [ 12], used by many libraries to load images and perform
data augmentation, does not support multispectral imagery. Simi-
larly, deep learning models implemented by the torchvision library
only support 3 channel (RGB) inputs, and must be adapted or re-
implemented to support multispectral data. Datasets of geospatial
data can be made up of a heterogenous mix of files with differing file
formats, spatial resolutions, projections, and coordinate reference
systems (CRS). Libraries such as GDAL [ 20] can interface with most
types of geospatial data, however further abstractions for using
such data in arbitrary deep learning pipelines are limited. Indeed,
the gap between loading geospatial data from disk, and using it
in a modeling pipeline, is large for all of the reasons mentioned
above. As illustrated in Figure 1, users will often need pixel-aligned
crops from multiple layers of data: imagery from different points in
time over the same space, imagery and corresponding label masks,
high-resolution and low resolution imagery from the same space,
etc. In contrast, there are a wide variety of software libraries at
the intersection of machine learning and other domains. In the
PyTorch ecosystem, torchvision [ 38], torchtext [ 73], and torchau-
dio [70] provide the tools necessary for abstracting the peculiarities
of domain-specific data away from the details of deep learning
training pipelines.
To address these challenges, we propose TorchGeo, a Python
package that allows users to transparently use heterogenous geospa-
tial data in PyTorch-based deep learning pipelines. Specifically,
TorchGeo provides:
TorchGeo: Deep Learning With Geospatial Data
Figure 2: Different layers of geospatial data often have differing coordinate reference systems and spatial resolutions. (Top row)
The same physical area cropped from four raster layers with different coordinate reference systems and spatial resolutions—
this data is not pixel-aligned and cannot yet be used in modelling pipelines. (Bottom row) The same data after reprojecting
into the same coordinate system and resampling to the highest spatial resolution—this data is pixel aligned and can serve as
inputs or masks to deep neural networks.
(1)data loaders for geospatial datasets common in the literature,
(2) data loaders for combining uncurated geospatial raster and
vector data layers with the ability to sample pixel-aligned
patches on the fly,
(3) augmentations appropriate for multispectral imagery,
(4) data samplers appropriate for geospatial data, and
(5)pre-trained models for many common remotely sensed im-
agery sources.
In this paper, we formally describe TorchGeo, propose and test
methods for sampling from large geospatial datasets, and test the
effect of ImageNet pretraining versus random weight initialization
on several benchmark datasets. We achieve close to state-of-the-
art results on all experimental datasets, despite focusing only on
creating simple and reproducible results to serve as baselines for
future work to build on. We further find that ImageNet pre-training
significantly improves spatial generalization performance in a land
cover mapping task. We believe that these results are interesting in
their own right, and that they highlight the importance of TorchGeo
to the larger machine learning community. TorchGeo is open source
and available on GitHub: https://github.com/microsoft/torchgeo.
2 DESIGN
Remotely sensed imagery datasets are usually formatted as scenes ,
i.e., tensors𝑋∈R𝐻×𝑊×𝐶, where𝐻is height,𝑊is width, and 𝐶is
the number of spectral channels, with corresponding spatial and
temporal metadata. This metadata includes a coordinate reference
system (CRS) that maps pixel coordinates to the surface of the Earth,
aspatial resolution (the size of each pixel when mapped onto thesurface of the Earth), spatial bounds (a bounding box representing
the area on Earth that the data covers), and a timestamp or time
range to indicate when the data was collected. We say that two
datasets,𝑋1and𝑋2, are pixel-aligned if𝑋1
𝑖,𝑗and𝑋2
𝑖,𝑗represent data
from the same positions on Earth for all 𝑖,𝑗. Most pairs of datasets
are not aligned by default. For example, 𝑋1and𝑋2can be captured
by two satellites in different orbits and will only have partially
overlapping spatial bounds, or 𝑋1will be satellite imagery while
𝑋2will be from a dataset of labels with a different CRS (see Figure
2). However, deep learning model training requires pixel-aligned
patches of imagery—i.e., smaller crops from large scenes. Most
models are trained with mini-batch gradient descent and require
input tensors in the format 𝐵×𝐻×𝑊×𝐶where𝐵is the number of
samples in a mini-batch and 𝑊and𝐻are constant over all samples
in the batch. At a higher level, training semantic segmentation
models requires pairs of pixel-aligned imagery and masks.
Aligning two datasets requires reprojecting the data from one
file in order to match the CRS of the other file, cropping the data
to the same spatial bounds, and resampling the data to correct for
differences in resolution or to establish the same underlying pixel
grid. Typically, these are performed as pre-processing steps using
GIS software such as QGIS, ArcGIS, or tools provided by GDAL—see
Listing 1 for an example of a GDAL command to align two data
layers. This requires some level of domain knowledge to perform
correctly and does not scale to large datasets as it requires creating
duplicates of the layer to be aligned. Further, this approach still
requires an implementation of a dataset or data loader that can
sample patches from the pre-processed imagery.
Stewart et al.
In TorchGeo, we facilitate this process by performing the align-
ment logic on the fly to create pixel-aligned patches of data sampled
from larger scenes. Specifically, we implement the alignment logic
in custom PyTorch Dataset classes that are indexed in terms of
spatiotemporal coordinates. Given a query in spatiotemporal coor-
dinates, a desired destination CRS, and a desired spatial resolution,
the custom dataset is responsible for returning the corresponding
reprojected and resampled data for the query. We further imple-
ment geospatial data samplers that generate queries according to
different criteria (e.g., randomly or in a regular grid pattern). See
Section 3 for a discussion on the implementation of these.
As our datasets are indexed by spatiotemporal coordinates, we
can easily compose datasets that represent different data layers by
specifying a valid area to sample from. For example, if we have two
datasets,𝐷1and𝐷2, we may want to sample data from the union
of the layers, 𝐷1∪𝐷2, if both layers cover disparate geospatial loca-
tions, or from the intersection of the layers, 𝐷1∩𝐷2, if one layer is
imagery and the other layer is labels. The latter is particularly pow-
erful for use in applications like multimodal learning [ 4] and data
fusion [ 74]. As we describe in the following section, we implement
generic dataset classes for a variety of common remotely-sensed
datasets (e.g., Landsat imagery) that can be composed in this way.
This allows users of the library to create their own multimodal
datasets without having to write custom code.
Most importantly, these abstractions that TorchGeo creates—
geospatial datasets and samplers—can be combined with a standard
PyTorch data loader class to produce fixed size batches of data to
be transferred to the GPU and used in training or inference. See
Listing 2 for a working example of TorchGeo code for setting up a
functional data loader that uses Landsat and Cropland Data Layer
(CDL) dataset implementations. Our approach trades off required
storage space for data loading time compared to pre-processing
all data layers, but, crucially, does not require knowledge of GIS
tooling. We benchmark our implementations in Section 4.3.
3 IMPLEMENTATION
The implementation of TorchGeo follows the design of other Py-
Torch domain libraries to reduce the amount of new concepts that
a user must learn to integrate it with their existing workflow. We
split TorchGeo up into the following submodules:
Datasets Our dataset implementations consist of both benchmark
datasets that allow users to interface with common datasets
used in RS literature and generic datasets that allow users to
interface with common geospatial data layers such as Land-
sat or Sentinel-2 imagery. Either of these types of datasets
can also be geospatial datasets , i.e., datasets that contain
geospatial metadata and can be sampled as such. These are
part of the core contribution of TorchGeo and we describe
them further in the following section.
Samplers We implement samplers for indexing into any of our
geospatial datasets . Our geospatial datasets are indexed by
bounding boxes in spatiotemporal coordinates (as opposed to
standard fixed-length datasets of images which are usually
indexed by an integer). The samplers generate bounding
boxes according to specific patterns: randomly across all
scenes in a dataset, random batches from single scenes ata time, or in grid patterns over scenes. Different sampling
patterns can be useful for different model training strategies,
or for running model inference over datasets.
Models Most existing model implementations (e.g., in torchvision)
are fixed to accept 3 channel inputs which are not compatible
with multispectral imagery. We provide implementations
(or wrappers around well-established implementations) of
common deep learning model architectures with variable-
sized inputs and pre-trained weights, e.g., models that use
all of the Sentinel-2 multispectral bands as inputs. We also
implement architectures from recent geospatial ML work
such as the Fully Convolutional Siamese Network [13].
Transforms Similar to existing model implementations, some ex-
isting deep learning packages do not support data augmenta-
tion methods for multi-spectral imagery. We provide wrap-
pers for augmentations in the Kornia [ 45] library (which
does support augmentations over arbitrary channels) and
implement transforms specific to geospatial data.
Trainers Finally, we implement model training recipes using the
PyTorch Lightning library [ 68]. These include both dataset-
specific training code, and general training routines, e.g., an
implementation of the BYOL self-supervision method [22].
3.1 Datasets
We organize datasets based on whether they are generic datasets
orbenchmark datasets and based on whether or not they contain
geospatial metadata—i.e., are a geospatial dataset .
Benchmark datasets are datasets released by the community that
consist of both inputs and target labels for a specific type of task
(scene classification, semantic segmentation, instance segmentation,
etc.). These may or may not also contain geospatial metadata that
allows them to be joined with other sources of data. In our opinion,
one of the strongest components of existing deep learning domain
libraries is the way that they make the use of existing datasets easy.
We aim to replicate this and, for example, include options that let
users automatically download the data for a corresponding dataset.
Table 1 lists the set of benchmark datasets that TorchGeo currently
supports.
Generic datasets are not created with a specific task in mind,
but instead represent layers of geospatial data that can be used for
any purpose. For example, we implement datasets for represent-
ing collections of scenes of Landsat imagery that let users index
into the imagery and use it in arbitrary PyTorch-based pipelines.
These are not limited to imagery; for example, we also implement
a dataset representing the Cropland Data Layer labels, an annual
raster layer that gives the estimated crop type or land cover at a
30 m/px resolution for the contiguous United States (see Table 2).
3.2 Samplers
As our geospatial datasets are indexed with bounding boxes using
spatiotemporal coordinates and do not have a concept of a dataset
“length”, they cannot be sampled from by choosing a random inte-
ger. We provide three types of samplers for different situations: a
RandomGeoSampler that returns a fixed-sized bounding box from
the valid spatial extent of a dataset uniformly at random, a Ran-
domBatchGeoSampler that returns a set of randomly positioned
TorchGeo: Deep Learning With Geospatial Data
Dataset Task Source # Samples # Categories Size (px) Resolution (m) Bands
ADVANCE [26] C Google Earth, Freesound 5,075 13 512×512 0.5 RGB
BigEarthNet [59] C Sentinel-1/2 590,326 19–43 120×120 10 SAR, MSI
EuroSAT [24] C Sentinel-2 27,000 10 64×64 10 MSI
PatternNet [76] C Google Earth 30,400 38 256×256 0.06–5 RGB
RESISC45 [10] C Google Earth 31,500 45 256×256 0.2–30 RGB
So2Sat [77] C Sentinel-1/2 400,673 17 32×32 10 SAR, MSI
UC Merced [69] C USGS National Map 21,000 21 256×256 0.3 RGB
COWC [40] C, R UAV 388,435 2 256×256 0.15 RGB
Tropical Cyclone [37] R GOES 8–16 108,110 - 256×256 4K–8K MSI
USAVars [49] R NAIP 100K - - 4 MSI
Benin Cashews [29] S Airbus Pléiades 70 6 1, 186×1,122 0.5 MSI
ETCI 2021 Floods [41] S Sentinel-1 66,810 2 256×256 5–20 SAR
GID-15 [63] S Gaofen-2 150 15 6, 800×7,200 3 RGB
Kenya Crop Type [17] S Sentinel-2 4,688 7 3, 035×2,016 10 MSI
LandCover.ai [6] S Aerial 10,674 5 512×512 0.25–0.5 RGB
Potsdam [51] S Aerial 38 6 6, 000×6,000 0.05 MSI
SEN12MS [54] S Sentinel-1/2, MODIS 180,662 33 256×256 10 SAR, MSI
Vaihingen [51] S Aerial 33 6 1,281–3,816 0.09 RGB
FAIR1M [61] O Gaofen, Google Earth 15K 37 1, 024×1,024 0.3–0.8 RGB
IDTReeS [21] O, C Aerial 591 33 200×200 0.1–1 RGB
NWPU VHR-10 [11] O, I Google Earth, Vaihingen 800 10 358–1,728 0.08–2 RGB
SpaceNet [65] I WorldView, Planet Dove 1,889–28,728 2 102–900 0.5–4 MSI
ZueriCrop [64] I, T Sentinel-2 116K 48 24×24 10 MSI
Seasonal Contrast [35] T Sentinel-2 100K–1M - 264×264 10 MSI
LEVIR-CD+ [55] D Google Earth 985 2 1, 024×1,024 0.5 RGB
OSCD [14] D Sentinel-2 24 2 40–1,180 60 MSI
xView2 [23] D Maxar 3,732 4 1, 024×1,024 0.8 RGB
C = classification, R = regression, S = semantic segmentation, O = object detection, I = instance segmentation, T = time series, D = change detection
Table 1: Benchmark datasets implemented in TorchGeo.
fixed-sized bounding boxes from a random scene within a dataset,
and a GridGeoSampler that returns bounding boxes in a grid pattern
over subsequent scenes within a dataset. These samplers are bench-
marked in Section 4.2. This abstraction also allows for methods that
rely on specific data sampling patterns. For example, Tile2Vec [ 28]
relies on sampling triplets of imagery where two of the images are
close to each other in space while the third is distant. This logic can
be implemented in several lines of code as a custom sampler class,
that would then operate over any of the generic imagery datasets.
Finally, all TorchGeo samplers are compatible with PyTorch data
loader objects, allowing them to be fit into any PyTorch-based
pipeline.4 EXPERIMENTS AND RESULTS
4.1 Datasets
We use the following datasets in our experiments:
Landsat and CDL A collection of multispectral imagery from 114
Landsat 8 [ 52] scenes and the Cropland Data Layer (CDL) [ 8]
dataset. This data is 151 GB on disk and is stored in cloud
optimized GeoTIFF (COG) format. We use this dataset to
benchmark our GeoDataset and sampler implementations.
So2Sat Aclassification dataset consisting of 400,673 image patches
classified with one of 42 local climate zone labels [ 77]. The
patches are 30×30pixels in size with 18 channels consisting
of Sentinel-1 and Sentinel-2 bands. They are sampled from
different urban areas around the globe. We use the second
version of the dataset as described on the project’s GitHub
Stewart et al.
Type Dataset
ImageryLandsat [52]
Sentinel [16]
NAIP [18]
ASTER Global DEM [1]
European DEM
LabelsAboveground Woody Biomass [67]
Canadian Buildings Footprints [60]
Chesapeake Land Cover [46]
Global Mangrove Distribution [56]
Cropland Data Layer [8]
EDDMapS [5]
EnviroAtlas [48]
Esri 2020 Land Cover [30]
GBIF [19]
GlobBiomass [53]
iNaturalist [19]
Open Buildings [57]
Table 2: Generic datasets implemented in TorchGeo.
page1in which the training split consists of data from 42
cities around the world, the validation split consists of the
western half of 10 other cities, and the testing split covers
the eastern half of the 10 remaining cities.
LandCover.ai Asemantic segmentation dataset consisting of high-
resolution (0.5 m/px and 0.25 m/px) RGB aerial imagery from
41 tiles over Poland where each pixel has been classified as
one of five land cover classes [ 6]. The scenes are divided into
10,674 512×512pixel patches and split according to the
script on the dataset webpage2.
Chesapeake Land Cover Asemantic segmentation dataset con-
siting of high-resolution (1 m/px) imagery from the USDA’s
National Agriculture Imagery Program (NAIP) and high-
resolution (1 m/px) 6-class land cover labels from the Chesa-
peake Conservancy [ 46]. The dataset contains imagery and
land cover masks for parts of six states in the Northeastern
US. The data for each state is split into ∼7×6km tiles, then
divided into pre-defined train, validation, and test splits3.
RESISC45 Aclassification dataset consisting of 31,500 256×256
pixel RGB image patches of varying spatial resolutions where
each patch is classified into one of 45 classes [ 75]. As the
dataset does not have official splits, we use the train/val/test
splits defined in Neumann et al. [42].
ETCI 2021 Asemantic segmentation dataset used in a flood detec-
tion competition [ 41]. It consists of 66,810 256×256pixel
Sentinel-1 SAR images. We use the official train/test splits,
and we further randomly subdivide the train split 80/20 into
train/val splits.
EuroSAT Aclassification dataset consisting of 27,000 64×64pixel
Sentinel-2 images and 10 target classes [ 24]. We use the
train/val/test splits defined in Neumann et al. [42].
1https://github.com/zhu-xlab/So2Sat-LCZ42
2https://landcover.ai
3https://lila.science/datasets/chesapeakelandcoverUC Merced Aclassification dataset consisting of 21,000 256×256
pixel RGB images from the USGS National Map and 21 target
classes [ 69]. We use the train/val/test splits defined in Neu-
mann et al. [42].
COWC Counting Aregression dataset consisting of 317,230 train-
ing and 81,161 testing images [ 40]. Each 256×256pixel
image is labeled with the number of cars in the image. We
reserved 81,161 images from the training set for validation.
4.2 Data loader benchmarks
We first benchmark the speed at which TorchGeo can sample
patches of imagery and masks from the Landsat and CDL dataset.
We believe this dataset is typical of a large class of geospatial ma-
chine learning problems—where users have access to a large amount
of satellite imagery scenes covering a broad spatial extent and, sep-
arately, per-pixel label masks where each scene is not necessarily
projected in the same coordinate reference system. The end goal
of such a problem is to train a model with pixel-aligned patches
of imagery and label masks as described in Section 2. As such, we
measure the rate at which our dataset and sampler implementations
can provide patches to a GPU for training and inference.
In Figure 3a, we calculate the rate at which samples of vary-
ing batch size can be drawn from a GeoDataset using various
GeoSampler implementations. Compared to the other samplers,
GridGeoSampler is significantly faster due to the repeated access
of samples that are already in GDAL’s least recently used (LRU)
cache. For small batch sizes, RandomGeoSampler and RandomBat-
chGeoSampler are almost identical, since overlap between patches
is uncommon. However, for larger batch sizes, RandomBatchGeoSam-
pler starts to outperform RandomGeoSampler as the cache is used
more effectively.
In Figure 3b, we demonstrate the difference that preprocessing
and caching data makes. This is most easily demonstrated by Grid-
GeoSampler and to a lesser extent the other samplers. GDAL’s
LRU cache only saves raw data loading times, so warping must
always be done on the fly if the dataset CRSs or resolutions do not
match. When the necessary storage is available, preprocessing the
data ahead of time can lead to significantly faster sampling rates.
Although RandomGeoSampler and RandomBatchGeoSampler are
much slower than GridGeoSampler, most users will only need to
use GridGeoSampler for inference due to our pre-trained model
weights.
4.3 Dataset benchmarks
We also use TorchGeo to create simple, reproducible benchmark
results on 8 of the datasets described in Section 4.1. We report
the uncertainty in the results such that future work can evaluate
whether a proposed improvement is due to methodological innova-
tion or variance in results due to the stochastic nature of training
deep learning models. To ensure reproducibility, we include a model
training and evaluation framework in TorchGeo based around the
PyTorch Lightning library and release all of our results. To quantify
uncertainty, we report the mean and standard deviation metrics
calculated over 10 training runs with different random seeds. The
torchmetrics library [ 7] was used to compute all performance values
TorchGeo: Deep Learning With Geospatial Data
(a) Sampler performance vs. batch size
16 32 64 128 256
Batch size100200300400500600700800900Sampling rate
(patches/sec)RandomGeoSampler
RandomBatchGeoSampler
GridGeoSampler (b) Effect of preprocessing and caching
GridGeoSampler
RandomBatchGeoSamplerRandomGeoSampler0100200300400500600700Sampling rate
(patches/sec)Raw Data, Not Cached
Preprocessed, Not Cached
Raw Data, Cached
Preprocessed, Cached
Figure 3: Sampling performance of various GeoSampler implementations. (a) Solid lines represent average sampling rate,
while shaded region represents minimum and maximum performance across random seeds. (b) Average sampling rate under
different data loading conditions.
(overall top-1 accuracy for the classification datasets, mean inter-
section over union (mIoU) for the semantic segmentation datasets,
and root mean square error (RMSE) for the regression dataset).
Our main results are shown in Table 3. We find that our sim-
ple training setup achieves competitive results on several datasets.
The in-domain pre-training method in Neumann et al . [42] trains
models starting from ImageNet weights, then further trains on re-
mote sensing (in domain) datasets, before fine-tuning on the actual
target dataset, which performs better than simply starting from
ImageNet weights. In contrast, our best result across the RESISC45,
EuroSAT, and UC Merced datasets come from simply using Ima-
geNet pre-trained models and training on the target dataset with a
low learning rate. The difference between these approaches is likely
entirely due to different learning rate selection in the hyperparam-
eter search, and data augmentation in the in-domain pre-training
setup. On the So2Sat dataset, we find that the ImageNet pre-trained
models are able to achieve similar results as in-domain pretraining,
but only when using all Sentinel-2 bands. The previously reported
baseline methods on the LandCover.ai dataset all use a DeepLabV3+
segmentation model with a Xception71 + Dense Prediction Cell
(DPC) encoder that has been pre-trained on Cityscapes. We are able
to achieve a result within 0.75% mIoU of this setup using a simple
U-Net and ResNet50 encoder pre-trained on ImageNet. Finally, we
report the first set of basic benchmark results on the Chesapeake
Land Cover dataset.
4.4 Effect of ImageNet pre-training on
generalization performance
Two of the datasets we test with, So2Sat and Chesapeake Land
Cover, contain splits that are designed to measure the generaliza-
tion performance of a model. The validation and test splits from
So2Sat include data from urban areas that are not included in the
training split while the Chesapeake Land Cover dataset contains
separate splits for six different states. In these settings, we observea large performance boost when training models from ImageNet
weights versus from a random initialization, however we do not ob-
serve the boost on in-domain data. Table 4 shows the performance
of models that are trained on the Delaware split and evaluated on
the test splits from every state. The in-domain performance (i.e., in
Delaware) of the ImageNet pre-trained model and a model trained
from scratch are the same, however, in every other setting the Ima-
geNet pre-trained model performs better. In all cases but Maryland,
this difference is greater than 6 points of mIoU with the most ex-
treme difference being 12 points in Virginia. In the So2Sat case, we
find that most models are not able to significantly reduce validation
loss (where in this case validation data is out-of-domain), while,
unsurprisingly, achieving near perfect results on the training data.
Despite this overfitting, there remains a large gap between the best
and worst models, with ImageNet pre-trained models achieving
+7% and +10% accuracy over randomly initialized models. These re-
sults extend existing lines of research in computer vision that show
how pre-training can improve out-of-domain performance [ 25],
and how, specifically, ImageNet pretraining is useful for transfer
learning tasks [27].
5 DISCUSSION
We introduce TorchGeo, a Python package for enabling deep learn-
ing with geospatial data. TorchGeo provides data loaders for com-
mon geospatial datasets, composable data loaders for uncurated
geospatial raster and vector data, samplers appropriate for geospa-
tial data, models pre-trained on satellite imagery, multispectral
transforms, and model trainers. Importantly, TorchGeo allows users
to bypass the common pre-processing steps necessary to align
geospatial imagery with labels and performs this pre-processing
on the fly. We benchmark TorchGeo data loader speed and demon-
strate how TorchGeo can be used to create reproducible benchmark
results in several geospatial datasets.
Finally, TorchGeo serves as a platform for performing geospa-
tial machine learning research. Existing works in self-supervised
Stewart et al.
Dataset Method Weight Initialization Bands Performance
RESISC45 [10]ResNet50 ImageNet RGB 95.42 ±0.23%
ResNet18 random RGB 79.90 ±0.25%
ResNet50 v2 [42] In domain RGB 96.86%
ViT B/16 [58] ImageNet-21k RGB 96.80%
ResNet50 [72] Sup-Rotation-100% RGB 96.30%
So2Sat [77]ResNet50 ImageNet (+ random) MSI 63.99±1.38%
ResNet50 random MSI 56.82 ±4.32%
ResNet50 ImageNet RGB 59.82 ±0.94%
ResNet50 random RGB 49.46 ±2.67%
ResNeXt + CBAM [77] random MSI 61%
ResNet50 v2 [42] In domain RGB 63.25%
LandCover.ai [6]U-Net, ResNet50 encoder ImageNet RGB 84.81 ±0.21%
U-Net, ResNet50 encoder random RGB 79.73 ±0.67%
DeepLabv3+, Xception71 with
DPC encoder [6]Cityscapes RGB 85.56%
Chesapeake Land Cover [46]
Delaware splitU-Net, ResNet50 encoder ImageNet (+ random) MSI 69.40±1.39%
U-Net, ResNet18 encoder random MSI 68.99±0.84%
ETCI 2021 [41] U-Net, ResNet50 encoder random SAR 45.77±3.19%
EuroSAT [24]ResNet50 ImageNet (+ random) MSI 97.86 ±0.23%
ResNet50 random MSI 96.07 ±0.28%
ResNet50 ImageNet RGB 98.11 ±0.31%
ResNet50 random RGB 87.33 ±0.76%
ResNet50 v2 [42] In domain RGB 99.20%
UC Merced [69]ResNet50 ImageNet RGB 98.15% ±0.46%
ResNet50 v2 [42] In domain RGB 99.61%
COWC Counting [40]ResNet50 ImageNet RGB 0.573±0.005
ResNet18 ImageNet RGB 0.667 ±0.007
ResCeption [40] random RGB 0.657
Table 3: Benchmark results comparing TorchGeo trained models to previously reported results over 8 datasets. Classification
dataset results are reported as overall top-1 accuracy, semantic segmentation dataset results are reported as mean class IoU, and
regression dataset results are reported as RMSE. Results from TorchGeo models are reported as the mean with one standard
deviation over 10 training runs from different random seeds. Results from related work are reported as is.
Weight init Delaware Maryland New York Pennsylvania Virginia West Virginia
ImageNet (+ random) 69.40±1.39% 59.57±0.70% 57.95±1.10% 55.13±1.25% 45.56±1.54% 20.76±1.95%
random 68.99 ±0.84% 57.30±0.78% 49.26±2.40% 47.67±2.40% 33.14±3.73% 14.95±2.72%
Table 4: Mean IoU performance of models trained in Delaware, with and without ImageNet weight initialization, on the test
splits from Chesapeake Land Cover dataset.
learning with geospatial data rely on spatiotemporal metadata and
can be naturally implemented in TorchGeo and scaled over large
amounts of geospatial imagery without the need for pre-processing
steps. Similarly, augmentation methods appropriate for training
with geospatial imagery are under-explored, however, can be easily
integrated with TorchGeo. Other interesting future research direc-
tions include building inductive biases appropriate for geospatial
imagery into deep learning models (similar to the work done on ro-
tation equivariant networks [ 36]), data fusion techniques (e.g., howto incorporate spatial information into models, or appropriately
use multi-modal layers), and learning shape-based models. Finally,
TorchGeo exposes a catalog of benchmark geospatial datasets (Ta-
ble 1) through a common interface, and, with the results in this
paper, has begun to include corresponding benchmark results. This
makes it easy for researchers to compare new ideas to existing
work without having to repeat expensive computations. We hope
TorchGeo can help drive advances at the intersection of machine
learning and remote sensing.
TorchGeo: Deep Learning With Geospatial Data
ACKNOWLEDGMENTS
This research is part of the Blue Waters sustained-petascale comput-
ing project, which is supported by the National Science Foundation
(awards OCI-0725070 and ACI-1238993), the State of Illinois, and
as of December, 2019, the National Geospatial-Intelligence Agency.
Blue Waters is a joint effort of the University of Illinois at Urbana-
Champaign and its National Center for Supercomputing Applica-
tions. The research was supported by NSF grants IIS 21-31335, OAC
21-30835, DBI 20-21898, and a C3.ai research award. We’d like to
thank TorchGeo contributors for their efforts in creating the library.
We’d also like to thank Siyu Yang, Md Nasir, Shahrzad Gholami,
and Thomas Roca for their feedback and support.
REFERENCES
[1]Michael Abrams, Robert Crippen, and Hiroyuki Fujisada. 2020. ASTER global
digital elevation model (GDEM) and ASTER global water body dataset (ASTWBD).
Remote Sensing 12, 7 (2020), 1156.
[2]Wingtra AG. 2021. WingtraOne mapping drone and Sony RX1R II camera.
https://wingtra.com/mapping-drone-wingtraone/image-quality/
[3]Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke,
David Lobell, and Stefano Ermon. 2021. Geography-aware self-supervised learn-
ing. In Proceedings of the IEEE/CVF International Conference on Computer Vision .
10181–10190.
[4]Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2019. Multi-
modal Machine Learning: A Survey and Taxonomy. IEEE Transactions on Pattern
Analysis and Machine Intelligence 41, 2 (2019), 423–443. https://doi.org/10.1109/
TPAMI.2018.2798607
[5]Charles T Bargeron and David J Moorhead. 2007. EDDMapS—early detection and
distribution mapping system for the southeast exotic pest plant council. Wildland
weeds 10, 4 (2007), 4–8.
[6]Adrian Boguszewski, Dominik Batorski, Natalia Ziemba-Jankowska, Tomasz
Dziedzic, and Anna Zambrzycka. 2021. LandCover.ai: Dataset for Automatic
Mapping of Buildings, Woodlands, Water and Roads From Aerial Imagery. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition .
1102–1110.
[7]Jirka Borovec et al .2020. TorchMetrics: Machine learning metrics for distributed,
scalable PyTorch applications. https://github.com/PyTorchLightning/metrics
[8]Claire Boryan, Zhengwei Yang, Rick Mueller, and Mike Craig. 2011. Monitoring
US agriculture: the US Department of Agriculture, National Agricultural Statistics
Service, Cropland Data Layer Program. Geocarto International 26, 5 (2011), 341–
358.
[9] Gustau Camps-Valls, Devis Tuia, Xiao Xiang Zhu, and Markus Reichstein. 2021.
Deep learning for the Earth Sciences: A comprehensive approach to remote sensing,
climate science and geosciences . John Wiley & Sons.
[10] Gong Cheng, Junwei Han, and Xiaoqiang Lu. 2017. Remote Sensing Image
Scene Classification: Benchmark and State of the Art. Proc. IEEE 105, 10 (2017),
1865–1883. https://doi.org/10.1109/JPROC.2017.2675998
[11] Gong Cheng, Junwei Han, Peicheng Zhou, and Lei Guo. 2014. Multi-class geospa-
tial object detection and geographic image classification based on collection of
part detectors. ISPRS Journal of Photogrammetry and Remote Sensing 98 (2014),
119–132. https://doi.org/10.1016/j.isprsjprs.2014.10.002
[12] Alex Clark et al .2010. Pillow: The friendly PIL fork (Python Imaging Library).
https://github.com/python-pillow/Pillow
[13] Rodrigo Caye Daudt, Bertr Le Saux, and Alexandre Boulch. 2018. Fully convo-
lutional Siamese networks for change detection. In 2018 25th IEEE International
Conference on Image Processing (ICIP) . IEEE, 4063–4067.
[14] Rodrigo Caye Daudt, Bertr Le Saux, Alexandre Boulch, and Yann Gousseau. 2018.
Urban change detection for multispectral earth observation using convolutional
neural networks. In IGARSS 2018-2018 IEEE International Geoscience and Remote
Sensing Symposium . IEEE, 2115–2118.
[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im-
ageNet: A large-scale hierarchical image database. In 2009 IEEE conference on
computer vision and pattern recognition . Ieee, 248–255.
[16] Matthias Drusch, Umberto Del Bello, Sébastien Carlier, Olivier Colin, Veron-
ica Fernandez, Ferran Gascon, Bianca Hoersch, Claudia Isola, Paolo Laberinti,
Philippe Martimort, et al .2012. Sentinel-2: ESA’s optical high-resolution mission
for GMES operational services. Remote sensing of Environment 120 (2012), 25–36.
[17] Radiant Earth Foundation. 2020. CV4A Competition Kenya Crop Type Dataset.
https://doi.org/10.34911/RDNT.DW605X Version 1.0, Radiant MLHub.
[18] USDA Farm Service Agency (FSA). 2015. National Agriculture Imagery Program
(NAIP). USDA Geospatial Data Gateway.
[19] GBIF. 2020. GBIF: The Global Biodiversity Information Facility. gbif.org[20] GDAL/OGR contributors. 2022. GDAL/OGR Geospatial Data Abstraction software
Library . Open Source Geospatial Foundation. https://doi.org/10.5281/zenodo.
5884351
[21] Sarah J. Graves, Sergio Marconi, Dylan Stewart, Ira Harmon, Ben G. Weinstein,
Yuzi Kanazawa, Victoria M. Scholl, Maxwell B. Joseph, Joseph McClinchy, et al .
2021. Data science competition for cross-site delineation and classification of
individual trees from airborne remote sensing data. bioRxiv (2021).
[22] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhao-
han Daniel Guo, et al .2020. Bootstrap your own latent: A new approach to
self-supervised learning. arXiv preprint arXiv:2006.07733 (2020).
[23] Ritwik Gupta, Richard Hosfelt, Sandra Sajeev, Nirav Patel, Bryce Goodman, Jigar
Doshi, Eric Heim, Howie Choset, and Matthew Gaston. 2019. xBD: A Dataset for
Assessing Building Damage from Satellite Imagery. arXiv:1911.09296 [cs.CV]
[24] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. 2019.
EuroSAT: A novel dataset and deep learning benchmark for land use and land
cover classification. IEEE Journal of Selected Topics in Applied Earth Observations
and Remote Sensing 12, 7 (2019), 2217–2226.
[25] Dan Hendrycks, Kimin Lee, and Mantas Mazeika. 2019. Using pre-training
can improve model robustness and uncertainty. In International Conference on
Machine Learning . PMLR, 2712–2721.
[26] Di Hu, Xuhong Li, Lichao Mou, Pu Jin, Dong Chen, Liping Jing, Xiaoxiang Zhu,
and Dejing Dou. 2020. Cross-Task Transfer for Geotagged Audiovisual Aerial
Scene Recognition. In Computer Vision – ECCV 2020 , Andrea Vedaldi, Horst
Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International
Publishing, Cham, 68–84.
[27] Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. 2016. What makes ImageNet
good for transfer learning? arXiv preprint arXiv:1608.08614 (2016).
[28] Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, and Ste-
fano Ermon. 2019. Tile2Vec: Unsupervised representation learning for spatially
distributed data. In Proceedings of the AAAI Conference on Artificial Intelligence ,
Vol. 33. 3967–3974.
[29] Z. Jin, C. Lin, C. Weigl, J. Obarowski, and D. Hale. 2021. Smallholder Cashew
Plantations in Benin. https://doi.org/10.34911/rdnt.hfv20i Radiant MLHub.
[30] Krishna Karra, Caitlin Kontgis, Zoe Statman-Weil, Joseph C. Mazzariello, Mark
Mathis, and Steven P. Brumby. 2021. Global land use / land cover with Sentinel 2
and deep learning. In 2021 IEEE International Geoscience and Remote Sensing Sym-
posium IGARSS . 4704–4707. https://doi.org/10.1109/IGARSS47720.2021.9553499
[31] Krishna Karra, Caitlin Kontgis, Zoe Statman-Weil, Joseph C Mazzariello, Mark
Mathis, and Steven P Brumby. 2021. Global land use/land cover with Sentinel
2 and deep learning. In 2021 IEEE International Geoscience and Remote Sensing
Symposium IGARSS . IEEE, 4704–4707.
[32] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,
Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. 2018. Progressive
neural architecture search. In Proceedings of the European conference on computer
vision (ECCV) . 19–34.
[33] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.
arXiv preprint arXiv:1711.05101 (2017).
[34] Dengsheng Lu, Qi Chen, Guangxing Wang, Lijuan Liu, Guiying Li, and Emilio
Moran. 2016. A survey of remote sensing-based aboveground biomass estimation
methods in forest ecosystems. International Journal of Digital Earth 9, 1 (2016),
63–105.
[35] Oscar Mañas, Alexandre Lacoste, Xavier Giro-i Nieto, David Vazquez, and Pau
Rodriguez. 2021. Seasonal Contrast: Unsupervised Pre-Training from Uncurated
Remote Sensing Data. arXiv preprint arXiv:2103.16607 (2021).
[36] Diego Marcos, Michele Volpi, Benjamin Kellenberger, and Devis Tuia. 2018. Land
cover mapping at very high resolution with rotation equivariant CNNs: Towards
small yet accurate models. ISPRS Journal of Photogrammetry and Remote Sensing
145 (2018), 96–107.
[37] Manil Maskey, Rahul Ramachandran, Muthukumaran Ramasubramanian, Iksha
Gurung, Brian Freitag, Aaron Kaulfus, Drew Bollinger, Daniel J. Cecil, and Jeffrey
Miller. 2020. Deepti: Deep-Learning-Based Tropical Cyclone Intensity Estimation
System. IEEE Journal of Selected Topics in Applied Earth Observations and Remote
Sensing 13 (2020), 4271–4281. https://doi.org/10.1109/JSTARS.2020.3011907
[38] Francisco Massa et al .2016. torchvision: Datasets, Transforms and Models specific
to Computer Vision. https://github.com/pytorch/vision
[39] David J. Mulla. 2013. Twenty five years of remote sensing in precision agriculture:
Key advances and remaining knowledge gaps. Biosystems Engineering 114, 4
(2013), 358–371. https://doi.org/10.1016/j.biosystemseng.2012.08.009 Special
Issue: Sensing Technologies for Sustainable Agriculture.
[40] T Nathan Mundhenk, Goran Konjevod, Wesam A Sakla, and Kofi Boakye. 2016.
A large contextual dataset for classification, detection and counting of cars with
deep learning. In European Conference on Computer Vision . Springer, 785–800.
[41] NASA-IMPACT. 2021. ETCI 2021 Competition on Flood Detection. https://nasa-
impact.github.io/etci2021/
[42] Maxim Neumann, Andre Susano Pinto, Xiaohua Zhai, and Neil Houlsby.
2019. In-domain representation learning for remote sensing. arXiv preprint
arXiv:1911.06721 (2019).
Stewart et al.
[43] NOAA Office of Satellite and Product Operations. 1994. NOAA Geostationary
Operational Environmental Satellite (GOES) I-M and N-P Series Imager Data.
NOAA National Centers for Environmental Information. https://doi.org/10.
25921/Z9JQ-K976
[44] Jay S Pearlman, Pamela S Barry, Carol C Segal, John Shepanski, Debra Beiso, and
Stephen L Carman. 2003. Hyperion, a space-based imaging spectrometer. IEEE
Transactions on Geoscience and Remote Sensing 41, 6 (2003), 1160–1173.
[45] Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, and Gary Bradski.
2020. Kornia: an open source differentiable computer vision library for PyTorch.
InProceedings of the IEEE/CVF Winter Conference on Applications of Computer
Vision . 3674–3683.
[46] Caleb Robinson, Le Hou, Kolya Malkin, Rachel Soobitsky, Jacob Czawlytko,
Bistra Dilkina, and Nebojsa Jojic. 2019. Large Scale High-Resolution Land Cover
Mapping With Multi-Resolution Data. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition . 12726–12735.
[47] John Rogan, Janet Franklin, Doug Stow, Jennifer Miller, Curtis Woodcock, and Dar
Roberts. 2008. Mapping land-cover modifications over large areas: A comparison
of machine learning algorithms. Remote Sensing of Environment 112, 5 (2008),
2272–2283.
[48] Esther Rolf, Nikolay Malkin, Alexandros Graikos, Ana Jojic, Caleb Robinson, and
Nebojsa Jojic. 2021. Resolving label uncertainty with implicit generative models.
(2021).
[49] Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar,
Miyabi Ishihara, Benjamin Recht, and Solomon Hsiang. 2021. A generalizable and
accessible approach to machine learning with global satellite imagery. Nature
communications 12, 1 (2021), 1–11.
[50] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste,
Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques,
Anna Waldman-Brown, et al .2019. Tackling climate change with machine
learning. arXiv preprint arXiv:1906.05433 (2019).
[51] Franz Rottensteiner, Gunho Sohn, Jaewook Jung, Markus Gerke, Caroline Bail-
lard, Sebastien Benitez, and Uwe Breitkopf. 2012. The ISPRS benchmark on
urban object classification and 3D building reconstruction. ISPRS Annals of the
Photogrammetry, Remote Sensing and Spatial Information Sciences I-3 (2012), Nr. 1
1, 1 (2012), 293–298.
[52] David P Roy, Michael A Wulder, Thomas R Loveland, Curtis E Woodcock,
Richard G Allen, Martha C Anderson, Dennis Helder, James R Irons, David M
Johnson, Robert Kennedy, et al .2014. Landsat-8: Science and product vision for
terrestrial global change research. Remote sensing of Environment 145 (2014),
154–172.
[53] Maurizio Santoro. 2018. GlobBiomass - global datasets of forest biomass. https:
//doi.org/10.1594/PANGAEA.894711
[54] Michael Schmitt, Lloyd Haydn Hughes, Chunping Qiu, and Xiao Xiang Zhu.
2019. SEN12MS–A Curated Dataset of Georeferenced Multi-Spectral Sentinel-1/2
Imagery for Deep Learning and Data Fusion. arXiv preprint arXiv:1906.07789
(2019).
[55] Li Shen, Yao Lu, Hao Chen, Hao Wei, Donghai Xie, Jiabao Yue, Rui Chen, Yue
Zhang, Ao Zhang, Shouye Lv, et al .2021. S2Looking: A Satellite Side-Looking
Dataset for Building Change Detection. arXiv preprint arXiv:2107.09244 (2021).
[56] M. Simard, T. Fatoyinbo, C. Smetanka, V. H. Rivera-Monroy, E. Castaneda-Mova,
N. Thomas, and T. Van der Stocken. 2019. Global Mangrove Distribution, Above-
ground Biomass, and Canopy Height. https://doi.org/10.3334/ORNLDAAC/1665
[57] Wojciech Sirko, Sergii Kashubin, Marvin Ritter, Abigail Annkah, Yasser Salah Ed-
dine Bouchareb, Yann Dauphin, Daniel Keysers, Maxim Neumann, Moustapha
Cisse, and John Quinn. 2021. Continental-scale building detection from high
resolution satellite imagery. arXiv preprint arXiv:2107.12283 (2021).
[58] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob
Uszkoreit, and Lucas Beyer. 2021. How to train your ViT? Data, Augmentation,
and Regularization in Vision Transformers. arXiv preprint arXiv:2106.10270
(2021).
[59] Gencer Sumbul, Marcela Charfuelan, Begüm Demir, and Volker Markl. 2019.
Bigearthnet: A Large-Scale Benchmark Archive for Remote Sensing Image Un-
derstanding. In IGARSS 2019 - 2019 IEEE International Geoscience and Remote
Sensing Symposium . 5901–5904. https://doi.org/10.1109/IGARSS.2019.8900532
[60] Timu Sumisu. 2019. Canadian Building Footprints. https://github.com/microsoft/
CanadianBuildingFootprints
[61] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin
Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Cheng
Wang, and Kun Fu. 2021. FAIR1M: A Benchmark Dataset for Fine-grained Object
Recognition in High-Resolution Remote Sensing Imagery.
[62] Maxar Technologies. 2021. WorldView satellite imagery.
[63] Xin-Yi Tong, Gui-Song Xia, Qikai Lu, Huanfeng Shen, Shengyang Li, Shucheng
You, and Liangpei Zhang. 2020. Land-cover classification with high-resolution
remote sensing images using transferable deep models. Remote Sensing of Envi-
ronment 237 (2020), 111322.
[64] Mehmet Ozgur Turkoglu, Stefano D’Aronco, Gregor Perich, Frank Liebisch,
Constantin Streit, Konrad Schindler, and Jan Dirk Wegner. 2021. Crop mapping
from image time series: deep learning with multi-scale label hierarchies. arXivpreprint arXiv:2102.08820 (2021).
[65] Adam Van Etten, Dave Lindenbaum, and Todd M Bacastow. 2018. SpaceNet:
A remote sensing dataset and challenge series. arXiv preprint arXiv:1807.01232
(2018).
[66] Cees J Van Westen. 2013. Remote sensing and GIS for natural hazards assessment
and disaster risk management. Treatise on geomorphology 3 (2013), 259–298.
[67] Global Forest Watch. 2002. Global forest watch. World Resources Institute, Wash-
ington, DC Available from http://www. globalforestwatch. org (accessed March 2002)
(2002).
[68] Falcon William and The PyTorch Lightning team. 2019. PyTorch Lightning.
https://doi.org/10.5281/zenodo.3828935
[69] Yi Yang and Shawn Newsam. 2010. Bag-of-visual-words and spatial extensions
for land-use classification. In Proceedings of the 18th SIGSPATIAL international
conference on advances in geographic information systems . 270–279.
[70] Yao-Yuan Yang, Moto Hira, Zhaoheng Ni, Anjali Chourdia, Artyom Astafurov,
Caroline Chen, Ching-Feng Yeh, Christian Puhrsch, David Pollack, Dmitriy Gen-
zel, Donny Greenberg, Edward Z. Yang, Jason Lian, Jay Mahadeokar, Jeff Hwang,
Ji Chen, Peter Goldsborough, Prabhat Roy, Sean Narenthiran, Shinji Watanabe,
Soumith Chintala, Vincent Quenneville-Bélair, and Yangyang Shi. 2021. Tor-
chAudio: Building Blocks for Audio and Speech Processing. arXiv preprint
arXiv:2110.15018 (2021).
[71] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable
are features in deep neural networks? arXiv preprint arXiv:1411.1792 (2014).
[72] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos
Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann,
Alexey Dosovitskiy, et al .2019. A large-scale study of representation learning
with the visual task adaptation benchmark. arXiv preprint arXiv:1910.04867
(2019).
[73] Guanheng Zhang et al .2016. torchtext: Data loaders and abstractions for text
and NLP. https://github.com/pytorch/text
[74] Jixian Zhang. 2010. Multi-source remote sensing data fusion: status and trends.
International Journal of Image and Data Fusion 1, 1 (2010), 5–24.
[75] Wei Zhang, Ping Tang, and Lijun Zhao. 2019. Remote sensing image scene
classification using CNN-CapsNet. Remote Sensing 11, 5 (2019), 494.
[76] Weixun Zhou, Shawn Newsam, Congmin Li, and Zhenfeng Shao. 2018. Pat-
ternNet: A benchmark dataset for performance evaluation of remote sensing
image retrieval. ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018),
197–209.
[77] Xiao Xiang Zhu, Jingliang Hu, Chunping Qiu, Yilei Shi, Jian Kang, Lichao Mou,
Hossein Bagheri, Matthias Häberle, Yuansheng Hua, Rong Huang, et al .2019.
So2Sat LCZ42: A benchmark dataset for global local climate zones classification.
arXiv preprint arXiv:1912.12171 (2019).
A DATA LOADER BENCHMARKING
In order to benchmark the performance of our data loaders, we
download 114 Landsat 8 collection 2 level-2 scenes and 1 Cropland
Data Layer (CDL) file for the year of 2019. All files are stored as
Cloud Optimized GeoTIFFs (COGs) with a block size of 512, and
take up 151 GB of space on disk, uncompressed. All files are kept
in their original CRS (Albers Equal Area for CDL and UTM for
Landsat). Experiments are run on Microsoft Azure with a 6-core
Intel Xeon E5-2690 CPU. All data is stored on a local SSD attached
to the compute node. Batch size and random seed are varied while
the remaining hyperparameters are kept fixed. Total epoch size is
4096, patch size is 224, stride is 112, and the number of workers for
parallel data loading is set to 6.
B PRE-PROCESSING ALIGNMENT WITH
GDAL
As an example of an alignment pre-processing workflow, we assume
that we have a Landsat 8 scene and a Cropland Data Layer (CDL)
raster (“cdl.tif”) which completely covers the extent of the Landsat
scene. We would like to create a pixel-aligned version of these two
layers. Given that the Landsat 8 scene has a CRS of “EPSG:32619”,
a height of 8011 pixels, a width of 7891 pixels, and spatial bounds
of (186585, 4505085, 423315, 4745415), the corresponding GDAL
TorchGeo: Deep Learning With Geospatial Data
command to create a cropped version of the CDL layer that is
aligned to the Landsat layer would look like:
1$ gdalwarp \
2 -t_srs EPSG :32619 \
3 -of COG \
4 -te 186585 4505085 423315 4745415 \
5 -ts 7891 8011 \
6 cdl . tif aligned_cdl . tif
Listing 1: Command-line example of manual reprojection of
CDL using GDAL.
The spatial metadata of the Landsat scene can be determined
through other GDAL command-line tools (gdalinfo command),
geospatial data packages such as the rasterio package in Python, or
through GIS software such as QGIS or ArcGIS.
C TORCHGEO CODE EXAMPLE
TorchGeo is designed to be simple and easy to use, and provides
a familiar API for users who have experience using libraries like
torchvision [ 38]. In Listing 2, we provide an example code snippet
showing how to use TorchGeo.
In this example, we show how easy it is to work with geospatial
data and to sample small image patches from a combination of
Landsat [ 52] and Cropland Data Layer (CDL) [ 8] data using Torch-
Geo. First, we assume that the user has Landsat 7 and 8 imagery
downloaded. Since Landsat 8 has more spectral bands than Landsat
7, we only use the bands that both satellites have in common. We
create a single dataset including all images from both Landsat 7
and 8 data by taking the union between these two datasets.
Next, we take the intersection between this dataset and the CDL
dataset. We want to take the intersection instead of the union to
ensure that we only sample from regions where we have both
Landsat and CDL data. Note that we can automatically download
and checksum CDL data. Also note that each of these datasets
may contain files in different CRSs or resolutions, but TorchGeo
automatically ensures that a matching CRS and resolution is used.
This dataset can now be used with a PyTorch data loader. Unlike
benchmark datasets, geospatial datasets often include very large
images. For example, the CDL dataset consists of a single image
covering the entire contiguous United States. In order to sample
from these datasets using geospatial coordinates, TorchGeo defines
a number of samplers. In this example, we use a random sampler
that returns 256×256pixel images and 10,000 samples per epoch.
We also use a custom collation function to combine each sample
dictionary into a mini-batch of samples. This data loader can now
be used in your normal training/evaluation pipeline.
Many applications involve intelligently composing datasets based
on geospatial metadata like this. For example, users may want to:
•Combine datasets for multiple image sources and treat them
as equivalent (e.g., Landsat 7 and 8)
•Combine datasets for disparate geospatial locations (e.g.,
Chesapeake NY and PA)
These combinations require that all queries are present in at least
one dataset, and can be created using a UnionDataset. Similarly,
users may want to:1from torch . utils . data import DataLoader
2from torchgeo . datasets import (
3 Landsat7 , Landsat8 , CDL , stack_samples
4)
5from torchgeo . samplers import (
6 RandomGeoSampler
7)
8
9# Take the union of all Landsat imagery
10landsat7 = Landsat7 ( root =" ... ")
11landsat8 = Landsat8 (
12 root =" ... ",
13 bands = Landsat8 . all_bands [1: -2]] ,
14)
15landsat = landsat7 | landsat8
16
17# Take the intersection of Landsat and CDL
18cdl = CDL (
19 root =" ... ", download =True , checksum = True
20)
21dataset = landsat & cdl
22
23# Sample 10 ,000 256 x256 image patches
24# from the intersection of the datasets
25sampler = RandomGeoSampler (
26 dataset , size =256 , length =10000
27)
28
29# Use the dataset and sampler as normal
30# in a PyTorch DataLoader
31dataloader = DataLoader (
32 dataset ,
33 batch_size =128 ,
34 sampler = sampler ,
35 collate_fn = stack_samples
36)
37for batch in dataloader :
38 image = batch [" image "]
39 mask = batch [" mask "]
40
41 # Train a model , or make predictions
42 # using a pre - trained model
Listing 2: Example TorchGeo code for creating a joint
Landsat 7/8 [52] and Cropland Data Layer (CDL) [8]
dataset and using such a dataset with a standard PyTorch
DataLoader class.
•Combine image and target labels and sample from both si-
multaneously (e.g., Landsat and CDL)
•Combine datasets for multiple images sources for multimodal
learning or data fusion (e.g., Landsat and Sentinel)
Stewart et al.
These combinations require that all queries are present in both
datasets, and can be created using an IntersectionDataset. TorchGeo
automatically composes these datasets for you when you use the
intersection ( &) and union ( |) operators.
D BENCHMARK DATASET EXPERIMENTS
For experiments, we use the pre-defined training, validation, and
testing splits in all datasets. We perform a small hyperparameter
search with a single fixed random seed on each dataset. Specifically,
we perform a hyperparameter search for the best validation perfor-
mance over the grid: model architecture ∈{ResNet18, ResNet50},
weight initialization ∈{random, ImageNet}, learning rate ∈{0.01,
0.001, 0.0001}, and loss function ∈{cross entropy, jaccard}. For theclassification and regression datasets, we use the ResNets as is, and
for the semantic segmentation datasets, we use the ResNets as the
encoder model in a U-Net. With the So2Sat and EuroSAT datasets
we also run experiments that use all the Sentinel-2 bands vs. only
the RGB bands. In the cases where we use ImageNet weights with
imagery that has more than RGB bands we randomly initialize the
non-RGB kernels in the first convolutional layer of the network
and denote this setting as ImageNet (+ random). In all cases, we
use the AdamW optimizer [ 33], reduce learning rate on validation
loss plateaus, and early stop based on validation loss. We repeat the
training process with 10 different seeds using the best performing
hyperparameter configuration and report the test set performance
over these models.
The Thirty-Third AAAI Conference on Artiﬁcial Intelligence (AAAI-19)
Tile2Vec: Unsupervised Representation Learning for Spatially Distributed Data
Neal Jean,1,2Sherrie Wang,3,4Anshul Samar,1George Azzari,4David Lobell,4Stefano Ermon1
1Department of Computer Science, Stanford University, Stanford, CA 94305
2Department of Electrical Engineering, Stanford University, Stanford, CA 94305
3Institute of Computational and Mathematical Engineering, Stanford University, Stanford, CA 94305
4Department of Earth System Science, Stanford University, Stanford, CA 94305
{nealjean, sherwang, asamar, gazzari, dlobell }@stanford.edu, ermon@cs.stanford.edu
Abstract
Geospatial analysis lacks methods like the word vector repre-
sentations and pre-trained networks that signiﬁcantly boost
performance across a wide range of natural language and
computer vision tasks. To ﬁll this gap, we introduce Tile2Vec,
an unsupervised representation learning algorithm that ex-
tends the distributional hypothesis from natural language —
words appearing in similar contexts tend to have similar
meanings — to spatially distributed data. We demonstrate
empirically that Tile2Vec learns semantically meaningful
representations for both image and non-image datasets. Our
learned representations signiﬁcantly improve performance in
downstream classiﬁcation tasks and, similarly to word vec-
tors, allow visual analogies to be obtained via simple arith-
metic in the latent space.
1 Introduction
Remote sensing, the measurement of the Earth’s surface
through aircraft- or satellite-based sensors, is becoming in-
creasingly important to many applications, including land
use monitoring, precision agriculture, and military intelli-
gence (Foody 2003; Mulla 2013; Oshri et al. 2018). Com-
bined with recent advances in deep learning and computer
vision (Krizhevsky, Sutskever, and Hinton 2012; He et al.
2016), there is enormous potential for monitoring global is-
sues through the automated analysis of remote sensing and
other geospatial data streams. However, recent successes in
machine learning have largely relied on supervised learn-
ing techniques and the availability of very large annotated
datasets. Remote sensing provides a huge supply of data,
but many downstream tasks of interest are constrained by a
lack of labels.
The research community has developed a number of tech-
niques to mitigate the need for labeled data. Often, the key
underlying idea is to ﬁnd a low-dimensional representa-
tion of the data that is more suitable for downstream ma-
chine learning tasks. In many NLP applications, pre-trained
word vectors have led to dramatic performance improve-
ments. In computer vision, pre-training on ImageNet is a de
facto standard that drastically reduces the amount of train-
ing data needed for new tasks. Existing techniques, how-
ever, are not suitable for remote sensing data that, while su-
Copyright c⃝2019, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.perﬁcially resembling natural images, have unique charac-
teristics that require new methodologies. Unlike natural im-
ages — object-centric, two-dimensional depictions of three-
dimensional scenes — remote sensing images are taken
from a bird’s eye perspective, and they are also often multi-
spectral . These differences present both challenges and op-
portunities. On one hand, models pre-trained on ImageNet
do not transfer well and cannot take advantage of additional
spectral bands (Xie et al. 2016). On the other, there are fewer
occlusions, permutations of object placement, and changes
of scale to contend with — this spatial coherence provides a
powerful signal for learning representations.
Our main assumption is that image tiles that are geo-
graphic neighbors (i.e., close spatially) should have similar
semantics and therefore representations, while tiles far apart
are likely to have dissimilar semantics and should therefore
have dissimilar representations. This is akin to the distribu-
tional hypothesis used to construct word vector representa-
tions in natural language: words that appear in similar con-
texts should have similar meanings. The main computational
(and statistical) challenge is that image patches are them-
selves complex, high-dimensional vectors, unlike words.
In this paper, we propose Tile2Vec, a method for learn-
ing compressed yet informative representations from unla-
beled remote sensing data. We evaluate our algorithm on a
wide range of remote sensing datasets and ﬁnd that it gen-
eralizes across data modalities, with stable training and ro-
bustness to hyperparameter choices. On a difﬁcult land use
classiﬁcation task, our learned representations outperform
other unsupervised features and even exceed the perfor-
mance of supervised models trained on large labeled train-
ing sets. Tile2Vec learns a meaningful embedding space,
demonstrated through visual query by example, latent space
interpolation, and visual analogy experiments. Finally, we
apply Tile2Vec to the non-image task of predicting coun-
try health indices from economic data, suggesting that real-
world applications of Tile2Vec may extend to domains be-
yond remote sensing.
2 Tile2Vec
For clarity, in this section we focus on the application of
Tile2Vec to remotely sensed image datasets. The extension
to non-image spatial data is straightforward, and we revisit
this setting in Section 4.4.
3967
2.1 Distributional semantics
The distributional hypothesis in linguistics is the idea that
“a word is characterized by the company it keeps”. In NLP,
algorithms like Word2vec and GloVe leverage this assump-
tion to learn continuous representations that capture the nu-
anced meanings of huge vocabularies of words. The strategy
is to build a co-occurrence matrix and solve an implicit ma-
trix factorization problem, learning a low-rank approxima-
tion where words that appear in similar contexts have sim-
ilar representations (Levy and Goldberg 2014; Pennington,
Socher, and Manning 2014; Mikolov et al. 2013b).
To extend these ideas to geospatial data, we need to an-
swer the following questions:
•What is the right atomic unit, i.e., the equivalent of indi-
vidual words in NLP?
•What is the right notion of context?
For atomic units, we propose to learn representations at
the level of remote sensing tiles, a generalization of im-
age patches to multispectral data. This introduces new chal-
lenges as tiles are high-dimensional objects — computations
on co-occurrence tensors of tiles would quickly become in-
tractable, and statistics almost impossible to estimate from
ﬁnite data. Convolutional neural networks (CNNs) will play
a crucial role in projecting down the dimensionality of our
inputs.
For context, we rely on spatial neighborhoods . Distance
in geographic space provides a form of weak supervision:
we assume that tiles that are close together have similar se-
mantics and therefore should, on average, have more similar
representations than tiles that are far apart. By exploiting this
fact that landscapes in remote sensing datasets are highly
spatially correlated, we hope to extract enough learning sig-
nal to reliably train deep neural networks.
2.2 Unsupervised triplet loss
To learn a mapping from image tiles to low-dimensional em-
beddings, we train a convolutional neural network on triplets
of tiles, where each triplet consists of an anchor tile ta, a
neighbor tile tnthat is close geographically, and a distant
tiletdthat is farther away. Following our distributional as-
sumption, we want to minimize the Euclidean distance be-
tween the embeddings of the anchor tile and the neighbor
tile, while maximizing the distance between the anchor and
distant embeddings. For each tile triplet t= (ta,tn,td), we
seek to minimize the triplet loss
L(t) = [||fθ(ta)−fθ(tn)||2−||fθ(ta)−fθ(td)||2+m]+(1)
To prevent the network from pushing the distant tile farther
without restriction, we introduce a rectiﬁer [·]+with margin
m. Once the distance to the distant embedding exceeds the
distance to the neighbor embedding by at least the margin,
we are satisﬁed. Here, fθis a CNN with parameters θthat
maps from the domain of image tiles Xtod-dimensional
real-valued vector representations, fθ:X→Rd.
Notice that when ||fθ(ta)−fθ(tn)||2<||fθ(ta)−
fθ(td)||2, all embeddings can be scaled by some constant
in order to satisfy the margin and bring the loss to zero. We
observe this behavior empirically — beyond a small number
of iterations, the CNN learns to increase embedding mag-
nitudes and the loss decreases to zero. By penalizing the
Figure 1: Top: Light blue boxes denote anchor tiles, dark
blue neighbor tiles, and red distant tiles. Bottom: Tile
triplets corresponding to the top panel. The columns show
anchor, neighbor, and distant tiles and their respective CDL
class labels. Anchor and neighbor tiles tend to be the same
class, while anchor and distant tend to be different.
embeddings’ l2-norms, we constrain the network to gener-
ate embeddings within a hypersphere and encourage a better
representation, not just a bigger one. Given a dataset of N
tile triplets, our full training objective is
min
θN∑
i=1[
L(t(i)) +λ(
||z(i)
a||2+||z(i)
n||2+||z(i)
d||2)]
,(2)
whereλcontrols the regularization strength and z(i)
a=
fθ(t(i)
a)∈Rdand similarly for z(i)
nandz(i)
d.
2.3 Triplet sampling
The sampling procedure for ta,tn, andtdis described by
two parameters:
•Tile size deﬁnes the pixel width and height of a single tile.
•Neighborhood deﬁnes the region around the anchor tile
from which to sample the neighbor tile. In our implemen-
tation, if the neighborhood is 100 pixels, then the center of
the neighbor tile must be within 100 pixels of the anchor
tile center both vertically and horizontally. The distant tile
is sampled at random from outside this region.
Tile size should be chosen so that tiles are large enough
to contain information at the scale needed for downstream
tasks. Neighborhood should be small enough that neighbor
tiles will be semantically similar to the anchor tile, but large
3968
Algorithm 1 SampleTileTriplets( D,N,s,r )
1:Input: Image dataset D, number of triplets N, tile size s,
neighborhood radius r
2:Output: Tile triplets T={(t(i)
a, t(i)
n, t(i)
d)}N
i=1
3:
4:Initialize tile triplets T={}
5:fori←1, Ndo
6: t(i)
a←SAMPLE TILE(D, s )
7: t(i)
n←SAMPLE TILE(NEIGHBORHOOD (D, r, t(i)
a), s)
8: t(i)
d←SAMPLE TILE(¬NEIGHBORHOOD (D, r, t(i)
a), s)
9: Update T←T∪(t(i)
a, t(i)
n, t(i)
d)
10:end for
11:return T
12:
13:function SAMPLE TILE(A, s)
14: t←Sample tile of size suniformly at random from A
15: return t
16:end function
17:
18:function NEIGHBORHOOD (D, r, t )
19: A←Subset of Dwithin radius rof tile t
20: return A
21:end function
enough to capture intra-class (and potentially some inter-
class) variability. In practice, we ﬁnd that plotting some ex-
ample triplets as in Fig. 1 allowed us to ﬁnd reasonable val-
ues for these parameters. Results across tile size and neigh-
borhood on our land cover classiﬁcation experiment are re-
ported in Table A3.1
Pseudocode for sampling a dataset of triplets is given in
Algorithm 1. Note that no knowledge of actual geographi-
cal locations is needed, so Tile2Vec can be applied to any
dataset without knowledge of the data collection procedure.
2.4 Scalability
Like most deep learning algorithms, the Tile2Vec objec-
tive (Eq. 2) allows for mini-batch training on large datasets.
More importantly, the use of the triplet loss allows the train-
ing dataset to grow with a quadratic relationship relative to
the size of the available remote sensing data. Concretely,
assume that for a given remote sensing dataset we have a
sampling budget of Ntriplets. If we train using the straight-
forward approach of Eq. 2, we will iterate over Ntraining
examples in each epoch. However, we notice that in most
cases the area covered by our dataset is much larger than
the area of a single neighborhood. For any tile t, the likeli-
hood that any particular t′in the other (N−1)tiles is in its
neighborhood is extremely low. Therefore, at training time
we can match any (ta,tn)pair with any of the 3Ntiles in
the dataset to increase the number of unique example triplets
that the network sees from O(N)toO(N2).
In practice, we ﬁnd that combining Tile2Vec with this data
augmentation scheme to create massive datasets results in
an algorithm that is easy to train, robust to hyperparameter
choices, and resistant to overﬁtting. This point will be revis-
ited in section 4.1.
1Appendix available at https://arxiv.org/abs/1805.02855.3 Datasets
We evaluate Tile2Vec on several widely-used classes of re-
mote sensing imagery, as well as a non-image dataset of
country characteristics. A brief overview of data organized
by experiment is given here, with more detailed descriptions
in Appendix A.5.
3.1 Land cover classiﬁcation
We ﬁrst evaluate Tile2Vec on a land use classiﬁcation task
— predicting what is on the Earth’s surface from remotely
sensed imagery — that uses the following two datasets: The
USDA’s National Agriculture Imagery Program (NAIP)
provides aerial imagery for public use that has four spectral
bands — red (R), green (G), blue (B), and infrared (N) —
at 0.6 m ground resolution. We obtain an image of Central
Valley, California near the city of Fresno for the year 2016
(Fig. 2), spanning latitudes [36.45,37.05]and longitudes
[−120.25,−119.65]. The Cropland Data Layer (CDL) is a
raster geo-referenced land cover map collected by the USDA
for the continental United States (USDA-NASS 2016). Of-
fered at 30 m resolution, it includes 132 class labels span-
ning crops, developed areas, forest, water, and more. In our
NAIP dataset, we observe 66 CDL classes (Fig. A10). We
use CDL as ground truth for evaluation by upsampling it to
NAIP resolution.
3.2 Latent space interpolation and visual analogy
We explore Tile2Vec embeddings by visualizing linearly in-
terpolated tiles in the learned feature space and performing
visual analogies on two datasets. Tiles sampled from NAIP
are used in the latent space interpolation evaluation. The
USGS and NASA’s Landsat 8 satellite provide moderate-
resolution (30 m) multispectral imagery on a 16-day col-
lection cycle. Landsat datasets are public and widely used
in agricultural, environmental, and other scientiﬁc applica-
tions. We generate median Landsat 8 composites containing
7 spectral bands over the urban and rural areas of three ma-
jor US cities — San Francisco, New York City, and Boston
— for a visual analogy evaluation.
3.3 Poverty prediction in Uganda
We evaluate the regression task of predicting local poverty
levels from Landsat 7 composites of Uganda from 2009-
2011 containing 5 spectral bands. The World Bank’s Living
Standards Measurement Study (LSMS) surveys measure
annual consumption expenditure at the household and vil-
lage levels — these measurements are the basis for deter-
mining international standards for extreme poverty. We use
the Uganda 2011-12 survey as labels for the poverty predic-
tion task described in (Jean et al. 2016).
3.4 Worldwide country health index prediction
Lastly, to demonstrate that Tile2Vec can be used with other
high-dimensional vector data within a spatial context, we
predict a subset of country characteristics from other country
features. The CIA World Factbook is an annual document
3969
Figure 2: Left: Our NAIP aerial imagery covers 2500 km2around Fresno, California. Center: Land cover types as labeled by
the Cropland Data Layer (CDL, see Section 3.1) show a highly heterogeneous landscape; each color represents a different CDL
class. Right: For the land cover classiﬁcation task, we split the dataset spatially into train, validation, and test sets.
compiled by the U.S. Central Intelligence Agency contain-
ing information on the governments, economies, energy sys-
tems, and societies of 267 world entities (Factbook 2015).
We extract a dataset from the 2015 Factbook that contains
73 real-valued features (e.g., infant mortality rate, GDP per
capita, crude oil production) for 242 countries.
4 Experiments
4.1 Land cover classiﬁcation using aerial imagery
We train Tile2Vec embeddings on 100k triplets sampled
from the NAIP dataset. The Tile2Vec CNN is a ResNet-18
architecture (He et al. 2016) modiﬁed for 28×28CIFAR-
10 images (1) with an additional residual block to handle
our larger input and (2) without the ﬁnal classiﬁcation layer.
Each of the 300k 50×50NAIP tiles is labeled with the mode
CDL land cover class and our evaluation metric is classiﬁ-
cation accuracy on this label.
To ensure that training and test sets are spatially disjoint,
we split the area into a 12×12grid of rectangular blocks,
which we then partitioned randomly into train (104 blocks),
validation (20 blocks), and test (20 blocks) (Fig. 2, right).
Each of these blocks is just over 17 km2in size, roughly
5000 times the size of each tile. By splitting the dataset at the
block level, we are able to reduce the spatial autocorrelation
and estimate generalization error with minimal inﬂation.
Tile2Vec hyperparameter optimization We tune the two
main hyperparameters of Algorithm 1 by searching over
a grid of tile sizes and neighborhoods. We run the CDL
land cover classiﬁcation experiment 20 times in total, us-
ing combinations of tile size in [25,50,75,100] and neigh-
borhood radius in [50,100,500,1000,None], where None
indicates that both the neighbor and distant tiles are sam-
pled from anywhere in the dataset (i.e., inﬁnite radius). The
resulting accuracies are reported in Table A1. Results sug-
gest that on this task and dataset, a neighborhood radius of
100 pixels strikes the ideal balance between sampling se-
mantically similar tiles and capturing intra-class variabil-
ity, though classiﬁcation accuracy remains higher than themodel with inﬁnite radius even when the neighborhood is
increased to 1000 pixels. Accuracy also increases with tile
size, which can be attributed to greater imbalance of labels
at larger tile sizes (Appendix A.4) as well as greater avail-
able spatial context for classiﬁcation.
Because CDL labels are at a resolution (30 m) equivalent
to 50 NAIP pixels (0.6 m), we ultimately choose a tile size of
50 and neighborhood of 100 pixels for the land cover classi-
ﬁcation task. For consistency, subsequent experiments also
use these default hyperparameters. Although these default
hyperparameters yield high performance in most cases, they
should generally be optimized for new datasets and tasks.
Unsupervised learning baselines We compare Tile2Vec
to a number of unsupervised feature extraction methods. We
describe each baseline here, and provide additional training
details in Appendix A.1.
•Autoencoder : A convolutional autoencoder is trained on
all 300k multispectral tiles, split 90% training and 10%
validation. We train until the validation reconstruction er-
ror ﬂattens; the encoder is then used to embed tiles into
the feature space. The autoencoder achieves good recon-
structions on the held-out test set (examples in Appendix
A.1).
•Pre-trained ResNet-18 : A modiﬁed ResNet-18 was
trained on resized CIFAR-10 images and used as a fea-
ture extractor. Since CIFAR-10 only has RGB channels,
this approach only allows for use of the RGB bands of
NAIP and illustrates the limitations of transferring mod-
els from natural images to remote sensing datasets.
•PCA/ICA : Each RGBN tile of shape (50,50,4)is un-
raveled into a vector of length 10,000 and then PCA/ICA
is used to compute the ﬁrst 10 principal components for
each tile.
•K-means : Tiles are clustered in pixel space using k-
means with k= 10 , and each tile is represented as 10-
dimensional vectors of distances to each cluster centroid.
3970
n= 1000 n= 10000
Unsupervised features RF LR MLP RF LR MLP
Tile2Vec 52.6±1.1 53.7±1.3 55.1±1.2 56.9±0.3 59.7±0.3 58.4±0.3
Autoencoder 49.1 ±0.7 44.7±1.0 52.0±1.0 53.1±0.2 55.6±0.2 57.2±0.4
Pre-trained ResNet-18 47.7 ±0.6 48.4±0.8 49.9±1.7 50.6±0.2 53.7±0.2 54.4±0.4
PCA 46.9 ±0.8 50.2±0.4 43.6±5.3 50.1±0.3 51.1±0.1 52.4±0.3
ICA 47.7 ±0.6 50.1±0.6 46.7±3.1 50.4±0.4 51.1±0.1 52.5±0.2
K-Means 43.1 ±0.8 49.4±0.4 44.5±3.9 45.6±0.5 50.0±0.1 50.5±0.2
Table 1: Comparison of Tile2Vec features to unsupervised baselines on the CDL classiﬁcation task in Section 4.1. Random
forest (RF), logistic regression (LR), and multilayer perceptron (MLP) classiﬁers are trained over 10 trials of n= 1000 and
n= 10000 randomly sampled labels, with mean accuracies and standard deviations reported.
Figure 3: Left: Logistic regression on Tile2Vec unsupervised features outperforms supervised CNNs until 50k labeled exam-
ples. Right: The Tile2Vec triplet loss decreases steadily and downstream classiﬁcation accuracy tracks the loss.
As shown in Table 1, the features learned by Tile2Vec out-
perform other unsupervised features when used by random
forest (RF), logistic regression (LR), and multilayer percep-
tron (MLP) classiﬁers trained on n= 1000 orn= 10000
labels. We also trained a DCGAN (Radford, Metz, and Chin-
tala 2015) as a generative modeling approach to unsuper-
vised feature learning. Although we were able to gener-
ate reasonable samples, features learned by the discrimina-
tor performed poorly — samples and results can be found
in Appendix A.1. Approaches based on variational autoen-
coders (V AEs) would also provide intriguing baselines, but
we are unaware of existing models capable of capturing
complex multispectral image distributions.
Supervised learning comparisons Surprisingly, our
Tile2Vec features are also able to outperform fully-
supervised CNNs trained directly on the classiﬁcation task
with large amounts of labeled data. Fig. 3 shows that ap-
plying logistic regression on Tile2Vec features beats several
state-of-the-art supervised architectures (He et al. 2016;Szegedy et al. 2015; Huang et al. 2017) trained on as many
as 50k CDL labels. We emphasize that the Tile2Vec CNN
and the supervised ResNet share the same architecture,
so logistic regression in Fig. 3 is directly comparable to
the classiﬁcation layers of the supervised architectures.
Similar results for random forest and multilayer perceptron
classiﬁers can be found in Appendix A.4.
Latent space interpolation We further explore the
learned representations with a latent space interpolation ex-
periment shown in Fig. 4. Here, we start with the Tile2Vec
embeddings of a ﬁeld tile and an urban tile and linearly in-
terpolate between the two. At each point along the interpo-
lation, we search for the ﬁve nearest neighbors in the la-
tent space and display the corresponding tiles. As we move
through the semantically meaningful latent space, we re-
cover tiles that are more and more developed.
Training details Tile2Vec is easy to train and robust to the
choice of hyperparameters. We experimented with margins
3971
Figure 4: Left: Linear interpolation in the latent space at equal intervals between representations of rural and urban images.
Below, we show 5 nearest neighbors in the latent space to each interpolated vector. Right: Starting with a rural NYC embedding,
we add urban SF and subtract rural SF to successfully discover urban NYC tiles. More visual analogies are shown in Fig. A6.
ranging from 0.1 to 100 and found little effect on accuracy.
Using a margin of 50, we trained Tile2Vec for 10 trials with
different random initializations and show the results in Fig.
3. The training loss is stable from epoch to epoch, consis-
tently decreasing, and most importantly, a good proxy for
unsupervised feature quality as measured by performance on
the downstream task (Fig. 3, bottom right). By combining
explicit regularization (Eq. 2) with the data augmentation
scheme described in Section 2.4, we observe that Tile2Vec
does not seem to overﬁt even when trained for many epochs.
4.2 Visual analogies across US cities
To evaluate Tile2Vec qualitatively, we explore three major
metropolitan areas of the United States: San Francisco, New
York City, and Boston. First, we train a Tile2Vec model
on the San Francisco dataset only. Then we use the trained
model to embed tiles from all three cities. As shown in Fig.
4 and A6, these learned representations allow us to perform
arithmetic in the latent space, or visual analogies (Radford,
Metz, and Chintala 2015). By adding and subtracting vec-
tors in the latent space, we can recover image tiles that are
semantically expected given the operations applied.
Here we use Landsat images with 7 spectral bands,
demonstrating that Tile2Vec can be applied effectively to
highly multispectral datasets. Tile2Vec can also learn rep-
resentations at multiple scales: each Landsat 8 (30 m resolu-
tion) tile covers 2.25 km2, while the NAIP and DigitalGlobe
tiles are 2500 times smaller. Finally, Tile2Vec learns robust
representations that allow for domain adaptation or transfer
learning, as the three datasets have widely varying spectral
distributions (Fig. A9).
4.3 Poverty prediction from satellite imagery
Next, we apply Tile2Vec to predict annual consumption ex-
penditures in Uganda from satellite imagery. Accurate mea-
surements of poverty are essential for both research and pol-
icy, but reliable data is limited in the developing world —
machine learning methods that are still effective when la-
beled data is scarce could help to ﬁll these critical gaps.Features d kNN RF RR
Tile2Vec 10 77.5±1.0 76.0±1.3 69.6±1.0
Non-health 60 62.8 ±1.5 72.1±1.6 68.7±1.7
Locations 2 69.3 ±1.0 67.7±2.6 11.6±1.5
Table 2: Predicting health index using Tile2Vec features ver-
sus non-health features and locations (i.e., {lat, lon}). Here,
dis feature dimension, kNN is k-nearest neighbors, RF is
random forest, and RR is ridge regression. Hyperparameters
(e.g.,kand regularization strength) are tuned for each fea-
ture set. We report average r2and standard deviation for 10
trials of 3-fold cross-validation.
The previous state-of-the-art result used a transfer learn-
ing approach in which a CNN is trained to predict night-
time lights (a proxy for poverty) from daytime satellite im-
ages — the features from this model are then used to pre-
dict consumption expenditures (Jean et al. 2016). We use
the same LSMS survey preprocessing pipeline and ridge re-
gression evaluation (see Appendix A.2). Evaluating over 10
trials of 5-fold cross-validation, we report an average r2of
0.496±0.014compared to r2= 0.41for the transfer learn-
ing approach — this is achieved with publicly available im-
agery with much lower resolution than the proprietary im-
ages used in (Jean et al. 2016) (30 m vs. 2.4 m).
4.4 Generalizing to other spatial data: Predicting
country health indices
The CIA Factbook contains 73 features spanning economic,
energy, social, and other characteristics of countries around
the world. To demonstrate that Tile2Vec can leverage spatial
coherence for non-image datasets as well, we use 13 of the
features in the CIA Factbook related to public health and
compute a health index, then attempt to predict this health
index from the remaining 60 features. We train Tile2Vec by
sampling triplets of countries and feeding the feature vectors
into a small MLP with one hidden layer.
3972
Figure 5: Left: The 60 non-health country features visualized using t-SNE. Spatial relationships are preserved for some clusters,
but not for others. Right: The 10-dimensional Tile2Vec embeddings visualized using t-SNE. The latent space now respects
both spatial and characteristic similarities. Several countries are annotated to highlight interesting relationships: North Korea
and South Korea are embedded far apart even though they are spatial neighbors; USA, South Korea, and China are embedded
close together though they are geographically separated.
As shown in Table 2, the embeddings learned by Tile2Vec
on this small spatial dataset ( N= 242 ) outperform both the
original features and approaches that explicitly use spatial
information. Fig. 5 shows the original 60-dimensional fea-
ture vectors as well as the 10-dimensional learned Tile2Vec
embeddings projected down to two dimensions using t-SNE
(van der Maaten and Hinton 2008). While there is some ge-
ographic grouping of countries in projecting down the orig-
inal features, the Tile2Vec embeddings appear to capture
both geographic proximity and socioeconomic similarity.
In this experiment, the Haversine formula was used to
compute the great-circle distance between pairs of countries
in kilometers — future work could explore using distance
functions more meaningful to the application at hand, e.g.,
shared borders or trade volume.
5 Related Work
Our inspiration for using spatial context to learn repre-
sentations originated from continuous word representations
like Word2vec and GloVe (Mikolov et al. 2013b; 2013a;
Pennington, Socher, and Manning 2014). In NLP, the distri-
butional hypothesis can be summarized as “a word is char-
acterized by the company it keeps” — words that appear in
the same context likely have similar semantics. We apply
this concept to remote sensing data, with multispectral im-
age tiles as the atomic unit analogous to individual words in
NLP, and geospatial neighborhoods as the “company” that
these tiles keep. A related, supervised version of this idea
is the patch2vec algorithm (Fried, Avidan, and Cohen-Or
2017), which its authors describe as learning “globally con-
sistent image patch representations”. Working with naturalimages, they use a very similar triplet loss (ﬁrst introduced in
(Hoffer and Ailon 2015)), but sample their patches with su-
pervision from an annotated semantic segmentation dataset.
Unsupervised learning for visual data is an active area
of research and thus impossible to summarize concisely,
but we attempt a brief overview of the most relevant topics
here. The three main classes of deep generative models —
likelihood-based variational autoencoders (V AEs) (Kingma
and Welling 2013), likelihood-free generative adversarial
networks (GANs) (Goodfellow et al. 2014), and various au-
toregressive models (Oord, Kalchbrenner, and Kavukcuoglu
2016; van den Oord et al. 2016) — attempt to learn the
generating data distribution from training samples. Other re-
lated lines of work use spatial or temporal context to learn
high-level image representations. Some strategies for using
spatial context involve predicting the relative positions of
patches sampled from within an image (Noroozi and Favaro
2016; Doersch, Gupta, and Efros 2015) or trying to ﬁll in
missing portions of an image (in-painting) (Pathak et al.
2016). In videos, nearby frames can be used to learn tem-
poral embeddings (Ramanathan et al. 2015); other methods
leveraging the temporal coherence and invariances of videos
for feature learning have also been proposed (Misra, Zitnick,
and Hebert 2016; Wang and Gupta 2015).
6 Conclusion
We demonstrate the efﬁcacy of Tile2Vec as an unsuper-
vised feature learning algorithm for spatially distributed data
on tasks from land cover classiﬁcation to poverty predic-
tion. Our method can be applied to image datasets spanning
moderate to high resolution, RGB or multispectral bands,
3973
and collected via aerial or satellite sensors, and even to
non-image datasets. Tile2Vec outperforms other unsuper-
vised feature extraction techniques on a difﬁcult classiﬁ-
cation task — surprisingly, it even outperforms supervised
CNNs trained on 50k labeled examples.
In this paper, we focus on exploiting spatial coherence,
but many geospatial datasets also include sequences of data
collected over time. Temporal patterns can be highly infor-
mative (e.g., seasonality, crop cycles), and we plan to ex-
plore this aspect in future work. Remote sensing data have
largely been unexplored by the machine learning commu-
nity — more research in these areas could result in enormous
progress on many problems of global signiﬁcance.
Acknowledgements
This research was supported by NSF (#1651565, #1522054,
#1733686), ONR, Sony, and FLI. NJ was supported by the
Department of Defense (DoD) through the National Defense
Science & Engineering Graduate Fellowship (NDSEG) Pro-
gram.
References
Doersch, C.; Gupta, A.; and Efros, A. A. 2015. Unsupervised vi-
sual representation learning by context prediction. In Proceedings
of the IEEE International Conference on Computer Vision , 1422–
1430.
Factbook, C. 2015. The World Factbook; 2010. http://www.cia.
gov/library/publications/the-world-factbook. Accessed: 2018-01-
20.
Foody, G. M. 2003. Remote sensing of tropical forest environ-
ments: Towards the monitoring of environmental resources for sus-
tainable development. International Journal of Remote Sensing
24(20):4035–4046.
Fried, O.; Avidan, S.; and Cohen-Or, D. 2017. Patch2Vec: Glob-
ally consistent image patch representation. In Computer Graphics
Forum , volume 36, 183–194. Wiley Online Library.
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-
Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y . 2014. Gen-
erative adversarial nets. In Advances in Neural Information Pro-
cessing Systems , 2672–2680.
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition , 770–778.
Hoffer, E., and Ailon, N. 2015. Deep metric learning using triplet
network. In International Workshop on Similarity-Based Pattern
Recognition , 84–92. Springer.
Huang, G.; Liu, Z.; Weinberger, K. Q.; and van der Maaten, L.
2017. Densely connected convolutional networks. In Proceedings
of the IEEE conference on Computer Vision and Pattern Recogni-
tion, volume 1, 3.
Jean, N.; Burke, M.; Xie, M.; Davis, W. M.; Lobell, D. B.; and Er-
mon, S. 2016. Combining satellite imagery and machine learning
to predict poverty. Science 353(6301):790–794.
Kingma, D. P., and Welling, M. 2013. Auto-encoding variational
bayes. arXiv preprint arXiv:1312.6114 .
Krizhevsky, A.; Sutskever, I.; and Hinton, G. 2012. ImageNet clas-
siﬁcation with deep convolutional neural networks. In Advances in
Neural Information Processing Systems , 1097–1105.Levy, O., and Goldberg, Y . 2014. Neural word embedding as im-
plicit matrix factorization. In Advances in Neural Information Pro-
cessing Systems , 2177–2185.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013a. Efﬁcient
estimation of word representations in vector space. arXiv preprint
arXiv:1301.3781 .
Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J.
2013b. Distributed representations of words and phrases and their
compositionality. In Advances in Neural Information Processing
Systems , 3111–3119.
Misra, I.; Zitnick, C. L.; and Hebert, M. 2016. Shufﬂe and learn:
unsupervised learning using temporal order veriﬁcation. In Euro-
pean Conference on Computer Vision , 527–544. Springer.
Mulla, D. J. 2013. Twenty ﬁve years of remote sensing in pre-
cision agriculture: Key advances and remaining knowledge gaps.
Biosystems Engineering 114(4):358 – 371. Special Issue: Sensing
Technologies for Sustainable Agriculture.
Noroozi, M., and Favaro, P. 2016. Unsupervised learning of visual
representations by solving jigsaw puzzles. In European Conference
on Computer Vision , 69–84. Springer.
Oord, A. v. d.; Kalchbrenner, N.; and Kavukcuoglu, K. 2016. Pixel
recurrent neural networks. arXiv preprint arXiv:1601.06759 .
Oshri, B.; Hu, A.; Adelson, P.; Chen, X.; Dupas, P.; Weinstein, J.;
Burke, M.; Lobell, D.; and Ermon, S. 2018. Infrastructure qual-
ity assessment in africa using satellite imagery and deep learning.
Proc. 24th ACM SIGKDD Conference .
Pathak, D.; Krahenbuhl, P.; Donahue, J.; Darrell, T.; and Efros,
A. A. 2016. Context encoders: Feature learning by inpainting.
InProceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , 2536–2544.
Pennington, J.; Socher, R.; and Manning, C. 2014. Glove: Global
vectors for word representation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language Processing
(EMNLP) , 1532–1543.
Radford, A.; Metz, L.; and Chintala, S. 2015. Unsupervised rep-
resentation learning with deep convolutional generative adversarial
networks. arXiv preprint arXiv:1511.06434 .
Ramanathan, V .; Tang, K.; Mori, G.; and Fei-Fei, L. 2015. Learn-
ing temporal embeddings for complex video analysis. In Proceed-
ings of the IEEE International Conference on Computer Vision ,
4471–4479.
Szegedy, C.; Liu, W.; Jia, Y .; Sermanet, P.; Reed, S.; Anguelov,
D.; Erhan, D.; Vanhoucke, V .; Rabinovich, A.; et al. 2015. Going
deeper with convolutions. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition .
USDA-NASS. 2016. USDA National Agricultural Statistics Ser-
vice Cropland Data Layer. published crop-speciﬁc data layer [on-
line].
van den Oord, A.; Kalchbrenner, N.; Espeholt, L.; Vinyals, O.;
Graves, A.; et al. 2016. Conditional image generation with pix-
elcnn decoders. In Advances in Neural Information Processing
Systems , 4790–4798.
van der Maaten, L., and Hinton, G. 2008. Visualizing high-
dimensional data using t-SNE. Journal of Machine Learning Re-
search 9:2579–2605.
Wang, X., and Gupta, A. 2015. Unsupervised learning of visual
representations using videos. arXiv preprint arXiv:1505.00687 .
Xie, M.; Jean, N.; Burke, M.; Lobell, D.; and Ermon, S. 2016.
Transfer learning from deep features for remote sensing and
poverty mapping. In Proceedings of the Thirtieth AAAI Conference
on Artiﬁcial Intelligence , AAAI’16, 3929–3935. AAAI Press.
3974
Predicting Ground-Level Scene Layout from Aerial Imagery
Menghua Zhai
ted@cs.uky.eduZachary Bessinger
zach@cs.uky.eduScott Workman
scott@cs.uky.eduNathan Jacobs
jacobs@cs.uky.edu
Computer Science, University of Kentucky
Abstract
We introduce a novel strategy for learning to extract se-
mantically meaningful features from aerial imagery. In-
stead of manually labeling the aerial imagery, we propose
to predict (noisy) semantic features automatically extracted
from co-located ground imagery. Our network architecture
takes an aerial image as input, extracts features using a
convolutional neural network, and then applies an adaptive
transformation to map these features into the ground-level
perspective. We use an end-to-end learning approach to
minimize the difference between the semantic segmentation
extracted directly from the ground image and the semantic
segmentation predicted solely based on the aerial image.
We show that a model learned using this strategy, with no
additional training, is already capable of rough semantic
labeling of aerial imagery. Furthermore, we demonstrate
that by ﬁnetuning this model we can achieve more accu-
rate semantic segmentation than two baseline initialization
strategies. We use our network to address the task of esti-
mating the geolocation and geo-orientation of a ground im-
age. Finally, we show how features extracted from an aerial
image can be used to hallucinate a plausible ground-level
panorama.
1. Introduction
Learning-based methods for pixel-level labeling of aerial
imagery have long relied on manually annotated training
data. Unfortunately, such data is expensive to create. Fur-
thermore, its value is limited because a method trained on
one dataset will typically not perform well when applied
to another source of aerial imagery. The difﬁculty in ob-
taining datasets of sufﬁcient scale for all modalities has
hampered progress in applying deep learning techniques
to aerial imagery. There have been a few notable excep-
tions [ 22,24], but these have all used fairly coarse grained
semantic classes, covered a small spatial area, and are lim-
ited to modalities in which human annotators are able to
manually assign labels.
We propose a novel strategy for obtaining semantic la-
Cross Entropy Loss
LabelExtract
Transform
Transfer
SemanticsFigure 1. We learn to predict the ground image segmentation di-
rectly from an aerial image of the same location, thereby transfer-
ring the semantics from the ground to the aerial image domain.
bels for aerial image segmentation. See Figure 1for a
schematic overview of the approach. Our idea is to use
existing methods for semantic image segmentation, which
are tailored for ground images, and apply these to a large
dataset of geotagged ground images. We use these seman-
tically labeled images as a form of weak supervision and
attempt to predict these semantic labels from an aerial im-
age centered around the location of the ground image. We
do not use a parametric transformation between the aerial
and ground-level viewpoints. Instead, we use a dense rep-
resentation, similar in spirit to the general representation,
dubbed ﬁlter ﬂow, described by Seitz and Baker [ 27].
There has been signiﬁcant interest recently in predicting
ground image features from aerial imagery for the task of
1
867

ground image geolocalization [ 34]. Our work is unique in
that it is the ﬁrst to attempt to predict a dense pixel-level seg-
mentation of the ground image. We demonstrate the value
of this approach in several ways.
Main Contributions: The main contributions of this
work are: (1) a novel convolutional neural network (CNN)
architecture that relates the appearance of an aerial image to
the semantic layout of a ground image of the same location,
(2) demonstrating the value of our training strategy for pre-
training a CNN to understand aerial imagery, (3) extensions
of the proposed technique to the tasks of ground image lo-
calization, orientation estimation, and synthesis, and (4) an
extensive evaluation of each of these techniques on large,
real-wold datasets. Together these represent an important
step in enabling deep learning techniques to be extended to
the domain of aerial image understanding.
2. Related Work
Learning Viewpoint Transformations Many methods
have been proposed to represent the relationship between
the appearance of two viewpoints. Seitz and Baker [ 27]
model image transformations using a space-variant linear
ﬁlter, similar to a convolution but varying per-pixel. They
highlight that a linear transformation of a vectorized rep-
resentation of all the pixels in an image is very general;
it can represent all standard parametric transformations,
such as similarity, afﬁne, perspective, and more. More re-
cently, Jaderberg et al. [13] describe an end-to-end learn-
able module for neural networks, the spatial transformer,
which allows explicit spatial transformations ( e.g., scaling,
cropping, rotation, non-rigid deformation) of feature maps
within the network that are conditioned on individual data
samples. Practically, including a spatial transformer allows
a network to select regions of interest from an input and
transform them to a canonical pose. Similarly, Tinghui
et al . [35] address the problem of novel view synthesis.
They observe that the visual appearance of different views
is highly correlated and propose a CNN architecture for es-
timating appearance ﬂows, a representation of which pixels
in the input image can be used for reconstruction.
Relating Aerial and Ground-Level Viewpoints Several
methods have been recently proposed to jointly reason
about co-located aerial and ground image pairs. Luo et
al. [20] demonstrate that aerial imagery can aid in rec-
ognizing the visual content of a geotagged ground image.
M´attyus et al. [21] perform joint inference over monocular
aerial imagery and stereo ground images for ﬁne-grained
road segmentation. Wegner et al. [31] build a map of street
trees. Given the horizon line and the camera intrinsics,
Ghouaiel and Lef `evre [ 8] transform geotagged ground-levelpanoramas to a top-down view to enable comparisons with
aerial imagery for the task of change detection. Recent work
on cross-view image geolocalization [ 18,19,33,34] has
shown that convolutional neural networks are capable of ex-
tracting features from aerial imagery that can be matched
to features extracted from ground imagery. V o et al. [30]
extend this line of work, demonstrating improved geolocal-
ization performance by applying an auxiliary loss function
to regress the ground-level camera orientation with respect
to the aerial image. To our knowledge, our work is the ﬁrst
work to explore predicting the semantic layout of a ground
image from an aerial image.
Semantic Segmentation of Aerial/Satellite Imagery
There is a long tradition of using computer vision tech-
niques for aerial and satellite image understanding [ 11,32,
4]. Historically these two domains were distinct. Satellite
imagery was typically lower-resolution, from a strictly top-
down view, and with a diversity of spectral bands. Aerial
imagery was typically higher-resolution, with a greater di-
versity of viewing angles, but with only RGB and NIR sen-
sors. Recently these two domains have converged; we will
use the term aerial imagery as we are primarily working
with high-resolution RGB imagery. However, our approach
could be applied to many types of aerial and satellite im-
agery. Kluckner et al . [17] address the task of semantic
segmentation using a random forest to combine color and
height information. More recent work has explored the use
of CNNs for aerial image understanding. Mnih and Hinton
propose a CNN for detecting roads in aerial imagery [ 22]
using GIS data as ground truth. They extend their approach
to handle omission noise and misregistration between the
imagery and the labels [ 23]. These approaches require ei-
ther extensive pixel-level manual annotation or existing GIS
data. Our work is the ﬁrst to demonstrate the ability to trans-
fer a dense pixel-level labeling of ground imagery to aerial
imagery.
Visual Domain Adaptation Domain adaptation ad-
dresses the misalignment of source and target domains [ 7].
A signiﬁcant amount of work has explored domain adap-
tation for visual recognition [ 25]. Jhuo et al. [14] propose
a low-rank reconstruction approach where the source fea-
tures are transformed to an intermediate representation in
which they can be linearly reconstructed by the target sam-
ples. Our work is most similar to that of Sun et al. [29], who
propose a method for transferring scene categorizations and
attributes from ground images to aerial imagery. Similar
to our approach, they learn a transformation matrix which
minimizes the distance between a source feature and the tar-
get feature. Our work differs in several ways: 1) we carry
out the linear transformation not only in the semantic di-
mensions but also in the spatial dimensions, 2) we constrain
868

256
1✕1✕289
interpolate to size
 17 x 17 x channels
64128512
aerial label
S
1✕1✕4
(i, j, y, x)
transformation 
matrixaerial image
VGG16
A
FT
Figure 2. A visual overview of our network architecture. We extract features from an aerial image using the VGG16 architecture and
form a hypercolumn using the PixelNet approach. These features are processed by three networks that consist of 1 ×1 convolutions:
networkAconverts the hypercolumn into semantic features; network Sextracts useful features from the aerial image for controlling the
transformation; and network Fdeﬁnes the transformation between viewpoints. The transformation, T, is applied to the aerial semantic
features to create a ground-level semantic labeling.
the transformation matrix such that the semantic meaning of
the source feature and the target feature remains the same,
3) our transformation matrix is input dependent, and 4) we
learn the transformation matrix as well as the source feature
at the same time, in an end-by-end manner, which simpliﬁes
training.
3. Cross-view Supervised Training
We propose a novel training strategy for learning to ex-
tract useful features from aerial imagery. The idea is to pre-
dict the semantic scene layout, Lg, of a ground image, Ig,
using only an aligned aerial image, Ia, from the same lo-
cation. This strategy leverages existing methods for ground
image understanding at training time, but does not require
any ground imagery at testing time.
We represent semantic scene layout, Lg, as a pixel-level
probability distribution over classes, such as road,vegeta-
tion, and building . We construct a training pair by collecting
a georegistered ground panorama and an aerial image of the
same location, orienting the panorama to the aerial image
(panoramas are originally aligned with the road direction),
and then extracting the semantic scene layout, Lg, of the
panorama using an off-the-shelf method [ 2] with four se-
mantic classes. We then use an end-to-end training strategy
to learn to extract pixel-level features from the aerial image
and transform them to the ground-level viewpoint.
3.1. Network Architecture
Our proposed network architecture is composed of four
modules. A convolutional neural network (CNN), La=A(Ia;ΘA), is used to extract semantic labels from the aerial
imagery. Another CNN, S(Ia;ΘS), uses features extracted
from aerial imagery to help estimate the transformation
matrix,M=F(xr,yr,ic,jc,S(Ia;ΘS);ΘF), based on
aerial image features and the pixel location in the respec-
tive images. Finally, we have a transformation module,
Lg′=T(La,M), that converts from the aerial viewpoint
to the ground-level using the estimated transformation ma-
trix,M. There are many choices for these components, and
the remainder of this section describes the particular choices
we made for this study. See Figure 2for a visual overview
of the architecture.
Aerial Image Feature Extraction ForA(Ia;ΘA), we
use the VGG16 [ 28] base architecture and convert it
to a pixel-level labeling method using the PixelNet ap-
proach [ 3]. The core idea is to interpolate intermediate fea-
ture maps of the base network to a uniform size, then con-
catenate them along the channels dimension to form a hy-
percolumn . In our experiments, we form the hypercolumn
from conv-{12,22,33,43}of the VGG16 network. The hy-
percolumn, which is now 256 ×256×960, is followed by
three 1×1 convolutional layers, with 512, 512, and 4 out-
put channels respectively. The ﬁrst two 1 ×1 convolutions
have ReLU activations, the ﬁnal is linear. We designate the
output of the ﬁnal convolution as La=A(Ia;ΘA). The
output of this stage is transformed from a aerial viewpoint
to a ground viewpoint by the ﬁnal stage of the network.
Cross-view Semantic Transformation We represent the
transformation between the aerial and ground-level view-
869

points as a linear operation applied channel-wise to La.
To transform from the ha×wa×4 aerial label, La, to
thehg×wg×4 ground label, Lg′, we need to estimate a
hgwg×hawarow-stochastic matrix, M. Given M, the
transformation process is as follows: reshape the aerial la-
bel,La, into ahawa×4 matrix, la; multiply it by Mto
getlg′; then reshape lg′to the size of the ground label, Lg,
to form our estimate of the ground label, Lg′. To account
for the expected layout of the scene, and to handle the sky
class (which is not visible from the aerial image), we carry
out the transformation on the logits of la,fa, and add a bias
term,bto get the logits of lg′,fg′:fg′=Mfa+b.
There are many ways of representing the transformation
matrix,M, in a neural network. The na ¨ıve approach is to
treatMas a matrix of learnable variables. However, this
approach has two downsides: (1) the transformation does
not depend on the content of the aerial image and (2) the
number of parameters scales quadratically with the number
of pixels in LaandLg.
We represent each element, Mrc, in the transformation
matrix,M, as the output of a neural network, F, which is
conditioned on the aerial image, Ia, and the location in the
input and output feature maps. More precisely, each ele-
mentMrc=F(xr,yr,ic,jc,S(Ia;ΘS)), where(ic,jc)∈
[0,1], is the aerial image pixel of the corresponding element,
(yr,xr)∈[0,1]is the ground image pixel of the corre-
sponding element.
We now deﬁne the architecture of the transformation es-
timation neural network, F. The value of the transformation
matrix at location (r,c)is computed through a neural net-
work,˜F, followed by a softmax function to normalize the
impact of all pixels sampled from the aerial image:
Mrc=F(r,c,S(Ia;ΘS)) =e˜Fr,c
∑
c′e˜Fr,c′,
where:
˜Fr,c=˜F(i,j,y,x,S (Ia;ΘS)),and
i=⌊c/wa⌋/ha, j= mod(c,wa)/wa,
y=⌊r/wg⌋/hg, x= mod(r,wg)/wg.
The base network, ˜F, is a multilayer perceptron, with ReLU
activation functions, that takes as input a 293-element vec-
tor. The network has three layers, with 128, 64, and 1 output
channels respectively (refer to the lower part of Figure 2).
The na ¨ıve approach can be considered a special case of this
representation where we ignore the aerial image and use a
one-hot encoding representation of rows and columns.
As described above, there are two main advantages of
our approach of representing the transformation matrix: a
reduction in the number of parameters when Mis large
and the ability to adapt to different aerial image layouts.
An additional beneﬁt is that if we change the resolution of
reshape
(i, j)(y, x)
Transformation matrix 
elements of locations: 
{ (i, j, y, x ) | ∀i, j }
Figure 3. Visualization of the transformation matrix. (left) trans-
formation matrix, M; (right-top) An alternative visualization, M′,
of the transformation matrix. M′containshg×wgcells (square
heat maps). Each cell, m′
yx, is reshaped to size ha×wa, from
one row of Mthat corresponds to locations, {(i,j,y,x)| ∀i,j}.
We also present the aerial image (overlapped with m′
yx) and the
ground image to illustrate how the hot spot of m′
yxcorresponds to
the location, (y,x), on the ground image.
Figure 4. Examples of aligned aerial/ground image pairs from our
dataset. (row 1) In the aerial images, north is the up direction.
In the ground images, north is the central column. (row 2-4) Im-
age dependent receptive ﬁelds estimated by our algorithm as fol-
lows: 1) ﬁx ground locations (y,x)(locations in squares); 2) select
all(i,j)(locations in contours) with high ˜F(i,j,y,x,S (Ia;ΘS))
values. Corresponding ﬁelds between the aerial image and the
ground image are shown in the same color.
our input and output feature maps it is easy to create a new
transformation matrix, M, without needing to resort to in-
terpolation. The transformation matrix learned by our al-
gorithm encodes pixel correspondences between the aerial
image and ground image (see Figure 3). We present more
examples of pixel correspondences in Figure 4.
3.2. Dataset
We collect our training and testing dataset from the
CVUSA dataset [ 34]. CVUSA contains approximately 1.5
870

million geotagged pairs of ground and aerial images from
across the United States. We use the Google Street View
panoramas of CVUSA as our ground images. For each
panorama, we also download an aerial image at zoom level
19 from Microsoft Bing Maps in the same location. We
ﬁlter out panoramas with no available corresponding aerial
imagery. Using the camera’s extrinsic information, we then
warp the panoramas to align with the aerial images. We
also crop the panoramas vertically to reduce the portion of
the sky and ground pixels. In total, we collected 35,532
image pairs for training and 8,884 image pairs for testing.
Some examples of aerial/ground image pairs in our dataset
are shown Figure 4.
3.3. Implementation Details
We implemented the proposed architecture using
Google’s TensorFlow framework [ 1]. We train our net-
works for 10 epochs with the Adam optimizer [ 16]. We
enable batch normalization [ 12] with decay 0.9 in all con-
volutional and fully-connected layers (except for the output
layers) to accelerate the training process. Our implementa-
tion is available at [ 6].
The training procedure is as follows: for a given cross-
view image pair, (Ia,Ig), we ﬁrst compute the ground
semantic pixel label: Ig→Lg, using SegNet [ 2].
We then minimize the cross entropy between Lgand
T(A(Ia;ΘA);ΘT)with respect to the model parameters,
ΘAandΘT. The resulting architecture requires a signiﬁ-
cant amount of memory to output the full ﬁnal feature map,
which would normally result in very small batch sizes for
GPU training. Due to the PixelNet approach of using inter-
polation to scale the feature maps, we are able to perform
sparse training. Instead of outputting the full-size feature
map, we only extract a dense grid of points, the resulting
feature map is 17 ×17×4. Despite this, at testing time, we
can provide an aerial image and generate a full-resolution,
semantically meaningful feature map.
4. Evaluation and Applications
In this section, we will show that our network architec-
ture can be used in four different tasks: 1) weakly super-
vised semantic learning, 2) aerial imagery labeling, 3) ori-
entation regression and geocalibration, and 4) cross-view
image synthesis. Additional qualitative results and the com-
plete network structure used for cross-view image synthesis
can be found in our supplemental materials.
4.1. Weakly Supervised Learning
We trained our full network architecture (with randomly
initialized weights) to predict ground-level semantic label-
ing using the dataset described in Section 3.2. Figure 5
shows example output, La, from the aerial image under-
standing CNN. This demonstrates that the resulting network
Figure 5. Example outputs from our weakly supervised learning
method on test images. For each aerial image (top), we show the
pixel-level labeling inferred by our model, which uses only noisy
ground image segmentation as labels. We visualize three classes:
road (red), vegetation (green), and man-made (blue).
has learned to extract semantic features from an aerial im-
age, all without any manual annotated aerial imagery.
While these results are compelling, they could be better
with a higher quality ground image segmentation method.
The method we use, SegNet [ 2], was trained on mostly ur-
ban scenes, but many of our images are from rural and sub-
urban areas. The end result is that certain classes are often
mislabeled in the ground imagery, including dirtandbuild-
ing. In addition, because the panoramas and aerial images
were not captured at the same time, we are unable to accu-
rately model transient objects, such as vehicles and pedes-
trians. All these factors make the dataset very challenging
for training. Given these limitations, it is surprising that the
resulting aerial image segmentation method works so well.
In the following section, we show using this network as a
starting point for strongly supervised aerial image segmen-
tation outperforms two standard initialization methods.
4.2. Cross­view for Pre­training
We evaluate our proposed technique as a pre-training
strategy for the task of semantic pixel labeling of aerial
imagery. Starting from the optimal weights from the pre-
vious section, we ﬁnetune and evaluate using the ISPRS
dataset [ 26]. This dataset contains 33 true orthophotos cap-
tured over Vaihingen, Germany. The ground sampling dis-
tance is 9 cm/px and there are over 168 million pixels in
total. Ground truth is provided for 16 photos; each pixel is
assigned one of six categories: Impervious surfaces, Build-
ing, Low vegetation, Tree, Car, andClutter/background .
Image Processing Compared to the Bing Maps imagery
we used for pre-training, images in the ISPRS dataset are
at a different spatial scale and the color channels represent
different frequency bands (the R channel is actually a near
infrared channel). To ensure that the pre-trained network
weights are appropriate for the new dataset, we adjusted the
scale and color channels as follows. We ﬁrst resize the IS-
PRS images to the equivalent of Bing Maps zoom level 19.
We then label a pixel as vegetation ifR
R+G+Bis greater
871

Figure 6. An example from the ISPRS dataset [ 26]. (left) Near
infrared image; (middle) The same image after pre-processing;
(right) Ground-truth annotation of the image.
than 0.4. For each pixel labeled as vegetation, we halve the
R channel intensity and swap the R and G channels. The
resulting images, shown in Figure 6, are much closer in ap-
pearance to the Bing Maps imagery than the raw imagery.
Evaluation Protocol We split the 16 annotated images
into training (images 5, 7, 11, 13, 15, 17, and 21), vali-
dation sets (images 1 and 3) and testing (images 23, 26, 28,
30, 32, 37, and 40). From each set we extracted a set of
224×224 subwindows (respectively 82, 12, and 34 from
training, validation, and testing respectively). We then com-
pared performance with different numbers of training im-
ages: 1, 2, 7, 20, 54, and 82. We evaluated the performance
in terms of the average precision for all pixels. We ignore
theClutter/background pixels because of the low number of
assigned pixels.
Training and Testing We used the same architecture with
the aerial feature extractor, A(Ia;ΘA), deﬁned in Sec-
tion 3.1, to do semantic labeling on ISPRS. During train-
ing, we use the Adam optimizer to minimize the cross en-
tropy between the network outputs and the labels. We use a
batch size of 8, randomly sample 1,000 pixels per image for
sparse training, and train the network. We run the valida-
tion set every 1,000 training iterations and save the optimal
network weights for testing. During testing, we sample all
pixels on the image to generate the dense labeling.
We experiment using three different initializations of the
VGG16 convolutional layers and ﬁnetune the remaining
layers of the network: 1) Ours: initialize with model pre-
trained using our framework; 2) Random: initialize using
Xavier initialization [ 9]; 3) VGG16: initialize with model
pre-trained on ImageNet.
Since the VGG16 model we used in this experiment is
trained without batch normalization, it may be less com-
petitive. To achieve a fair comparison, we turned off batch
normalization in this experiment and re-trained the network
for 15 epochs to get the pre-trained model.
Our results (Figure 7) show that ﬁnetuning from the
VGG16 model performs poorly on the aerial image label-
ing task. We think that the patterns it learned mostly from
the ground image may hinder pattern learning for aerial im-1 2 7 20 54 8200.10.20.30.40.50.60.70.80.91
  
Ours
Random
VGG16
Figure 7. Performance comparison of different initialization meth-
ods on the ISPRS segmentation task. The x-axis is the number of
training images and the y-axis is average precision.
Table 1. Per-Class Precision on the ISPRS Segmentation Task
Class Init.Number of training samples
1 2 7 20 54 82
Imp.Ours 0.67 0.74 0.63 0.64 0.66 0.64
Random 0.70 0.70 0.54 0.62 0.61 0.73
VGG16 0.60 0.55 0.55 0.61 0.70 0.59
BldgOurs 0.72 0.76 0.76 0.80 0.75 0.78
Random 0.56 0.62 0.63 0.64 0.82 0.71
VGG16 0.78 0.72 0.71 0.69 0.70 0.75
Low.Ours 0.37 0.43 0.51 0.65 0.67 0.67
Random 0.29 0.29 0.29 0.37 0.67 0.64
VGG16 0.25 0.25 0.29 0.44 0.53 0.57
TreeOurs 0.68 0.54 0.71 0.71 0.74 0.74
Random 0.42 0.46 0.49 0.56 0.71 0.69
VGG16 0.36 0.44 0.50 0.55 0.65 0.74
CarOurs 0.13 0.46 0.67 0.48 0.48 0.49
Random 0.05 0.08 0.10 0.25 0.45 0.57
VGG16 0.05 0.11 0.20 0.20 0.25 0.23
agery. Our method outperforms both of the other initial-
ization strategies. We also present the prediction precision
per class in Table 1. We highlight that our method does
better especially on the Building ,Low vegetation , and Tree
classes, which can also be found in pre-training annotations.
4.3. Cross­view for Geocalibration
We show how the ground-level feature maps we estimate
from aerial imagery can be used to estimate the orientation
and location of a ground image. We show quantitative re-
sults for the orientation estimation task and qualitative re-
sults for simultaneous orientation and location estimation.
We use the following datasets for all experiments:
•CVUSA: We use the test set introduced in Section 3.2to
create this dataset; it has two parts for orientation estima-
872

Figure 8. Qualitative results of orientation predictions on Cityscapes dataset (top) and CVUSA (bottom). The Ig,LgandLg′are stacked
vertically on the left side of the aerial image. We visualize three classes on the labels: road (red), vegetation (green), and man-made (blue).
The discrete PDFs of the ground camera orientation are visualized with red arrows, whose lengths indicate the magnitudes. In the CVUSA
results, the ground truth (green) and the optimal prediction (blue) are also shown with the orientation PDF. The last prediction result is a
typical failure case of our method, where the scene is symmetric from the top-down view.
-180 -120 -60 0 60 120 1800200400600800100012001400
Figure 9. A histogram of orientation errors on the CVUSA dataset.
tion and geocalibration, respectively. For the orientation
regression task, we rotate the aerial image to a random
angle. For the ﬁne-grained geocalibration experiment,
we center-crop the aerial image around a random x,y
offset, then rotate the image to a random angle. In both
experiments, the heading direction of the ground images
are the same. We center crop a 224 ×448 cutout from
each ground image as the query image.
•Cityscapes: The Cityscapes dataset [ 5] is a recently re-
leased benchmark dataset designed to support the task of
urban scene understanding through semantic pixel label-
ing. It consists of stereo video from 50 different cities and
ﬁne pixel-level annotations for 5,000 frames and coarse
pixel-level annotations for 20,000 frames.
Orientation Estimation For this task, we assume the lo-
cation and focal length of the ground image, Ig, is known
but the orientation is not. The intuition behind our method is
that the semantic labeling of the ground image will be most
similar to the feature map of the aerial image at the actual
orientation. For a query ground image, Ig, the ﬁrst step isto download the corresponding aerial image, Ia. We then
infer the semantic labeling of the query image, Ig→Lg,
and predict the ground image label from the aerial image
using our learned network, Ia→Lg′. We assign an energy
to each possible orientation by computing the cross entropy
betweenLgandLg′in a sliding window fashion across all
possible orientations. We select the orientation with the
lowest energy. We present sample results in Figure 8and
a histogram of the orientation errors on the CVUSA dataset
in Figure 9.
Fine-grained Geocalibration For this task, we assume
that we know the focal length of the camera and have a
rough estimate of the camera location (i.e., within 100 me-
ters). We extract 256 ×256 aerial images from the area
around our rough estimate and extract the corresponding
ground-level feature maps. We apply our orientation esti-
mation procedure to each feature map. The result is a distri-
bution over orientations for each location. Figure 10shows
several example results, including the most likely direction
for each location, as well as the most likely location and
orientation pair.
4.4. Aerial to Ground Image Synthesis
We propose a novel application to infer a ground image
by using features extracted from our network. Our network
architecture is based on the deep, directed generative model
proposed by Kim et al. [15]. Their model consists of two
parts: a deep generator, G, which generates images that try
to minimize a deep energy model, E. A low energy im-
plies the image is real and high energy implies the image is
fake. The architecture and training methods are inspired by
generative adversarial networks [ 10], however it provides
an energy-based formulation of the discriminator to address
common instabilities of adversarial training.
873

Figure 10. Fine-grained geocalibration results on CVUSA. (left) From top to bottom are the Ig,Lg, andLg′respectively. We visualize
three classes on the labels: road (red), vegetation (green), and man-made (blue). (right) Orientation ﬂow map (red), where the arrow
direction indicates the optimal direction at that location and length indicates the magnitude. We also show the optimal prediction and the
ground-truth frustums in blue and green respectively.
Figure 11. Synthesized ground-level views. Each row shows an
aerial image (left), its corresponding ground-level panorama (top-
right), and predicted ground-level panorama (bottom-right).
We begin by extracting an 8 ×40×512 cross-view fea-
ture map, f, that has been learned to relate an aerial and
ground image pair. The generator is given falong with
random noise, z, as input. The generator outputs a 64 ×
320 panorama, Iˆg, that represents the predicted ground im-
age. The cross-view feature, predicted panorama, and the
ground truth panorama, Ig, are input to the energy model.
Batch normalization [ 12] is applied in every layer of both
models, except for the ﬁnal layers. ReLU activations are
used throughout the generator and Leaky ReLU, with leak
parameter α= 0.2, are used in the energy model. The mod-
els are updated in an alternating fashion, where the genera-
tor is updated twice for every update of the energy model.
Both the generator and energy model are optimized using
the Adam optimizer, with moment parameters β1= 0.5
andβ2= 0.999. We train using batch sizes of 32 for 30
epochs. A complete description of the architecture used in
this section is provided in our supplemental materials.Example outputs generated by our network are shown in
Figure 11. Each row contains an aerial image (left), its re-
spective ground panorama (top-right), and our prediction of
the ground scene layout (bottom-right), which would ide-
ally be the same. The network has learned the most com-
mon features, such as roads and their orientations, as well
as trees and grass. However, it has difﬁculty hallucinating
buildings and the sky, which is likely caused by highly vari-
able appearance factors.
We note that the resolution of the synthesized ground-
level panoramas is much lower than the original panorama,
however adversarial generation of high-resolution images is
an active area of research. We expect that in the near future
we will be able to use our learned features in a similar man-
ner to generate full-resolution panoramas. Additionally, al-
gorithmic improvements to our ground image segmentation
method would provide more photo-realistic predictions.
5. Conclusion
We introduced a novel strategy for using labeled ground
images as a form of weak supervision for learning to un-
derstand aerial images. The key is to simultaneously learn
to extract features from the aerial image and learn to map
from the aerial to the ground image. We demonstrated that
by using this process we are able to automatically extract se-
mantically meaningful features from aerial imagery, reﬁne
these to obtain more accurate pixel-level labeling of aerial
imagery, estimate the location and orientation of a ground
image, and synthesize novel ground-level views. The pro-
posed technique is equally applicable to other forms of im-
agery, including NIR, multispectral, and hyperspectral. For
future work, we plan to explore richer ground image anno-
tation methods to explore the limits of what is predictable
about a ground-level view from an aerial view.
Acknowledgements
We gratefully acknowledge the support of NSF CA-
REER grant (IIS-1553116), a Google Faculty Research
Award, and an AWS Research Education grant.
874

References
[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,
C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al.
Tensorﬂow: Large-scale machine learning on heterogeneous
distributed systems. arXiv preprint arXiv:1603.04467 , 2016.
5
[2] V . Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A
deep convolutional encoder-decoder architecture for image
segmentation. arXiv preprint arXiv:1511.00561 , 2015. 3,5
[3] A. Bansal, X. Chen, B. Russell, A. Gupta, and D. Ramanan.
Pixelnet: Towards a General Pixel-level Architecture. arXiv
preprint arXiv:1609.06694 , 2016. 3
[4] G. Cheng and J. Han. A survey on object detection in optical
remote sensing images. CoRR , abs/1603.06201, 2016. 2
[5] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
R. Benenson, U. Franke, S. Roth, and B. Schiele. The
cityscapes dataset for semantic urban scene understanding.
InCVPR , 2016. 7
[6]https://github.com/viibridges/crossnet .5
[7] H. Daume III and D. Marcu. Domain adaptation for statis-
tical classiﬁers. Journal of Artiﬁcial Intelligence Research ,
26:101–126, 2006. 2
[8] N. Ghouaiel and S. Lef `evre. Coupling ground-level panora-
mas and aerial imagery for change detection. Geo-spatial
Information Science , 19(3):222–232, 2016. 2
[9] X. Glorot and Y . Bengio. Understanding the difﬁculty of
training deep feedforward neural networks. In International
Conference on Artiﬁcial Intelligence and Statistics , 2010. 6
[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
D. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio. Gen-
erative adversarial nets. In NIPS , 2014. 7
[11] A. Huertas and R. Nevatia. Detecting buildings in aerial im-
ages. Computer Vision, Graphics, and Image Processing ,
41:131–152, 1988. 2
[12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating
deep network training by reducing internal covariate shift. In
ICML , 2015. 5,8
[13] M. Jaderberg, K. Simonyan, A. Zisserman, et al. Spatial
transformer networks. In NIPS , 2015. 2
[14] I.-H. Jhuo, D. Liu, D. Lee, and S.-F. Chang. Robust visual
domain adaptation with low-rank reconstruction. In CVPR ,
2012. 2
[15] T. Kim and Y . Bengio. Deep directed generative models
with energy-based probability estimation. arXiv preprint
arXiv:1606.03439 , 2016. 7
[16] D. Kingma and J. Ba. Adam: A method for stochastic opti-
mization. In ICLR , 2015. 5
[17] S. Kluckner, T. Mauthner, P. M. Roth, and H. Bischof. Se-
mantic classiﬁcation in aerial imagery by integrating appear-
ance and height information. In ACCV , 2009. 2
[18] T.-Y . Lin, S. Belongie, and J. Hays. Cross-view image ge-
olocalization. In CVPR , 2013. 2
[19] T.-Y . Lin, Y . Cui, S. Belongie, and J. Hays. Learning
deep representations for ground-to-aerial geolocalization. In
CVPR , 2015. 2[20] J. Luo, J. Yu, D. Joshi, and W. Hao. Event recognition: view-
ing the world with a third eye. In ACM Conference on Mul-
timedia , 2008. 2
[21] G. M ´attyus, S. Wang, S. Fidler, and R. Urtasun. Hd maps:
Fine-grained road segmentation by parsing ground and aerial
images. In CVPR , 2016. 2
[22] V . Mnih and G. E. Hinton. Learning to detect roads in high-
resolution aerial images. In ECCV , 2010. 1,2
[23] V . Mnih and G. E. Hinton. Learning to label aerial images
from noisy data. In ICML , 2012. 2
[24] S. Paisitkriangkrai, J. Sherrah, P. Janney, V .-D. Hengel, et al.
Effective semantic pixel labelling with convolutional net-
works and conditional random ﬁelds. In IEEE/ISPRS Work-
shop: Looking From Above: When Earth Observation Meets
Vision , 2015. 1
[25] V . M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual do-
main adaptation: A survey of recent advances. IEEE Signal
Processing Magazine , 32(3):53–69, 2015. 2
[26] F. Rottensteiner, G. Sohn, M. Gerke, and J. D. Wegner. Is-
prs test project on urban classiﬁcation and 3d building re-
construction. Commission III-Photogrammetric Computer
Vision and Image Analysis, Working Group III/4-3D Scene
Analysis , pages 1–17, 2013. 5,6
[27] S. M. Seitz and S. Baker. Filter ﬂow. In ICCV , 2009. 1,2
[28] K. Simonyan and A. Zisserman. Very deep convolutional
networks for large-scale image recognition. In ICLR , 2015.
3
[29] H. Sun, S. Liu, S. Zhou, and H. Zou. Unsupervised cross-
view semantic transfer for remote sensing image classi-
ﬁcation. IEEE Geoscience and Remote Sensing Letters ,
13(1):13–17, 2016. 2
[30] N. N. V o and J. Hays. Localizing and orienting street views
using overhead imagery. In ECCV , 2016. 2
[31] J. D. Wegner, S. Branson, D. Hall, K. Schindler, and P. Per-
ona. Cataloging public objects using aerial and street-level
images-urban trees. In CVPR , 2016. 2
[32] W. Willuhn and F. Ade. A rule-based system for house re-
construction from aerial images. In ICPR , 1996. 2
[33] S. Workman and N. Jacobs. On the location dependence of
convolutional neural network features. In IEEE/ISPRS Work-
shop: Looking From Above: When Earth Observation Meets
Vision , 2015. 2
[34] S. Workman, R. Souvenir, and N. Jacobs. Wide-area im-
age geolocalization with aerial reference imagery. In ICCV ,
2015. 2,4
[35] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View
synthesis by appearance ﬂow. In ECCV , 2016. 2
875

arXiv:2104.11757v2  [cs.CY]  3 May 2021BecomingGood at AIforGood
MeghanaKshirsagar∗
Microsoft AIforGood
USACalebRobinson∗
MicrosoftAI forGood
USASiyu Yang∗
MicrosoftAIfor Good
USA
ShahrzadGholami
MicrosoftAI forGood
USAIvanKlyuzhin
MicrosoftAIfor Good
USASumit Mukherjee
MicrosoftAIfor Good
USA
MdNasir
MicrosoftAI forGood
USAAnthony Ortiz
MicrosoftAIfor Good
USAFelipeOviedo
MicrosoftAIfor Good
USA
DarrenTanner
MicrosoftAI forGood
USAAnusuaTrivedi
MicrosoftAIfor Good
USAYixi Xu
MicrosoftAIfor Good
USA
MingZhong
MicrosoftAI forGood
USABistraDilkina
University of SouthernCalifornia
USARahulDodhia
MicrosoftAIfor Good
USA
JuanM. LavistaFerres
MicrosoftAIfor Good
USA
ABSTRACT
AI for good (AI4G) projects involve developing and applying ar-
tiﬁcial intelligence (AI) based solutions to further goals in areas
such as sustainability, health, humanitarian aid, and soci al justice.
Developing and deploying such solutions must be done in coll ab-
oration with partners who are experts in the domain in questi on
andwhoalreadyhaveexperienceinmakingprogresstowardss uch
goals. Based on our experiences, we detail the diﬀerent aspe cts of
this type of collaboration broken down into four high-level cat-
egories: communication, data, modeling, and impact, and di still
eleven takeaways to guide such projects in the future. We bri eﬂy
describetwocasestudiestoillustratehowsomeofthesetak eaways
were applied inpracticeduringourpast collaborations.
CCS CONCEPTS
•General and reference →Surveys and overviews ; •Human-
centered computing →Collaborative and social computing; •
Social andprofessional topics →Sustainability.
∗Equal ﬁrstauthorcontribution.
Permission to make digital or hard copies of all or part of thi s work for personal or
classroomuseisgrantedwithoutfeeprovidedthatcopiesar enotmadeordistributed
for proﬁt or commercial advantage and that copies bear this n otice and the full cita-
tionontheﬁrstpage.Copyrightsforcomponents of thiswork owned byothersthan
ACMmustbehonored.Abstractingwithcreditispermitted.T ocopyotherwise,orre-
publish,topostonserversortoredistributetolists,requ irespriorspeciﬁcpermission
and/or afee. Request permissionsfrompermissions@acm.or g.
AIES ’21,May 19–21,2021,Virtual Event,USA.
© 2021 Associationfor Computing Machinery.
ACM ISBN978-1-4503-8473-5/21/05...$15.00
https://doi.org/10.1145/3461702.3462599KEYWORDS
AI forgood;collaboration;sustainability; casestudy
ACMReference Format:
Meghana Kshirsagar, Caleb Robinson, Siyu Yang, Shahrzad Gholami, Iv an
Klyuzhin,Sumit Mukherjee,MdNasir,AnthonyOrtiz,FelipeOviedo,Da r-
ren Tanner, Anusua Trivedi, Yixi Xu, Ming Zhong, Bistra Dilkina, Rahul
Dodhia, and Juan M. Lavista Ferres. 2021. Becoming Good at AI for Go od.
InProceedings of the 2021 AAAI/ACM Conference on AI, Ethics, a nd Society
(AIES ’21),May 19–21, 2021, VirtualEvent, USA. ACM,NewYork, NY,USA,
11pages. https://doi.org/10.1145/3461702.3462599
1 INTRODUCTION
Advances inartiﬁcialintelligence (AI) andcomputingpowe rhave
given rise to powerful AI tools ubiquitous in many people’s p er-
sonalandprofessionallives.Theseabilitiesareintegrat edintoour
phones and computers, and are driven mainly by businesses th at
have productizedadvances in AI at massive scales. Many of th ese
tools are broadly available and provide some social beneﬁt ( e.g.,
search engines, navigation tools). However, the promise of AI to
improve lives and protect vulnerable people and ecosystems has
notyet reached its potential.
AI for Good(AI4G) is a movement within the larger ﬁeld of AI
that aims to develop and use AI methods to further progress to -
wards goals in sustainability, health, humanitarian aid, a nd social
justice, guided loosely by the UN Sustainable Development G oals
(SDGs) and priorities within local communities. Excellent litera-
ture reviews on the topic are oﬀered by [17, 54, 61, 70]. A key
diﬀerence from commercial applications of AI is that those A I4G
problemsand successes areoftennot deﬁnedbymarket need, b ut
ratherbynon-proﬁts,socialenterprises andgovernments s eeking
tosolveproblemsthathave notfoundsolutionsintheprivat esec-
tor.Forexample,researchers intheﬁeldofcomputationals ustain-
ability[24]developandapplymethodstotackleproblemssu chas
wildlifeconservation [19],bioacoustics[82],bird-migr ation track-
ing [59] and poverty detection [34]. In recent years, a numbe r of
pieces of criticism have been directed at the AI4G movement [ 10,
26,40].Whilethesecritiquesraiseimportantconcernssuc hasthe
biasofmodelstrainedonlimiteddata,shiftingattentiona wayfrom
rootcausesofsocietalproblems,andapaternalisticunder standing
of the aﬀected community, the discussion largely centers on the
diﬃcultyof deﬁning what is “good”in oursocietalcontext.
Inthisarticlewedistillﬁrst-handexperiencesfromourre search
lab focused on AI4G projects spanning several application a reas
over two years. Cognizant of the complexity of problems in th e
AI4Gdomainandourexpertiserestrictedtothetechnicalsi deofAI
(statistics, modeling,and engineering), wecollaboratee xtensively
with external partner organizations (PO) to deﬁne good outcomes
forourprojects,sourceandcuratedata,andrealizereal-w orldim-
pact from our modeling solutions. These AI4G projects contribute
to solving problems in two ways: we develop and apply AI tech-
niques toacceleratepreviouslymanualtaskssuchasdatapr ocess-
ing to enable the PO to arrive at their solutionfaster, and we ana-
lyze and model collecteddata for additional insights. The c ollabo-
rativeandpracticalnatureofsuchprojectsmeans thatthe deliver-
ablesarenotjustmodelweights,sourcecode,andtechnicalpaper s;
they crucially involve working with these POs (whose techni cal
capabilities and infrastructure vary greatly) to develop w orkable
engineering solutions for deployment that respect resourc e con-
straintsthatPOsface,aswellascommunicatinganddocumen ting
oursolutions–and theirlimitations –forthedomainexpert sout-
side of computer science and engineering who use our models t o
impact society.
WehighlightchallengesthataremorepronouncedinAI4Gpro jects
compared to machine learning (ML) projects in the academic a nd
corporate spheres, outline strategies we have learned for u nder-
takingsuchprojects,andreﬂectondiﬃcultieswehavefaced mea-
suring our impact. We break these down into four sections in t he
restofthediscussion:communication,data,modeling,and impact.
Finally, we describetwo case studies,i.e. AI4G projects ,that exem-
plifythesediﬃcultiesandhowweapproachedtheminareal-w orld
setting.
2 COMMUNICATION
Therelationshipandinteractionbetweendatascientistsa ndPOs–
whoarethedomainexpertsthatdeﬁneproblems,curatedata, and
actonmodeloutputs–isanimportantﬁrsttopic.Domainexpe rts
sometimes have decades worth of experience working in a prob -
lem area. Communicating all of this accumulated knowledge t o
datascientistswithinafewdaysorweeksduringprojectpla nning
canbediﬃcult,butdatascientistsmustbewillingandready toin-
corporate this knowledge into their modeling approaches. W hile
straightforward,accuratebi-directionalcommunication atallstages
ofanAI4Gprojectisessentialforitssuccess,wefocusbelo wonar-
eas wheredatascientists mayneedtodrivetheconversation with
thePO.2.1 Settingrealisticexpectations from AI
It is often the case that POs have inﬂated expectations about the
capabilities of modern AI-based techniques due to the hype s ur-
rounding the ﬁeld1and its misrepresentation in the media [38].
In our experience, when initially proposing and scoping pro jects,
somePOsmaybelievethattherearepre-existingAItoolstha tcan
beimmediatelyadaptedforanichepurposewithlittletonot rain-
ing data (cf. [15]). However, our experience also shows that POs
respond well to open, honest communication about AI’s capab ili-
ties,ortheneedfor(labeled)trainingdata.Earlyconvers ationsof-
ten involved directing POs’ expectations away from a model t hat
achievesalloftheirgoals,whichwouldrequiremoretraini ngdata
than is currently available, to more targeted models for whi ch ap-
propriate and suﬃcient training data is available, and whic h will
still bring them signiﬁcantly closer to their goals. For cer tain use
cases, an adequateand achievable approach is to use AI as a co m-
plementary tool to streamline and accelerate current workﬂ ows,
ratherthanentirely supplantingthem.
One such example we have worked on involved a PO who ini-
tiallyapproachedusaboutbuildinganaturallanguageproc essing
(NLP)modeltoextractverynuancedauthorintentfromdecon tex-
tualized 1- or 2-sentence texts in a small corpus of unlabele d doc-
uments. After gaining a better understanding of the speciﬁc use
caseforthemodel,weworkedwiththePOtoreframetheproble m
in terms of multi-label topic classiﬁcation, and also worke d with
them to label the data. The result was a model that allowed the
POtoincorporateanentirelynewdatasource(text)intoabr oader
initiative focusedon quantifying human activities’ impac ts onen-
vironmental resources.
InadditiontodiscussingthegenerallimitationsofAI-bas edmeth-
odswith regards to what can beachieved, depending on the PO’ s
domain and technical expertise, it may also be crucial to mak e
them aware of more concrete issues encountered while buildi ng
MLmodels.Theseincludeoverﬁttinginsmalldataregimes,m odel
bias, generalization issues after deploying a model,adver sarial at-
tacks,dataandmodelprivacyconcerns,limitationsofinte rpretable
models, etc. For example, lesion shape irregularity is one o f the
mostcriticalfeaturesintheclinicaldiagnosisofmelanom a[1].On
theotherhand,recentstudieshavedeterminedthatconvolu tional
neural networks are negatively-biased in capturing shape- related
information from images [5, 21]. Clinical researchers who m ay
wishtodevelopaconvolutionalmodelformelanomadetectio nare
likely to be unaware of this ﬁnding, and eﬀective communicat ion
of this knowledge may facilitate the development of alterna tive
approaches. We believe that knowledge transfer should be a c ore
componentofan AI4Gproject.
In contrast to these positive cases, there may be circumstan ces
in which POs need to be informed when their goals – even with
proper reframing – may not be achievable with AI. For example ,
a model trained to detect ﬁsh species from underwater camera s
may be highly accurate in identifying a few common species, b ut
theresearchers mayliketodetectveryrarespeciesif,fore xample,
detectionof therare species is key todeciding whether some eco-
nomic development opportunityis allowed. In these cases, m odel
1For example, a Gartner 2020 report on emerging technologies places AI at the peak
intermsof inﬂated expectations [46].
creationmayneedtobepostponedfortime-consumingdataco llec-
tionand labeling. And in some cases itmay benecessary toask if
the current stateofmachine learning is abletomeaningfull y help
thePO inmeeting its goals.
Takeaway 1.EducatingPOs aboutAI’s limits and op-
portunitiesisacorepartofanAI4Gproject.Potentially
unrealistic expectations for AI can often be reframed
into achievable goals that streamline the PO’s work-
ﬂows.
2.2 Project scoping andimplementation
CollaborationsbetweenPOsandtechnicalexpertsonAI4Gpr ojects
are typically short-term or phased, hence it is important to set
priorities and expectations before beginning a project. Fo r exam-
ple, one project we have worked on involves counting herd ani -
mals (e.g. cattle) over large areas from satellite imagery t o moni-
tortheeﬀectiveness ofconservationpolicies.Thisis anex pensive
task to perform manually, but can potentially be automated w ith
computervisionmodels.Instancesegmentationoftheanima lsisa
straightforwardapproach,however itisriskyduetothelac kofla-
beleddataorlowspatialresolutionoftheimagery.Lessstr aightfor-
wardmodelingapproaches,suchascoarseranimaldensityes tima-
tionmodels,cansatisfythesameprojectgoals,butmaytake more
eﬀorttodeﬁneupfront.Suchapproachesmayalsobeinformed by
domain knowledge – herd animals move in groups, resulting in
dense crowds more easily identiﬁable in imagery than indivi dual
animals. The back-and-forth communication with the PO to un -
derstand the goal and explore appropriate modeling solutions is a
crucial partoftheprojectlifecycle.
Wesummarizequestionstoexplorewhenscopingaprojectwit h
a PO as aguideline forfutureprojects:
(1) Setting accurate goals: is the project oriented towards the
prediction or estimation of quantities, or is it towards the
visualization and representation of data? If the project is
oriented towards prediction/estimation, then whatwill be
done with the output from the model, whowill use it (say,
for decision-making), and wherewill it be deployed? If the
project is oriented towards visualization/representatio n or
to interpretability/inference, whatformat is expected, who
will be consuming the end result, and – again – wherewill
itbedeployed?
(2) What are the resource constraints of the PO? Thecompute
andstorageresourcesavailableandthetypeofdevicewhere
thesolutionwillbedeployed(cloud-based,battery-power ed
devices) should be taken into account to determine what
modelingsolutionsarepossible.
(3) WhatisthetechnicalexpertiseavailableatthePO?This will
determine theamountof supportavailablefor maintaining
a deliverablebeyond thedurationof theproject.
(4) Does the problem require developing novel machine learn -
ing techniques? This will inform the project timeline and
the risk. It could present an opportunity to identify larger
researchchallengesandbringthemintotheMLcommunity
atlarge.
(5) To what degree is data/model privacy a concern? The data
availabletothePOcanoftencontainpersonallyidentiﬁabl einformation about individuals. This would not only make
thedatasetnotreleasableforthesakeofreproducibility, but
also limit the public release of models since they may be
subjecttoprivacyattacks [62].
(6) Ispreciseinformationonthedatawelldocumented?TheP O
should convey the assumptions and the procedures under-
lyingdatacollection.Ifthedatahasalreadybeenprocesse d,
the processing steps should be communicated because cer-
tainpre-processingstepscanintroducebiasinthedata[20 ].
(7) How could the PO’s domain knowledge be incorporatedin
modeling?Duringtheprojectdevelopment stage,itmaybe
necessarytonotrelyentirelyondata-driventechniques(f or
instance, for feature learning), but also utilize PO’s know l-
edgeofdomain-relevantmetricsandfeaturesthathavebeen
identiﬁed as signiﬁcant in their ﬁeld. For example, in radi-
ology, dozens of hand-crafted radiomic features have been
previously found to be signiﬁcant predictors of cancer sur-
vival and response totherapy[4].
Takeaway 2.Toensure we develop solutionsthatare
practicallyuseful,projectscopingneedstobeanongo-
ing dialoguewiththePO.
3 DATA
Most AI4G projects start with the PO sharing a dataset they ha ve
previously collected and labeled, or pointing out public da tasets
that are relevant to the problem. While speciﬁc funding oppo rtu-
nitiesforcreatingdatasetsforMLapplicationsandnovelm ethods
to take advantage of weakly labeled public data [73, 83] or ge n-
erated data [8] exist, exploiting these is often not possibl e. This
is in contrast to enterprise applications where ML teams are also
responsible for collecting training data and have budget al located
forcuratingdatasetswithspeciﬁcMLtasksinmind.Inthiss ection
we reﬂect on recurring challenges related to data availabil ity and
qualityinthecontext ofAI4Gprojects;foranin-depth disc ussion
ofdata issues present inhigh-stakes AI applications,refe r to[57].
3.1 Adaptingtopreviously collecteddatasets
Limitations in the data collection process often inﬂuence t he de-
gree to which an AI4G project can be successful, possibly mor e
so than they aﬀect the outcomes of commercial or academic AI
projects,which have morecontrolover datacollection.
First, there is a discrepancy between the purpose of data col -
lection by POs that have the goal of solving an application pr ob-
lem and data collectionbygroups that have the goal of creati ng a
generalizable model.ThePOs need not necessarily care abou tthe
metadataassociatedwithdatapointsthattheylabelforfur thering
theirgoals,whilesuchmetadatamaybeimportantforquanti fying
how a model trained on such data will generalize. Put diﬀeren tly,
ifdatacollectionisconductedbyaPOwithafocusonthecont ent,
ratherthantechnicalspeciﬁcationsofthedata,thenthisc ancause
problemsin post-hocmodelingsteps.
Extending the example of counting herd animals from the last
section,thePO mightlabelherd animals fromsatelliteimag ery at
a variety of spatial resolutions. The imagery could be at a sp atial
resolution of 0.1, 0.3, or 0.5 m/pixel; as long as the PO can co unt
the types of animals of interest over a given area and date, th ey
canmeettheirgoals.Ontheotherhand,amachinelearningmo del
thathasbeentrainedtoidentifyherdanimalsfromonly0.1m /pixel
imagerywilllikelynotgeneralizeto0.5m/pixelimageryas objects
willbe5timessmallerineachdimension.Themetadataassoc iated
with the labeled data is necessary to inform modeling decisi ons
(e.g.augmentingthescaleofsatellitedataandlabelsduri ngmodel
training willallow themodelto generalize over a range of sc ales).
Thisisincontrasttosettingswheredataiscollectedspeci ﬁcallyfor
thepurposeofbuildingeﬀectivemodels,wheremetadatawou ldbe
considered explicitlyatthedatacollectionstage.
In addition to the completeness of metadata, quality and con -
sistency of data collection, which do not aﬀect experts’ abi lity to
discernthecontentbutwhichposeadditionalgeneralizati on chal-
lenges tomachinelearningmodels,areafrequentissue.For exam-
ple, in a study applying speech feature modeling to analyze m en-
tal health issues, the recording of conversations between m ilitary
personnel with suicide risk and their therapists is used to p redict
theiremotionalbond[43].Here,theconsistentuseofdedic atedmi-
crophones for the two speakers in a controlled environment w as
foundtoimprovemodelingoutcomes,however thisaspectofd ata
collectionwouldnotaﬀect themanual analysis of therecord ings.
Second,AI4Gprojectsofteninvolvesensitivedatanecessi tating
the adherence to strict ethical and legal restrictions that make us-
ing such data more diﬃcult. For example, many agencies would
like to train computer vision models to detect images of Chil d
Sexual Abuse Material (CSAM), but databases such as the Chil d
AbuseImageDatabase(CAID) maintainedbytheUKGovernment
are not accessible to other organizations [11]. Other examp les in-
cludeapplicationswithhealthcaredata–modelsthatident ifypul-
monaryfeaturesfromchestx-rayimageryneedtobetrainedw ith
labeledchestx-rays,adatasourcethatinitiallyrequires pairingpa-
tient records with their chest x-rays. These sensitive appl ications
require specialized privacy-preserving modeling methods and a
layer ofcomplexitythatotherapplicationsdonotentail.P Os may
choose to share synthetic data generated using generative m od-
els instead ofreal data butthese approaches also may bepron eto
membership inference attacks [30]. Recent work on privacy p re-
servingMLhasdevelopedbothdefenses[41,77]andwaystome a-
suretheprivacy risks from releasing models[33, 37].
Third, the amount and quality of data available in some AI4G
projectswillbelimited.Thisis notspeciﬁctoAI4Gproject s,how-
ever is worth mentioning due to the frequency with which such
projectscomeup.Forexample,projectsthatinvolvedetect ingpoul-
trybarns,solarfarms,orotherrelativelyuncommonfeatur esfrom
satelliteimagery requirehaving a labeleddataset ofsuch f eatures.
However, creating labeled data in these applications is exp ensive
as it requires annotators to ﬁrst ﬁnd examples of the objects in
questionoverlargelandscapesbeforelabelingthemapprop riately.
In our experience, these type of satellite image annotation s will
notbeinaformatthatisimmediatelyuseable(e.g.pointlab elsfor
a segmentation problem),or willbea biased sample(e.g. man y la-
belsfromaspeciﬁcarearatherthanasampleoflabelsfromab road
area).
Finally,opendatasetsoftenhavesigniﬁcantdatacuration issues.
For example, a Kaggle dataset for predicting the outcomeof p reg-
nancies in India has been shown to be missing key data from the
original survey, resulting in misleading predictions [67] . POs thatwanttohelpreduceinfantmortalitycanbemisledbysuchdat asets
or the promise of eﬀective models from open competitions tha t
use such data. Another issue is the lack of query infrastruct ure
aroundpublicdatasetswhichcreatessigniﬁcantfrictioni nprojects
that might use such data. A positive example is Google Earth E n-
gine[25],whichallowsresearchers toquicklyqueryacross itscol-
lection of public satellite imagery, visualize sample patc hes, and
assess if the data is of suﬃcient quantity and resolution for the
intended analysis.
Takeaway 3.DatasetsinAI4Gprojectsmaynotbeim-
mediately useful for creating models. When creating
models with such data, it is important to understand
theassociatedmetadata,collectionprocess,andanyse-
curityorprivacyconcerns.
3.2 Dealingwithsubjective data annotation
Thevariables of interest in several sociallyimportant dom ains in-
volvinghumanperceptionanddecision-makingareambiguou sand
ill-deﬁned. Diﬀerent annotators might interpret the deﬁni tion of
labels diﬀerently, leading to inconsistent and noisy label s. For ex-
ample, the colloquial meaning of “depression” might be diﬀe rent
from its meaning in a clinical context [79]. The data annotat ion
process for an AI system aimed at diagnosing clinical depres sion
should be cognizant of the diﬀerence. Creating a taxonomy of la-
belscouldminimizetheannotator’ssubjectivity.Thisals orequires
transparencyintheinterpretationofthelabelsandclearl ycommu-
nicating its limitations during diﬀerent stages of the proj ect life
cycle, including annotation, modelling and deployment to m ini-
mize the semantic ambiguity of labels. An in-depth discussi on on
ambiguous labels in the context of computational social sci ences
can be found in [14]. When subjectivity of the labels are inhe rent
duetothevariabilityinhumanperception,itshouldbeasta ndard
practicetoemploymultipleannotatorsforeachexampleand judge
thefeasibilityofthetaskbyevaluatinginter-annotatora greement.
Severalmethodshavebeenproposedtoobtainestimatesofth etrue
groundtruthlabelsfromthenoisy/subjectivelabelscolle ctedfrom
multipleannotators[42,48].
Takeaway 4.Inseveralsociallyimportantdomains,la-
bels suﬀer from subjective annotation. Such situation
should be identiﬁed upfront to avoid introducing in-
consistencies inthemodelingpipeline.
3.3 Creating trainingand testsetswiththe
application scenario in mind
Poorchoicesoftraining, validationandtestsetsplitscan resultin
an estimated model performance that does not reﬂect actual p er-
formance when deployed (for other lessons learnt in evaluat ing
model performance, see section 4.3). This is especiallyrel evant in
humanitarianaidandconservationapplicationswheremode lsare
expectedtogeneralize wellspatiallyand/or temporally.
Forinstance, withmarine mammal sounddetection[81],whil e
generatingtrain/testsplits,considerationshouldbegiv entodiﬀer-
ent types of underwater and anthropogenic noises such as tho se
from commercial ship generators, mining, aircraft, and sea sonal
eﬀectsonoceanwaves.Asanotherexample,thexBDdatasetas so-
ciatedwiththexViewChallenge[28]isalarge-scalepublic dataset
designed to enable building damage assessment for humanita rian
assistance and disaster recovery. It consists of data from 1 9 dis-
asters from around the world between 2011 and 2019. However,
scenes from the 19 disasters are present in all of the oﬃcial t rain,
testandhold-outsplits,whereasitwouldbemoreusefultor eport
performance on unseen locations as the next disaster will li kely
strike elsewhere. The same is true for the SpaceNet Challeng e Se-
ries for building footprint extraction, where the default s plits cre-
atedbythechallenge’sutilitiescontainoverlappingloca tions[69].
Subsequentstudieshaveshownthatperformancedropsdrast ically
when applying a trained building damage classiﬁer to an unse en
location, even within the same region [29, 68]. As another ex am-
plerequiringspatialgeneralization, untilveryrecently ,allstudies
of animal species classiﬁcation on camera trap images were s plit
across sequences of images but not across locations. This re sults
in precision and recall metrics of greater than 90% [44]. In r eality,
performanceismuchworseifwesplitthedatabycameralocat ion,
and even worseif we splitbyecosystem [9,58].
Many past studies in wildﬁre risk prediction, another probl em
important to both disaster relief and environmental conser vation,
assumecertainrandomvariablestobeindependentandident ically
distributed in the evaluation phase [12, 53, 56]. However, n atural
hazards like wildﬁres are stochastic events with spatio-te mporal
dimensions, and evaluating models of such events based on ra n-
domizedtraining and testsplits leads toinformationleaka ge, mis-
leading organizations whooperationalizesuch models[22] .
In applying machine learning to medical imaging, Zech et al.
[80]foundthatadatasetofchestx-rayscuratedforscreeni ngpneu-
monia cases could be used to train a model to accurately predi ct
which hospital system the x-ray comes from, indicating that the
pneumoniadetectionmodeldevelopedfromthedatasetcould have
been aided by features not related to the medical condition. The
training and testsplits should bechosen tomeasure how well the
modelwillwork forunseen x-raymachines.
Takeaway 5.Carefully consider how to splitthe data
intotrainingandtestsetssothatthemodel’sabilityto
generalize tounseen instances of inputis measured.
4 MODELING
The models used in AI4G contexts usually involve a diﬀerent s et
of requirements and constraints compared to general AI appl ica-
tions. First, AI4G models are developed in applied ML contex ts.
Most of the models are developed with domain-speciﬁc motiva -
tions and limitations in mind. In consequence, models devel oped
formainstreamMLﬁelds,suchasNLPorcomputervision,requ ire
cautious adaptation and deployment to the speciﬁc domain. F ur-
thermore, the process of model development is motivated ﬁrs t by
application requirements instead of pure novelty or state- of-the-
art performance.
4.1 Incorporatingdomain expertise
Domain expertise from the PO can help in model development as
POsoftenhavedecadesofexperienceandaccumulatedknowle dgein deﬁning and solving related problems. Speciﬁcally, doma in ex-
pertiseisusefulin:i)determiningadequatefeaturesandd atarepre-
sentations, ii) enforcing inductive bias and regularizati on in mod-
els, iii) choosing simpliﬁed parameterizations, iv) inter preting the
learned modelsand outputs.
Domain expertise is invaluable for collecting features tha t are
relevanttoaproblem.ThisisespeciallyrelevantinAI4Gpr oblems
where there are few samples compared to the number of feature s
or where informative features are mixed with noisy features . For
instance, topredict the lateeﬀects of chemotherapy oncanc er pa-
tients, we found that including all chemotherapy drugs gave us a
higher predictive performance as compared to prior work. Ho w-
ever, clinical researchers know that only certain drugs hav e been
clinicallylinkedtocertainlateeﬀectsinotherprioranal yses.This
suggests that the additional confounding features were pro bably
spuriouscontributorstopredictiveperformanceandhence should
notbeincludedin themodel.
In addition to helping determine data representation, doma in
expertise can help improve performance by embedding speciﬁ c
knowledge, often in the form of inductive bias or regulariza tion,
in model design. For example, ﬁnding promising solar cell te ch-
nologiesisoftenadiﬃcultandatime-consumingtask.Asola rcell
consistsofastackofvarioussemiconductormaterials,whe reeach
layer performs a certain electrical and optical function an d fabri-
cationparameters areoptimizedformaximizing solarenerg y cap-
tured.A machine learning modelcan avoid theneed for resour ce-
intensive physical experiments and accelerate the paramet er op-
timization step. Combining a supervised machine learning m odel
with a physical model of solar cell operation calibrated by a n ex-
pertallowed foramodelregularizationmethodbasedonphys ical
principles [49] and an order of magnitude reduction in the ti me
and resources requiredtocreate a solarcell[45, 49].
Further, AI4G projects often involve collaborations spann ing
multiplecountriesandPOsinadiﬀerent countryarelikelyt oface
unique challenges that only local experts would be aware of. For
example, work on the anti-poaching PAWS project [18] uses th e
localknowledgeofparkrangers toconstrainpredictedsear ch pat-
terns toareas that canbefeasiblyvisited.
If an interpretable model is used, then domain experts may be
able to use aggregated model predictions to draw larger conc lu-
sionsaboutaproblem.Forexample,inourcollaborationstu dying
thefoodsecurityinlow-resourcecommunitiesinMalawibas edon
survey panel dataof households,domainexperts were ableto use
the outputs of interpretable models to recognize spatial an d sea-
sonalpatternsassociatedwiththefoodsecuritystatusoft hecom-
munitiesandvillages.Thesecommunity-levelinsightscan helplo-
cal governments manage their resources more eﬃciently acro ss
thecommunities and over time.
Takeaway 6.Endeavor to incorporate the PO’s do-
main expertise in model development when possible
through methods such as feature selection and engi-
neering, modelchoice, and modelregularization.
4.2 Modeldevelopment withresource
constraints
Since deployed models are maintained by the PO who often has
lessresourcesthanenterprisesfocusedonmainstreamMLap plica-
tions,resourceconstraints oncethemodelis operationali zedlimit
the choice of models. In addition to the ﬁnancial cost of runn ing
sophisticatedmodelsonpotentiallylargedatasets,deplo yingtore-
mote regions in battery-powered devices and carbon emissio n re-
latedenvironmental costarealso importantconsideration s.
For example, Robinson et al. [51] trained a fully convolutio nal
neuralnetwork(CNN)onover55terabytesofaerialimageryt ocre-
ateahigh-resolutionlandcovermapovertheUnitedStates. Diﬀer-
ences in seconds of running time per batch translate to hundr eds
ofdollarsinthecostoftheﬁnalcomputation.Here,alarger ,state-
of-the-art model would incur a ~270% increase in the cost of t he
ﬁnalcomputationforafractionalincreaseinperformancem etrics
suchasaccuracyandintersection-over-union,andsoatrad e-oﬀin
favor of lowering thecostwas made.
Inwildlifeconservation [74]and accessibilityapplicati ons[75],
models need to be deployed to edge or mobile devices of vary-
ingcapacity.Forinstance, theSeeing AI2mobileapp,whichhelps
peoplewith visionimpairment orlow vision tobetterunders tand
their surroundings, uses deep learning architectures spec iﬁcally
designed forlow resourcesettings.
State-of-the-art deep learning models have large carbon fo ot-
prints from training and operation [63], which is a concern f or
AI4Gprojectsinparticular.Applicationssuchastheonede scribed
in Lacoste et al. [35] allow the model developer to choose a da ta
center location powered to a large extent by renewable energ y
sourcesandacloudproviderwhooﬀsetstheremaining emissi ons.
Takeaway 7.Carefully consider a project’s con-
straints during deployment in advance before settling
onamodelingapproach.
4.3 Evaluation and metrics
ModelvalidationisacrucialpartofanyAIproject.InAI4Gp rojects,
validation metrics will not only need to measure how well the
model is performing in standard ways (e.g. accuracy, AUC-RO C,
intersection over union), but how well the model is performi ng
with respecttoany domain-speciﬁc requirements.
For example, the common part of commuters (CPC) [36] is a
domain-speciﬁcmetricusedinmeasuringhowwellpredicted com-
mutingﬂowsalignwithgroundtruthdata.Thismetricjointl ycon-
siders all commutingﬂows together,as opposedtoa commonML
metric such as mean squared error that only considers pairwi se
errors between a single origin and destination. ReportingC PC re-
veals more about the overall structure of a predicted set of ﬂ ows
andisthusimportanttoreportinAI4Gprojectsthatconside rcom-
muter ormigrationﬂows [50].
Another example comes from cancer imaging, where special-
izedextensionsofthereceiveroperatingcharacteristic( ROC)anal-
ysis are used to evaluate the lesion detection performance b y ra-
diology readers: localization ROC (LROC) quantiﬁes not onl y the
correct binary diagnosis, but also takes into account the ac curacy
2https://www.microsoft.com/en-us/ai/seeing-aioflesionlocalizationwithinanimage.Thefree-responseo perating
characteristic(FROC) extends thenotionofLROC tothemult iple-
lesiondetectiontask [6].
These and other domain-speciﬁc metrics can potentially be i n-
cluded in modeling as well as in evaluation. For example, if t he
domain-speciﬁc metric is diﬀerentiable with respect to the pre-
dicted quantity and computed on a per-sample basis, then it c an
be used in combination, or in place of, common loss functions
whentrainingmodelswithgradientdescentbasedmethods.I nthe
medical-imagery domain, such loss functions have been used to
bettercapturedomain-speciﬁc problemcharacteristics [6 5].
In other cases, a domain-speciﬁc metric is not necessary, ho w-
ever domain experts will care littleabout commonly reporte dML
metrics. For example, mean average precision (mAP) involve s av-
eraging the precision of a model at all possiblerecall value s. This
averagewillinclude,forexample,theprecisionofthemode lat1%
recall which is not informative as such performance would ne ver
beacceptable. Precision@ /u1D458, where/u1D458is the lowest tolerablerecall,
is a more appropriate metric. As we discuss in Section 2, arri ving
atthis metric involves extended dialoguewith domainexper ts.
Finally, the data collection process by the PO can be imperfe ct,
thereforemodelevaluationbasedsolelyonsuchdatasetsmi ghtbe
insuﬃcient. For example, in the case of the anti-poaching PA WS
project,thedatasetiscollectedbylimitedparkrangersvi afootpa-
trolling over a vast area. As such, many regions in the protec ted
areas arenotthoroughlycovered bytherangers every montha nd
thedatasetdoesnotperfectlyrepresenttheareaunderstud y.Build-
ingalong-termcollaborationwithPOstodeploymachinelea rning
solutionsforpilottestsbeforeafullcommitmentcanbring impor-
tant insights about the performance of the trained model in t he
wild[23,78].
Takeaway 8.Check if domain-speciﬁcmetricscanbe
incorporatedduringtrainingand validationofmodels
and determine which ML metrics are relevant to solv-
ing theproblemathand.
4.4 Humans in theloop
Intheindustry,continueddatacollectionanduser-suppli edlabels
allow models to be improved over time, the so-called “data ﬂy -
wheel” eﬀect [16, 66]. In a similar manner, many scientiﬁc ﬁe lds
have come up with labeled datasets and models [13, 64, 71] tha t
arecontinuouslyupdatedas labelingtechniques(such as ph ysical
simulationsordataacquisitionmethods)areimproved.Aco mmon
industrypracticeforimprovingmodelperformanceistoite rateon
improving datasetsinstead of iterating on improving models[31].
In both cases, having humans continually in the loop – whethe r
bylabelingortuningmodelbehaviorbasedonfeedbackfroma de-
ployed system – provide large beneﬁts to the overall project out-
comes.
In general, the one-oﬀ nature of AI4G projects preclude this
commonwayofimprovingmodelperformance.Accumulatingex pert-
annotatedlabels,even thosecreatedwitheﬃciency gains en abled
by the ﬁrst version of the model, lies outsideof formal infra struc-
turesand therefore modelre-training is not doneas often.
At the same time, most of the models used in AI4G projects do
notenablecompleteautomation.Forexample,theoutputofa medi-
caldiagnosismodelwillbeinterpretedbyhealthprofessio nals,and
ﬁnal decision may be made based on the patient’s clinical his tory
and the presence of secondary signs and symptoms that are not
captured by the model [39]. Here, a human mustbe in the mod-
eling loop evaluating every output of a model. More broadly, the
output of allAI4G models will be inspected by domain experts
in the PO and their feedback will constitute a form of weak su-
pervision that must be included in the modeling process in or der
to produce a suitable deliverable. For example, POs that rel y on
highly accurate land cover data will often need to make manua l
corrections to modeled outputs, and, as such, will have a diﬃ cult
time using land cover predictions with artifacts such as rou nded
cornersonbuildingthatarenoteasilycorrectableinGISso ftware.
This type of feedback is only apparent after one iteration of mod-
elling and inspecting the results with the PO. Glacier monit oring
is another scenario where incorporatinghumans in the looph ave
beenshownvaluable.Barakaet.alproposedaglaciermappin gtool
thatusessemanticsegmentationpredictionsasastartingp ointand
allows domain experts for easy adjustments of those predict ions
for afaster glacier mappingsystem [7].
Thus, achieving a balance between having human feedback in-
cluded in the modeling process and staying within-scope is a cru-
cial part of ﬁnishing such AI4G projects. We have found activ e
learningpipelinestobebeneﬁcialtowardsthisend.Withan active
learning pipeline, participants at the PO can be engaged dir ectly
during the modeling process and will allow their feedback (i n the
form oflabels) tobedirectlyincorporatedin theﬁnaldeliv erable.
We note that the active learning loop can also incorporate hu -
mansmoretightly.Forexample,whenhumansarefurtherallo wed
tochoosethelocationstolabel(versus beingpresented loc ations),
and can observe the eﬀect labeling those locations has on mod el
output after a retraining period, they can more eﬃciently tr ain
land cover models [52]. Finally, as active learning methods more
tightly couple dataset collection with model training, the y show
promise in reducing the total amount of manual eﬀort require d
to produce a ﬁnal product. For instance, active learning tra ining
of camera trap species identiﬁcation models has been found t o
match state-of-the-art accuracy with orders of magnitude f ewer
annotated training samples [55].
Takeaway 9.AI4G projects require humans in the
loop to some extent. Active learning pipelines can en-
ablePOstoengagewiththemodelingprocessdirectly
duringa project.
5 IMPACT
Lacking in theusualbusiness indicators such as revenue and user
engagement, oneofthemostdiﬃcultaspectsofanAI4Gprojec tis
measuring the degree to which it is successful, and weighing the
success by the potential impact in advancing a PO’s mission. In
turn, an AI4G project’s potential impact will not be realize d with-
out the PO or the broader communities adopting the technolog y.
Indeed,therearemanymorepressreleases,blogposts,andp romis-
ing published results than functioning AI systems actively “doing
good”intheworld.Inthissection,weattempttounderstand whythat is by exploring the three related issues of deployment, adop-
tionand impact.
5.1 Uphillpathto deployment andadoption
Unlikeresearch developing novel techniques ortheoretica lunder-
standing, AI4G projects necessitate the deployment of any d evel-
opedmodels,andseparatelyandoftenmorediﬃcult,theadop tion
of such technologies by the PO and related communities, for t he
eﬀort tobemeaningful [72].This “last mile” problem can bee spe-
cially challenging in AI4G projects since engineering is of ten not
afocusfor aPO ora research group.
Inourexperience, deployment entails threescenarios:
i) aone-timescoringofrelevantinputdatatoproducederiv ed
datafor thePO’s downstream analysis and publication,
ii) areal-timeAPIexposingthemodelforapplicationssuch as
anti-poaching and invasive species monitoring,
iii) a batch processing mechanism triggered automatically or
bytheusertoprocessalargequantityofrawdataforrecur-
ringanalysis,suchaswhileprocessingconﬂictvideosfrom
a region forweapondetection[2].
Theﬁrstscenariorequirestheleastengineeringeﬀortbeyo ndmodel
development,butmaynotresultinlastingimpact.Real-tim eAPIs
have recurring cost, in addition to requiring upkeep and int egra-
tion with the client application consuming model output. Ba tch
processingcouldtakeadvantageofdiscountcloudcomputea tlow-
traﬃc times and parallelize model scoring. It is often neces sary
to guide the PO in understanding whether they require real-t ime
always-onmodeldeploymentorifoﬄinebatchprocessingiss uﬃ-
cient.ThereisalsoalotofenthusiasmfordeployingMLmode lson
edge devices so that the PO can avoid uploading the raw data fo r
processinginlow-connectivityregions.Inthiscase,itis important
to communicate any trade-oﬀs compressing the model through
techniquessuchasquantizationmayhaveonperformance[60 ,76].
To truly enable productivity gains from using AI tools and cl oud
infrastructure,thePOsoftenneedamuchlargerpieceofsof tware
toorchestratescoring rawdatausing themodeland interact with
themodeloutputs,ofwhichWildlifeInsights isanotableex ample
foracceleratingwildlifesurveysusingcomputervisionmo dels[3].
Adoption is the harder problem because, in many ways, it is
outside of the control of the technical team. Identifying ho w ML
metrics translate into time saved in the PO’s workﬂows is par a-
mount. Taking input from human-computer interaction exper ts
maybehelpfulat this stage, as is thinking abouthow tointeg rate
modeloutputwith thesoftwareusedindownstream analysis. For
example,beingabletopreview modeloutputsaboveacertain con-
ﬁdencethresholdcanhelpthedomainexperttoﬁlteroutinpu tﬁles
thatdonot containany subjects ofinterest; pre-populatin gthela-
bel ﬁeld with the most common class may save many keystrokes
duringmanualreview[27].Wehavealsofoundthatopen-sour cing
themodeldevelopmentcodebuildstrustinthemodel,andthe code
repositorywithdiscussionboardscanactasahubforthecom mu-
nityinvolved intransfusing AI into their domain.
Takeaway 10.Maintaining deployed modelsrequires
long-term engineering resource commitments. Focus-
ingontimesavedinsteadofpureMLmetricshelpsor-
ganizations adoptthetechnology.
5.2 Measuringimpact
Typicalmachinelearningprojectsmeasuresuccessinterms ofmodel
evaluation metrics (e.g. F1-score, ROC-AUC, etc.) (also se e discus-
sioninthelastsection)orbusinesskeyperformanceindica tor(KPIs)
(e.g.click-throughrate,dailyactiveusers,etc.).Howev er,inthelife
cycle of AI4G projects, model evaluation metrics serve more as a
basisofdiscussionswithPOsaboutamodel’scapabilitiesa ndlim-
itations; the KPIs important to the POs are outcomes that can be
several steps removed from the model outputs. It is importan t to
learn about the PO’s KPIs in the scoping phase of the project t o
inform theapproach.
A challenge in creating lastingimpact comes from the lack of
a business model for these AI4G endeavors. We are ﬁnding ways
to step out of a funding mindset and grow the technical capabi l-
ities of the PO so that they could be self-suﬃcient in subsequ ent
datacollectionandre-trainingeﬀorts.Thisisanotherpla cewhere
two-waycommunicationisimportant(seeSection2):thetec hnical
teamoftendoesnotgettoseetheimpactoftheirworkintheﬁe ld.
Maintaining arelationship withthePOafterthetechnicalp ortion
of the project is complete to get updates on how their workﬂow s
have been impacted is important in maintaining a long-term c ol-
laboration and acts as part of a larger feed-back loop for sol ving
theapplicationproblem.
InourengagementswithPOs,wedonotconcernourselveswith
deﬁningwhatisapositiveimpactagainsttheﬁnalproblemth ePO
aims to tackle. We rely on the domain experts at the PO to deter -
mine what the intended eventual impact is and to what extent a
project furthers the PO’s mission. More proximal to data mod el-
ing,incollaboratingwithPOsonAI4Gprojectswehavefound two
ways forAI techniques torealize impact:ﬁnding structurea nd in-
sightsfromlargedatasets,andmakingdomainexperts’work ﬂows
more eﬃcient so that they may scale outtheir work. Therefore , it
may requireworking with thePOto ﬁnd ways totrack theimme-
diateimpact of anAI4G modelontheir dataanalysis orworkﬂo w
eﬃciency, inadditiontoimpacts onthePO’s end mission.
Takeaway 11.Domain experts within POs should
deﬁne mission-related impacts. When quantiﬁcation
of direct model impact is needed, work with the PO
to identify opportunities to quantify both immedi-
ate (workﬂow or analysis enhancement) and farther-
removed(mission-related)impactsoftheAI4Gproject.
6 CASESTUDIES
6.1 NLP tomap Syrian conﬂict
Problem :
The Carter Center (TCC) has beenworking onsupportinga poli t-
ical solutiontothewars inSyria3.Since 2012,TCC has initiated a
conﬂict mapping project that analyzes an unprecedented vol ume
of citizen-generated information about the conﬂict. Every week,
3https://www.cartercenter.org/countries/syria.htmlTCC compiles a report using the information it receives from the
Armed Conﬂict Location and Event Data (ACLED) Project4[47],
whichcuratesnewsstoriesandarticlesrecordingincident srelated
to the war in Syria. This report is read by various committees in
the UN, foreign ministries and NGOs. Given the weekly timeli ne
(Takeaway7),collatingtheincomingdataintoastructures uitable
for their analysis manually has been diﬃcultgiven the incre asing
volumeofreports.Automatingthiscurationprocesswouldr educe
thethousandsof hoursofwork neededbyprofessional analys ts.
Solution :
Our work automated this process by classifying the informat ion
into several categories such as shelling, artillery ﬁre, an d aerial
bombardment.WehelpedTCCbuildahigh-precision,neuraln etwork-
based natural language processing (NLP) model that reclass iﬁes
the input conﬂict events at the granularity desired by TCC (T ake-
away 6). This improvement in data processing allowed TCC em-
ployeestothenfocusonsubsequentanalysisoftheconﬂicte vents
(Takeaway 9).
6.2 Mappingsolarfarmsacross India
Problem :
At the end of 2020, India was only 2% away from the target of
40% installed non-fossil fuel electricity capacity, one of its Paris
Climate Agreement targets [32]. While it is encouraging to s ee
renewable energy production systems, such as solar farms, b eing
rapidly built, it is also important to locate such installat ions in a
waythatavoidsencroachingonthehabitatsofendangeredsp ecies
andotherecologicalreserves.Aninternationalconservat ionNGO
has been working with states in India to create a tool for iden ti-
fying areas where solar and wind developments are less likel y to
cause ecological harm. However, information on where solar in-
stallations are located is only available for two states, an d so we
workedtogetherwiththeNGOtousesatelliteimagerytotryi den-
tifysolarinstallations across allof India.
Solution :
Finding solar farms from satellite imagery is straight-for ward to
formulateasasemanticsegmentationtask,butthelabelsth atwere
availablefortheprojectwerebothfewandnotintheformatn eeded
forthis MLtask:only72pointlabelsoflocationsofsolarfa rmsin
two states were available (Takeaway 3). To overcome this lim ita-
tion,weﬁrstpre-trainedaconvolutionalneuralnetworkto cluster
pixels in the input satellite imagery by color (i.e. in an uns uper-
vised manner). We then used an interactive training applica tion
toquicklyﬁne-tunethenetworktosegmenttheclassesofint erest
andusedthisﬁne-tunedmodeltoobtainweaksegmentationla bels
fortheentiretyofthestudyarea.Theseweaklabelsmakeitp ossi-
bletotrainasupervisedsemantic segmentationnetworktha twas
capable of accurately detecting solar farms. Solar farms fo und by
thissupervisedmodelwerevalidatedbyanalystsattheNGO. Into-
talwewereabletoﬁndandvalidate1422solarinstallations across
India. The human-in-the-loop process we used was a crucialc om-
ponentinbothtrainingand evaluation,enablingMLmodelsy ield
reliableresultsgiventhesmallamountsoflabelsavailabl einitially
(Takeaway 9). Given the large area of interest, we must also r ely
4https://www.acleddata.com/data/
on free satellite imagery, which is lower in resolution than com-
mercial imagery; we accept this constraint to ensure our sol ution
remains practically useful in the long term as the NGO update s
the map each year (Takeaway 10). To reduce the number of false
positive identiﬁcations, we incorporated OpenStreetMap d ata to
remove areas of roads, snow and water bodies, a post-process ing
step informed byexpertiseingeospatial analysis (Takeawa y 6).
7 CONCLUSION
Our work presents a broad overview of the considerations nec es-
sarywhileworkingonAI4Gproblemsandthechallenges encou n-
teredtherein.WeobservethatthemostusefulAI4Gprojects result
from working closely with speciﬁc stakeholders and underst and-
ing their operations and needs; attention to the particular char-
acteristics of the problem while developing ML models, metr ics
and evaluation; a deep understanding of ethics and fairness con-
cerns;acommitmenttosoundscientiﬁcandengineeringprac tices
andatransferoftechnologythatempowersthebeneﬁciaries toun-
derstand and learnfrom thesolution,and hopefully,adapti t with
theirchangingneeds.Tosupportourobservationswepresen tsev-
eral examples from our own experiences and relevant literat ure
and summarize the learned lessons in takeaways. We hope that
our endeavor helps researchers who are passionate about soc ial
good causes by bridging the gap between ML methodologies and
their potential for relevant impact. However, we note that t here
are many problems and questions still outstanding, and that we
are continually learning and growing our own repertoire in t ack-
ling challenging issues and working with POs. Becoming good at
AI4G is a process that we are actively engaged in, and we hope
others joinus and learnwith us.
ACKNOWLEDGMENTS
WewouldliketothankKarthikeyanRamamurthyforhelpfulfe ed-
backonaninitialdraft.Wewouldalsoliketothankthemanyp art-
nerorganizationswhohavecollaboratedwithusoverthepas tsev-
eral years.
REFERENCES
[1] NaheedRAbbasi,HelenMShaw,DarrellSRigel,RobertJFr iedman,WilliamH
McCarthy,ImanOsman,AlfredWKopf,andDavidPolsky.2004. Earlydiagnosis
of cutaneous melanoma: revisiting the ABCD criteria. JAMA292, 22 (2004),
2771–2776.
[2] Raja Abdulrahim. 2021. AI Emerges as Crucial Tool for Gro ups Seek-
ing Justice for Syria War Crimes. The Wall Street Journal (2021).
https://www.wsj.com/articles/ai-emerges-as-crucial- tool-for-groups-seeking-justice-for-syria-war-crime s-11613228401
[3] Jorge A Ahumada, Eric Fegraus, Tanya Birch, Nicole Flore s, Roland Kays, Tim-
othy G O’Brien, Jonathan Palmer, Stephanie Schuttler, Jenn ifer Y Zhao, Walter
Jetz,et al. 2020. Wildlife insights:A platform to maximize the potential of cam-
era trap and other passive sensor wildlife data for the plane t.Environmental
Conservation 47,1(2020), 1–6.
[4] MicheleAvanzo,JosephStancanello, and IssamElNaqa.2 017. Beyondimaging:
the promiseofradiomics. PhysicaMedica 38(2017), 122–139.
[5] N. Baker, H. Lu, Gennady Erlikhman, and P. Kellman. 2020. Local features and
global shape information in object classiﬁcation by deep co nvolutional neural
networks. VisionResearch 172 (2020), 46–61.
[6] Andriy I Bandos, Howard E Rockette, and David Gur. 2013. S ubject-centered
free-responseROC(FROC) analysis. Medical physics 40,5 (2013), 051706.
[7] ShimaaBaraka,BenjaminAkera,BibekAryal,TenzingShe rpa,FinuShresta,An-
thony Ortiz,KrisSankaran,JuanLavistaFerres,MirMatin, and Yoshua Bengio.
2020. Machine Learning for Glacier Monitoring in the Hindu K ush Himalaya.
arXiv preprint arXiv:2012.05013 (2020).
[8] SaraBeery,YangLiu,DanMorris,JimPiavis,AshishKapo or,NeelJoshi,Markus
Meister,andPietroPerona.2020. Syntheticexamplesimpro vegeneralizationforrare classes. In The IEEE Winter Conference on Applications of Computer Visi on.
863–873.
[9] Sara Beery, Grant Van Horn, and Pietro Perona. 2018. Reco gnition in Terra
Incognita.In ProceedingsoftheEuropeanConferenceonComputerVision .Munich,
Germany.
[10] BettinaBerendt.2019.AIfortheCommonGood?!Pitfall s,challenges,andethics
pen-testing. Paladyn, Journal ofBehavioral Robotics 10, 1(2019), 44–65.
[11] Roderic Broadhurst. 2020. Child sex abuse images and ex ploitation materials.
InTheHumanFactorofCybercrime ,RutgerLeukfeldtand ThomasJ.Holt (Eds.).
Routledge, 310–336.
[12] MauroCastelli,LeonardoVanneschi,andAlešPopovič. 2015. Predictingburned
areasof forest ﬁres:anartiﬁcialintelligence approach. Fireecology 11, 1 (2015),
106–118.
[13] Lowik Chanussot, Abhishek Das,Siddharth Goyal, Thiba utLavril, Muhammed
Shuaibi, Morgane Riviere, Kevin Tran, Javier Heras-Doming o, Caleb Ho, Wei-
hua Hu, et al. 2020. The Open Catalyst 2020 (OC20) Dataset and Community
Challenges. arXiv preprintarXiv:2010.09990 (2020).
[14] Nan-Chen Chen, Margaret Drouhard, Rafal Kocielnik, Ji na Suh, and Cecilia R
Aragon. 2018. Using machine learning to support qualitativ e coding in social
science:Shifting thefocustoambiguity. ACMTransactionsonInteractiveIntelli-
gent Systems(TiiS) 8, 2(2018), 1–20.
[15] Michael Chui, James Manyika, and Mehdi Miremadi. 2018. What AI can and
can’t do (yet) foryourbusiness. McKinseyQuarterly 1 (2018), 97–108.
[16] JimCollins.2019. Turningtheﬂywheel:amonographtoaccompanygoodtogreat .
Random House.
[17] Josh Cowls, Thomas King, MariarosariaTaddeo, and Luci ano Floridi. 2019. De-
signing AI for socialgood: Sevenessential factors. SSRN 3388669 (2019).
[18] FeiFang,ThanhHongNguyen,RobPickles,WaiYLam,Gopa lasamyRClements,
BoAn,AmandeepSingh,MilindTambe,AndrewLemieux,etal.2 016.Deploying
PAWS: Field Optimization of the Protection Assistant for Wi ldlife Security.. In
AAAI,Vol. 16. 3966–3973.
[19] FeiFang, Milind Tambe,BistraDilkina,andAndrew JPlu mptre. 2019. Artiﬁcial
intelligence and conservation . CambridgeUniversityPress.
[20] SalvadorGarcía,JuliánLuengo,andFranciscoHerrera .2015.Datapreprocessing
in data mining . Springer.
[21] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A.
Wichmann, and Wieland Brendel. 2019. ImageNet-trained CNN s are biased to-
wardstexture;increasingshapebiasimprovesaccuracyand robustness.In Inter-
national Conferenceon Learning Representations .
[22] Shahrzad Gholami, Narendran Kodandapani, Jane Wang, a nd Juan M. Lav-
ista Ferres. 2021. Where there’s Smoke, there’s Fire: Wildﬁ re Risk Predictive
ModelingviaHistoricalClimateData.In AnnualConferenceonInnovativeAppli-
cationsof ArtiﬁcialIntelligence (IAAI) .
[23] Shahrzad Gholami, Sara Mc Carthy, Bistra Dilkina, Andr ew J Plumptre, Milind
Tambe,MargaretDriciru,FredWanyama,AggreyRwetsiba,Mu staphaNsubaga,
Joshua Mabonga, et al. 2018. Adversary Models Account for Im perfect Crime
Data: Forecasting and Planning against Real-world Poacher s. InAAMAS. 823–
831.
[24] Carla Gomes, Thomas Dietterich, Christopher Barrett, Jon Conrad, Bistra Dilk-
ina,Stefano Ermon, FeiFang, Andrew Farnsworth,AlanFern, Xiaoli Fern,et al.
2019. Computationalsustainability:Computingforabette rworldandasustain-
ablefuture. Commun.ACM 62,9 (2019), 56–65.
[25] Noel Gorelick, Matt Hancher, Mike Dixon, Simon Ilyushc henko, David Thau,
andRebeccaMoore.2017. GoogleEarthEngine:Planetary-sc alegeospatialanal-
ysisforeveryone. Remote sensingofEnvironment 202 (2017), 18–27.
[26] Ben Green. 2019. “Good” isn’t good enough. In Proceedings of the AI for Social
Good workshopatNeurIPS .
[27] Saul Greenberg. 2020. Automated Image Recognition for Wildlife Camera Traps:
Making itWork forYou . Technical Report. Science.
[28] Ritwik Gupta, Bryce Goodman, Nirav Patel, Ricky Hosfel t, Sandra Sajeev, Eric
Heim,JigarDoshi,KeaneLucas,HowieChoset,andMatthewGa ston.2019. Cre-
ating xBD:A DatasetforAssessingBuilding DamagefromSate llite Imagery.In
Proceedings ofthe IEEE/CVFConferenceonComputerVisionan d PatternRecogni-
tion (CVPR)Workshops .
[29] Hanxiang Hao, Sriram Baireddy, Emily R Bartusiak, Lati sha Konz, Kevin La-
Tourette, Michael Gribbons, Moses Chan, Mary L Comer, and Ed ward J Delp.
2020. An Attention-Based System for Damage Assessment Usin g Satellite Im-
agery.arXiv preprint arXiv:2004.06643 (2020).
[30] JamieHayes,LucaMelis,GeorgeDanezis,andEmilianoD eCristofaro.2019. LO-
GAN: Membershipinference attacksagainstgenerative mode ls.Proceedings on
Privacy EnhancingTechnologies 2019,1 (2019), 133–152.
[31] Fred Hohman, Kanit Wongsuphasawat, MaryBeth Kery,and KayurPatel. 2020.
Understanding and VisualizingData Iteration inMachine Le arning. In Proceed-
ingsof the 2020CHIConference onHuman Factorsin Computing Systems.1–13.
[32] Anjali Jaiswal and Madhura Joshi. 2020. Climate Action : All Eyes on India.
https://www.nrdc.org/experts/anjali-jaiswal/climate -action-all-eyes-india
[33] Bargav Jayaraman, Lingxiao Wang, David Evans, and Quan quan Gu. 2020. Re-
visiting Membership Inference Under Realistic Assumption s.arXiv preprint
arXiv:2005.10881 (2020).
[34] NealJean,MarshallBurke,MichaelXie,W MatthewDavis ,DavidBLobell,and
Stefano Ermon. 2016. Combining satellite imagery and machi ne learning to
predict poverty. Science353,6301 (2016), 790–794.
[35] Alexandre Lacoste, Alexandra Luccioni, Victor Schmid t, and Thomas Dandres.
2019. Quantifying the Carbon Emissions of Machine Learning .arXiv preprint
arXiv:1910.09700 (2019).
[36] Maxime Lenormand, Sylvie Huet, Floriana Gargiulo, and Guillaume Deﬀuant.
2012. A UniversalModel of Commuting Networks. PLoS ONE 7,10 (2012).
[37] XiyangLiu,YixiXu,Sumit Mukherjee,and JuanLavistaF erres.2020. MACE:A
Flexible Framework for Membership Privacy Estimation in Ge nerative Models.
arXiv preprint arXiv:2009.05683 (2020).
[38] Gary Marcus. 2019. An Epidemic of AI Misinformation. The Gradient (2019).
https://thegradient.pub/an-epidemic-of-ai-misinform ation/
[39] D Douglas Miller and Eric W Brown. 2018. Artiﬁcial intel ligence in medical
practice: the question to the answer? The American journal of medicine 131, 2
(2018), 129–133.
[40] JaredMoore.2019. AI for not bad. FrontiersinBig Data 2(2019), 32.
[41] Sumit Mukherjee, Yixi Xu, Anusua Trivedi,and Juan Lavi staFerres. 2019. Pro-
tecting GANs against privacy attacks by preventing overﬁtt ing.arXiv preprint
arXiv:2001.00071 (2019).
[42] Md Nasir,BrianBaucom,Panayiotis Georgiou, and Shrik anth Narayanan.2015.
Redundancy analysis of behavioral coding for couples thera py and improved
estimation of behavior from noisy annotations. In 2015 IEEE International Con-
ference onAcoustics,Speech and Signal Processing(ICASSP) . IEEE, 1886–1890.
[43] MdNasir,BrianRBaucom,CraigJBryan,ShrikanthSNara yanan,andPanayio-
tisGGeorgiou.2017. ComplexityinSpeech and itsRelationt o Emotional Bond
in Therapist-Patient Interactions During Suicide Risk Ass essment Interviews..
InINTERSPEECH .3296–3300.
[44] Mohammad Sadegh Norouzzadeh,Anh Nguyen, MargaretKos mala, Alexandra
Swanson,MeredithS.Palmer,CraigPacker,andJeﬀClune.20 18. Automatically
identifying, counting, and describing wild animals in came ra-trapimages with
deep learning. Proceedings of the National Academy of Sciences (2018).
[45] Felipe Oviedo, Zekun Ren, Xue Hansong, Siyu Isaac Parke r Tian, Kaicheng
Zhang,MariyaLayurova,ThomasHeumueller,NingLi,ErikBi rgersson,Shijing
Sun, et al. 2020. Bridging the gap betweenphotovoltaics R&D and manufactur-
ing with data-drivenoptimization. arXiv preprint arXiv:2004.13599 (2020).
[46] Kasey Panetta. 2020. 5 Trends Drive the Gart-
ner Hype Cycle for Emerging Technologies, 2020.
https://www.gartner.com/smarterwithgartner/5-trends -drive-the-gartner-hype-cycle-for-emerging-technolo gies-2020/
[47] Clionadh Raleigh, Andrew Linke, Håvard Hegre, and Joak im Karlsen. 2010. In-
troducing ACLED: an armed conﬂict location and event datase t: special data
feature.Journal of peace research 47,5 (2010), 651–660.
[48] VikasCRaykar,ShipengYu,LindaHZhao,GerardoHermos illoValadez,Charles
Florin, Luca Bogoni, and Linda Moy. 2010. Learning from crow ds.Journal of
Machine Learning Research 11, 4(2010).
[49] Zekun Ren, Felipe Oviedo, Maung Thway, Siyu IP Tian, Yue Wang, Hansong
Xue, Jose Dario Perea, MariyaLayurova,Thomas Heumueller, Erik Birgersson,
et al. 2020. Embedding physics domain knowledge into a Bayes ian network
enableslayer-by-layerprocessinnovationfor photovolta ics.npj Computational
Materials 6,1 (2020), 1–9.
[50] CalebRobinsonandBistraDilkina.2018. Amachinelear ningapproachto mod-
eling human migration. In Proceedings of the 1st ACM SIGCAS Conference on
Computingand Sustainable Societies . 1–8.
[51] CalebRobinson,LeHou,KolyaMalkin,RachelSoobitsky ,JacobCzawlytko,Bis-
traDilkina,andNebojsaJojic.2019. Largescalehigh-reso lutionlandcovermap-
ping with multi-resolution data. In Proceedings of the IEEE Conference on Com-
puter Visionand Pattern Recognition . 12726–12735.
[52] CalebRobinson,AnthonyOrtiz,KolyaMalkin,BlakeEli as,AndiPeng,DanMor-
ris, Bistra Dilkina, and Nebojsa Jojic. 2019. Human-Machin e Collaboration for
FastLand CoverMapping. arXiv preprint arXiv:1906.04176 (2019).
[53] MarcosRodriguesandJuandelaRiva.2014.Aninsightin tomachine-learningal-
gorithmstomodelhuman-causedwildﬁreoccurrence. EnvironmentalModelling
&Software 57(2014), 192–201.
[54] David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochan ski, Alexandre La-
coste, Kris Sankaran, Andrew Slavin Ross, Nikola Milojevic -Dupont, Natasha
Jaques, Anna Waldman-Brown, et al. 2019. Tackling climate c hange with ma-
chine learning. arXiv preprint arXiv:1906.05433 (2019).
[55] Mohammad Sadegh Norouzzadeh, DanMorris,Sara Beery, N eel Joshi, Nebojsa
Jojic, and Jeﬀ Clune. 2019. A deep active learning system for species identiﬁ-
cation and counting in camera trap images. Methods in Ecology and Evolution
(2019).
[56] YoussefSaﬁand AbdelazizBouroumi.2013. Predictiono f forestﬁresusingarti-
ﬁcialneuralnetworks. Applied MathematicalSciences 7,6 (2013), 271–286.
[57] Nithya Sambasivan, Shivani Kapania, Hannah Highﬁll, D iana Akrong,
Praveen Kumar Paritosh, and Lora Mois Aroyo. 2021. "Everyon e wants to do
the model work,not the datawork":DataCascadesinHigh-Sta kesAI.[58] Stefan Schneider, Saul Greenberg, Graham W Taylor, and Stefan C Kremer.
2020. Three critical factors aﬀecting automated image spec ies recognition per-
formance forcameratraps. Ecologyand evolution 10,7 (2020), 3503–3517.
[59] Daniel Sheldon, Andrew Farnsworth, Jed W Irvine, Benja min Van Doren,
Kevin F Webb, Thomas G Dietterich, and Steve Kelling. 2013. A pproximate
Bayesianinferenceforreconstructingvelocitiesofmigra tingbirdsfromweather
radar.InProceedings of theTwenty-Seventh AAAIConferenceon Artiﬁc ialIntelli-
gence.
[60] TaoSheng,ChenFeng,ShaojieZhuo,XiaopengZhang,Lia ngShen,andMickey
Aleksic.2018. A quantization-friendly separableconvolu tion formobilenets. In
20181stWorkshoponEnergyEﬃcientMachineLearningandCog nitiveComputing
for Embedded Applications (EMC2) .IEEE, 14–18.
[61] Zheyuan Ryan Shi, Claire Wang, and Fei Fang. 2020. Artiﬁ cial intelligence for
social good: A survey. arXiv preprint arXiv:2001.01818 (2020).
[62] Reza Shokri, Marco Stronati, Congzheng Song, and Vital y Shmatikov. 2017.
Membership inference attacks against machine learning mod els. In2017 IEEE
Symposium on Security and Privacy (SP) . IEEE, 3–18.
[63] EmmaStrubell,Ananya Ganesh,and AndrewMcCallum.201 9. Energy and pol-
icyconsiderationsfordeeplearninginNLP.In 57thAnnual MeetingoftheAsso-
ciation forComputational Linguistics(ACL) .
[64] ShijingSun,ArmiTiihonen,FelipeOviedo,ZheLiu,Jan akThapa,NoorTitanPu-
tri Hartono, Anuj Goyal, Clio Batali, Alex Encinas, Jason Yo o, et al. 2021. A
Physical Data Fusion Approach to Optimize Compositional St ability of Halide
Perovskites. Matter(2021).
[65] Saeid Asgari Taghanaki, Yefeng Zheng, S Kevin Zhou, Bog dan Georgescu,
Puneet Sharma, Daguang Xu, Dorin Comaniciu, and GhassanHam arneh. 2019.
Comboloss:Handlinginputandoutputimbalanceinmulti-or gansegmentation.
Computerized Medical Imagingand Graphics 75(2019), 24–33.
[66] Erik Trautman. 2018. The Virtuous Cycle of AI Products.
https://www.eriktrautman.com/posts/the-virtuous-cyc le-of-ai-products
[67] Anusua Trivedi,Sumit Mukherjee, Edmund Tse, Anne Ewin g, and JuanLavista
Ferres.2019. RisksofUsingNon-veriﬁedOpenData:Acasest udyonusingMa-
chine Learning techniques for predicting Pregnancy Outcom es in India. arXiv
preprint arXiv:1910.02136 (2019).
[68] TinkaValentijn,JacopoMargutti,MarcvandenHomberg ,andJormaLaaksonen.
2020.Multi-HazardandSpatialTransferabilityofaCNNfor AutomatedBuilding
DamageAssessment. Remote Sensing 12,17 (2020), 2839.
[69] Adam Van Etten, Dave Lindenbaum, and Todd M Bacastow. 20 18. Spacenet:
A remote sensing dataset and challenge series. arXiv preprint arXiv:1807.01232
(2018).
[70] Ricardo Vinuesa, Hossein Azizpour, Iolanda Leite, Mad eline Balaam, Virginia
Dignum, Sami Domisch, Anna Felländer, Simone Daniela Langh ans, Max
Tegmark, and Francesco Fuso Nerini. 2020. The role of artiﬁc ial intelligence
in achieving the Sustainable Development Goals. Nature communications 11, 1
(2020), 1–10.
[71] RandiVita,SwapnilMahajan,JamesAOverton,Sandeep K umarDhanda,Sheri-
dan Martini, Jason R Cantrell, Daniel K Wheeler, Alessandro Sette, and Bjoern
Peters. 2019. The immune epitope database (IEDB): 2018 upda te.Nucleic acids
research47,D1 (2019), D339–D343.
[72] Kiri Wagstaﬀ. 2012. Machine learning that matters. arXiv preprint
arXiv:1206.4656 (2012).
[73] SherrieWang,WilliamChen,SangMichaelXie,GeorgeAz zari,andDavidBLo-
bell.2020. Weaklysuperviseddeeplearningforsegmentati onofremotesensing
imagery. Remote Sensing 12,2 (2020), 207.
[74] BenGWeinstein.2018. Acomputervisionforanimalecol ogy.JournalofAnimal
Ecology87,3 (2018), 533–545.
[75] Christine T Wolf. 2020. Democratizing AI? experience a nd accessibility in the
age of artiﬁcialintelligence. XRDS: Crossroads,The ACMMagazine for Students
26, 4(2020), 12–15.
[76] Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen , Sy Choudhury,
Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bil l Jia, et al. 2019.
Machinelearningatfacebook:Understandinginferenceatt heedge.In 2019IEEE
International Symposium on High Performance Computer Archi tecture (HPCA) .
IEEE, 331–344.
[77] Liyang Xie, Kaixiang Lin, Shu Wang, Fei Wang, and Jiayu Z hou. 2018. Diﬀer-
entially privategenerativeadversarialnetwork. arXiv preprintarXiv:1802.06739
(2018).
[78] Lily Xu, Shahrzad Gholami, Sara Mc Carthy, Bistra Dilki na, Andrew Plumptre,
MilindTambe,RohitSingh,MustaphaNsubuga,JoshuaMabong a,MargaretDri-
ciru, et al. 2020. Stay Ahead of Poachers: Illegal Wildlife P oaching Prediction
and Patrol Planning Under Uncertainty with Field Test Evalu ations (Short Ver-
sion). In 2020 IEEE 36th International Conference on Data Engineerin g (ICDE).
IEEE, 1898–1901.
[79] AmirHosseinYazdavar,HusseinSAl-Olimat,MonirehEb rahimi,GoonmeetBa-
jaj,TanviBanerjee,KrishnaprasadThirunarayan,Jyotish manPathak,andAmit
Sheth. 2017. Semi-supervisedapproachto monitoring clini cal depressivesymp-
toms in social media. In Proceedings of the 2017 IEEE/ACMInternational Confer-
ence on Advances in Social NetworksAnalysisand Mining .1191–1198.
[80] John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Cos ta, Joseph J Ti-
tano, and Eric Karl Oermann. 2018. Variable generalization performance of a
deeplearningmodeltodetectpneumoniainchestradiograph s:across-sectional
study.PLoS medicine 15,11 (2018), e1002683.
[81] Ming Zhong, Manuel Castellote, Rahul Dodhia, Juan Lavi sta Ferres, Mandy
Keogh,andArialBrewer.2020. Belugawhaleacousticsignal classiﬁcationusing
deep learning neural network models. The Journal of the Acoustical Society ofAmerica147, 3(2020), 1834–1841.
[82] MingZhong,JackLeBien,MarconiCampos-Cerqueira,Ra hulDodhia,JuanLav-
ista Ferres, Julian P Velev, and T Mitchell Aide. 2020. Multi species bioacous-
tic classiﬁcationusing transferlearning of deep convolut ional neural networks
with pseudo-labeling. Applied Acoustics 166 (2020), 107375.
[83] Zhi-Hua Zhou. 2018. A brief introduction to weakly supe rvised learning. Na-
tional Science Review 5,1 (2018), 44–53.
 
Abstract — In this work we consider the application of 
convolutional neural networks (CNNs) for pixel -wise labeling 
(a.k.a., semantic segmentation) of remote sensing imagery (e.g., 
aerial color or hyperspectral imagery). R emote sensing imagery is 
usually stored in the form of very large images, referred to as  
“tiles”, which are too large to be segmented directly using most 
CNNs and their associated hardware.  As a result, during label 
inference, smaller sub -images, called “patches”, are processed 
individually and then “stitched” back together to create a tile -sized 
label map. There are many variants of stitching in the literature 
involving, for example, averaging overlapping labels, or cli pping 
labels near the edges of the output label image .  There is relatively 
little explanation or justification offered for these variants in the 
literature, and little experimental evidence of the impact or 
superiority of any particular approach.  To addr ess these 
limitations, we provide a survey of existing stitching approaches, 
and then explain how all approaches are fundamentally motivated 
by translational variance  of segmentation networks – that is, the 
label predicted for a particular pixel depends up on its relative 
position in the input patch.   We explore the primary causes of 
translational variance in modern CNNs, and support this with 
experimental evidence.  Finally, we recommend a stitching 
strategy to maximize label accuracy and minimize computati onal 
costs.  The proposed method contributed to our winning entry in 
the INRIA building labeling competition .   
 
Index Terms — semantic segmentation, convolutional neural 
networks, deep learning, aerial imagery, building detection  
I. INTRODUCTION  
Convolutional  neural networks (CNNs) are now the dominant 
method for semantic segmentation (i.e., dense pixel -wise 
labeling) of remote sensing imagery, such as color or 
hyperspectral satellite imagery [1]–[8]. For example, 
performance in several recent benchmark problems has been 
dominated by CNNs including a recent Kaggle competition for 
building labeling [5], the INRIA building labeling competition 
[7], and the recent ISPRS labeling competition [8].   
Here we co nsider the unique challenges of performing label 
inference on large remote sensing imagery  using semantic 
segmentation CNNs, termed segmentation networks (SNs) . 
Raw remote sensing imagery is often stored as large image 
“tiles”, which cannot be processed directly, as a whole, because 
of limited memory on the graphics processing units (GPUs) 
used by modern CNNs.  A common solution to this problem is 
to extract smaller sub -images, termed patches , and process 
them individually. This process – termed stitching  - is 
illustrated in Fig. 1.  Once label patches are inferred for each 
input patch, they are placed back into position in order to form 
a label tile.   A. Variations of stitching  
Label stitching is not simply the process of concatenati ng 
label patches however, and there are three major variants in the 
literature .  These variants employ additional processing with the 
goal of improving label accuracy.  Perhaps t he most common 
class of approaches  involves overlapping the output label 
image s so that the labels overlap and can be averaged [2], [9] –
[12].  The amount of overlap varies across approaches, resulting 
in different numbers of labels being involved in the averages . 
One study also applied a weighted averaging scheme among 
overlapping pixels  [9].  
A second class of approaches involves clipping the edges of 
the output label patches , and then concatenating the remainder 
of the patch without overlap or averaging [5], [7], [13] .  A final 
class of approaches simply c oncatenate the output label patches 
without any modification [3].    
We provide additional explanati on and motivation for these 
approaches in Section IV.  For now , it suffices to say that, 
despite the existence of several classes of stitching  approaches , 
little justification or experimental evidence has been provided 
for any particular approach.  In gene ral, it is unclear why one 
approach may be preferable.     
B. Contributions of this work  
In this work , we aim to elucidate the process of stitching, and 
provide guidance on how to achieve the best tradeoffs between 
computational costs and label accuracy. Towa rds this goal, we 
analyze d existing stitching approaches  and their motivations .  A 
central finding of that investigation is that all existing methods 
are motivated by  the absence  of perfect translational  Tiling and s titching segmentation output for remote 
sensing : basic challenges and recommendations  
Bohao Huang1, Daniel Reichman1, Leslie M. Collins1, Kyle Bradbury2, and Jordan M.  Malof1 
 
1Department of Electrical & Computer Engineering, Duke University, D urham, NC 27708  
2Energy Initiative, Duke University, Durham, NC 27708  
 
 
 
Fig. 1.  Illustration of stitching for building segmentation in color overhead 
imagery.  Small sub -images, termed patches , of imagery are extracted from 
large tiles of remote sensing imagery (left).  Each patch is processed 
individually, producing a label patch ( top right).  The output label patches are 
then placed back into position in the tile, or “stitched”, forming a large 
contiguous label tile  (bottom right).  “Stitching” includes many variations 
such as densely extracting label patches so that the overlappin g labels can be 
averaged; clipping the edges of label patches; or simply concatenating 
neighboring label patches.  The goal of this work is to elucidate the motivation 
for these different designs, and understand which are best.  

 
equivariance  in modern CNNs. A network is translatio nally 
equivariant  if a translation of the input patch results in a 
translation of the corresponding label patch , without any other 
changes in the predicted labels .  However,  modern CNNs are 
not often perfectly equivariant, and yield changes to their label 
predictions after [14]. 
We use the term translational variance  to refer to the 
variability of the predicted labels with respect to translation.   
This behavior is illustrated in Fig. 2. In this work we identify 
the underlying causes of translational variance  in many 
modern segmentation networks: non -unary  strides, and zero -
padding s.  We support these assertions with mathematical and 
experi mental evidence,  and we quantify the relative impact of 
each cause on label accuracy .     
Based on our findings we recommend a stitching strategy to 
maximize label accuracy and minimize computational time  – 
the major goal of this work .  We support our recommendations 
with experimental results to illustrate the tradeoffs between 
label accuracy and computation. The proposed stitching 
approach was employed in our winning entry to the recent 
INRIA building labeling competition [7], [15] , and is an 
extension of preliminary work [13]. 
C. Paper organization  
The remainder of this work is or ganized as follows:  Section 
II and Section IV discuss the remote sensing datasets and the 
segmentation networks considered in our experiments, 
respectively. Section IV reviews existing motivations for 
stitching approaches, and discusses the central role of  
translational variance.  Section V discusses the two major 
causes of translational variance, including theory and 
experimental evidence to support the discussion.   In Section VI 
we recommend a general stitching approach, and provide 
experiments to suppor t its advantages.  In Section VII we 
present conclusions.  
II. THE SEGMENTATION DATA SETS  
To evaluate our hypotheses in this work we run experiments 
on two large aerial imagery datasets. One of them is The INRIA 
building labeling dataset (D1) [15] and another one is the solar 
array labeling dataset (D2) [16].  
A. The INRIA building labeling dataset (D1)  
Dataset 1 (D1) is the INRIA Aerial Image Labeling Challenge 
Dataset [15]. We select ed D1 because it is a popular benchmark 
dataset, and because its geographic diversity will include many 
different stitching conditions: different shapes and sizes of 
objects, and different class priors.  This dataset contains aerial 
RGB imagery collected from 10 cities in both the U.S. and 
Europe, however in this work, we only used the 5 cities with 
publicly ava ilable ground truth labeling: Austin, Chicago, 
Kitsap, Western Tyrol, and Vienna.  A total of 36 images were 
captured over each city at a ground sampling rate of 0.3 m.  
Each of the 36 images encompasses 2.25 km2, which translates 
to 5000 × 5000 pixels in each tile.  
B. The solar array labeling dataset (D2)  
Dataset 2 (D2) is a color (RGB) dataset of ortho -rectified 
aerial photography for the problem of pixel -wise solar 
photovoltaic array labeling [16].  D2 was included principally to help quantify the variability of our results across different 
remote sensing problems . Covered more than twice of the area 
as D1, D2 also has the advantage of ab undant data . We used a 
subset of the data comprising roughly 19,000 solar arrays, over 
1000 km2 of area collected over three municipalities in 
California, U.S.A: Fresno, Modesto, and Stockton.  This subset 
was chosen because all of the imagery was collecte d at the same 
0.3-meter resolution.  This dataset has been employed in several 
studies of semantic segmentation [9], [17] .  
 
 
III. SEGMENTATION NETWORK S 
In this work we used two popular semantic segmentation 
models: U-Net [18] and DeepLab V2 [19]. We select these two 
models because  (i) they achieved state -of-art performance in 
several benchmark datasets [8], [15]  and (ii) they represent two 
popular design choices  for segmentation CNNs, making our 
findings more relevant.  The U -Net uses an encoder -decoder  
structure  [7], [20] , while the DeepLab model uses a Res Net 
encoder (or “backbone”) with Spatial -Pyramid -Pooling [21]–
[24].  We will see that these two approaches also result in 
somewhat different behaviors with respect to stitching, making  
Fig. 2. Illustration of translational variance  in the label patches predicted 
by a modern segmentation network – the U -Net.  Each row shows the same 
(color) image patches, with the predicted labels from a segmentation 
network overlaid in transparent red.  The boundaries between neighboring 
label patches are indicated by red dashed lines.  The term Δ𝑗 indicates how 
much the imagery is horizontally translated with respect to the CNN.  Each 
row shows a different level of translation.  Horizontal translati ons manifest 
themselves as horizontally translated dashed lines in the borders because 
the area of the input imagery is held constant – this causes it to appear as 
though the network is translating.  This was done deliberately so that each 
input pixel does  not move in the visualization, and therefore it is easy to 
compare the predicted labels across different translations.  We see that the 
same pixel receives a different label, depending upon Δ𝑗.  This often 
manifests itself as discontinuities at the bo rders of neighboring label 
patches.  
 

 
it important to analyze both of them.  We briefly review these 
architectures here.  
A. The U-Net architecture  
The U -Net is a popular semantic segmentation CNN 
architecture [18] that was originally proposed for the 
segmentation of medical imagery [18]. We use the U -Net 
architecture  as it was proposed in [18], with the single exception 
that we use half as many filters in e ach convolutional layer. This 
modification was adopted because it was used by the winning 
entry in the INRIA building labeling competition  [15]. Note 
that the U -Net model does not employ any zero -padding in its 
intermediate convolutional (feature) layers. Therefore,  its 
output feature maps (and final label ma ps) are smaller than its 
input.   
B. The DeepLab V2 architecture  
The DeepLab V2 models adapt Atrous Convolution to 
maintain the spatial extent of the output feature maps 
throughout convolutional layers. In [19], the DeepLab V2 
implementation based on the ResNet -101 architecture  [25] was 
reported to outperform the implementation based on VGG16 
[26] for the PASCAL -VOC 2012 dataset [27]. Therefore, we  
chose to utilize the DeepLab V2 implementation with the 
ResNet-101 network architecture  in our experiments . Since we 
are focusing on addressing stitching problems specifically 
associated with CNNs, we did not include the Conditional 
Random Field (CRF)  often applied after post processing .  
Note that , in contrast  to the U -Net model , the DeepLab V2 
model uses zero-padding throughout its intermediate 
convolutional (feature) layers  in order to help maintain the 
spatial extent of it s output label patches . As we hypothesize in 
Section V , this has the effect of reducing the accuracy of the 
label predictions of the Deeplab V2 at the edges of its output 
label maps.  
C. Network training  
We trained both of the CNN models  (DeepLab V2 and U -Net 
using D1 and D2. For D1, the first five tiles in each city formed 
the validation set and the remaining 31 tiles in each city were 
used for training. For D2, we used the first half of the images in 
each city as the validation set, and the used the remaining h alf 
as the training set.  
The optimization procedure and the related parameter 
settings in all of the experiment s are consistent across models. 
The optimization objective function is the discrete cross 
entropy loss, which is widely used [28].  Unless specified, we 
use a batch size of 5 and patch size of 572 × 572 pixels  for the 
U-Net models  (with no zero padd ing) and 321 × 321 pixels for 
the DeepLab V2 models  as the default choice of their original 
implementations . An Adam optimizer  [29] with β1=0.9, β2=
0.999, ϵ=10−8 is used. The models are trained for 100 epochs 
with 8,000 patches per epoch . We did a grid search of hyper -
parameters and select corresponding parameters that yield best 
results. For the experiments with the U -Net no zero -padding 
model, we trained the networks with a learning rate of 10−4 and 
dropped to 10−5after 60 epochs.  For the experiments with the 
DeepLab V2 model, we trained them with a learning rate of 
10−5 and dropped to 10−6after 60 epochs.  IV. MOTIVATIONS FO R STITCHING  AND THE ROLE OF 
TRANSLATIONAL VARIAN CE 
In this section we discuss motivations for the three major 
stitching approaches  in the literature : label averaging, label  
clipping, and concatenation.  For each approach we explain the 
major motivations for it that have been provided in the 
literature, and any theoretical or experimental support that has 
been provided.   In all cases , we also explain how each of these 
approaches make some assumptions about translational 
variance in segmentation networks.   
A. Label clipping  
These approaches involve clipping (or removing) labels at the 
edge of the output label patches .  This is based on the notion 
that these edge labels exhibit relatively high error rates, on 
average, compared to labels near the center of the la bel patches .  
If edge labels do indeed exhibit higher error rates, then it is 
beneficial (for label accuracy) to clip the label -image edges to 
remove lower -accuracy labels .    We will see in IV.B that this 
is also sometimes cited as a justification for ave raging 
overlapping labels as well.  
Evidence for this notion has been provided in terms of zero -
padding in one work, and the authors  in [5] showed 
experimentally that label errors did indeed increase towards the 
edges of the  output label image for their network (a U -Net).  We 
reproduce these results on another dataset, and with an 
additional modern segmentation network (DeepLabV2).   
The notion that edge labels exhibit greater error rates implies 
that any pixel, if it happens  to reside at the edge of an input 
patch, may receive a worse prediction than if it happened to 
reside towards the center.  This is a special case of translational 
variance because it implies that any pixel, once moved to the 
edge of an input patch via a t ranslation, will occasionally 
receive a worse (i.e., different) label .  We assume here that 
imagery at the edges of the input patches is not inherently more 
challenging to label – an alternative explanation.    
B. Label averaging  
This approach involves extract ing patches densely, so that the 
output label patches overlap. The overlapping labels are then 
averaged in order to improve their accuracy.  This approach 
implicitly assumes that overlapping (i.e., spatially coincident) 
labels will occasionally disagree, s ince there is no benefit to 
averaging labels that are always identical.  However, the only 
major difference between any two labels that overlap is that the 
input imagery has been translated with respect to the 
segmentation network.  In turn, this implies t hat spatially 
coincident  label predict ions will occasionally disagree due only 
to translation, which is an implication of translational variance.   
Although averaging labels assumes the presence of 
translational variance, authors have implicated different forms 
of translational variance.  In many cases authors have suggested 
that edge labels exhibit higher error rates, and that these error 
rates can be reduced through averaging  [10]–[12].  This is the 
same motivation often given for edge clipping, but with an 
alternative solutio n.  Relatively little experimental evidence h as 
been provided to support averaging  edge labels , except in [10] 
it was reported that it provided a 1% increase in overall label 
accuracy.   As we discuss in Section VI, averaging may be 
beneficial, but clipping is probably a superior approach.   
 
In other studies , no particular type of translational variance 
was implicated , but some type of label averagin g was 
employed.  For example, [4], [6], [11]  employed label 
averaging without citin g any particular motivation.  Several 
papers have also employed label averaging so that it involves 
averaging labels that are not necessarily near the edges of the 
imagery [10]–[12], suggesting that the authors believe there are 
indeed other causes of translational variance , that may 
introduce variance across the entire label map .  As we will show 
in Section V, such translational variance does exist , although it 
is much less impactful.  
C. Label concatenation  
The last class of approaches simply involves concatenating  
the output lab el patches , without any clipping or averaging  [3]. 
This approach implies that averaging or clipping would not be 
beneficial .  This could be because the labels simply do not vary 
with respect to translations (i.e., the networks are translationally 
equivariant) or that the translational variance exists, but it is 
inconsequential enough that averaging or cl ipping would not be 
beneficial.  A s we discuss in Section V, translational variance 
of at least two kinds exists in CNNs, and ignoring it could lead 
to lower label accuracy.   
V. COMMON CAUSES  OF TRANSLATIONAL VA RIANCE  
In this section we explore two major ca uses of translational 
variance in modern segmentation networks : zero -padding and 
max-pooling.  We focus on these two causes of translational 
variance because they are widely used operations in modern 
segmentation networks , and as we show, they explain the 
translational variance motivating all variants of stitching .  As 
we describe subsequently , zero -padding results in poorer label 
accuracy at the edges of label patches.  Max -pooling does not 
lower label accuracy in any general way, but it introduces 
translat ional variance in the network  that can be reduced via 
averaging and improve label accuracy .   
While both operations give rise to translational variance, their 
impact on label accuracy is very different , and the 
computational costs of addressing them are di fferent .  This 
makes it useful to disentangle and address them separately.  For 
both zero -padding and max -pooling we explain quantifying 
their relationship to translational variance  and support our 
assertions with experiments.  
A. Zero -padding  
Zero -padding i s a common method used in convolutional 
operations (e.g., filtering) , and within CNNs . Zero -padding 
involves adding zero -valued pixels around the perimeter of an 
image (or other data), with the goal of maintaining the spatial 
or temporal size of the data a fter processing with convolution, 
max-pooling, and other common CNN operations.  Padding is 
illustrated in Fig. 3 for convolution with a 3 ×3 pixel receptive 
field (i.e., input size).   
The plausible problem with zero -padding is th at it introduces 
zeros rather than real  data into processing, potentially yield ing 
lower -quality output. In CNNs, this output usually also serves 
as input to subsequent layers, propagating the errors over a 
wider spatial area due to the non -unary  receptive  fields of 
subsequent layers . This propagation of error is illustrated in Fig. 
3.      
The precise impact of zero -padding will depend upon the 
particular statistical characteristics of the data, and therefore 
will vary.  Fig. 4 quantifies the impact of ze ro-padding on one 
of our datasets  (D1)  for U -Net.  We use a controlled experiment 
in which we apply U -Net with, and without, zero -padding.  The 
results indicate that, when zero -padding is present, it increases 
the error label rates by as much as 35% at the  edges.  A similar 
result was reported in [5] for the U -Net model but with a 
different dataset.  Thus are results here provide further support 
for this finding.  Here we also present the error rate of the 
DeepLab V2 model.  Due to the number of layers, it is infeasible 
to remove zero -padding from the model, and so there is no 
control, but it still appears that the label error rates rise towards 
the edge of the label patch.    
Note that this zero -padding effect is equivalent to  the “lack of 
context” some authors have cited in the literature for less -
accurate edge labels  [5].  That is, predicted labels at the edge of 
the label patch were based upon less data than those towards the 
center, by virtu e of being at the edge.  However, if you remove 
all zero -padding, then the label patch becomes smaller 
(sometimes substantially) than the input patch, and every label 
predicted by the network is based upon a complete set of real 
data.   This is precisely wh at happens in the U -Net, resulting in 
relatively uniform label accuracy in Fig. 4.  
  
Fig. 3. An example of how errors caused by zero -padding propagate 
through convolutional layers. Original pixels are in green cells, zero padded 
pixels are in blue and affected pixels are mark ed in yellow. Passing a 5 by 
5 pixel data cube through two 3 by 3 convolutional layers will result in only 
the center pixel being unaffected.  
 
 
Fig. 4: Percentage of errors across patches for U -Net and DeeplabV2 CNN 
models when a pplied to our INRIA dataset (D2). T he length of each curve is 
determined by its input patch size.  The distance in the x -axis represents 
max (lh,lv) where lh,lv are the corresponding horizontal and vertical distances 
in the Manhattan -distance. We  are excluding the pixels with distance smaller 
than 30 pixels because with total number of patches fixed, there are only few 
pixels within this range and the results are therefore noisy.  

 
B. Non-unary  stride  
Perhaps the primary cause of translational variance in CNNs 
are non -unary  strides (i.e., greater than one) in the network 
layers. The term stride  refers t o the pixel -wise step size that 
convolutional operations, such as pooling or filtering , take 
across an input image.  We will denote the stride of a 
convolutional operation with the symbol 𝛿. A non -unary  stride 
is one in which 𝛿>1, in which case the output image is smaller 
by a factor of 𝛿.   
Non-unary  strides introduce translational variance because 
they cause the image output to be smaller than its input by a 
factor of 𝛿.  This decimat ion of the input image causes unary  
translation s of the input image to result in f ractional (sub -unary ) 
translations in the output  image.  The result of these fractional 
strides is that the output labels simply change value, rather than 
translating. This effect is illustrated in Fig. 5 for the  popular 
max-pooling operation (e.g., the U-Net and DeepLabV2 
considered in this work both use max -pooling operations) .   
Let 𝐼 denote a 1-dimensional input image that we will input 
into a single -layer network. The network is comprised of a 
convolutional operation acting on the input image, 𝑓(⋅), with a 
stride of 𝛿0.  We have 𝐼′≔𝑓(𝐼), where 𝐼′ is the processed 
image.   The image has a new coordinate system that is related 
to the old coordinate system by    
𝑗′=𝑔(𝑗)=𝑗/𝛿    ∀𝑘′∈ℕ (1) 
The implication here is that translating the input image by a 
value, 𝑘<𝛿  results in a f raction al translation of the output 
image.  Let 𝛿 =2, then 𝑗′+1
2=𝑔(𝑗+1;𝛿 =2).  This 
translation causes us to sample 𝐼′ at a fractional location, which 
has different values than the output if no translation is applied.  
Alternatively, 𝑗′+1=𝑔(𝑗+2;𝛿 =2), in which case 𝐼′ 
exhibits the same values, but translated by one.  Both of these 
scenarios are illustrated in Fig. 5 for the max -pooling operation.   
The aforementioned observation implies that a network is 
translationally equivariant  with respect to input -image 
translations that are multiples of 𝛿 .  We specified no particular 
form for the function, 𝑓(⋅), and therefore this result applies to 
any common convolutional operation that can have non -unary  
strides, such as regular, Atrous [19], and transpose 
convolutional  or upsampling  layers; or max, averag e, or learned 
pooling layers.   
This result generalizes to larger networks that involve several 
successive pooling operations  that may be interspersed with 
other layers .  If 𝑁 denotes the number of pooling layers in a 
network,  each with stride 𝛿𝑖, where 𝑖𝑡ℎ non-unary -strided 
operation, then we have  
𝛿𝑡𝑜𝑡=∏ 𝛿𝑖𝑁
   (2) 
where 𝛿𝑡𝑜𝑡 is the effective stride between the input and output 
of a multi -layer network.  If the  individual strides are all the 
same  value , then 𝛿𝑡𝑜𝑡=𝛿𝑁.  We provide a sketch of the proof 
for this assertion in Appendix I.   
To demonstrate the relationship between pooling and 
translational equivariance , we conduct an experiment with the 
DeepLabV2 and U -Net segmentation networks. Th e 
experiment is designed to measure the correlation between the labels predicted for a single pixel, as the pixel is translated with 
respect to the segmentation network.  We expect that the labels 
will have correlations of approximately one (exact match) w hen 
we translate the imagery by factors of 𝛿.    
 
To conduct this experiment, we select four 100x100  pixel 
regions from each tile in dataset one (D1) . Then we record the 
labels predicted for each pixel in each 100x100 pixel region as 
we shift the input patch by Δ𝑗,Δ𝑖 pixels in  both the ho rizontal 
and vertical axis, respectively while maintain ing that  the patch 
covers the 100x100  pixel region. The labels from each 
translation are used to obtain a correlation matrix for U -Net and 
Deeplab V2, presented in  Fig. 6.  
The U-Net has 4 max -pooling layers, each with 𝛿=2.  
Furthermore, the U -Net has no zero -padding that may also 
introduce  translational variance.  Therefore we expect 
translations that are multiples of 24=16 pixels to yield a 
perfect correlation, and indeed that is the case for the U -Net.  
The DeepLab V2 is based on the ResNet -101 architecture, 
which has three  layers  with 𝛿=2, and we see that the 
correlations have peaks at translations of 𝛿3=8.  However, the 
correlations never reach a value of one , we hypothesize due to 
the substantial zero -padding , which introduces additional 
translational variance  at across all translations .  We can see also 
that the peaks fall farther from a value of one as the total 
translation size increases (i.e., pixels move closer to the edge),  
which is consistent with this hypothesis.    
C. Trans. variance  is only caused by two sources  
We want to draw additional attention to the correlation results 
with the U -Net in Fig. 6 .  The fact that the U -Net achieves full 
translational equivariance  - after accounting for non -unary  
stride s – suggests that the only two apparent  source s of 
translational variance  in the U -Net are (i) zero -padding, if it is 
used, and (ii) non-unary  strides . Since most networks are 
comprised of the same components as the U -Net, this is a fairly 
general statemen t about the sources of translational variance in 
CNNs .   
VI. LABEL STITCHING RECO MMENDATIONS  
In this section we provide recommendations for stitching 
based on the discussions in Section V and VI.  The two main  
Fig. 5. Illustration of a convolutional max-pooling layer, with a stride 𝛿=
2, operating on a 1 -dimensional input image, 𝐼.  This operation returns the 
maximum value of the two input values.  The output of the max -pooling 
layer is shown with respect to three different translations of the input.   The 
output image, 𝐼′(𝑗′), is presented for each possible translation.  A 
translation of one in the input equates to a translation of ½ in the output, 
which manifests itself as a change in the output, and no translation. This is 
a form of translat ional variance in the network.  Notice however, that if we 
translate by two pixels, the original output has simply been translated.  This 
indicates that the network is equivariant  with respect to input translations 
that are multiples of 𝛿.   
 

 
performance metrics we consider are computational time and 
label accuracy, in terms of the popular intersection -over-union 
(IoU) metric [27].  We make three primary recommendations, 
each described in its own subsection.  
 
A. Increase the output size during label inference  
We first make a novel strategy for stitching, which involves 
increasing the input -image size of the segmentation network 
only during label inference .  Because modern networks are 
usually fully convolutional, their input size can be altered at any 
time, including for label inference, after the network has already 
been trained.  We recommend the input size to the maximum 
memory cap acity of the hardware (usually the RAM of the  
GPU).  We find that this allows us to increase our input imagery 
from roughly 500 x500  pixels to 2500 x2500  pixels.     
This approach has several advantages.  First, it minimizes the 
amount of pixels in the outpu t label patch that are impacted by 
zero-padding.  As we show, this can improve label accuracy and 
is compatible with other processing such as label clipping, 
averaging, or doing nothing.   Another advantage is reduced 
computational time.  Much of the compu tational time required 
for modern CNNs is comprised of data handling (e.g., passing 
data between the GPU and the CNN, subsampling  and stitching  
patches, etc.).  Using this approach reduces this overhead, and 
results in substantially lower computational tim e. Next we 
conduct experiments to demonstrate the impact of this approach 
on computational time and label accuracy (IoU).  
1) Computational time improvements  
The computational time for generating label predictions with 
the U -Net and the DeepLab V2 models is sh own in  Fig. 7. The 
trend in performance is similar for both of the models that were 
considered: as the patch size increases, the running time 
decreases.  The running time decreases due to (i) reduced data 
handling overhead because  fewer  forward network pass es are 
needed and (ii) because less stitching operations are required.  
The simplicity of this approach and its faster running time are 
likely its greatest advantages .   
 
2) Label accuracy comparisons  
In Fig. 8 we compare the segmentation performance of the U -
Net and DeepLab V2 models by measuring the IoU on 
validation images . For the DeepLab V2 model, there is a large 
performance gain when increasing the input size at testing. As 
shown in  Fig. 4, the DeepLab V2 model generates poorer results 
at the boundary of input patch. By increasing the input patch 
size, the percentage of the output pixels impacted by this effect 
can be reduced, resulting in an overall performance 
improvement in t he stitched output label maps .  
 
Another more surprising experimental result is the small, but 
consistent, performance improvement the U -Net model  as the 
patch size increases.  We see in Fig. 4 that the U -Net has similar 
errors a cross its output label patch, and therefore a performance 
improvement due to label clipping is surprising.  We 
hypothesize that there may still be  relatively higher error rates 
at the edge s of its output , even though they are not apparent in 
Fig. 4 once zero -padding is removed .   
  
 
 
Fig. 6. The correlation matrices of the U -Net and the DeepLabV2 models 
on D1. We translated images by 32 pixels both horizontally and vertically 
– one pixel at a time – and then calculated the correlation of the same center 
100 by 100 pix el region. Each row shows the correlation between horizontal 
translations, Δ𝑗, and columns shows the correlation between vertical 
translations, Δ𝑖. The U -Net models has high correlations globally and also 
a value of 1 every 16 pixels. The DeepLabV2  shows peak correlations every 
8 pixels, however they are not quite equal to 1.  Also the peaks become 
smaller as the total translation increases.  See the text for explanations of 
these phenomenon.  
 
Fig. 7. Running time compari son for the U -Net and DeepLabV2 generating 
label predictions on all of the images in the validation data set in D1 and 
D2. In each plot, the input size at testing is shown on the X -axis.  
 
 
 
Fig. 8.  IoU of U -Net and DeepLabV2 on  the validation data set from D1 
and D2. In each plot, the input size at testing is shown on the X -axis. 
 
 
 

 
B. There is little benefit to averaging  labels  
As we noted in Section II.C, a popular stitching strategy 
involves averaging the labels from overlapping label patches.   
As discussed in Section V, the only rea son to average labels (in 
the manner that others have done) is if they vary due to 
translation.  As we discussed in Section VI.C , once zero -
padding is removed from networks, the only likely source of 
translational variance is due to non -unary  strides withi n the 
network layers.  Therefore an important question is whether it 
is useful to reduce this variance through averaging.   
To examine this hypothesis, we input translated versions of 
the input imagery into the U -Net and DeepLabV2 models , 
averaged all over lapping labels, and then evaluate d whether 
IoU improves.  We perform this experiment on both of our 
experimental datasets, and the results are presented in Fig. 9 
below. Note that these experiments are performed using the 
strategy recommended in VII.A of i ncreasing the input -patch 
size, and therefore the impact of zero -padding is substantially 
reduced in the DeepLabV2, and it was already absent from the 
U-Net.   
The experimental results indicate that the U -Net receives little 
or no performance improvement f rom averaging, indicating that 
averaging out the translational variance from non -unary strides 
is not very beneficial.  The DeepLabV2 includes additional 
translational variance due to the zero -padding, which we 
established is detrimental to label accuracy (see Fig. 4).  As 
we’d expect therefore, the DeepLabV2 model does exhibit 
slightly greater performance improvements from averaging, 
and it does so consistently, yielding improvements over both 
datasets.  However, once again, the p erformance improvements 
are modest, despite requiring 15x more processing because 15 
translated version of the input imagery must be processed.     
Although the computation -versus -performance tradeoffs are 
unfavorable with averaging, there appears to be li ttle risk to this 
operation (i.e., of lowering performance), and therefore, when 
label accuracy is important, it appears to be somewhat 
beneficial.   
 C. Clip label edges to maximize accuracy  
In Section V.A we showed that clipping the edges can 
mitigate label errors.  Clipping the edge labels is intended to 
remove the labels that are most negatively impacted by zero -
padding.  With the U -Net, or other relatively shallow networks, 
this can be done simply by not zero-padding convolutional 
layers  of the network .   This shrinks the output of the network 
somewhat, but if the network is shallow enough, it doesn’t 
introduce substantial computational overhead: this is especially 
true given our recommendations in Section VII.A  to increase 
the input size , in which case clipping has minimal impact on 
processing time, while maximizing performance.   
Our recommendations are a little more complicated for deep 
networks  (i.e., those with many layers) , such as DeepLabV2, in 
which there are very many layers with zero-padding .  In these 
cases, many labels will be contaminated by zero -padding . For 
example, DeepLabV2  is based on the ResNet -101, and 
therefore all predicted labels will involve some zero -values .   
Furthermore, l abels that are closer to the edge will have relied 
upon more zero -values, and therefore they might exhibit lower 
accuracy.  The results of Fig. 4 imply that there is indeed some 
increase in the error rates at the edges of DeepLabV2 , although 
it suggests that it is only noticeably detrimental for a small set 
of pi xels that are closest to the edge .  Therefore, i f label 
accuracy is most important, it may be desirable to clip the 20 -
40 pixels that are closest to the edges of label patches.  This 
approach introduces additional computational costs, but the 
proposed appr oach in Section VII.A (to increase the input -patch 
size) substantially mitigates this cost, and therefore we 
generally recommend clipping.   
VII. CONCLUSIONS  
This work is focused on the problem of stitching - a strategy 
for processing large data that cannot be stored in its entirety on 
modern hardware (GPUs), and therefore it must be processed in 
a piecemeal fashion.  A generic stitching approach is illustrated 
in Fig. 1, but there are several variants in the literature, and they 
are usually employed with little  motivation or with little 
provided experimental support.  
In this work we address the absence of investigation on 
stitching in the literature. W e provide a survey of existing 
approaches and opinion on this topic.  Based on this survey, we 
find that all va riants of stitching are essentially motivated by 
translational variance in modern segmentation networks .  We 
explain the two likely causes of translational variance, 
including experimental evidence of their existence and impact.   
Based on our investigatio ns we recommend a stitching 
strategy with the following guidelines :  
• Enlarge the network’s input -size only during label 
inference .   
• Clip the edges of the output label images  to remove higher -
error labels  
• There is little benefit to averaging  labels  from tr anslated 
input,  if recommendation (1) is followed.  If label accuracy 
is a priority, then averaging labels from many (10 or more ) 
translated input images can yield small, but consistent , 
accuracy  improvements .  
The first recommendatio n is particularly impo rtant – we show 
that it  substantially reduces computation time, while also  
Fig. 9. IoU performance comparison for both the U -Net and the 
DeepLabV2 model on D1. The number of shifted pixels being aggregated 
is labeled in the X axis. 
 

 
providing modest improvements in label accuracy.  The 
proposed stitching approach was employed in our winning 
entry to the recent INRIA building labeling competition [7], 
[15], and is an extension o f our preliminary work [13].   
We note th at, although our segmentation datasets are both 
focused on remote sensing applications, we believe our major 
conclusions should not depend upon the dataset or problem, and 
should apply to other application domains.  
ACKNOWLEDGEMENTS  
We thank the NVIDIA corp oration  for donating a GPU  for this 
work, and the NSF XSEDE computational environment and the 
Duke Compute Cluster s for providing computing resources. We 
would also like to thank the Duke University Energy Initiative 
for supporting our work.  
APPENDIX I 
Appendix I provides a sketch of the proof for the relationship 
between translational equivariance  and non -unary -strided 
network layers in Section VI.B. To begin, l et 𝛿1 refer to the 
stride of a network layer 𝑓1(⋅) that is operating (without loss of 
generality) on a 1 -dimensional input 𝐼′(𝑗′).  So, 𝑓1(⋅) could be 
a convolutional filtering operation or a pooling operation, and 
𝐼′(𝑗′) could be the input imagery, or the output of some 
intermediate layer.   If we apply 𝑓1(⋅) to 𝐼′, we have a ne w 
image 𝐼′′ and its new coordinate system, denoted by 𝑗′′.  
Furthermore, the relationship between 𝑗′′ and 𝑗′ is given by 
equation (1) in Section V.B.   
Based on the results discussed in Section V.B and illustrated 
in Fig. 5, we found that transla tions that are multiples of 𝛿1 will 
result in translational equivariance .  We can write this 
equivariance relationship as   
𝐼′′(𝑗′′+𝑘)=𝑓1(𝐼′(𝑗′+δ1𝑘))    ∀𝑘 ∈ℕ (3) 
This simply states that translating 𝐼′ by factors of δ𝟏 results  in 
a translated, but otherwise equal, version of 𝐼′′.  Conversely, 
this will not be true for alternative translations of the input.    
Let us now assume that 𝐼′ is actually the output of a previous 
function, 𝑓0  with a s tride of δ0 being applied to preceding input 
data,  denoted  𝐼(𝑗).  We want to know what kinds of translations 
in 𝐼 will satisfy the constraint in (3) for equivariance.  To 
achieve this, we can use the relationship in equation (1) to map 
the permissibl e equivariant translation s in (3) into the 
coordinate system of the input image 𝐼.  Th is is given by  
𝑗=𝑔−1(𝑗′+𝛿1𝑘 )=𝛿1𝑗′+𝛿1𝛿0 𝑘 (4) 
This equation suggests that translations in 𝐼 that are factors of 
𝛿1𝛿0 𝑘 will satisfy equation (3), ensuring that 𝐼′′ is equivariant 
with respect to translations in 𝐼.  Note that we can ignore 𝛿1𝑗′ 
because it is a fixed arbitrary translation of 𝐼, and in practice 
this can be any value.  Notice that maintaining the equi variance 
constraint for equation (3), also satisfies the constraint for 
translational equivariance between 𝐼 and 𝐼′.  More precisely, if 
we let 𝑘′=𝛿1𝑘 then 𝑘′∈ℕ, satisfies the equivariance 
constraint (not explicitly written, but similar to ( 3)) with respect 
to 𝐼′.   Extending this result inductively (i.e., supposing there are 
more preceding layers  in the network)  yields equation (2) in the 
text.  If all non -unary  strides are the same, then we find that 
strides of 𝛿𝑁 in a network’s input  imagery should yield 
translation equivariant outputs from the it, where 𝑁 is the 
number of (not necessarily consecutive) layers.    
REFERENCES  
[1] S. Piramanayagam, W. Schwartzkopf, F. W. Koehler, and E. Sab er, 
“Classification of remote sensed images using random forests and 
deep learning framework,” no. October 2016, p. 100040L, 2016.  
[2] J. Sherrah, “Fully Convolutional Networks for Dense Semantic 
Labelling of High -Resolution Aerial Imagery,” pp. 1 –22, 2016 . 
[3] M. Volpi and D. Tuia, “Dense semantic labeling of subdecimeter 
resolution images with convolutional neural networks,” IEEE Trans. 
Geosci. Remote Sens. , vol. 55, no. 2, pp. 881 –893, 2017.  
[4] N. Audebert, B. Le Saux, and S. Lefèvre, “Semantic Segmenta tion of 
Earth Observation Data Using Multimodal and Multi -scale Deep 
Networks,” 2016.  
[5] V. Iglovikov, S. Mushinskiy, and V. Osin, “Satellite Imagery Feature 
Detection using Deep Convolutional Neural Network: A Kaggle 
Competition,” 2017.  
[6] D. Marmanis, J. D. Wegner, S. Galliani, K. Schindler, M. Datcu, and 
U. Stilla, “Semantic Segmentation of Aerial Images With an 
Ensemble of Cnns,” ISPRS Ann. Photogramm. Remote Sens. Spat. 
Inf. Sci. , vol. III -3, no. July, pp. 473 –480, 2016.  
[7] B. Huang et al. , “Large -scale semantic classification: outcome of the 
first year of inria aerial image labeling benchmark,” in International 
Geoscience and Remote Sensing Symposium , 2018.  
[8] J. D. W. Franz Rottensteiner, Gunho Sohn, Markus Gerke, “ISPRS 
Test Project on Urban Clas sification and 3D Building 
Reconstruction,” ISPRS - Comm. III - Photogramm. Comput. Vis. 
Image Anal. Work. Gr. III / 4 - 3D Scene Anal. , pp. 1 –16, 2013.  
[9] J. Camilo, R. Wang, L. M. Collins, K. Bradbury, and J. M. Malof, 
“Application of a semantic segment ation convolutional neural 
network for accurate automatic detection and mapping of solar 
photovoltaic arrays in aerial imagery,” in IEEE Applied Imagery 
Pattern Recognition Workshop , 2017.  
[10] N. Audebert and B. Le Saux, “Semantic segmentation of earth 
observation data using multimodal and multi -scale deep networks,” 
in Asian conference on Computer Vision , 2017, pp. 180 –196. 
[11] H. Wang, Y. Wang, Q. Zhang, S. Xiang, and C. Pan, “Gated 
convolutional neural network for semantic segmentation in high -
resoluti on images,” Remote Sens. , vol. 9, no. 5, pp. 1 –15, 2017.  
[12] D. Marmanis, K. Schindler, J. D. Wegner, S. Galliani, M. Datcu, and 
U. Stilla, “Classification with an edge: Improving semantic image 
segmentation with boundary detection,” ISPRS J. Photogramm. 
Remote Sens. , vol. 135, pp. 158 –172, 2018.  
[13] B. Huang, L. M. Collins, K. Bradbury, and J. M. Malof, “Deep 
convolutional segmentation of remote sensing imagery: a simple and 
efficient alternative to output image stitching,” in International 
Geoscience an d Remote Sensing Symposium , 2018.  
[14] S. Soatto and A. Chiuso, “Visual Representations: Defining 
Properties and Deep Approximations,” pp. 1 –20, 2014.  
[15] E. Maggiori et al. , “Can Semantic Labeling Methods Generalize to 
Any City ? The Inria Aerial Image Labeling Benchmark,” pp. 3226 –
3229, 2017.  
[16] K. Bradbury et al. , “Distributed solar photovoltaic array location and 
extent dataset for remote sensing object identification,”  Sci. data , vol. 
3, p. 160106, 2016.  
[17] J. M. Malof, K. Bradbury, L. M. Collins, and R. G. Newell, 
“Automatic detection of solar photovoltaic arrays in high resolution 
aerial imagery,” Appl. Energy , vol. 183, 2016.  
[18] O. Ronneberger, P. Fischer, and T.  Brox, “U -Net: Convolutional 
Networks for Biomedical Image Segmentation,” pp. 1 –8, 2015.  
[19] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. 
Yuille, “DeepLab: Semantic Image Segmentation with Deep 
Convolutional Nets, Atrous Convolution, and F ully Connected 
CRFs,” pp. 1 –14, 2016.  
[20] V. I. Iglovikov, S. Seferbekov, A. V. Buslaev, and A. Shvets, 
“TernausNetV2: Fully Convolutional Network for Instance 
Segmentation,” 2018.  
 
[21] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking 
Atrou s Convolution for Semantic Image Segmentation,” 2017.  
[22] L. Chen, Y. Zhu, G. Papandreou, F. Schroff, and C. V Aug, “Encoder -
Decoder with Atrous Separable Convolution for Semantic Image 
Segmentation.”  
[23] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyr amid scene parsing 
network,” Proc. - 30th IEEE Conf. Comput. Vis. Pattern Recognition, 
CVPR 2017 , vol. 2017 –Janua, pp. 6230 –6239, 2017.  
[24] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. 
Belongie, “Feature Pyramid Networks for Object Detec tion,” 2016.  
[25] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for 
image recognition,” in Proceedings of the IEEE conference on 
computer vision and pattern recognition , 2016, pp. 770 –778. 
[26] K. Simonyan and A. Zisserman, “Very Deep Convol utional 
Networks for Large -Scale Image Recoginition,” Intl. Conf. Learn. 
Represent. , pp. 1 –14, 2015.  
[27] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. 
Zisserman, “The pascal visual object classes (VOC) challenge,” Int. 
J. Comput. Vis. , vol. 88, no. 2, pp. 303 –338, 2010.  
[28] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning . MIT 
Press, 2016.  
[29] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic 
Optimization,” pp. 1 –15, 2014.  
 
REPORT
A global coral reef probability map generated using convolutional
neural networks
Jiwei Li1•David E. Knapp1•Nicholas S. Fabina1•Emma V. Kennedy2•
Kirk Larsen3•Mitchell B. Lyons2•Nicholas J. Murray2,4•Stuart R. Phinn2•
Chris M. Roelfsema2•Gregory P. Asner1
Received: 14 April 2020 / Accepted: 11 September 2020 / Published online: 24 September 2020
/C211Springer-Verlag GmbH Germany, part of Springer Nature 2020
Abstract Coral reef research and management efforts can
be improved when supported by reef maps providing local-
scale details across global extents. However, such maps aredifﬁcult to generate due to the broad geographic range of
coral reefs, the complexities of relating satellite imagery to
geomorphic or ecological realities, and other challenges.However, reef extent maps are one of the most commonly
used and most valuable data products from the perspective
of reef scientists and managers. Here, we used convolu-tional neural networks to generate a globally consistent
coral reef probability map—a probabilistic estimate of the
geospatial extent of reef ecosystems—to facilitate scien-tiﬁc, conservation, and management efforts. We combined
a global mosaic of high spatial resolution Planet Dove
satellite imagery with regional Millennium Coral ReefMapping Project reef extents to build training, validation,
and application datasets. These datasets trained our reef
extent prediction model, a neural network with a dense-unet architecture followed by a random forest classiﬁer,
which was used to produce a global coral reef probability
map. Based on this probability map, we generated a globalcoral reef extent map from a 60% threshold of reef prob-
ability (reef: probability C60%, non-reef: probabil-
ity\60%). Our ﬁndings provide a proof-of-concept
method for global reef extent estimates using a consistent
and readily updateable methodology that leverages modern
deep learning approaches to support downstream users.These maps are openly-available through the Allen Coral
Atlas.
Keywords Coral reef /C1Deep learning /C1Earth observation /C1
Planet Dove /C1Millennium Coral Reef Mapping Project /C1
Remote sensing
Introduction
Accurate and reliable maps are a prerequisite for quanti-fying and analyzing geospatial patterns and the processes
that underpin those patterns. With coral reefs experiencingunprecedented change (Hughes et al. 2018 ; Eakin et al.
2019 ), reef management and monitoring agencies, as well
as the science community, require information on the
location and extent of these environments from local (km)
to global scales, to understand and manage these biodiverseand valuable ecosystems. Global reef maps are funda-
mental to the valuation of reef ecosystem services
(Pendleton et al. 2016 ; Spalding et al. 2017 ), understanding
the past (Heron et al. 2016 ), present (Burke et al. 2017 ),
and future threats to reefs (Van Hooidonk et al. 2016 ),
supporting more effective conservation (Beyer et al. 2018 )
and reef restoration strategies (Foo and Asner 2019 ), and
facilitating scientiﬁc collaborations and research outcomesTopic Editor Morgan S. Pratchett
Electronic supplementary material The online version of this
article ( https://doi.org/10.1007/s00338-020-02005-6 ) contains sup-
plementary material, which is available to authorized users.
&Gregory P. Asner
gregasner@asu.edu
1Center for Global Discovery and Conservation Science,Arizona State University, Arizona 85281, USA
2Remote Sensing Research Centre, School of Earth andEnvironmental Sciences, The University of Queensland,
Queensland 4072, Australia
3Vulcan Inc, Washington 98104, USA
4College of Science and Engineering, James Cook University,
Queensland 4811, Australia
123Coral Reefs (2020) 39:1805–1815
https://doi.org/10.1007/s00338-020-02005-6
(McManus 1994 ). Now, increasingly sophisticated Earth
observation data and analytical tools provide an opportu-
nity to improve the spatial and thematic resolution of reef
maps, both over large spatial extents and at increasingtemporal frequencies (Roelfsema et al. 2020 ).
A single coral reef map has been the basis of most
modern scientiﬁc and management efforts. In 1994, theUnited Nations Environment Program (UNEP) World
Conservation Monitoring Centre (UNEP-WCMC) began
compiling and digitizing 150 years of reef maps into a
single layer ‘‘World Atlas of Coral Reefs’’ (Spalding et al.
2001 ). Concurrently, Landsat 7 imagery made it possible to
begin estimating reef extents from space (Millennium
Coral Reef Mapping Project; hereafter, MCRMP), espe-
cially in remote or inaccessible locations (Andrefouet et al.2006 ). The combination of these efforts and others has
resulted in the UNEP-WCMC Global Distribution of Coral
Reefs data product (Spalding et al. 2001 ; IMaRS-USF
2005 ; UNEP-WCMC et al. 2018 ), leading to many of the
outcomes already cited and innumerable others. At the
same time, its accuracy can be inconsistent because theunderlying data are derived from multiple sources with
different methodologies, and regions have different levels
of sampling intensity.
There have been efforts to iterate on the UNEP-WCMC
reef map with direct observations, remote sensing, and
modeling approaches, often to produce maps with detailedgeomorphic and ecological features. The NOAA National
Ocean Service has developed their mapping effort, cover-
ing 43,000 km
2from 0 to 150 m depth within US waters in
the Paciﬁc and Caribbean (Monaco et al. 2012 ). More
recently, the Allen Coral Atlas is developing a global reef
map using ﬁeld and satellite techniques carried out on aregion-by-region basis (Lyons et al. 2020 ;http://allencor
alatlas.org ), while the Living Oceans Foundation has
mapped, by their estimates, 95,000 km
2(5% of the global
reef area) using seaﬂoor observations, depth sounding, and
WorldView satellite imagery (Purkis et al. 2019 ). These
various approaches are promising and may lead to syner-gistic outputs, i.e., methodological differences between
regions in a single map can be problematic, whereas map
‘‘ensembles’’ can be created and analyzed when maps areinternally consistent but differ from one another in terms of
goals, assumptions, and methodologies.
Convolutional neural networks (CNNs) are commonly
used for computer vision and image processing tasks. CNNs
have been used extensively in remote sensing (Cheng et al.
2017 ; Ma et al. 2019 ) and are rapidly being adopted for
ecological applications of remote sensing data (Brodrick
et al. 2019 ; Christin et al. 2019 ; Li et al. 2020b ). Similar to
other conventional approaches, CNNs can incorporatespectral or color information, but they have the advantage of
incorporating spatial context, local or global texture features,and can self-generate features rather than rely on hand-
crafted features from experts. At the same time, CNNs also
have greater programming and computational overhead, and
they require large training datasets. Even so, CNNs are anatural choice for developing geospatial data resources from
high-resolution satellite imagery.
We present a global coral reef probability map and a
global coral reef extent map generated by convolutional
neural networks and Planet Dove satellite imagery, with the
ultimate goal of linking coral reef ecology and monitoring
groups. Our maps are a useful resource because they: (1)
use a single methodology and are therefore globally con-sistent and updateable, (2) leverage modern deep learning
methods to build on previous approaches, while incorpo-
rating new ones, and (3) provide a unique comparison withother mapping resources. Here we describe the develop-
ment of the satellite imagery and reef extent inputs, the
model architecture, and training strategy. We then reporton the results from the global coral reef probability map
and global coral reef extent map at global and regional
scales. Finally, we discuss opportunities for improving theapproach and future maps.
Materials and methods
Planet Dove satellite imagery
The features used to train the model came from a global
natural color composite mosaic generated from the PlanetDove satellite constellation. The mosaic was generated by
the Planet team ( www.planet.com ) using 554,663 individ-
ual scenes collected between 1 Oct. 2017 and 1 Sept. 2018.It contains three spectral bands corresponding to red,
green, and blue portions of the solar-reﬂected spectrum.
The global visual mosaic is split into sub-regional imagescalled ‘‘quads’’, with each image being 4096 94096 pix-
els at approximately 4.5 m resolution, or almost
20,000 920,000 m. We ﬁltered the visual mosaic down to
any quads in tropical and subtropical regions which may
have any amount of reef area, for a total of 50,084 quads.
The ﬁltering was achieved by making a global mask ofwaters less than 20 m deep using the GEBCO bathymetry
layer (GEBCO Group 2019 ), and large turbid or rocky
embayments (non-reef) were additionally ﬁltered out usinga normalized difference index threshold from a 2017 global
median MODIS mosaic ([band 1 -band 3]/[band
1?band 3] [-0.8). This mask was then buffered by
10 km, and we then conﬁrmed that all existing reef poly-
gons from the UNEP-WCMC mapping were covered.
The Planet mosaicking process modiﬁed band values
and inter-band relationships to create a ‘‘Dove visual
mosaic’’ that was meant to be a seamless representation of1806 Coral Reefs (2020) 39:1805–1815
123
the globe with limited cloud and cloud shadow. In addition
to losing the inter-band relationships, this also meant that
raw reﬂectance values and the near-infrared band are not
available. As a result, this visual mosaic of red, green, andblue bands precluded an effective classiﬁcation of land
features, correction for sun glint or waves, or water depth
estimation, among other challenges. While these signiﬁ-cantly limit the use of the visual mosaic for some scientiﬁc
applications, the curated removal of cloud and cloud sha-
dow by Planet in creating the visual mosaic was considered
invaluable in the often cloud-covered tropics.
Millennium Coral Reef Mapping Project (MCRMP)
reef features
The reef data used to train the model were acquired from
the open-access MCRMP maps, which cover a subset of
the world’s reefs (IMaRS-USF 2005). The MCRMP-clas-siﬁed reef features were based on a supervised classiﬁca-
tion method and manual post-processing. The features were
classiﬁed into several hierarchical categories, from coarseto detailed, and covered a representative sample of global
reef diversity. We chose to train the model with only this
dataset to ensure that reef features were created with con-sistent deﬁnitions and methodologies. The MCRMP maps
were manually reviewed to assess consistency with the
Dove visual mosaic. The MCRMP maps for Chagos, SriLanka, Tobago, and Vietnam were removed due to low
correspondence between the visual mosaic and the
MCRMP reef features, that is, sometimes reef featureswould not be visually apparent due to water depth or clarity
and therefore would not be effective training examples for
the model. This helped to ensure that the model would ﬁndmore consistent relationships between the visual mosaic
and reef features. The remaining MCRMP training data
features were spread across 2518 of the 50,084 visualmosaic quads (2425 after reducing the training dataset
further, see below).
The MCRMP reef classes were reduced to a subset that
would be tractable to model and sufﬁcient for predicting
global reef extent. The MCRMP classiﬁcations have ﬁve
hierarchical levels of resolution, from coarse (‘‘L1’’) todetailed (‘‘L5’’). We chose the L4 attribute level as the
starting point because the L4 level appeared to be adequate
for our needs, that is, the L3 attributes were not ﬁne enoughto separate spectrally-distinct reef features into distinct
classes, while the L5 attributes appeared to sometimes
partition spectrally- and ecologically-similar reef featuresinto distinct classes. We mapped the 62 L4 attribute classes
to 11 combined classes for our model training process
(Table 1): one for land, two for water, ﬁve for reef features,
and three for non-reef features (submerged, but visible
features which did not represent reef area). The reef classeswere separated into fore reefs, shallow reef ﬂats, variable
depth reef ﬂats, pinnacles, and lagoons.
Several assumptions guided this mapping process as a
result of our manual exploration of the data, with threecritical assumptions driving the majority of our work. First,
we focused on mapping reef and non-reef classes sepa-
rately. Second, depth plays a signiﬁcant role in theappearance (or lack thereof) of reef features in the imagery,
so we largely kept shallow and variable depth classes
separate, and removed most deep features completely,
using the MCRMP depth attribute to group these classes.
The two deep classes retained in the model were the ‘‘shelfhardground’’ and ‘‘shelf-slope’’, as they were more often
visible in the Dove visual mosaic and represent a signiﬁ-
cant portion of the benthos in some regions, including theCaribbean. Third, we redeﬁned lagoon classes from non-
reef to reef because they are fundamental geological,
ecological, and socio-economic components of reefecosystems (Aswani and Vaccaro 2008 ; Montaggioni and
Braithwaite 2009 ).
Other assumptions were made to best facilitate the
modeling process (detailed information can be found in
Table 1). First, we removed features with classes contain-
ing ‘‘with constructions’’ from the training data, including‘‘terrace with constructions’’ or ‘‘lagoon with construc-
tions’’. Each of these classes represents multiple spectral or
geomorphic features in a single class—e.g., both terraceand constructions, or both lagoon and constructions—and
we instead train the model to predict these classes sepa-
rately and at a ﬁner spatial resolution. Second, we mappedthe ‘‘aquatic land features’’ and ‘‘brackish atoll lagoon’’
classes to their own water class, as they were visually
distinct from other water features in many cases and aseparate class would likely improve model performance.
Third, we removed features in the ‘‘reticulated fringing’’
class as their boundaries were not always precisely oraccurately deﬁned, at least relative to our visual mosaic.
All modiﬁcations above did not have a large effect on the
results because they impacted a small proportion of fea-tures, or focused on non-reef classes.
Supplemental training dataThe MCRMP data alone are not sufﬁcient for training a
global reef model because they do not include classes forother non-reef geospatial features found in the Dove visual
mosaic. Speciﬁcally, the MCRMP data have no classes for
deep water or clouds, and our sampling process (describedbelow) initially under-sampled land areas. We addressed
these issues by manually creating supplementary training
data with three additional classes—deep water, clouds, andsupplementary land—which led to the ﬁnal 14 classes used
for the model (Table 1).Coral Reefs (2020) 39:1805–1815 1807
123
We used early versions of the model to identify imagery
with high-value supplemental training data. For deep water
and land classes, the model would misclassify areas withunique spectral patterns or textures, and we were able to
iteratively train the model, generate additional training data
to improve performance, and retrain the model. Forinstance, in addition to deep water with a rich, sapphire
color, we also collected samples from areas with distinct
waves or sunglint patterns. For land, we selected additionalsamples from areas with grassland, forests, jungles, and
cities, both in ﬂat areas and in areas with topography that
could cause shadows. Similarly, the supplementary cloudtraining data were selected by running earlier versions of
the model and ﬁnding areas where the model had mis-
classiﬁed clouds, most often as reef features. Because themodel output can be converted to vector format, we were
able to easily convert the ‘‘outlines’’ that the model had
drawn around clouds (and misclassiﬁed as reef) to a newcloud class.Convolutional neural network
We used a CNN to model the relationship between the red,
green, and blue band pixel values in the Dove visual
mosaic and the 14 reef training classes. We tested severalCNN architectures [i.e., u-net, dense u-net, Fully Convo-
lutional Network (FCN)] and selected a dense u-net
architecture because it showed the highest performance(Figs. 1,2; Zhang et al. 2018 ; Guan et al. 2019 ). The dense
u-net architecture is a combination of a u-net architecture
(Iandola et al. 2014 ; Ronneberger et al. 2015 ) and a dense
net architecture. The u-net structure is deﬁned by the
encoder-decoder pattern, through which the image features
are downsampled to a smaller resolution and then upsam-pled to the original resolution for the ﬁnal predictions
(Fig. 1). This pattern drives the model to reduce the
number of features to a subset that is most useful for pre-dicting the responses, while also using pass-through layers
to localize those predictions at the original image resolu-
tion during the upsampling process. The dense net com-ponent is that each convolutional layer additionally hasTable 1 Cross-connections between original Millennium Coral Reef Mapping Project reef feature classes, model training classes, and aggre-
gated model classes
MCRMP classes Model training
classesAggregated model
classes
Land on reef, main land Land Land
N/A Supplemental
landLand
Channel, deep lagoon, double barrier lagoon, drowned lagoon, haa enclosed lagoon, pass Water Water
Aquatic land features, brackish atoll lagoon Water other WaterN/A Supplemental
deep waterWater
Faro forereef, forereef Forereef Reef
Enclosed basin, enclosed lagoon, enclosed lagoon or basin, faro enclosed lagoon, shallow lagoon Lagoon ReefBay exposed fringing, crest, faro reef ﬂat, fractal reef ﬂat, intermediate reef ﬂat, linear reef ﬂat, pass
reef ﬂat, reef ﬂat, ridge and fossil crest, uplifted reef ﬂatShallow reef ﬂat Reef
Immature reef ﬂat, subtidal reef ﬂat Variable depth
reef ﬂatReef
Barrier reef pinnacle/patch, lagoon pinnacle, pinnacle Pinnacle Reef
Diffuse fringing Shallow non-reef Non-reef
Cross-shelf, inner slope, inner terrace, outer terrace, shallow lagoonal terrace, shallow terrace, shelf
terraceVariable depth
non-reefNon-reef
Shelf hardground, shelf slope Deep non-reef Non-reef
N/A Supplemental
cloudsNon-reef (clouds)
Bridge, deep drowned reef ﬂat, deep terrace, drowned bank, drowned inner slope, deep lagoon with
constructions, deep terrace with constructions, drowned pass, drowned patch, drowned rim, enclosedlagoon or basin with constructions, enclosed lagoon with constructions, forereef or terrace, haa
subtidal reef ﬂat, reticulated fringing, shallow lagoon with constructions, shallow lagoonal terrace
with constructions, shallow terrace with constructions, shelf terrace with constructions, undeterminedenvelopeRemoved N/A1808 Coral Reefs (2020) 39:1805–1815
123
access to the initial model inputs and inputs from each
preceding layer (Fig. 2), rather than simply the outputs
from the previous layer. This gives the model the oppor-
tunity to propagate information through the network across
paths of varying lengths, rather than restricting informationﬂow to a limited set of ﬁxed-length paths, and ultimately
increases model performance. The combination of thesetwo architectures performed better than either architecture
separately in early tests.
We tested a variety of dense-unet network parameteri-
zations to ﬁnd a model variant with adequate performance.
In general, our models took approximately 2 to 8 h to trainon modern computer graphics processing units (GPU),
limiting our ability to do an extensive search of all possible
architectures. We found it sufﬁcient to conduct a gridFig. 1 Sample dense u-net architecture, with four levels of dense
blocks and 16 ﬁlters. The RGB input is passed into the ﬁrst layer ofthe network in the upper left. The ﬁrst half of the network, or encoder,feeds the data through three levels of dense blocks and downsam-
pling, i.e., max pooling, represented by the three dark blue rectangles
and arrows. The bottom gray square represents the dense block andtransition between the encoder on the right and decoder on the left.
The second half of the network, or decoder, feeds the data throughthree levels of dense blocks and upsampling, represented by the light
yellow rectangles and arrows, which return the data to the originalspatial resolution. Finally, the features pass through one lastconvolution layer and a fully connected layer with softmax activa-
tions to calculate the ﬁnal reef class probabilities. Note that features
from the encoder are ‘‘passed-through’’ (dashed teal) to the decoder tobe concatenated with the upsampled features prior to each dense
block, helping to localize predictions
Fig. 2 Sample dense block, with four convolutions and 16 ﬁlters.
The input data are passed to the ﬁrst convolution on the left (grayblock) and has a variable number of channels (X) depending on thenetwork level and whether features from the pass-through layers are
concatenated prior to the dense block. The input to each convolution
layer is the concatenation of the input to the dense block (dark purple)and the output of each preceding convolution layer, e.g., the input tothe third convolution is the concatenation of the dense block input(dark purple) and the output from convolution layers one (dark blue)
and two (teal). The ﬁrst operation in each convolution layer is a 1 91
convolution (white arrow) to reduce the number of channels to thespeciﬁed ﬁlter number. The second operation in each convolution
layer is a 3 93 convolution (gray arrow) to derive new features.
Finally, the output of the dense block is the concatenation of the denseblock input and all convolution layer outputsCoral Reefs (2020) 39:1805–1815 1809
123
search to identify reasonable model architectures and
hyperparameters, even though random parameter searches,
Bayesian optimization approaches, or network architecture
searches can improve model performance (Dernoncourtand Lee 2016 ). We varied the number of dense blocks at
different model resolutions from two to four blocks, varied
the number of convolution layers within each dense blockat four, six, and eight, and varied the number of ﬁlters in
multiples of two from eight to 32. We split training data
into 256px 9256px images, included no growth in ﬁlter
number throughout the network, used 3 93 convolution
kernels, and both upsampled and downsampled with 2 92
kernels.
Model trainingWe sampled *5000 256 9256 pixel images from the
visual mosaic and response data. We restricted samples toareas within 64 pixels of reef classes, which was necessary
to keep classes balanced and get a sufﬁcient number of reef
samples, given the massive amount of mainland availablein the MCRMP data. In selecting model training data, we
only used samples with at least 75% feature coverage (i.e.,
at least 75% pixels are in ocean region) and at least 10%response coverage. Asymmetric thresholds were appropri-
ate because feature data were relatively complete, except
near the boundaries of the visual mosaic, while theMCRMP data were sparse in areas where reef segments
were surrounded by unlabeled deep water.
We split samples into training and validation sets rep-
resenting 90% and 10% of the data, respectively. Weincluded 16 samples in each neural network training
batch—i.e., the number of samples used for a single update
of the model parameters—due to model size and aug-mented samples during the training process. Image aug-
mentations included ﬂips, rotations, and transpositions;
crops and scalings; color shifts and brightenings; distor-
tions, blurs, noise additions, and dropouts; and the addition
of ‘‘fog-like’’ whitening. These augmentations improvedmodel performance and are a standard practice for neural
network model training (Christin et al. 2019 ; Ma et al.
2019 ). Response classes were weighted proportional to the
inverse of their abundance, which over-weighted classes
like reefs that appeared infrequently and under-weighted
classes like land or deep water that were more common.
Global coral reef probability map through model
classiﬁcation
We generated a global coral reef probability map through
model classiﬁcation. The ﬁnal layer in the CNN was con-ﬁgured to use softmax output activations, such that the
model output is the relative probability that a pixel repre-
sents one of the 14 training classes. Because the model isbased on only the three Dove satellite bands, it was limited
in its ability to distinguish between spectrally-similar
classes. As a result, we achieved greater accuracy byallowing the model to predict the 14 classes, and then
aggregating those probabilities further into land, water,
reef, and non-reef classes. We tested several methods ofconverting the 14 classes into the four aggregated classes,
including directly in the CNN, using a maximum-likeli-
hood approach on the CNN probabilities, and using arandom forest classiﬁer trained on the CNN probabilities.
Here, we present the results for the random forest classiﬁer
because it outperformed the other two methods. We splitthe validation data into ﬁvefold for training the random
forest classiﬁers, ultimately settling on 200 trees with a
max depth of 40 splits and weighted subsamples. Theoutput of the random forest classiﬁer is the probability that
a pixel is one of the aggregated model classes (reef or non-
reef (e.g., land, water, etc.); Table 1).
Global coral reef extent map and testing
We generated a global coral reef extent map from a 60%
threshold of reef probability (reef: probability C60%, non-
reef: probability \60%). To test this map, we created a
global coral reef ﬁeld location dataset. We collected coral
reef locations from public datasets, in particular, the Allen
(a)
(b)
(c)
Fig. 3 Global summaries of the data inputs and model outputs in the
context of Marine Ecoregions of the World (MEOW). In panel a,training quads (yellow) come from a manually-selected subset of the
MCRMP data, while testing quads come from all potential coral reef
locations. Area is buffered for visibility and not proportional to actualcoverage, as only 5% of the satellite imagery has associated trainingdata. In panel b, the total area tested within each of the 130 MEOW
ecoregions with quad coverage. In panel c, the average reef
probability within each MEOW ecoregion. Please note that ecoregionsummaries are not sufﬁcient for data interpretation because values aredependent on ecoregion size, the proportion of ecoregion covered by
satellite data, and the proportion of land and open water relative to
reef area1810 Coral Reefs (2020) 39:1805–1815
123
Coral Atlas ( https://allencoralatlas.org/ ), Atlantic and Gulf
Rapid Reef Assessment ( https://www.agrra.org/ ), Khaled
bin Sultan Living Oceans Foundation ( https://www.livin
goceansfoundation.org/ ), the National summary of
NOAA’s shallow-water benthic habitat mapping of U.S.
coral reef ecosystems ( https://repository.library.noaa.gov/
view/noaa/748 ), the USGS Coral Reef Project, and Red Sea
Biodiversity Surveys ( http://redseabiodiversity.sencken
berg.de/ ). We also used coral location records from pub-
lished literature (Lang 2003 ; Rezai et al. 2004 ; Bertels
et al. 2008 ; Solandt and Wood 2008 ; Bruckner 2011 ;
Madduppa et al. 2012 ; Monaco et al. 2012 ; Riegl et al.2012 , 2012; Torres-Pulliza et al. 2013 ; Pramudya et al.
2014 ; Jadot et al. 2015 ; Hossain et al. 2016 ; Fujii 2017 ;
Haﬁzt et al. 2017 ; Ampou et al. 2018 ; Edmunds and
Kuffner 2018 ; Purkis et al. 2019 ). In total, we accessed
1952 coral reef location points (Fig. S1). Moreover, we
also generated 1403 non-reef location points globally (e.g.,land, water, cloud, etc.) to test our reef extent map using a
confusion matrix.Table 2 Comparison of Spalding et al. ( 2001 ) reef estimates to reef estimates generated from our reef probability maps
Region Results, lower threshold Spalding et al. ( 2001 ) (km2) Results, upper threshold
Global 301,110 km2(60%) 284,300 154,049 km2(65%)
Atlantic and Caribbean 25,958 km2(65%) 21,600 16,446 km2(70%)
Caribbean 24,981 km2(65%) 20,000 16,087 km2(70%)
Atlantic 2640 km2(60%) 1600 976 km2(65%)
Indo-Paciﬁc 506,555 km2(55%) 261,200 257,323 km2(60%)
Red Sea and Gulf of Aden 18,088 km2(55%) 17,400 12,126 km2(60%)
Arabian Gulf and Sea 6327 km2(65%) 4200 2487 km2(70%)
Indian Ocean 35,630 km2(60%) 32,000 13,735 km2(65%)
Southeast Asia 180,065 km2(55%) 91,700 85,814 km2(60%)
Paciﬁc 188,471 km2(55%) 115,900 104,554 km2(60%)
Eastern Paciﬁc 2672 km2(55%) 1600 1312 km2(60%)
Reef extents estimates were generated for probability thresholds in 5% increments (i.e., 0%, 5%, 10%, …95%). We report the results for the
thresholds which cover the range inclusive of the Spalding estimates; e.g., the Spalding reef estimate falls between 65 and 70% for the Caribbean,but between 60 and 65% for the Atlantic. The selected regions and ordering were chosen to match the original reef estimate table from Spalding
et al. ( 2001 )
Fig. 4 Visual comparisons of the UNEP-WCMC reef map with our global coral reef extent map in different regions, including aGreat Barrier
Reef (GBR), Australia, Papua New Guinea, Indonesia; bMadagascar, East Africa; cRed Sea, Samoa, Virgin IslandsCoral Reefs (2020) 39:1805–1815 1811
123
Results
Our global coral reef probability map and coral reef extent
map is available for download and interactive viewing
from Allen Coral Atlas (allencoralatlas.org). Our globalDove mosaic quads covered coral reefs in tropical and
subtropical regions (Fig. 3). Within the Marine Ecoregions
of the World boundaries (MEOW) framework, the quadsspanned 9 of 12 realms, 39 of 62 provinces, and 130 of 232
ecoregions (Spalding et al. 2007 ). In total, these quads
included 10.26 million km
2of satellite imagery represented
by 121.75 billion pixels. The total area mapped in theglobal reef probability map sufﬁciently covered coral reefs
globally (Fig. 3).
We tested the feasibility of our reef probability map by
using it to derive reef area estimates compared to otherglobal and regional estimates (Spalding et al. 2001 ,
Table 2). We derived reef area by setting a probability
threshold and counting pixels as reef when their reefprobability exceeded that threshold. We tested probability
thresholds from 0 to 95% in increments of 5%. Depending
on the region, the estimates derived from a threshold
between 55 and 70% aligned well with Spalding’s esti-
mates, with most of the three regions or eight subregionshaving thresholds very close to 60% (Table 2). Therefore,
we created a global coral reef extent map using a 60%
threshold.
We compared our global coral reef extent map with the
UNEP-WCMC reef map worldwide (UNEP-WCMC et al.
2018 ). In some regions of the UNEP-WCMC map (i.e.,
GBR), some reef contours did not match, polygons
extended beyond reef structures, and there were gaps in
reef area (Fig. 4a–c). In contrast, our reef extent map
appeared to better track the actual reef extent (Fig. 4a–c).
These suggest that our mapping approach is especially
useful in delineating reef extent details in most regions,which cumulatively will generate more accurate uses for
our map at jurisdictional scales such as country- or island-
level applications.
Compared with a global coral reef ﬁeld dataset, our
global coral reef extent map had a producer’s accuracy of
87.3% (Table S2). Most of the coral reef location pointsfrom the ﬁeld dataset (1704) matched with reef regions of
our extent map. We plotted coral reef ﬁeld location points
and the reef extent map (Fig. 5, Fig. S2 to Fig. S5) and
found that coral reef regions were accurately illustrated in
the reef extent map. Only several deep reef locations (e.g.,
Fig. S5, Caribbean) did not appear on our reef extent map.
As expected, neural network architecture and parame-
ters had a large impact on model performance and results
(Table 3, Table S1). The two best performing models had
F-scores of 0.66, but one had slightly higher precision
Fig. 5 Visual comparisons of the coral reef ﬁeld locations with our
global coral reef extent map, including Great Barrier Reef (GBR),Australia, Indonesia, Madagascar, French Polynesia, Red Sea, andCaribbean
Table 3 Classiﬁcation
performance results (average
values using 14 classes) for
select model architecturesDense block number Convolution layers per block Filters Precision Recall F1-score Accuracy
2 4 16 0.68 0.38 0.49 0.91
2 6 32 0.74 0.55 0.63 0.9228 2 4 0.77 0.58 0.66 0.93
3 4 24 0.69 0.50 0.58 0.923 6 24 0.76 0.50 0.60 0.923 8 32 0.63 0.36 0.46 0.904 4 32 0.71 0.43 0.54 0.914 6 8 0.67 0.44 0.53 0.914 8 8 0.76 0.59 0.66 0.93
Bold means the best performances1812 Coral Reefs (2020) 39:1805–1815
123
(0.77 vs 0.76) while the other had slightly higher recall
(0.59 vs 0.58). We chose to use the model with higher
precision for global application after manual review indi-
cated that the choice was not likely to lead to qualitativedifferences, and because the map generation process was
too computationally expensive to generate multiple maps.
Discussion
Our global coral reef extent map and coral reef probabilitymap add value to existing reef products, as both a
methodological improvement and a new source of vitally
needed data. Methodological advantages include high-res-
olution satellite imagery, modern deep learning approa-ches, and consistent global methods. Our coral reef extent
map provides new high-resolution (i.e., 4.5 m) spatial
information of coral reefs at a global scale. Our coral reefprobability map can be used as a new data source by
combining the reef probabilities with other global maps to
create synergistic data products, or combining the reefprobabilities with other types of data layers to create
derived data or analytic products. Thus, our probability
map could be used in concert with other maps or datasets.
Our methods were composed of high-value and low-cost
approaches to modeling the global reef extent, but these
methods are not exhaustive and can be improved over time,with corresponding improvements in the generated reef
probabilities. Additional data sources could be incorpo-
rated, whether they include other satellite products likeSentinel-2 or Landsat-8, or derived data products like tur-
bidity, bathymetry, or geomorphology (Hedley et al.
2016 ,2018 ; Haﬁzt et al. 2017 ; Kerr and Purkis 2018 ;
Purkis 2018 ; Roelfsema et al. 2018 ,2020 ; Li et al.
2019a ,b; Purkis et al. 2019 ; Lyons et al. 2020 ). These
additional data sources could account for shortcomings inthe Dove feature data or add additional power to separate
response classes. Our model could be updated to include
both spatial and temporal resolutions with new networkarchitectures (e.g., Mou et al. 2018 ), again helping to dis-
tinguish between response classes. Supplemental response
categories could be enhanced to better handle a variety ofcloud patterns, turbid water, or other confounding spectral
or textural patterns. Multiple models could be generated for
different regions of the globe or different types of reefformations, or incorporated in model ensembles for more
accurate predictions.
Our model performances are affected by the technical
(e.g., mosaicking, misalignment) or environmental (e.g.,
clouds, turbid water) ‘‘noise’’ in the satellite data, and by
inaccuracies in the extent or category of reef in theMCRMP training data. Moreover, reef features could
simply not be visible due to excessive depth where bottomhabitats are invisible (Brando et al. 2009 ; Thompson et al.
2017 ; Li et al. 2020a ), even if the satellite data and reef
extents are otherwise accurate. For these reasons and
potentially others, we would expect a ‘‘perfect’’ model tohave less than perfect performance when tested against
validation data.
The growing quantity and quality of Earth observation
data, as well as the increasing sophistication and perfor-
mance of machine learning approaches, are double-edged
swords. They provide new opportunities for coral reef
research and applied outcomes, but present new barriers to
entry and effective and efﬁcient progress. In addition tocoral reef domain knowledge and the ability to communi-
cate and coordinate with policymakers and stakeholders,
teams need members with robust data science and softwareengineering skills. This project required the capacity to
store on the order of tens of terabytes of data and access to
hundreds of computer GPU hours. Teams of organizedcollaborators can produce better outcomes more quickly
than individual contributors, and continued collaborations
can iterate on existing datasets, codebases, and modelsmore quickly than teams starting from scratch. We
encourage others to join us in continuing to develop this
resource and others.
Acknowledgements This project was supported by Vulcan Inc.
Supercomputing was supported by Arizona State University’sKnowledge Enterprise Development program. Project partners pro-
viding ﬁnancial, service and personnel include: Arizona State
University, National Geographic, Planet Inc., and University ofQueensland. Additional support was also provided by Google Inc.and the Great Barrier Reef Foundation.
References
Ampou EE, Ouillon S, Andre ´foue¨t S (2018) Challenges in rendering
Coral Triangle habitat richness in remotely sensed habitat maps:The case of Bunaken Island (Indonesia). Marine pollution
bulletin 131:72–82
Andrefouet S, Muller-Karger FE, Robinson JA, Kranenburg CJ,
Torres-Pulliza D, Spraggins SA, Murch B (2006) Global
assessment of modern coral reef extent and diversity for regional
science and management applications: a view from space.2:1732–1745
Aswani S, Vaccaro I (2008) Lagoon ecology and social strategies:
habitat diversity and ethnobiology. Human Ecology 36:325–341
Bertels L, Vanderstraete T, Van Coillie S, Knaeps E, Sterckx S,
Goossens R, Deronde B (2008) Mapping of coral reefs usinghyperspectral CASI data; a case study: Fordata, Tanimbar,
Indonesia. International journal of remote sensing 29:2359–2391
Beyer HL, Kennedy EV, Beger M, Chen CA, Cinner JE, Darling ES,
Eakin CM, Gates RD, Heron SF, Knowlton N (2018) Risk-
sensitive planning for conserving coral reefs under rapid climate
change. Conservation Letters 11:e12587
Brando VE, Anstee JM, Wettle M, Dekker AG, Phinn SR, Roelfsema
C (2009) A physics based retrieval and quality assessment of
bathymetry from suboptimal hyperspectral data. Remote sensing
of Environment 113:755–770Coral Reefs (2020) 39:1805–1815 1813
123
Brodrick PG, Davies AB, Asner GP (2019) Uncovering ecological
patterns with convolutional neural networks. Trends in ecology
& evolution 34:734–745
Bruckner AW (2011) The Global Reef Expedition: Science without
Borders/C210. Journal of the Washington Academy of Sciences
19–32
Burke LM, Reytar K, Spalding M, Perry A (2017) Reefs at risk
revisited: World Resources Institute
Cheng G, Han J, Lu X (2017) Remote sensing image scene
classiﬁcation: Benchmark and state of the art. Proceedings ofthe IEEE 105:1865–1883
Christin S, Hervet E ´, Lecomte N (2019) Applications for deep
learning in ecology. Methods in Ecology and Evolution
10:1632–1644
Dernoncourt F, Lee JY (2016) Optimizing neural network hyperpa-
rameters with gaussian processes for dialog act classiﬁcation.
406–413
Eakin CM, Sweatman HP, Brainard RE (2019) The 2014–2017
global-scale coral bleaching event: insights and impacts. Coral
Reefs 38:539–545
Edmunds JRGPJ, Kuffner RDGIB (2018) Time-series coral-cover
data from Hawaii, Florida, Mo’orea, and the Virgin Islands
Foo SA, Asner GP (2019) Scaling Up Coral Reef Restoration Using
Remote Sensing Technology. Front Mar Sci 6:79
Fujii M (2017) Mapping the change of coral reefs using remote
sensing and in situ measurements: a case study in Pangkajeneand Kepulauan Regency, Spermonde Archipelago, Indonesia.
Journal of oceanography 73:623–645
Group GC (2019) GEBCO 2019 gridGuan S, Khan AA, Sikdar S, Chitnis PV (2019) Fully Dense UNet for
2-D Sparse Photoacoustic Tomography Artifact Removal. IEEE
journal of biomedical and health informatics 24:568–576
Haﬁzt M, Manessa MDM, Adi NS, Prayudha B (2017) Benthic
habitat mapping by combining lyzenga’s optical model and
relative water depth model in Lintea Island, Southeast Sulawesi.
Earth and Environmental Sciences (98)
Hedley J, Roelfsema C, Chollett I, Harborne A, Heron S, Weeks S,
Skirving W, Strong A, Eakin C, Christensen T (2016) Remote
sensing of coral reefs for monitoring and management: a review.
Remote Sensing 8:118
Hedley JD, Roelfsema C, Brando V, Giardino C, Kutser T, Phinn S,
Mumby PJ, Barrilero O, Laporte J, Koetz B (2018) Coral reef
applications of Sentinel-2: Coverage, characteristics, bathymetry
and benthic mapping with comparison to Landsat 8. Remotesensing of environment 216:598–614
Heron SF, Maynard JA, Van Hooidonk R, Eakin CM (2016) Warming
trends and bleaching stress of the world’s coral reefs 1985–2012.Scientiﬁc reports 6:38402
Hossain MS, Bujang JS, Zakaria MH, Hashim M (2016) Marine and
human habitat mapping for the Coral Triangle Initiative region
of Sabah using Landsat and Google Earth imagery. MarinePolicy 72:176–191
Hughes TP, Anderson KD, Connolly SR, Heron SF, Kerry JT, Lough
JM, Baird AH, Baum JK, Berumen ML, Bridge TC (2018)
Spatial and temporal patterns of mass bleaching of corals in theAnthropocene. Science 359:80–83
Iandola F, Moskewicz M, Karayev S, Girshick R, Darrell T, Keutzer
K (2014) Densenet: Implementing efﬁcient convnet descriptorpyramids. arXiv preprint arXiv:14041869
IMaRS-USF IRD (2005) Millennium coral reef mapping project.
Validated maps Cambridge (UK): UNEP World Conservation
Monitoring Centre
Jadot C, Darling ES, Brenier A (2015) MADAGASCAR: A Baseline
Assessment of Coral Reef Fisheries. Wildlife Conservation
SocietyKerr JM, Purkis S (2018) An algorithm for optically-deriving water
depth from multispectral imagery in coral reef landscapes in the
absence of ground-truth data. Remote Sensing of Environment210:307–324
Lang JC (2003) Status of coral reefs in the Western Atlantic: results
of initial surveys, Atlantic and Gulf Rapid Reef Assessment(AGRRA) program
Li J, Knapp DE, Schill SR, Roelfsema C, Phinn S, Silman M,
Mascaro J, Asner GP (2019a) Adaptive bathymetry estimation
for shallow coastal waters using Planet Dove satellites. RemoteSensing of Environment 232:111302
Li J, Schill SR, Knapp DE, Asner GP (2019b) Object-based mapping
of coral reef habitats using planet dove satellites. Remote
Sensing 11:1445
Li J, Fabina NS, Knapp DE, Asner GP (2020a) The Sensitivity of
Multi-spectral Satellite Sensors to Benthic Habitat Change.
Remote Sensing 12:532
Li X, Liu B, Zheng G, Ren Y, Zhang S, Liu Y, Gao L, Liu Y, Zhang
B, Wang F (2020b) Deep learning-based information mining
from ocean remote sensing imagery. National Science Review
Lyons MB, Roelfsema CM, Kennedy EV, Kovacs EM, Borrego-
Acevedo R, Markey K, Roe M, Yuwono DM, Harris DL, PhinnSR, Asner, Gregory P., Li, Jiwei, Knapp, David, Fabina,
Nicholas, Larsen, Kirk, Traganos, Dimosthenis, Murray, Nicho-
las (2020) Mapping the world’s coral reefs using a globalmultiscale earth observation framework. Remote Sensing inEcology and Conservation
Ma L, Liu Y, Zhang X, Ye Y, Yin G, Johnson BA (2019) Deep
learning in remote sensing applications: A meta-analysis andreview. ISPRS journal of photogrammetry and remote sensing
152:166–177
Madduppa HH, Ferse SC, Aktani U, Palm HW (2012) Seasonal trends
and ﬁsh-habitat associations around Pari Island, Indonesia:setting a baseline for environmental monitoring. Environmental
biology of ﬁshes 95:383–398
McManus J (1994) ReefBase-a global database of coral reef systems
and their resources. Naga, the ICLARM Quarterly 17:16
Monaco ME, Andersen SM, Battista TA, Kendall MS, Rohmann SO,
Wedding LM, Clarke AM (2012) National summary of NOAA’s
shallow-water benthic habitat mapping of US coral reefecosystems
Montaggioni LF, Braithwaite CJ (2009) Quaternary coral reef
systems: history, development processes and controlling factors.
Elsevier
Mou L, Bruzzone L, Zhu XX (2018) Learning spectral-spatial-
temporal features via a recurrent convolutional neural network
for change detection in multispectral imagery. IEEE Transac-tions on Geoscience and Remote Sensing 57:924–935
Pendleton L, Comte A, Langdon C, Ekstrom JA, Cooley SR, Suatoni
L, Beck MW, Brander LM, Burke L, Cinner JE (2016) Coral
reefs and people in a high-CO2 world: where can science make adifference to people? PloS one 11:e0164699
Pramudya FS, Wikantika K, Windupranata W (2014) Satellite-Based
Benthic Habitat Mapping Using LANDSAT 8 in Nusa Lembon-
gan and Nusa Ceningan Island. Faculty of Earth Science andTechnology Institut Teknologi Bandung
Purkis SJ (2018) Remote Sensing Tropical Coral Reefs: The View
from Above. Annual review of marine science 10:149–168
Purkis SJ, Gleason AC, Purkis CR, Dempsey AC, Renaud PG, Faisal
M, Saul S, Kerr JM (2019) High-resolution habitat and
bathymetry maps for 65,000 sq. km of Earth’s remotest coral
reefs. Coral Reefs 38:467–488
Rezai H, Wilson S, Claereboudt M, Riegl B (2004) Coral reef status
in the ROPME sea area: Arabian/Persian Gulf, Gulf of Oman
and Arabian Sea. Status of coral reefs of the world 1:155–1701814 Coral Reefs (2020) 39:1805–1815
123
Riegl BM, Bruckner AW, Rowlands GP, Purkis SJ, Renaud P (2012)
Red Sea coral reef trajectories over 2 decades suggest increasing
community homogenization and decline in coral size. PLoS One7:e38396
Roelfsema C, Kovacs E, Ortiz JC, Wolff NH, Callaghan D, Wettle M,
Ronan M, Hamylton SM, Mumby PJ, Phinn S (2018) Coral reefhabitat mapping: A combination of object-based image analysisand ecological modelling. Remote Sensing of Environment
208:27–41
Roelfsema CM, Kovacs EM, Ortiz JC, Callaghan DP, Hock K,
Mongin M, Johansen K, Mumby PJ, Wettle M, Ronan M (2020)Habitat maps to enhance monitoring and management of the
Great Barrier Reef. Coral Reefs 1–16
Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional
networks for biomedical image segmentation. 234–241
Solandt J-L, Wood C (2008) Maldives reef survey-June 13–30th,
2008. Marine Conservation Society, Ross on Wye, UK 1–18
Spalding M, Burke L, Wood SA, Ashpole J, Hutchison J, Zu
Ermgassen P (2017) Mapping the global value and distribution
of coral reef tourism. Marine Policy 82:104–113
Spalding M, Spalding MD, Ravilious C, Green EP (2001) World atlas
of coral reefs. Univ of California Press
Spalding MD, Fox HE, Allen GR, Davidson N, Ferdan ˜a ZA,
Finlayson MAX, Halpern BS, Jorge MA, Lombana AL, Lourie
SA (2007) Marine ecoregions of the world: a bioregionalizationof coastal and shelf areas. BioScience 57:573–583Thompson DR, Hochberg EJ, Asner GP, Green RO, Knapp DE, Gao
B-C, Garcia R, Gierach M, Lee Z, Maritorena S (2017) Airborne
mapping of benthic reﬂectance spectra with Bayesian linearmixtures. Remote sensing of Environment 200:18–30
Torres-Pulliza D, Wilson JR, Darmawan A, Campbell SJ, Andre ´foue¨t
S (2013) Ecoregional scale seagrass mapping: A tool to supportresilient MPA network design in the Coral Triangle. Ocean &coastal management 80:55–64
UNEP-WCMC, WorldFish Centre, WRI, TNC (2018) Global distri-
bution of warm-water coral reefs, compiled from multiplesources including the Millennium Coral Reef Mapping Project.Version 4.0. Includes contributions from IMaRS-USF and IRD
(2005), IMaRS-USF (2005) and Spalding et al. (2001). Cam-
bridge (UK): UN Environment World Conservation MonitoringCentre. URL: http://data.unep-wcmc.org/datasets/1
Van Hooidonk R, Maynard J, Tamelander J, Gove J, Ahmadia G,
Raymundo L, Williams G, Heron SF, Planes S (2016) Local-
scale projections of coral reef futures and implications of theParis Agreement. Scientiﬁc reports 6:1–8
Zhang Z, Liang X, Dong X, Xie Y, Cao G (2018) A sparse-view CT
reconstruction method based on combination of DenseNet anddeconvolution. IEEE transactions on medical imaging37:1407–1417
Publisher’s Note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.Coral Reefs (2020) 39:1805–1815 1815
123
Fairness and representation in satellite-based poverty maps:
Evidence of urban-rural disparities and their impacts on downstream policy
Emily Aiken1,∗,Esther Rolf2,∗,Joshua Blumenstock1
1UC Berkeley, School of Information
2Harvard University, Data Science Initiative & Center for Research on Computation and Society
∗These authors contributed equally to this work
{emilyaiken, jblumenstock }@berkeley.edu, erolf@seas.harvard.edu
Abstract
Poverty maps derived from satellite imagery are in-
creasingly used to inform high-stakes policy deci-
sions, such as the allocation of humanitarian aid
and the distribution of government resources. Such
poverty maps are typically constructed by training
machine learning algorithms on a relatively modest
amount of “ground truth” data from surveys, and
then predicting poverty levels in areas where im-
agery exists but surveys do not. Using survey and
satellite data from ten countries, this paper inves-
tigates disparities in representation, systematic bi-
ases in prediction errors, and fairness concerns in
satellite-based poverty mapping across urban and
rural lines, and shows how these phenomena affect
the validity of policies based on predicted maps.
Our ﬁndings highlight the importance of careful er-
ror and bias analysis before using satellite-based
poverty maps in real-world policy decisions.
1 Introduction
Satellite-based poverty maps are increasingly being used to
inform critical policy decisions, including estimating interim
subnational statistics [Hofer et al. , 2020 ], targeting human-
itarian aid [Aiken et al. , 2022; Smythe and Blumenstock,
2022 ], determining eligibility for social services [Gentilini et
al., 2022 ], and estimating the impacts of development pro-
grams [Huang et al. , 2021; Ratledge et al. , 2022 ]. These
maps are constructed by applying machine learning (ML) al-
gorithms to high-resolution imagery, based on the premise
that the algorithm can learn to predict poverty from pixel data
[Jean et al. , 2016; Yeh et al. , 2020; Chi et al. , 2022 ].
However, satellite-based poverty maps are not perfect.
When poverty predictions exhibit systematic errors, their use
in policy decisions can lead to disparate and unfair outcomes.
For example, a program that provides resources to the re-
gions of a country with lowest predicted wealth might dis-
proportionately “miss” poor regions with substantial infras-
tructure and large, developed settlements signaling wealth
from the sky. In such cases, the use of current satellite-based
poverty maps – which in principle could be used to address
the United Nation’s (UN) Sustainable Development Goals
and other pressing social issues – might in practice conﬂictwith goals of promoting equity (for example, as formalized
in the UN’s Leave No One Behind Principle).
The potential for satellite-based poverty maps to aid public
policy thus exists alongside the potential for such prediction-
based policies to introduce or exacerbate inequities. In set-
tings where policymakers may mis-perceive satellite-based
maps as technocratic and therefore “objective” measures of
poverty, it is imperative to document how systematic errors
and biases might arise or compound in satellite-based poverty
predictions and their uses in downstream policies.
This paper explores the interconnected phenomena of sys-
tematic prediction errors, representation, and unfairness in
satellite-based poverty maps, focusing on disparities between
urban and rural areas: are satellite-based maps as useful for
distinguishing poverty levels within urban and rural areas as
between them? Do satellite-based poverty maps tend to over-
estimate wealth in urban areas relative to rural ones (or vice
versa) – and if so, what are the consequences for downstream
policy decisions based on such maps? We focus our analy-
ses on urban-rural disparities because (1) previous work has
established urban build-up as as a key predictor of poverty
in satellite-based machine learning models [Yehet al. , 2020;
Engstrom et al. , 2022 ]and (2) many sensitive or protected
characteristics – including race, age, and religion – are corre-
lated with urbanization [Ghosh and Roy, 1997; Kuper, 2013 ].
Using survey data and satellite imagery from ten countries
(Table S1), our analysis produces four main results:
First, we document performance disparities across rural
and urban regions and connect them to potential represen-
tational limitations of current methods. It appears that in
many countries, satellite image representations can be used to
somewhat accurately differentiate between wealthy and poor
regions mainly because these representations capture differ-
ences between urban areas (which tend to be wealthy) and ru-
ral areas (which tend to be poorer). As a result, satellite-based
poverty maps are not as effective at differentiating wealth
within rural and urban parts a country as they are at estimating
wealth at a national scale.
Second, we document nuanced but systematic biases in
prediction errors for urban and rural areas. In countries where
poverty is concentrated in rural areas, predicted wealth in ur-
ban areas is under-ranked relative to predicted wealth in rural
areas. In contrast, in countries with a high degree of urban
poverty, predicted wealth in urban areas is consistently over-arXiv:2305.01783v1  [cs.LG]  2 May 2023
ranked relative to predicted wealth in rural areas.
Third, we study how these phenomena interact to impact
thefairness and effectiveness of downstream policies based
on predicted maps. We simulate hypothetical geographically
targeted aid programs which select beneﬁciary regions using
satellite-based poverty predictions. We observe two contrast-
ing phenomena with opposite effects on selection policies,
both tied to the underlying joint distribution of urbanization
and wealth. First, systematic over-ranking of rural wealth
results in under-allocation of aid to rural areas (particularly
when there is a strong correlation between urbanization and
ground-truth wealth). Second, overreliance on weaker cor-
relations between urbanization and wealth (arising from rep-
resentational limitations in satellite imagery) may result in
“missing” some of the urban poor.
Fourth, and ﬁnally, we explore options to reduce the ex-
posed disparities in satellite-based poverty mapping. We ﬁnd
that simple recalibration methods can improve predictive ac-
curacy and ameliorate prediction biases in some contexts, but
rely heavily on having reliable measures of regions being ur-
ban or rural with which to recalibrate.
1.1 Related work
Satellite-based poverty maps — which have been studied
in the research literature for some time [Jean et al. , 2016;
Yehet al. , 2020; Chi et al. , 2022; Rolf et al. , 2021 ]— are
now being used in real-world policy decisions, including the
geographic targeting of social assistance (in Togo [Aiken et
al., 2022 ], the Democratic Republic of the Congo [Gentilini
et al. , 2022 ], and Malawi [Paul et al. , 2021 ]) and policy im-
pact evaluation (in Uganda [Huang et al. , 2021 ]and Rwanda
[Ratledge et al. , 2022 ]). Broad calls to consider fairness and
responsibility in satellite-based machine learning – e.g. in
environmental applications [McGovern et al. , 2022 ], big data
for development [Blumenstock, 2018 ], and remote sensing
[Burke et al. , 2021 ]– underscore the importance of evaluat-
ing fairness and potential biases in these maps.
While the implications of algorithmic biases have been
documented in settings from criminal justice [Chouldechova
and G’Sell, 2017 ]and facial recognition [Buolamwini and
Gebru, 2018 ]to credit scoring [Liuet al. , 2018 ]and resource
allocation in healthcare [Obermeyer et al. , 2019 ], they have
received relatively little attention in the domain of poverty
mapping. Recent studies have highlighted speciﬁc fairness
concerns for particular regions and applications: Kondmann
et al. [2021 ]investigate statistical bias in estimation of
poverty and electriﬁcation rates across villages in rural India,
Zhang et al. [2022 ]expose performance gaps of unsupervised
transfer learning for landcover classiﬁcation across rural and
urban regions of China, and Smythe and Blumenstock [2022 ]
evaluate satellite-based poverty targeting in Nigeria.
However, to date there exists no systematic study of
broader fairness concerns in satellite-based poverty mapping
— partly because the data context of low- and middle-income
countries (LMICs), where the utility of satellite-derived maps
is most distinct, makes it difﬁcult to rigorously evaluate map
accuracy and fairness [Jerven, 2013; Bolliger et al. , 2017;
Burke et al. , 2021; Rolf, 2023 ]. Our work builds on previ-
ous studies by concretely illustrating how errors and biases insatellite-based poverty maps can translate into disparate out-
comes for downstream policy decisions.
2 Data and Methods
Our analysis relies on survey datasets from ten countries
matched to featurizations of satellite images.
2.1 Survey datasets
We use survey datasets from ten countries in our paper, de-
scribed in detail in Appendix A and Table S1. In short, we
use the following four categories of survey data:
Demographic and Health Surveys (DHS) from Colom-
bia, Honduras, Indonesia, Nigeria, Kenya, the Philippines,
and Peru. Each survey was conducted in 2010 or later and
interviewed 20,000-60,000 households in 1,000-5,000 clus-
ters. Clusters are small geographic groups of households,
sampled at random or stratiﬁed random in each country. Clus-
ters are roughly equivalent to a neighborhood in urban areas
(for which the provided cluster centroid is jittered with a 2km
radius) or a village in rural areas (for which the jitter is a 5km
radius). We use the DHS-constructed asset-based wealth in-
dex as the ground truth measure of poverty for each DHS sur-
vey, and calculate the average wealth index for each cluster.
The American Community Survey (ACS) from 2018,
which interviewed 1.5 million randomly selected households
from all 2,331 Public Use Microdata Areas (“PUMAs”) in
the United States. We use household income as the ground
truth poverty measure in the ACS, and calculate the average
household income per PUMA.
The Mexican Intercensal Survey from 2015, which in-
terviewed 2.8 million households in Mexico’s 2,446 munici-
palities. We construct an asset-based wealth index from the
survey data, using a principle components analysis to project
ownership of twelve assets to a unidimensional vector (Ap-
pendix A.1). Our ground truth measure of poverty in Mexico
is the average asset-based wealth per municipality.
TheIndian Socio Economic and Caste Census (SECC)
from 2012. We use estimated average per-capita consump-
tion at roughly the village/town level (shrid2) produced by
the Socioeconomic High-resolution Rural-Urban Geographic
Dataset for India (SHRUG) v2 (an updated version of [Asher
et al. , 2021 ]) as our reference measure of poverty. We spa-
tially aggregate small rural shrid2 regions together (Appendix
A.2) to ensure each observation is a large enough geography
and to reduce imbalance between the number of urban and
rural regions. This reduces the number of rural observations
from 522,344 to 59,832. There are 3,524 urban regions.
We normalize the poverty values for each country (logged
in the US and India1) to zero mean and unit variance. We
refer to these poverty measures as “wealth” throughout.
Categorizations of each region as either urban or rural are
deﬁned by these survey datasets. We refer to these binary
labels as “urbanization” throughout.
1We log poverty values in the US and India as these values rep-
resent consumption distributions, which are right-tailed. In the re-
maining countries, poverty is measured with asset indices and we do
not use a log transform.
2.2 Satellite image features
We obtain a set of tabularized features summarizing satel-
lite tiles in each country we study from MOSAIKS [Rolf
et al. , 2021 ], accessed via siml.berkeley.edu [Carleton et al. ,
2022 ]. The underlying satellite images are from Planet Labs
in 2019.2Features are generated through an unsupervised
machine learning approach based on random convolutional
features (RCFs), which are shown to carry skill across a vari-
ety of prediction tasks [Rolf et al. , 2021 ].
RCF embedding functions are essentially a wide and shal-
low feed-forward convolutional neural network with random
but ﬁxed (non-optimized) weights. We use RCFs as conve-
nient way to obtain images features with a single, ﬁxed fea-
turization method across countries.
The number of tiles per region varies widely between sur-
vey datasets: in the DHS, where each cluster has a 2-5km
radius, each cluster is represented with 16-88 tiles. In the
India, Mexico, and the United States, regions can overlap as
few as six tiles or as many as tens of thousands of tiles (Ta-
ble S1). For regions that intersect more than 100 tiles, we take
a random subset of 100 of the intersecting tiles. We then cal-
culate the average of each MOSAIKS feature for each region,
weighted by the overlap between the tiles and the region.
2.3 Problem formulation and simulation setup
Our machine learning simulations begin by randomly assign-
ing 75% of regions in each country to a training set and 25%
to a test set.3Following Rolf et al. [2021 ], in each country we
train a ridge regression model to predict average household
wealth in training set regions from the associated satellite-
derived MOSAIKS features. The objective function is mean
squared error, and we tune the ℓ2penalty via three-fold cross-
validation on the training set. We then use the trained model
to predict wealth for every region in the test set. To account
for idiosyncrasies in random train-test splits, we report the
mean±two std. errors across 100 simulations in all results.
2.4 Fairness analysis procedures
Our analysis focuses on bias and fairness in satellite-based
poverty maps along urban-rural lines. First, we document
performance disparities within and between urban and ru-
ral areas, by measuring predictive accuracy (measured with
R2and Spearman’s ρ) in the test set overall, in just urban re-
gions, and in just rural regions. Second, we measure system-
atic prediction biases between urban and rural regions when
using satellite-based poverty maps, quantiﬁed as (1) the mean
signed error in wealth prediction for rural and urban areas
2Satellite imagery (from 2019) is not obtained from the same
time period as all survey datasets, which range from 2010 to 2019.
Other work suggests that the impacts of this temporal mismatch are
limited [Yehet al. , 2020 ], and we observe no clear relationship be-
tween predictive accuracy and temporal mismatch in Figure 1.
3We use uniform random assignment of regions (PUMAs in the
US, municipalities in Mexico, aggregated Shrid2 units in India, and
clusters in DHS surveys) to train and test sets — rather than spatial
stratiﬁcation — as it allows for more consistency across countries,
and better reﬂects the “in-sample” scenarios in which satellite-based
poverty maps would be deployed [Wadoux et al. , 2021; Rolf, 2023 ].separately, and (2) the mean error in wealth ranking for rural
and urban areas separately.
We then measure how performance disparities and pre-
diction biases propagate to downstream policy decisions.
We simulate hypothetical aid programs using satellite-based
poverty predictions to select eligible geographies. To evaluate
the implications of performance disparities on simple metrics
ofallocational fairness , we compare the precision and recall
(equal by deﬁnition in this application [Brown et al. , 2018 ])
of hypothetical programs that target the poorest 20% of re-
gions in each country as a whole, the poorest 20% of urban
regions, and the poorest 20% of rural regions. To show how
systematic prediction biases propagate to downstream policy
decisions in nationwide aid programs, we measure aid allo-
cation (measured as the number of regions selected) to rural
areas and urban areas when satellite-based poverty maps are
used to select geographies, and compare to allocations when
ground truth measures of poverty are used.
2.5 Recalibration approaches
We explore two recalibration-based options for addressing
fairness issues in satellite-based poverty prediction: mean
calibration (adjusting the means of urban and rural predicted
wealth distributions to match the means of the ground truth
distributions), and selection threshold calibration (allocating
resources to urban and rural areas according to the share of
regions that are poor in each group). For both approaches, we
learn the parameters of the calibration procedure on the train-
ing set, and apply this learned calibration to the test. We in-
vestigate whether access to ground-truth urbanization values
affects the results our calibration approaches by also attempt-
ing calibration with predicted urbanization in test regions.
3 Results
3.1 Performance disparities and representation
Consistent with past work [Yehet al. , 2020; Engstrom et al. ,
2022 ], we ﬁnd that satellite-based wealth predictions explain
a signiﬁcant portion of the variance in ground-truth wealth
within each of the ten countries we study (mean R2= 0.47-
0.70), and there is a strong correlation between wealth pre-
dictions and ground truth (mean Spearman’s ρ= 0.71-0.83).
In all ten countries, the rank correlation is substantially
lower when predictions are evaluated just within urban ar-
eas (meanρ= 0.51-0.74) or just within rural areas (mean ρ=
0.40-0.82) (or both, Figure 1A).This systematically replicates
analysis in [Yehet al. , 2020 ](which documents performance
within-urban and within-rural areas for a pooled dataset from
several African countries) for ten countries across the globe.
There is heterogeneity across countries in terms of which ar-
eas are hardest to predict: in three countries (Colombia, Peru,
and the United States) predictive accuracy is higher among
urban areas than among rural areas, whereas in the remaining
seven countries (Honduras, India, Indonesia, Kenya, Mexico,
Nigeria, and the Philippines) predictive accuracy is higher
among rural areas. In all countries, at least one of urban or ru-
ral areas has substantially lower predictive accuracy than the
country as a whole (difference in mean ρ>0.09, Figure 1A).
Figure 1: Panel A: Rank correlation (Spearman’s ρ) between predicted and ground-truth wealth are higher in each country as a whole (gray)
than within urban (blue) and rural (red) regions in each country. Panel B: As a result, an aid program that targets the poorest 20% of regions
in urban (blue) or rural (red) parts of a country has lower accuracy than a program that targets within the entire country (gray).
Figure 2: Panel A: Averageℓ2distance between satellite image features for pairs of rural regions, pairs of urban regions, and pairs of
urban-rural regions. For India, we randomly sub-sample 2,000 rural and 2,000 urban regions to estimate average distances. Panel B: Two-
dimensional principle components analysis (PCA) projections of the MOSAIKS feature. Across countries, these dimensions explain between
90.2% and 98.5% of the variation in the 4000 features.
Why are satellite-based poverty maps consistently worse at
differentiating poverty levels within urban or rural areas than
within entire countries? Trends in the imagery and observed
wealth data point to the possibility that much of the accuracy
observed in country-scale satellite-based poverty maps is due
to their ability to distinguish between urban and rural areas.
In each country, there is a strong correlation between the
measured (“ground truth”) values of wealth and urbanization
(Table S3, Spearman’s ρ= 0.51-0.77 outside of India and the
United States).4We also ﬁnd that the overall performance
of poverty predictions tends to be higher for countries where
wealth and urbanization are more correlated (Figure S5).5
4In the India and United States, ρ= 0.28-0.30. The United States
is the only high-income country of the ten we study. The relatively
low correlation between wealth and urbanization in India in our data
might be due in part to the deﬁnition of shrid2 regions, in which
many urban regions have large spatial extent while a large majority
of region instances are rural (see Appendix A.1).
5This trend does not hold, and possibly reverses, when evaluating
across only urban or rural regions (also Figure S5).The potential inﬂuence of urbanization can also be seen in
the feature representations of the raw imagery — even be-
fore ﬁtting a predictive model —which already encode high
amount of signal as to whether a region is urban or rural (Fig-
ure 2B). As shown in Figure 2A, the average ℓ2distance be-
tween features of two rural regions is much lower than that
between an urban and a rural region (and two urban regions).
We ﬁnd that a similar overall trend holds when looking at
individual MOSAIKS tiles (Figure S1), and that satellite im-
agery is highly predictive of urbanization S3.
Finally, in countries where wealth and urbanizaton have
a strong correlation, the differences between the predictive
accuracy of satellite-based wealth predictions and satellite-
based predictions of a region being urban are small (mean
difference in Spearman’s ρ= 0.07-0.26 outside of the United
States and India, Table S3 and Figure S3). Along with the
results in Figure 2, the close relationship between predict-
ing urbanization and predicting wealth from satellite imagery
hints at potential concerns about representations of poverty in
satellite imagery akin to stereotype bias [Abbasi et al. , 2019;
Figure 3: Trends in allocative bias in using satellite-based poverty predictions to allocate aid. Panel A : Over-allocation of aid to rural areas vs.
mean signed error in poverty prediction in rural areas. Panel B : Over-allocation of aid to rural areas when vs. mean rank error in prediction
in rural areas. Filled in markers show biases in satellite-based predictions; faded markers show the noised wealth baseline.
Boyarskaya et al. , 2020 ], a particular type of representational
harm in which the observed data on individuals in a group are
more closely related than a more comprehensive characteri-
zation of those individuals would warrant.
Taken together, these results suggest that representations of
poverty in satellite imagery beyond urbanization are present
but often limited. As such, a concern for policy is that appli-
cations that “zoom in” on urban or rural areas (for example,
calculating interim subregional poverty statistics or running
an aid program in just urban or rural areas), predictive accu-
racy for identifying poverty from satellite imagery — and the
accuracy of downstream decisions — is likely to be substan-
tially lower than an overall accuracy estimate would suggest.
3.2 Systematic biases in prediction errors
In light of the limitations to poverty representations in satel-
lite imagery, a further concern for satellite-based poverty
mapping is possible systematic biases in prediction errors.
We begin by documenting mean signed errors in predic-
tions, ﬁnding that across countries, wealth in urban areas is
under-predicted and wealth in rural areas is over-predicted
(Figure 3A, Figure S4). This phenomenon may simply reﬂect
a statistical bias toward the mean prediction – in all countries
urban areas are on average richer than rural areas (Table S3).
The mean error in wealth ranking across countries exhibits
biased errors in both directions: in Nigeria, the Philippines,
and the United States, rural areas are under-ranked by wealth
predictions; in Colombia, India, Kenya, Mexico, and Peru,
rural areas are over-ranked; and in Honduras and Indonesia,
there is no statistically signiﬁcant difference in ranking be-
tween urban and rural areas (Figure 3B).
An important question is whether these same biases could
arise if simply using a lower-quality wealth label, rather than
satellite-based predictions. Figures 3 and S4 therefore in-
clude noised-wealth baselines , in which we add Gaussian
noise to the ground-truth wealth labels with zero mean and
isotropic covariance calibrated to the mean squared error of
the satellite-based predictions. This allows us to test whether
prediction biases of satellite-based models are systematically
different than those that would be observed under a model
of independent, additive prediction noise. Since urban areas
have higher average wealth than rural areas across countriesin our study, we expect the noised income baseline will over-
rank rural wealth and under-rank urban wealth.
Both the satellite-based poverty predictions and the noised-
income baseline over-rank wealth in rural areas in most coun-
tries (horizontal axis of Figure 3B). The degree of over-
ranking tends to be higher for the noised baseline than the
satellite-based predictions. The notable exceptions are the
United States and the Philippines, where prediction biases
from satellite imagery run in the opposite direction of those
from the noised wealth baseline (wealth is under-ranked in ru-
ral areas by satellite-based predictions and consistently over-
ranked by the noised wealth baseline in these two countries).
We explore possible drivers of these differences in Section 4.
3.3 Implications for downstream policies
To study the extent to which performance disparities and sys-
tematic prediction biases can propagate to allocative unfair-
ness in downstream policy decisions, we simulate hypotheti-
cal geographically targeted aid programs in each country, as
described in Section 2.4.
Geographic targeting effectiveness. We ﬁnd that the dis-
parities in predictive performance between urban and rural
areas documented in Section 3.1 reduce the effectiveness of
downstream decisions made using the satellite-based poverty
predictions. A simulated social protection program aiming to
select the poorest 20% of regions nationwide using satellite-
based poverty maps tends to have relatively high recall and
precision (54-71%), whereas programs identifying the poor-
est 20% of regions within urban or rural areas have lower re-
call and precision (38-73% in rural areas and 46-65% in urban
areas, Figure 1B).
Allocative unfairness. The systematic biases in ranking
of poverty by satellite-based predictions (Section 3.2) sug-
gests a risk of allocative unfairness when using satellite-based
poverty predictions to inform policy. In our simulated na-
tionwide aid programs, in countries where the relationship
between urbanization and wealth is strong (Colombia, Hon-
duras, India, Indonesia, Kenya, Mexico, Nigeria, and Peru),
aid tends to be under-allocated to rural areas (by 1-5 per-
centage points) compared to what would be allocated us-
ing ground truth wealth from the survey data. In countries
where correlation between urbanization and wealth is weaker
Figure 4: Two recalibration options. Panel A : Difference in allocation rates to rural regions (between using predictions and survey data to
assign allocations), for predictions with and without calibration, using the mean calibration strategy. Panel B : The same for selection threshold
calibration strategy. A difference of 0indicates an exact match with allocations based on survey data. Filled in markers show correction with
known values for urban/rural in satellite-based predictions; empty markers show the result of recalibrating with predicted urban/rural values.
(the Philippines and the United States), aid tends to be over-
allocated to rural areas (by 2-3 percentage points, Figure 3
and Figure S4). This latter pattern runs in the opposite di-
rection for the noised wealth baseline (faded markers in Fig-
ure 3), indicating that error structures speciﬁc to satellite-
based wealth predictions are driving allocative unfairness,
rather than general degradation of wealth estimates.
4 Investigating drivers of allocative unfairness
The nuanced patterns of allocative unfairness in Section 3.3
can be at least partially explained by characterizing two phe-
nomena driving errors in satellite-based predictions and rank-
ing of wealth between urban and rural areas:
Reversion towards the (sample) mean. One possible
driver of allocative unfairness is that predicted wealth can
be biased upward for low wealth regions and downwards for
high wealth regions, towards the overall mean wealth value
in the training data (as described in Section 3.2). In our sim-
ulated aid program, the upward bias of wealth rankings in
rural areas results in under-allocation of aid to rural areas.
Colombia, Honduras, India, Indonesia, Mexico, Nigeria, and
Peru are all emblematic of this pattern to varying degrees.
Notably, allocative biases are lesssevere for many of these
countries with satellite-based errors than would be expected
with classical Gaussian prediction errors (simulated with the
noised wealth baseline in Figure 3). One possible explanation
for this pattern is that a second driver of allocative unfairness
in satellite-based poverty predictions — described below —
works in the opposite direction of classical prediction error.
Reliance on correlations between urbanization and
wealth. A second potential driver of allocative unfairness is
a limited predictive power beyond identifying built-up areas
(established in Section 3.1). If variation in predicted wealth
is driven by urbanization, whereas variation in true wealth is
driven by more factors, satellite-based poverty prediction al-
gorithms might “miss” populations of urban poor, having as-
sociated them with urbanized regions tending to be wealthy.
The United States and the Philippines – which have the lowest
and third-lowest correlation between urbanization and wealthof all the countries we study, and the lowest overall prediction
performance (Table S3) – demonstrate this pattern.
While these two phenomena have different effects on the
allocation rate to urban and rural areas, it is possible (and
likely) for them to manifest jointly.6Summarized in Figure 3,
for most countries the ﬁrst driver seems to have the domi-
nant effect on allocation rates, excluding the United States
and the Philippines, where the allocative differences appear
to be driven mostly by the second phenomenon.
5 Addressing allocative unfairness
We test two approaches to addressing the issues of allocative
unfairness characterized in Section 3.3.
First, when we know which regions are classiﬁed as ur-
ban or rural, we can recalibrate the prediction distributions
within urban and rural areas to align with the true per-group
means in the training data. This addresses the “reversion to
the mean” phenomenon in an application-agnostic way. We
refer to this procedure as mean recalibration , and implement
it by learning an additive offset for each group so that the
predicted mean in each group matches the true group mean.
A second option is to directly address allocational unfair-
ness in the context of resource allocation by setting different
eligibility thresholds for urban and rural regions. We refer to
this option as selection threshold calibration , and implement
it by setting per-group allocation thresholds to match the frac-
tion of allocations that would be sent to urban and rural areas
using the reference wealth label values of the training set.
Mean calibration
Figure 4 shows that applying mean calibration often produces
downstream allocations that are closer to allocations based on
ground-truth wealth measures. Mean calibration successfully
reduces systematic prediction bias across urban and rural ar-
eas, and even slightly increases population level performance
for some countries (increase in R2of 0.00 - 0.02, increase in
Spearmanρof 0.00-0.02; Figure S6).
6We discuss this issue further and propose summary statistics to
help measure causes of each driver in Appendix B.
However, there are two important caveats to the mean cali-
bration strategy. First, it only addresses the ﬁrst driver of un-
fairness in Section 4 — reversion towards the mean. Across
countries, mean recalibration increases the allocation to rural
regions (evidenced by points above the y=xline in Figure 4)
due to the increased separation between predicted wealth of
rural and urban regions. In countries where the dominant
trend affecting allocation rates is missing the urban poor (the
Philippines and the United States), deploying this recalibra-
tion strategy can exacerbate allocative differences. For sim-
ulations in Mexico, mean recalibration also introduces an al-
locative bias toward over-targeting rural regions that was not
present in the original uncalibrated predictions.
Second, this simple mean recalibration strategy works
only when ground truth labels for being urban or rural are
known everywhere (that is, everywhere that the satellite-
based poverty map will be used — not just in the training
set). When we use satellite-based predictions for whether a
region is urban or rural to perform mean recalibration in the
test set, allocative bias is not signiﬁcantly improved in most
countries (non-ﬁlled-in points in Figure 4A).
Selection threshold calibration
When using ground truth indicators of urbanization, threshold
calibration results in allocations that are close to what would
be allocated with knowledge of true wealth values (conﬁ-
dence intervals for ﬁlled-in points in Figure 4B all overlap
they= 0line). This should be expected in our experimental
setup, so long as the distributions of urban and rural wealth
in the training set match those in the test set.
When satellite-based predictions for urbanization are used
to perform selection threshold calibration in the test set, al-
locative bias is not improved — the same pattern observed in
mean recalibration. It is possible that since wealth predictions
and urban build-up predictions are closely related (Section
3.1), there is little additional signal in urban build-up predic-
tions that is useful for calibration.
6 Discussion
Our work raises and investigates two main concerns relevant
to researchers and policymakers interested in building and de-
ploying satellite-based poverty maps for policymaking.
First, there are performance disparities in predictive ac-
curacy for identifying wealth levels within urban and ru-
ral areas in comparison to between them, explained partly
by somewhat limited representations of poverty in satellite
imagery beyond urbanization . In particular, wealth is bet-
ter differentiated between urban and rural areas than within
urban or rural parts of a country (Figure 1A). Simulated aid
programs that target only urban or only rural areas have lower
recall than national-scale programs that can leverage the dif-
ferences in urban and rural wealth (Figure 1B).
The main implication of this result for real-world deploy-
ments is that while satellite-based poverty programming at
a country scale may be relatively accurate (as documented
in past work [Jean et al. , 2016; Yeh et al. , 2020; Chi et al. ,
2022 ]), effectiveness may be substantially lower if programs
are deployed just for urban or rural areas (as is fairly common
in anti-poverty programming [Lindert et al. , 2020 ]).For researchers in machine learning, our results suggest
that a focus on building predictive models that represent and
distinguish wealth levels within urban and rural areas will
be essential for making satellite-based poverty maps a use-
ful and fair measurement tool. Other digital data sources,
such as mobile phone data [Blumenstock et al. , 2015;
Steele et al. , 2017 ], social media data [Fatehkia et al. , 2020;
Chiet al. , 2022 ], or information from crowdsourced maps
[Tingzon et al. , 2019 ]may be helpful for improving repre-
sentation and within-urban and within-rural differentiation.
Our second main ﬁnding is that systematic prediction bi-
ases in poverty predictions between urban and rural ar-
eas can result in allocative bias in downstream policy de-
cisions. The direction of prediction biases and downstream
disparities in allocations depends on the underlying joint dis-
tribution of poverty and urbanization: satellite-based poverty
maps may “miss” populations of urban poor in countries with
pockets of urban poverty, whereas in countries where poverty
is concentrated in rural areas, policies based on satellite-
based poverty maps are likely to over-allocate aid to urban
areas. The main implication of this result for policymakers is
that urban-rural biases may be present even in national-scale
policies using satellite-based poverty maps, and such maps
should always be audited for bias before deployment.
We test two simple yet promising approaches to address-
ing systematic prediction biases through recalibrating predic-
tions or selection thresholds, but both rely on having access
to ground-truth labels for regions being urban or rural in all
areas where the map is deployed. Imputed urban/rural values
are available at an increasingly high resolution globally [Rao
and Molina, 2015 ]; evaluating whether such estimates are
sufﬁcient for model recalibration will be an important topic
for future work. More generally, more sophisticated statis-
tical approaches to addressing prediction bias may improve
upon the ones we propose here [Proctor et al. , 2023 ].
The real-world implications of performance disparities and
prediction biases for downstream analyses and policies are
likely to be multi-faceted. We study in detail the implications
for one downstream use of satellite-based poverty maps: the
geographic targeting of humanitarian aid. A similar analysis
could be applied to understand implications of disparities and
biases for other uses of satellite-based predictions, such as
the estimation of sub-national statistics [Hofer et al. , 2020;
Sherman et al. , 2023 ]and causal inference on the effects of
anti-poverty programs [Huang et al. , 2021; Ratledge et al. ,
2022 ].
In summary, we ﬁnd consistent evidence of disparities in
satellite-based poverty maps across ten countries, with differ-
ent social structures, time scales, and modes of ground truth
data collection. An important complementary analysis, how-
ever, would seek to understand how the disparities we identify
interact within a single complex sociopolitical context. For
example, we studied disparities only across urban and rural
areas; developing a more comprehensive set of concerns will
crucially rely on local settings of model use. Such context-
driven work, along with the empirical results presented here,
can help policymakers realize the potential of satellite-based
poverty mapping while mitigating the risk that such maps in-
troduce bias or amplify existing inequities.
References
[Abbasi et al. , 2019 ]Mohsen Abbasi, Sorelle A Friedler,
Carlos Scheidegger, and Suresh Venkatasubramanian.
Fairness in representation: Quantifying stereotyping as a
representational harm. In Proceedings of the 2019 SIAM
International Conference on Data Mining , pages 801–809.
SIAM, 2019.
[Aiken et al. , 2022 ]Emily Aiken, Suzanne Bellue, Dean
Karlan, Chris Udry, and Joshua E Blumenstock. Machine
learning and phone data can improve targeting of humani-
tarian aid. Nature , 603(7903):864–870, 2022.
[Asher et al. , 2021 ]Sam Asher, Tobias Lunt, Ryu Matsuura,
and Paul Novosad. Development research at high geo-
graphic resolution: An analysis of night lights, ﬁrms, and
poverty in India using the SHRUG open data platform. The
World Bank Economic Review , 2021.
[Bada and Fox, 2022 ]X´ochitl Bada and Jonathan Fox. Per-
sistent rurality in Mexico and ‘the right to stay home’. The
Journal of Peasant Studies , 49(1):29–53, 2022.
[Blumenstock et al. , 2015 ]Joshua Blumenstock, Gabriel
Cadamuro, and Robert On. Predicting poverty and wealth
from mobile phone metadata. Science , 350(6264):1073–
1076, 2015.
[Blumenstock, 2018 ]Joshua Blumenstock. Don’t forget
people in the use of big data for development, 2018.
[Bolliger et al. , 2017 ]Ian Bolliger, Tamma Carleton,
Solomon Hsiang, Jonathan Kadish, Jonathan Proctor,
Benjamin Recht, Esther Rolf, and Vaishaal Shankar.
Ground control to Major Tom: The importance of ﬁeld
surveys in remotely sensed data analysis. arXiv preprint
arXiv:1710.09342 , 2017.
[Boyarskaya et al. , 2020 ]Margarita Boyarskaya, Alexandra
Olteanu, and Kate Crawford. Overcoming failures of
imagination in AI infused system development and de-
ployment. arXiv preprint arXiv:2011.13416 , 2020.
[Brown et al. , 2018 ]Caitlin Brown, Martin Ravallion, and
Dominique Van de Walle. A poor means test? Econo-
metric targeting in Africa. Journal of Development Eco-
nomics , 134:109–124, 2018.
[Buolamwini and Gebru, 2018 ]Joy Buolamwini and Timnit
Gebru. Gender shades: Intersectional accuracy dispari-
ties in commercial gender classiﬁcation. In Conference
on fairness, accountability and transparency , pages 77–
91. PMLR, 2018.
[Burke et al. , 2021 ]Marshall Burke, Anne Driscoll, David B
Lobell, and Stefano Ermon. Using satellite imagery to un-
derstand and promote sustainable development. Science ,
371(6535), 2021.
[Carleton et al. , 2022 ]Tamma Carleton, Trinetta Chong,
Hannah Druckenmiller, Eugenio Noda, Jonathan Proctor,
Esther Rolf, and Solomon Hsiang. Multi-Task Observation
Using Satellite Imagery and Kitchen Sinks (MOSAIKS)
API. https://siml.berkeley.edu, 2022.[Chiet al. , 2022 ]Guanghua Chi, Han Fang, Sourav Chatter-
jee, and Joshua E Blumenstock. Microestimates of wealth
for all low-and middle-income countries. Proceedings of
the National Academy of Sciences , 119(3):e2113658119,
2022.
[Chouldechova and G’Sell, 2017 ]Alexandra Chouldechova
and Max G’Sell. Fairer and more accurate, but for whom?
arXiv preprint arXiv:1707.00046 , 2017.
[Engstrom et al. , 2022 ]Ryan Engstrom, Jonathan Hersh,
and David Newhouse. Poverty from space: Using high
resolution satellite imagery for estimating economic well-
being. The World Bank Economic Review , 36(2):382–412,
2022.
[Fatehkia et al. , 2020 ]Masoomali Fatehkia, Isabelle
Tingzon, Ardie Orden, Stephanie Sy, Vedran Sekara,
Manuel Garcia-Herranz, and Ingmar Weber. Mapping
socioeconomic indicators using social media advertising
data. EPJ Data Science , 9(1):22, 2020.
[Gentilini et al. , 2022 ]Ugo Gentilini, Mohamed
Bubaker Alsaﬁ Almenﬁ, TMM Iyengar, Yuko Oka-
mura, John Austin Downes, Pamela Dale, Michael Weber,
David Locke Newhouse, Claudia P Rodriguez Alas, Ma-
reeha Kamran, et al. Social protection and jobs responses
to COVID-19. 2022.
[Ghosh and Roy, 1997 ]Rabindra Nath Ghosh and KC Roy.
The changing status of women in India: Impact of urban-
ization and development. International Journal of Social
Economics , 24(7/8/9):902–917, 1997.
[Hofer et al. , 2020 ]Martin Hofer, Tomas Sako, Arturo Mar-
tinez Jr, Mildred Addawe, Joseph Bulan, Ron Lester Du-
rante, and Marymell Martillan. Applying artiﬁcial intel-
ligence on satellite imagery to compile granular poverty
statistics. Asian Development Bank Economics Working
Paper Series , (629), 2020.
[Huang et al. , 2021 ]Luna Yue Huang, Solomon M Hsiang,
and Marco Gonzalez-Navarro. Using satellite imagery and
deep learning to evaluate the impact of anti-poverty pro-
grams. Technical report, National Bureau of Economic
Research, 2021.
[Jean et al. , 2016 ]Neal Jean, Marshall Burke, Michael Xie,
W Matthew Davis, David B Lobell, and Stefano Ermon.
Combining satellite imagery and machine learning to pre-
dict poverty. Science , 353(6301):790–794, 2016.
[Jerven, 2013 ]Morten Jerven. Poor numbers. In Poor Num-
bers. Cornell University Press, 2013.
[Kondmann and Zhu, 2021 ]Lukas Kondmann and Xiao Xi-
ang Zhu. Under the radar – auditing fairness in ML for
humanitarian mapping. arXiv preprint arXiv:2108.02137 ,
2021.
[Kuper, 2013 ]Leo Kuper. Religion and urbanization in
Africa. Reading in Race and Ethnic Relations: The Com-
monwealth and International Library: Reading in Sociol-
ogy, pages 129–148, 2013.
[Lindert et al. , 2020 ]Kathy Lindert, Tina George Karip-
pacheril, In ´es Rodr ´ıguez Caillava, and Kenichi Nishikawa
Ch´avez. Sourcebook on the foundations of social protec-
tion delivery systems . World Bank Publications, 2020.
[Liuet al. , 2018 ]Lydia T Liu, Sarah Dean, Esther Rolf, Max
Simchowitz, and Moritz Hardt. Delayed impact of fair ma-
chine learning. In International Conference on Machine
Learning , pages 3150–3158. PMLR, 2018.
[McGovern et al. , 2022 ]Amy McGovern, Imme Ebert-
Uphoff, David John Gagne, and Ann Bostrom. Why we
need to focus on developing ethical, responsible, and trust-
worthy artiﬁcial intelligence approaches for environmental
science. Environmental Data Science , 1:e6, 2022.
[Murray, 2022 ]Charles Murray. Data tools 1: Deciphering
the location of respondents in the American Community
Survey. 2022.
[Obermeyer et al. , 2019 ]Ziad Obermeyer, Brian Powers,
Christine V ogeli, and Sendhil Mullainathan. Dissecting
racial bias in an algorithm used to manage the health of
populations. Science , 366(6464):447–453, 2019.
[Paul et al. , 2021 ]Boban Varghese Paul, Chipo Msowoya,
Edward Archibald, Massimo Sichinga, Alejandra
Campero Peredo, and Muhammad Abdullah Ali Malik.
Malawi COVID-19 urban cash intervention process
evaluation report. 2021.
[Proctor et al. , 2023 ]Jonathan Proctor, Tamma Carleton,
and Sandy Sum. Parameter recovery using remotely
sensed variables. Technical report, National Bureau of
Economic Research, 2023.
[Rao and Molina, 2015 ]John NK Rao and Isabel Molina.
Small area estimation . John Wiley & Sons, 2015.
[Ratledge et al. , 2022 ]Nathan Ratledge, Gabe Cadamuro,
Brandon de la Cuesta, Matthieu Stigler, and Marshall
Burke. Using machine learning to assess the livelihood
impact of electricity access. Nature , 611(7936):491–495,
2022.
[Rolf et al. , 2021 ]Esther Rolf, Jonathan Proctor, Tamma
Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara,
Benjamin Recht, and Solomon Hsiang. A generalizable
and accessible approach to machine learning with global
satellite imagery. Nature communications , 12(1):1–11,
2021.
[Rolf, 2023 ]Esther Rolf. Evaluation challenges for geospa-
tial ml. arXiv preprint arXiv:2303.18087 , 2023.
[Sherman et al. , 2023 ]Luke Sherman, Jonathan Proctor,
Hannah Druckenmiller, Heriberto Tapia, and Solomon M
Hsiang. Global high-resolution estimates of the United
Nations Human Development Index using satellite im-
agery and machine-learning. Working Paper 31044, Na-
tional Bureau of Economic Research, March 2023.
[Smythe and Blumenstock, 2022 ]Isabella S Smythe and
Joshua E Blumenstock. Geographic microtargeting
of social assistance with high-resolution poverty maps.
Proceedings of the National Academy of Sciences ,
119(32):e2120025119, 2022.[Steele et al. , 2017 ]Jessica E Steele, P ˚al Roe Sundsøy, Carla
Pezzulo, Victor A Alegana, Tomas J Bird, Joshua Blu-
menstock, Johannes Bjelland, Kenth Engø-Monsen, Yves-
Alexandre De Montjoye, Asif M Iqbal, et al. Mapping
poverty using mobile phone and satellite data. Journal of
The Royal Society Interface , 14(127):20160690, 2017.
[Tingzon et al. , 2019 ]Isabelle Tingzon, Ardie Orden,
KT Go, S Sy, V Sekara, Ingmar Weber, M Fatehkia,
M Garc ´ıa-Herranz, and D Kim. Mapping poverty in the
Philippines using machine learning, satellite imagery,
and crowd-sourced geospatial information. International
Archives of the Photogrammetry, Remote Sensing &
Spatial Information Sciences , 2019.
[Wadoux et al. , 2021 ]Alexandre MJ-C Wadoux, Gerard BM
Heuvelink, Sytze De Bruin, and Dick J Brus. Spatial cross-
validation is not the right way to evaluate map accuracy.
Ecological Modelling , 457:109692, 2021.
[Yehet al. , 2020 ]Christopher Yeh, Anthony Perez, Anne
Driscoll, George Azzari, Zhongyi Tang, David Lobell, Ste-
fano Ermon, and Marshall Burke. Using publicly avail-
able satellite imagery and deep learning to understand
economic well-being in Africa. Nature communications ,
11(1):1–11, 2020.
[Zhang et al. , 2022 ]Miao Zhang, Harvineet Singh, Lazarus
Chok, and Rumi Chunara. Segmenting across places: The
need for fair transfer learning with satellite imagery. In
2022 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition Workshops (CVPRW) , pages 2915–2924,
2022.
Ethical Statement
This paper seeks to expose and quantify a potentially criti-
cal ethical issue in satellite-based poverty prediction: issues
of fairness within and between urban and rural areas. How-
ever, our work here still sits squarely within computational
and algorithmic aspects of fairness. By focusing on trends
across ten very different countries, the analysis in this paper
is largely devoid of the full social context of poverty mapping
and aid allocation in the each individual country we study.
Country-speciﬁc and human-centered work on local concep-
tions of fairness in such policies will complement the analysis
in this paper.
Acknowledgments
Aiken acknowledges support from a Microsoft Research PhD
Fellowship. Blumenstock acknowledges support from the
National Science Foundation under CAREER Grant IIS-
1942702. Rolf acknowledges support from the Harvard Data
Science Initiative and the Center for Research on Computa-
tion and Society.
We thank Paul Novosad and Sam Asher for sharing with
us with an early release of the SHRUG v2 dataset, and for
feedback on an earlier draft of this work. We thank Gabriel
Cadamuro, Tamma Carleton, Guanghua Chi, and Jonathan
Proctor for helpful feedback on the paper.
A Appendix: Data details
A.1 Categorization of rural vs. urban, calculation of wealth index
Demographic and Health (DHS) surveys. Each DHS survey uses a country-speciﬁc rule to deﬁne which areas are rural and
which are urban; rural ratios range from 30% in Colombia to 64% in the Philippines (Table S1).
American Community Survey (ACS). Categorizations of PUMAs as urban or rural are from Murray [2022 ]; 42% of PUMAs
are categorized as rural. Rural PUMAs are deﬁned by the ACS as “an agricultural or otherwise sparsely populated PUMA a
largest place of fewer than 20,000 people that is not contiguous with another place.”
Mexican Intercensal Survey. As discussed in Section 2.1, we construct an asset-based wealth index from the Mexican Inter-
censal data, using a principle components analysis to project ownership of the twelve assets (electricity, landline phone, mobile
phone, internet, car, hot water, air conditioning, computer, washing machine, refrigerator, TV , and radio) to a unidimensional
vector. The asset index explains 34% in the variance in ownership of the underlying assets. Our ground truth measure of
poverty is average asset-based wealth per municipality. Municipalities are assigned to urban or rural according to the Mexican
government’s defnition of rurality (recording in the intercensal survey): rural municipalities are those where the majority of the
population lives in communities of less than 2,500 people; the remaining municipalities are urban [Bada and Fox, 2022 ].
Indian Socio Economic and Caste Census (SECC) (via SHRUG). Shrid2s are geographic units deﬁned and used in the
SHRUG database to be consistent with census region deﬁnitions over time in India. As discussed in the documentation of
shrids for v1.57, each shrid2 region (shrid units for v2 of the dataset) can thus contain multiple villages or towns. The small-
area estimation method for computing per-capita consumption estimates for each shrid unit is described in [Asher et al. , 2021 ].
Rural and urban units are deﬁned by the 2012 SECC data in the SHRUG database, which separates per capita consumption
estimates by urban and rural. Of the 525,868 original shrid2 units, 126 (0.024%) have SECC values for both rural and urban
consumption. We categorize these units as be urban, taking rural regions to be those only with rural consumption. For the 126
units with both urban and rural consumption, we calculate total consumption as a weighted average of urban and rural con-
sumption in the shrid, weighting by the urban and rural population from the 2011 Indian population censuses (also aggregated
to shrids as in [Asher et al. , 2021 ]).
A.2 Combining rural SHRUG regions to larger geographical extents
In the original SHRUG v2 dataset, there are 3,524 urban (or both urban and rural) shrid2 units and 522,344 rural units. Because
some shrid2 regions are very small in geographic extent, we combine rural shrid2 to reduce the imbalance between the number
of urban and rural observations. This also ensures that several MOSAIKS tiles overlap with each cell for most observation
units. Only rural shrid2s are merged together; we do not alter the extents of urban shrid2s.
We use the following procedure to merge small rural shrids within the district administrative level (1 level ﬁner than states).
Within each district, we iteratively ﬁnd the smallest remaining rural extent (by area). We merge this extent with a neighboring
geometry according to the following rules: (1) only neighboring geometries currently made up of fewer than 25 regions are
eligible to be merged, (2) of the candidate neighbors, the one with the highest boundary overlap with the district to be merged
is chosen. If there are no feasible neighbors to merge with, the geometry will stay as is and be removed from the mergeable
list. We repeat this process until there are no more mergeable geometries (“mergeable” meaning geometries of area less than
25km2) with at least one neighbor that satisﬁes rule (1) above).
When geometries are merged according to this process, per capita consumption estimates are computed as a weighted average
of the per capita consumption estimates at shrid2 level, where weights in the average are proportional to the 2011 Indian census
population counts for each shrid2.
Before this procedure, there were 3,524urban geometries (median area 13.9km2) and522,344rural geometries (median ar-
eas 2.92km2). After aggregation, there are 3,524(median area 13.9km2) and59,832rural geometries (median areas 40.8km2).
A.3 MOSAIKS features
As mentioned in Section 3.1, for each instance (region), the random convolutional feature (RCF) representation is an average of
up to 100 MOSAIKS tiles overlapping with the geographic extent of the region. The minimum, maximum, and average number
of MOSAIKS tiles per region in each country is given in Table S1.
Figure 2A plots the average Euclidean distances between image features for pairs of regions, where the pairs considered are:
both rural regions, both urban regions, and one rural one urban region. In most countries we study, feature representations
of urban regions tend to be more similar to features of other urban regions than they are to features of rural instances. Rural
regions tend to be closer to each other in feature space than urban regions are to each other, though this could be partly due to
features in rural regions being an average over more MOSAIKS tiles on average than features corresponding to urban regions.
Thus, it is difﬁcult to say how differences in average feature distances measured in 2A affect performance differnces across
urban and rural regions (Figure 1). On the one hand, the smaller variation of feature representation within rural regions could
contribute to lower predictive performance for rural regions. On the other hand, when this lower variation is due to averaging
7https://shrug-assets-ddl.s3.amazonaws.com/static/main/assets/other/shrug-codebook.pdf
Figure S1: Average distance ℓ2between MOSAIKS tiles sampled from of pairs of rural instances, pairs of urban instances, and pairs of
urban-rural instances. For India, we randomly sub-sample 2,000 rural and 2,000 urban regions to estimate average distances.
of MOSAIKS more tiles across the rural geographies than urban geographies, the feature representation may be in some sense
more precise for rural regions, which could contribute to higher predictive performance in rural regions.
To understand the amount to which the averaging of the 1×1km tiles in the per-region MOSAIKS feature representation
affects the distances plotted in 2A, in Figure S1 we plot distances between the RCF featurizations of individual tiles in each
region, where one tile is sampled for each region. As in Figure 2A, the tile feature distances in Figure S1 are smaller between
pairs of rural instances than between pairs of urban instances or pairs of one rural and one urban instance. For all types of
pairs, the distances between tiles in Figure S1 tend to be larger than the distances between average feature representations in
Figure 2A. This is expected, since averaging many tiles reduces the variation in the feature representation for each region. The
averaging generally reduces the average rural-rural and urban-urban distances more than distances between urban-rural pairs.
This is consistent with the observation that the average feature representations for urban and rural regions are substantially
separated for many countries, reﬂected also in the PCA distribution plots in Figure 2B.
Dataset Deﬁnition of regionsNumber of
Regions% Rural
RegionsTiles Per Region
Minimum Mean Maximum
Colombia 2010 DHS Clusters 4,868 30.1% 16 52 88
Honduras 2011 DHS Clusters 1,128 56.2% 16 58 86
India 2012 SECC Aggregated shrid2s 63,356 94.4% 0 49 100
Indonesia 2017 DHS Clusters 1,319 57.8% 16 58 87
Kenya 2014 DHS Clusters 1,585 61.2% 16 60 86
Mexico 2015 survey Municipalities 2,446 56.1% 6 89 100
Nigeria 2018 DHS Clusters 1,359 58.8% 16 56 86
Peru 2012 DHS Clusters 1,131 38.8% 16 47 85
Philippines 2017 DHS Clusters 1,213 64.0% 16 62 88
US 2019 ACS PUMAs 2,331 41.8% 12 94 100
Table S1: Summary of datasets.
Figure S2: Maps of ground truth wealth and satellite-based wealth predictions, as well as categorizations of urban/rural, in each country we
study.
B Appendix: Supplementary analysis, ﬁgures, and tables
B.1 Quantifying summary statistics for drivers of allocative unfairness
In Section 3, we discussed two main phenomena that could drive allocative unfairness:
1. A reversion towards the sample mean, which biases predictions wealth of rural places to be higher than the true value, and
predictions of wealth of urban places to be lower, on average, and
2. A potential to miss the urban poor, due in part to relying on correlations between urbanization and wealth to produce
poverty predictions.
To study these two drivers more rigorously, we evaluate the correlation between summary statistics representing each of the
two phenomena and the difference in allocation in Figure 3 across 100 simulation runs with different random data splits. As a
summary statistic for the ﬁrst phenomenon – reversion to the sample mean – we use the difference in standard deviation between
satellite-predicted and true wealth distributions (we will notate this summary statistic as p1). As a summary statistic for the
second phenomenon — reliance on correlations between urbanization and wealth — we use the Spearman’s rank correlation
between wealth predictions and predictions for being urban (we will notate this summary statistic as p2).
To measure differences in allocations, we experiment with two summary statistics for allocational disparities: (a) the differ-
ence in allocations (between a targeting method that uses ground truth poverty data and a targeting method that uses satellite-
based predictions) to rural areas at a 20% selection threshold, as shown in Figure 3 and (b) the difference in area under the curves
in Figure S4, which summarizes the difference in allocations at all possible thresholds. For both these summary statistics, we
will notate the allocation to rural areas using ground truth poverty data as b, the allocation to rural areas using satellite-based
poverty predictions as ˆb, and the difference between the two as ˆb−b. We ﬁnd that, across countries, in simulations where the
ﬁrst driver is dominant (that is, reversion to the sample mean plays a key role — as measured by a large reduction in stan-
dard deviation), aid is under-allocated to rural areas. In simulations where the second driver is dominant (that is, the correlation
between predictions of wealth and predictions of urban build-up is high), aid tends to be over-allocated to rural areas (Table S2).
Country (A) ˆb (B)b (C)ˆb−b (D) Pearson’s r(ˆb−b,p1)(E) Pearson’s r(ˆb−b,p2)
Panel A: Using allocations at a 20% threshold as the summary statistic for allocations
Colombia 84.684 88.918 -4.234*** -0.026 0.105
Honduras 96.263 99.719 -3.456*** -0.097 -0.094
India 99.290 99.972 -0.682*** -0.091 0.115
Indonesia 96.515 97.818 -1.303*** -0.124 -0.056
Kenya 87.612 90.888 -3.275*** -0.306 0.014
Mexico 88.496 90.154 -1.659*** -0.100 0.111
Nigeria 90.652 93.072 -2.420*** -0.047 -0.110
Peru 92.439 97.351 -4.912*** -0.126 0.042
Philippines 93.623 90.279 3.344*** -0.057 0.193
US 52.077 50.000 2.077*** -0.110 0.469
Panel B: Using area under the targeting curves (Figure S4 Panel D) as the summary statistic for allocations
Colombia 0.572 0.594 -0.022*** 0.063 0.044
Honduras 0.816 0.827 -0.010*** -0.015 0.107
India 0.953 0.961 -0.007*** -0.090 -0.032
Indonesia 0.821 0.826 -0.005*** -0.046 0.129
Kenya 0.781 0.804 -0.023*** -0.273 -0.012
Mexico 0.718 0.745 -0.027*** 0.124 0.185
Nigeria 0.783 0.790 -0.007*** -0.049 -0.070
Peru 0.682 0.696 -0.015*** -0.159 0.169
Philippines 0.834 0.804 0.030*** -0.154 0.187
US 0.500 0.476 0.024*** -0.167 0.488
Table S2: Drivers of allocative unfairness between urban and rural areas. Columns A-B compare allocations when using true ( b) and predicted
(ˆb) values, averaged across runs. Column C documents the average difference in allocations ( b−ˆb), with statistical signiﬁcance determined
via a two-sided t test. Column D records the correlation between our summary statistic for the ﬁrst driver of allocative unfairness ( p1, the
magnitude of the gap in standard deviation between true and predicted values) and the difference in allocation to rural areas, across runs. A
negative correlation indicates that in general, on runs where the ﬁrst driver is strong, aid tends to be under-allocated to rural areas. Column E
records the correlation between our summary statistic for the second driver ( p2, the rank correlation between predicted poverty and predicted
urbanization) and the difference in allocation to rural areas, across runs. A positive correlation indicates that in general, on runs where the
ﬁrst driver is strong, aid tends to be over-allocated to rural areas.
B.2 Additional tables and ﬁgures
(A) Predicting poverty(B)
Predicting
urban(C) Relating poverty
and urban build-up(D) Using urban
predictions to
measure poverty
R2(w,ˆw) Pearson’s
r(w,ˆw)Spearman’s
ρ(w,ˆw)AUC(u,ˆu) Pearson’s
r(w,u)Spearman’s
ρ(w,u)Pearson’s
r(w,ˆu)Spearman’s
ρ(w,ˆu))
Colombia 0.70 (0.01) 0.84 (0.01) 0.83 (0.01) 0.94 (0.01) 0.77 (0.01) 0.72 (0.01) 0.71 (0.02) 0.70 (0.02)
Honduras 0.66 (0.04) 0.82 (0.02) 0.82 (0.02) 0.95 (0.01) 0.76 (0.03) 0.75 (0.02) 0.77 (0.02) 0.75 (0.02)
India 0.52 (0.01) 0.72 (0.00) 0.74 (0.00) 0.84 (0.01) 0.35 (0.01) 0.30 (0.01) 0.28 (0.01) 0.26 (0.01)
Indonesia 0.58 (0.03) 0.77 (0.02) 0.78 (0.02) 0.93 (0.02) 0.72 (0.02) 0.72 (0.02) 0.72 (0.03) 0.71 (0.03)
Kenya 0.58 (0.03) 0.77 (0.02) 0.77 (0.02) 0.84 (0.02) 0.59 (0.02) 0.60 (0.03) 0.58 (0.03) 0.55 (0.04)
Mexico 0.66 (0.03) 0.82 (0.01) 0.83 (0.01) 0.79 (0.02) 0.51 (0.03) 0.51 (0.03) 0.56 (0.04) 0.57 (0.04)
Nigeria 0.65 (0.02) 0.81 (0.01) 0.83 (0.01) 0.87 (0.02) 0.57 (0.03) 0.58 (0.03) 0.71 (0.02) 0.72 (0.03)
Peru 0.69 (0.03) 0.83 (0.02) 0.83 (0.02) 0.96 (0.01) 0.77 (0.02) 0.77 (0.02) 0.77 (0.02) 0.74 (0.02)
Philippines 0.47 (0.09) 0.70 (0.04) 0.72 (0.03) 0.90 (0.02) 0.53 (0.03) 0.53 (0.03) 0.62 (0.03) 0.63 (0.03)
US 0.49 (0.05) 0.72 (0.03) 0.71 (0.02) 0.99 (0.00) 0.28 (0.04) 0.28 (0.04) 0.28 (0.04) 0.24 (0.05)
Table S3: Relationship between urban build-up and predicting wealth from satellite imagery. Panel A evaluates the predictive accuracy of our
satellite-based wealth predictions using three metrics ( R2, Pearson’sr, and Spearman’s ρ). Panel B evaluates the predictive accuracy of our
satellite-based urban/rural classiﬁcations based on AUC. Panel C records the correlation between wealth and an indicator variable for being
urban in each country (using Pearson’s rand Spearman’s ρ). Panel D records the correlation between wealth and a satellite-based prediction
of being urban in each country (using Pearson and Spearman ρ).wrepresents ground-truth wealth; ˆwpredicted wealth; uground-truth urban
(a binary indicator), and ˆupredicted urban (a probabilistic prediction between 0 and 1). Standard deviations across bootstrapped runs are
shown in parentheses.
Figure S3: Comparing the predictive accuracy (measured with Spearman’s ρof satellite-based poverty predictions ( ˆw)for identifying wealth
(w), in comparison to using satellite-based probabilistic predictions of being urban ( ˆu) for identifying wealth ( w).
Figure S4: Allocative bias in using satellite-based wealth estimates. Panel A compares the mean signed error for satellite-based wealth
predictions (left) to the noised-wealth baseline (right). Panel B makes the same comparison for the mean rank error. Panel C records the share
of rural regions targeted in a hypothetical aid program targeting the poorest 20% of regions in each country, depending whether ground-truth
(green) wealth, satellite-based wealth estimates (yellow) or the noised-wealth baseline (gray) are used. Panel D records the sensitivity of the
allocations from Panel C to the eligibility threshold. In all panels error bars represent two standard errors above and below the mean.
Figure S5: Predicted performance ( R2score and Spearman ρ) vs. the degree of rank correlation between wealth and binary urbanization
values (urban or rural). Colors represent the evaluation regime: overall performance (black), performance across only rural regions (red), and
performance across only urban regions (blue). Each dot represents one evaluation regime for one country.
Figure S6: Additive recalibration by group raises the linear ﬁt and rank correlation of overall predictions (leftmost two panels) and reduces
statistical bias of predictions per-group (rightmost two panels).
IN-DOMAIN REPRESENTATION LEARNING
FOR REMOTE SENSING
Maxim Neumann⋆, Andr ´e Susano Pinto⋆, Xiaohua Zhai, and Neil Houlsby
{maximneumann,andresp,xzhai,neilhoulsby }@google.com
Google Research, Brain
Zurich, Switzerland
ABSTRACT
Given the importance of remote sensing, surprisingly little attention has been paid
to it by the representation learning community. To address it and to establish base-
lines and a common evaluation protocol in this domain, we provide simpliﬁed
access to 5 diverse remote sensing datasets in a standardized form. Speciﬁcally,
we investigate in-domain representation learning to develop generic remote sens-
ing representations and explore which characteristics are important for a dataset
to be a good source for remote sensing representation learning. The established
baselines achieve state-of-the-art performance on these datasets.
1 I NTRODUCTION
Remote sensing via computer vision and transfer learning is an important domain to address climate
change as outlined by Rolnick et al. (2019). Among others, research in remote sensing promises to
help in solving challenges in food security (precision farming), water sustainability, disaster preven-
tion (ﬂoods/landslides/earthquake forecasting), deforestation or wild ﬁre detection, urban planning,
and monitoring of carbon stocks and ﬂuxes or the air quality.
The number of Earth observing satellites is constantly increasing, with currently over 700 satellites
monitoring many aspects of the Earth’s surface and atmosphere from space, generating terabytes
of imagery data every day. However the ground truth data acquisition is costly, usually requiring
extensive campaign preparation, people and equipment transportation, and in-ﬁeld gathering of the
characteristics under question.
While there are remote sensing communities working on applying general deep learning methods to
remote sensing problems, this domain has received relatively little attention from the representation
learning community. Given its importance, it is still in an early development stage in comparison
to the progress made in representation learning on natural and medical images (eg. by Raghu et al.
(2019)).
Some reasons for this are the diversity of data sources (satellite types, data acquisition modes, reso-
lutions), the need of domain knowledge and special data processing, and the wide and scattered ﬁeld
of applications. The scarcity of standard recognized benchmark datasets and evaluation frameworks
is another.
For a long time there were only small labeled remote sensing datasets available. Only recently new
large-scale datasets have been generated in this domain (eg. by Zhu et al. (2018); Sumbul et al.
(2019)). However, a consistent evaluation framework is still missing and the performance is usually
reported on non-standard splits and with varying metrics, making reproduction and quick research
iteration difﬁcult.
To address this, we provide ﬁve representative and diverse remote sensing datasets in a standardized
form for easy reuse. In particular, we explore the importance of in-domain representation learning
for remote sensing at various data sizes and establish new state-of-the-art baseline results. The
main goal of this work is to develop general remote sensing representations that can be applied by
researchers to other unseen remote sensing tasks.
1arXiv:1911.06721v1  [cs.CV]  15 Nov 2019
By providing these standardized datasets, common problem deﬁnition and baselines, we hope this
work will simplify and enable faster iteration of research on remote sensing and inspire general
representation learning experts to test their newest methods in this critical domain.
In summary, the main contributions of this work are as follows:
1. Exploring in-domain supervised ﬁne-tuning to train generic remote sensing representations.
2. Generating 5 existing remote sensing datasets in a standardized format and establishing a
common evaluation protocol1. Publishing the best trained representation in TensorFlow
Hub for easy reuse in transfer learning applications2.
3. Establishing state-of-the-art baselines for the BigEarthNet, EuroSAT, RESISC–45, So2Sat,
and UC Merced datasets.
2 R ELATED WORK
REPRESENTATION AND TRANSFER LEARNING
Shortly after AlexNet (Krizhevsky et al., 2012) was trained on ImageNet (Deng et al., 2009), rep-
resentations obtained from it were used as off-the-shelf feature extractor networks (Razavian et al.,
2014). Fine-tuning approaches were also explored and led to better results than random-initialization
even when the tasks or domains differ (Yosinski et al., 2014; Kornblith et al., 2019). These ap-
proaches extend beyond natural images and have been used the in medical domain, leading Raghu
et al. (2019) to question on how they work. Simultaneously there have been many improvements on
training representations. Mahajan et al. (2018) explored training bigger models on larger datasets
with weaker labels and (Ngiam et al., 2018; Cui et al., 2018) showed gains by closer matching the
domains. Very promising are the more sample-efﬁcient approaches using semi- or self-supervision
(Zhai et al., 2019a).
DEEPLEARNING IN REMOTE SENSING
Common remote sensing problems include land-use and land cover (LULC) classiﬁcation, physical
and bio-geo-chemical parameter estimation, target detection, time-series analysis, pan-sharpening
and change detection. Many of these tasks can be solved or helped by deep learning approaches
related to classiﬁcation, object detection, semantic segmentation, or super-resolution. Reviews of
some of these approaches can be found for instance in (Ball et al., 2017; Zhu et al., 2017; 2019).
Remote sensing data is acquired in different modes: optical, multi- and hyper-spectral, synthetic
aperture radar (SAR), lidar, spectrometer (Elachi & Van Zyl, 2006). Each of these modes has its own
acquisition geometry and speciﬁc characteristics providing unique and complimentary information
about the Earth’s surface. In dependence of the instrument, the remote sensing imagery is available
from very high resolutions at centimeter scale (aerial optical or radar, eg. for urban monitoring) to
very low resolution at kilometer scale (eg. for atmosphere and ocean surface monitoring). Another
important satellite data characteristic is the revisit time (how fast does a satellite revisit the same
location, and constructs a time series), which can range from daily to multiple months.
The majority of deep learning approaches in remote sensing are currently based on optical imagery
at high (0.3-1 m) to medium (10-30 m) resolution, obtained from aerial imagery (such as seen on
Google Earth) or publicly available satellite data (eg. NASA’s Landsat or ESA’s Sentinel-2 satel-
lites), respectively. Though, multi-spectral, hyper-spectral, and radar imagery is increasingly being
used as well.
REPRESENTATION /TRANSFER LEARNING IN REMOTE SENSING
Since labeling remote sensing data is expensive, for a long time there was no equivalent to ImageNet
and most benchmark datasets were small. The probably most used remote sensing benchmarking
dataset, UC Merced (Yang & Newsam, 2010), has only 2100 images with 100 images per class.
1Published at https://www.tensorflow.org/datasets
2Published at https://tfhub.dev/google/collections/remote_sensing/1
2
Because it was not possible to train large state-of-the-art network on a small dataset from scratch,
remote sensing researchers started to build smaller networks, as for instance in Luus et al. (2015).
Another direction was to use models pre-trained on natural images. For instance, Marmanis et al.
(2016) used the extracted CNN features from the pre-trained Overfeat model (Sermanet et al., 2013)
to feed to another CNN model. Castelluccio et al. (2015) used GoogLeNet (Szegedy et al., 2015)
pre-trained on ImageNet to ﬁne-tune UC Merced. Similarly, Nogueira et al. (2017) used pre-trained
models on natural imagery to ﬁne-tune ﬁve models based of cross–validation splits. Afterwards,
they trained an additional SVM on the ﬁne-tuned features to achieve best performance.
Although there are works in using pre-trained representations, there is hardly any work on training
representations speciﬁc for solving tasks on this domain. An example is the multi-layer genera-
tive adversarial network (GAN) (Goodfellow et al., 2014) approach for unsupervised representation
learning presented by Lin et al. (2017). Xie et al. (2015) developed a transfer learning pipeline
for mapping poverty distribution based on transfering the nighttime lights prediction task. Not
speciﬁcally addressing representation learning, but a still related application using conditional GANs
(Mirza & Osindero, 2014) was demonstrated for cloud removal from RGB images by fusing RGB
and near infrared bands in Enomoto et al. (2017), or by fusing multi-spectral and synthetic aperture
radar data (Grohnfeldt et al., 2018).
3 D ATASETS
For this work, ﬁve diverse datasets were selected. We prioritized newer and larger datasets that are
quite diverse from each other, address scene classiﬁcation tasks, and include at least optical imagery.
Image data comes either from aerial high-resolution imagery or from satellites. Three datasets in-
clude imagery from the European Space Agency’s (ESA) Sentinel-2 satellite constellation, that pro-
vides medium resolution imagery of the Earth’s surface every three days. The multi-spectral imager
on Sentinel-2 delivers next to the 3 RGB channels additional channels at various frequencies (see
Appendix A.1). One dataset includes co-registered imagery from a dual-polarimetric synthetic aper-
ture radar (SAR) instrument of ESA’s Sentinel-1.
Besides the differences in data sources, number of training samples, number of classes, image sizes
and pixel resolutions (summarized in Table 1), the datasets are also quite diverse across:
•Intra- and inter-class visual diversity: some datasets have high in-class and low between-
classes diversity and vice versa.
•Label imbalance: some datatasets are perfectly balanced, while others are highly unbal-
anced.
•Label domain: land-use land cover (LULC), urban structures, ﬁne ecological labels.
•Label quality: from ﬁne human selection to weak labels from auxiliary datasets3.
While having only 5 datasets might not allow us to completely disentangle the confounding factors
of remote sensing representation learning, it should still help us in understanding the important
factors for good remote sensing representation learning datasets.
3.1 I NDIVIDUAL DATASETS
In addition to the descriptions of the individual datasets in this section, Appendix A shows example
images for all datasets and the distribution of classes.
BigEarthNet (Sumbul et al., 2019) is a challenging large-scale multi-spectral dataset consisting of
590,326 image patches from the Sentinel-2 satellite. 12 frequency channels (including RGB) are
provided, each covering an area of 1.2 ×1.2 km with resolutions of 10 m, 20 m, and 60 m per
pixel. This is a multi-label dataset where each image is annotated by multiple land-cover classes.
The label distribution is highly unbalanced ranging from 217k images of “mixed forest” label to
only 328 images with label “burnt areas”. About 12% of the patches are fully covered by seasonal
3Some datasets and images are affected by not ﬁltered-out cloud and snow coverage that makes the correct
classiﬁcation of the samples difﬁcult.
3
Table 1: Overview of considered remote sensing datasets.
Name year Source Size Classes Image size Resolution Problem
BigEarthNet 2019 Sentinel-2 590k 43 120x120* 10–60 m multi-label
EuroSAT 2019 Sentinel-2 27k 10 64x64 10 m multi-class
RESISC–45 2017 aerial 31.5k 45 256x256 0.2–60+ m multi-class
So2Sat 2019 Sentinel-1/2 376k 17 32x32 10 m multi-class
UC Merced 2010 aerial 2.1k 21 256x256 0.3 m multi-class
*Image size varies in dependence of resolution from 120x120 to 60x60 to 20x20.
snow, clouds or cloud shadows. The only available public baseline metrics include precision and
recall values of 69.93% and 77.1%, respectively, for using a shallow CNN on a reduced dataset,
after removing the snow and cloud affected samples.
EuroSAT (Helber et al., 2019) is another recently published dataset containing 27,000 images from
Sentinel-2 satellites. All 13 frequency bands of the satellite are included. Each image covers an area
of 640×640 meters and is assigned to one of 10 LULC classes, with 2000 to 3000 images per class.
Because the classes are quite distinctive, very high accuracies can be achieved when using the entire
dataset for training.
NWPU RESISC–45 (Cheng et al., 2017) dataset is an aerial dataset consisting of 31,500 RGB
images divided into 45 scene classes. Each class includes 700 images with a size of 256 ×256
pixels. This is the only dataset with varying spatial resolution ranging from 20 cm to more than 30
meters. The data covers a wide range of countries and biomes. During the construction, the authors
paid special attention to have classes with high same-class diversity and between-class similarity to
make it more challenging.
So2Sat LCZ-42 (Zhu et al., 2018) is a dataset consisting of co-registered SAR and multi-spectral
320×320 m image patches acquired by the Sentinel-1 and Sentinel-2 remote sensing satellites,
and the corresponding local climate zones (LCZ) (Stewart & Oke, 2012) labels. The dataset is
distributed over 42 cities across different continents and cultural regions of the world. This is another
challenging dataset and it is intended for learning features to distinguish various urban zones. The
challenge of this dataset is the relatively small image size (32 ×32) and the relatively high inter-class
visual similarity.
UC Merced Land-Use Dataset (Yang & Newsam, 2010) is a high-resolution (30 cm per pixel)
dataset that was extracted from aerial imagery from the United States Geological Survey (USGS)
National Map over multiple regions in the United States. The 256 ×256 RGB images cover 21
land-use classes, with 100 images per class. This is a relatively small datasets that has been widely
benchmarked for remote sensing scene classiﬁcation task since 2010 and for which nearly perfect
accuracy can be achieved with modern convolutional neural networks (Castelluccio et al., 2015;
Marmanis et al., 2016; Nogueira et al., 2017).
3.2 R ELEASE OF STANDARDIZED DATASETS
To simplify access to the data and its usage, we imported and published these datasets in Tensorﬂow
Datasets (TFDS)4.
For reproducability and a common evaluation framework, standard train, validation , and testsplits
using the 60%, 20%, and 20% ratios, respectively, were generated for all datasets except So2Sat.
For the So2Sat dataset, the source already provides train and validation splits. To generate the test
split, the original upstream validation is separated into validation and test splits with the 25% and
75% ratios, respectively.
4Published at https://www.tensorflow.org/datasets
4
4 R EMOTE SENSING DATA PROCESSING
The remote sensing domain is quite distinctive from natural image domain and requires special
attention during pre-processing and model construction. Some characteristics are:
•Remote sensing input data usually comes at higher precision (16 or 32 bits).
•The number of channels is variable, depending on the satellite instrument. RGB channels
are only a subset of a multi- or hyper-spectral imagery dataset. Other data sources might
have no optical channels (eg. radar or lidar) and the channels distribution can be determined
by polarimetric, interferometric or frequency diversity.
•The range of values varies largely from dataset to dataset and between channels. The values
distribution can be highly skewed.
•Many quantitative remote sensing tasks rely on the absolute values of the pixels.
•The images acquired from space are usually rotation invariant.
•Source data can be delivered at different product levels (for instance w/ or w/o atmospheric
correction, co-registration, orthorectiﬁcation, radiometric calibration, etc.).
•Especially lower resolution data aggregates a lot of information about the illuminated sur-
face in a single pixel since it covers a large area.
•Image axes might be non-standard, eg. representing range and azimuth dimensions.
This sets some requirements on pre-processing and encourages to adjust data augmentation of the
input pipeline for remote sensing data.
Speciﬁcally for the problems discussed in this paper, it is recommended to rescale and clip the range
of values per channel (accounting for outliers). Data augmentation that affects the intensity of the
values should be discarded. On the other hand, one can reuse the rotation invariance and extend
the augmentation to perform all rotations and ﬂipping (providing 7 additional images per sample).
Given multi-spectral data, such as Sentinel-2 based BigEarthNet, EuroSAT and So2Sat, one can use
other subsets of channels instead of RGB including all available ones.
5 E XPERIMENTAL RESULTS
The main goal is to develop representations that can be used across a wide ﬁeld of remote sensing
applications on unseen tasks. The training and evaluation protocol follows two main stages: (1)
upstream training of the representations model based on some out- or in-domain data, and (2) down-
stream evaluation of the representations by transferring the trained representation features to the new
downstream tasks. For the upstream training on in-domain proxy datasets the entire data is used, that
cannot include any data from the downstream tasks. The quality of the trained representations and
their generalizability is often evaluated on reduced downstream training sizes to assess efﬁciency of
the representations.
5.1 E XPERIMENTAL SETUP
In order to simplify the training procedure and to draw more general conclusions, all experiments
use the same ResNet50 V2 architecture (He et al., 2016) and conﬁguration. However, due to the
varied number of classes and training samples in the various datasets, we perform sweeps over a
small set of hyper-parameters and augmentations as described in detail in Appendix B.
5.2 E VALUATION METRICS
We report performance results using accuracy metrics commonly used in computer vision tasks:
for multi-class problems we use the Top-1 global accuracy metric, which denotes the percentage
of correctly labeled samples. For multi-label problems we use the mean average precision (mAP)
metric, which denotes the mean over the average precision (A V) values (A V is the integral over the
precision-recall curve) of the individual labels.
5
Table 2: Performance of trained In-Domain and ImageNet representations (rows) when using 1000
training examples for downstream tasks (columns). Emphasized (bold font) are the best accuracies
per downstream task (column).
Source\Target BigEarthNet EuroSAT RESISC–45 So2Sat UC Merced
ImageNet 25.10 96.84 84.89 53.69 99.02
BigEarthNet - 96.45 78.43 50.91 99.61
EuroSAT 27.10 - 79.59 52.99 98.05
RESISC–45 27.59 97.14 - 54.43 99.61
So2Sat 26.30 96.30 77.70 - 97.27
UC Merced 26.86 96.73 85.73 53.52 -
To measure and aggregate relative performance improvement over datasets performing at quite dif-
ferent accuracy levels, the logit transformation of the accuracy is preferred (Kornblith et al., 2019):
∆ = logit(ρ) = log(ρ
1−ρ)
=1
sigmoid (ρ)
whereρis the accuracy rate (Top-1 or mAP) and ∆is the corresponding logit accuracy. It captures
the importance of accuracy change at different accuracy levels to provide a more fair evaluation of
the achievement.
5.3 C OMPARING IN -DOMAIN REPRESENTATIONS
To obtain in-domain representations, ﬁrst we train models either from scratch or by ﬁne-tuning Im-
ageNet on each full dataset. The best of these models are then used as in-domain representations to
train models on other remote sensing tasks (excluding the one used to train the in-domain represen-
tation).
For an initial evaluation of the different in-domain representation source data, Table Table 2 shows a
cross-table of evaluating each trained in-domain and ImageNet representation on each of the down-
stream tasks. The representations were trained using full datasets upstream, while the down-stream
tasks used only 1000 training examples to better emphasize the differences.
It can be observed that with 1000 training examples, the best results all come from ﬁne-tuning the
in-domain representations.
Additionally, despite having 2 distinctive groups of high-resolution aerial (RESISC–45, UC Merced)
and medium-resolution satellite datasets (BigEarthNet, EuroSAT and So2Sat), the representations
trained on RESISC–45 were able to outperform the others in all tasks (BigEarthNet representa-
tions tied for the UC Merced dataset) and it was the only representation to consistently outperform
ImageNet-based representations.
That RESISC–45 would perform so good on both aerial and satellite data was unexpected. The
reason is most likely related to the fact that RESISC–45 is the only dataset that has images with
various resolutions. Combined with the large number of classes that have high within class diversity
and high between-class similarity it seems to be able to train good representations for a wide range
of remote sensing tasks, despite not being a very big dataset. This learning can be reused to adjust
augmentation schemes for remote sensing representation learning by stronger varying the scale of
images with randomized zooms and potentially other afﬁne transformations.
The best representations for the RESISC–45 itself come from the other aerial dataset, UC Merced.
The relatively small training size of it was counter-balanced by an aggressive augmentation ( crop &
rotas described in Appendix B.3).
Counter to the expectation that bigger datasets should train better representations, the two biggest
datasets, BigEarthNet and So2Sat, didn’t provide the best representations (except of BigEarthNet
representations for UC Merced). We hypothesize that this might be due to the weak labeling and the
low training accuracy obtained in these datasets. It is possible that the full potential of these large-
scale datasets was not yet fully utilized and other self- or semi-supervised representation learning
approaches could improve the performance.
6
BigEarthNetEuroSATRESISC-45So2Sat
UC Merced012Logit accuracy gain(a) Over datasets
1001000Full0.00.51.0Logit accuracy gain (b) Over sizes
ImageNet
InDomain
Figure 1: Aggregated mean relative improvement in logit accuracy of ﬁne-tuning from ImageNet
and in-domain representations in comparison to training from scratch.
5.4 L ARGE -SCALE COMPARISON
Having trained in-domain representations, we can now evaluate and compare the transfer quality of
ﬁne-tuning the best in-domain representations with ﬁne-tuning ImageNet and training from scratch
at various training data sizes. The results using the default experimental setup are shown in Table 3.
In all cases, ﬁne-tuning from ImageNet is better than training from scratch. And in all but one case,
ﬁne-tuning from an in-domain representation for transfer is even better.
The only exception is the BigEarthNet dataset at its full size. It is expected that having a large dataset
should reduce the need for pre-training, but the gap between in-domain and ImageNet pre-training
is quite big. We don’t have an explanation for this yet and this needs to be further investigated.
Overall, these results establish new state-of-the-art baselines for these datasets, as summarized in
Table 4. Note that some results are not comparable: RESISC–45 has been previously evaluated only
on 20% of data, So2Sat has no public benchmarking result to our knowledge, and the only published
result of BigEarthNet is based on a cleaner version of the dataset (after removing the noisy images
containing clouds and snow) and only precision and recall metrics were reported.
Fig. 1 summarizes the results of Table 3 across all datasets and training sizes. Logit accuracy metric
is used, which ﬁts better for aggregation of accuracies across wide ranges. Fig. 1a emphasizes that
the smaller datasets (EuroSAT, RESISC–45, UC Merced) proﬁt more from in-domain knowledge
than the larger datasets (BigEarthNet, So2Sat). As is re-iterated again in Fig. 1b, using in-domain
representations leads to higher accuracy gain when small number of training examples are available,
which is of most interest for remote sensing applications to reduce the ground truth data acquisition.
5.5 L IMITED NUMBERS OF TRAINING EXAMPLES
To look closer into in-domain representation learning for small number of training examples, we
trained models with small training sizes ranging from 25 to 2500 (samples were randomly drawn
disregarding class distributions). We used a simpliﬁed set of hyper-parameters that might not deliver
the most optimal performance, but still allows to observe the general trends. As shown in Fig. 2,
the improvement of using in-domain representations is clearly visible for the EuroSAT, RESISC–45
and UC Merced datasets. These are the 3 smaller datasets with higher quality labels. The results are
less conclusive for the BigEarthNet and So2Sat datasets that have more noisy labels.
Table 3: Accuracy over different training methods and number of used training samples.
BigEarthNet EuroSAT RESISC–45 So2Sat UC Merced
100 1k Full 100 1k Full 100 1k Full 100 1k Full 100 1k Full
Scratch 14.5 21.4 72.4 63.9 91.7 98.5 21.4 56.1 95.6 33.9 47.0 62.1 50.8 91.2 95.7
ImageNet 17.8 25.1 75.4 87.3 96.8 99.1 44.9 84.9 96.6 44.9 53.7 63.1 79.9 99.0 99.2
InDomain 18.8 27.6 69.7 91.3 97.1 99.2 49.0 85.7 96.8 46.4 54.4 63.2 91.0 99.6 99.6
7
Table 4: Results on selected remote sensing datasets. All results use Top-1 accuracy, except for
BigEarthNet which uses precision/recall and mean average precision. Ourbest results were obtained
by ﬁne-tuning in-domain representations except for BigEarthNet which was obtained by ﬁne-tuning
ImageNet.
Dataset Reference Result
BigEarthNetSumbul et al. (2019) 69.93% / 77.1% (P/R)
Ours 75.36 % (mAP)
EuroSATHelber et al. (2019) 98.57%
Ours 99.20 %
RESISC–45Cheng et al. (2017) 90.36%
Ours 96.83 %
So2Sat Ours 63.25 %
UC MercedMarmanis et al. (2016) 92.4%
Castelluccio et al. (2015) 97.1%
Nogueira et al. (2017) 99.41%
Ours 99.61 %
25 250 2500
Training samples0.20.3mAP
(a) BigEarthNet
25 250 2500
Training samples0.40.60.81.0Accuracy
 (b) EuroSAT
25 250 2500
Training samples0.250.500.75Accuracy
 (c) RESISC–45
25 250 2500
Training samples0.30.40.5Accuracy
(d) So2Sat
25 250 2500
Training samples0.250.500.751.00Accuracy
 (e) UC Merced
InDomain
ImageNet
FromScratch
Figure 2: Top-1 accuracy rate or mean average precision (mAP) on validation set after training with
a given method over a limited number of training examples on each dataset.
6 C ONCLUSION
We present a common evaluation benchmark for remote sensing representation learning based on
ﬁve diverse datasets. The results demonstrate the enhanced performance of in-domain represen-
tations, especially for tasks with limited number of training samples, and achieve state-of-the-art
performance. The ﬁve analyzed datasets and the best trained in-domain representations are pub-
lished for easy reuse by the public.
We investigate dataset characteristics to be a good source for remote sensing representation learning.
As the experimental results indicate, having a multi-resolution dataset helps to train more general-
izable representations. Other factors seem to be label quality, number of classes, visual similarity
across the classes and visual diversity within the classes. Surprisingly, we observed that representa-
tions trained on the large weakly-supervised datasets were not as successful as that of a smaller and
more diverse human-curated dataset.
8
However, some results were inconclusive and require more investigation. Understanding the main
factors of a good remote sensing dataset for representation learning is a major challenge, solving
which could improve performance across a wide range of remote sensing tasks and applications.
ACKNOWLEDGMENTS
We thank No ´e Lutz for useful comments, Jeremiah Harmsen for inspiration, and the Brain Zurich
VTAB team for insightful discussions and developing the benchmarking framework. Finally, we
would like to acknowledge Tensorﬂow Hub (TF-Hub) and Tensorﬂow Datasets (TFDS) teams for
their support on publishing datasets and models.
REFERENCES
John E. Ball, Derek T. Anderson, and Chee Seng Chan. Comprehensive survey of deep learning in
remote sensing: theories, tools, and challenges for the community. Journal of Applied Remote
Sensing , 11(04):1, September 2017. ISSN 1931-3195. doi: 10.1117/1.JRS.11.042609.
Marco Castelluccio, Giovanni Poggi, Carlo Sansone, and Luisa Verdoliva. Land Use Classiﬁcation
in Remote Sensing Images by Convolutional Neural Networks. August 2015. URL http:
//arxiv.org/abs/1508.00092 .
G. Cheng, J. Han, and X. Lu. Remote sensing image scene classiﬁcation: Benchmark and state
of the art. Proceedings of the IEEE , 105(10):1865–1883, October 2017. ISSN 0018-9219. doi:
10.1109/JPROC.2017.2675998.
Yin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge J. Belongie. Large scale ﬁne-grained
categorization and domain-speciﬁc transfer learning. CoRR , abs/1806.06193, 2018. URL http:
//arxiv.org/abs/1806.06193 .
Jia Deng, Wei Dong, Richard Socher, Li jia Li, Kai Li, and Li Fei-fei. Imagenet: A large-scale
hierarchical image database. In In CVPR , 2009.
Charles Elachi and Jakob J Van Zyl. Introduction to the physics and techniques of remote sensing ,
volume 28. John Wiley & Sons, 2006.
Kenji Enomoto, Ken Sakurada, Weimin Wang, Hiroshi Fukui, Masashi Matsuoka, Ryosuke Naka-
mura, and Nobuo Kawaguchi. Filmy cloud removal on satellite imagery with multispectral con-
ditional generative adversarial nets. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops , pp. 48–56, 2017.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems , pp. 2672–2680, 2014.
C. Grohnfeldt, M. Schmitt, and X. Zhu. A conditional generative adversarial network to fuse sar
and multispectral optical data for cloud removal from sentinel-2 images. In IGARSS 2018 - 2018
IEEE International Geoscience and Remote Sensing Symposium , pp. 1726–1729, July 2018. doi:
10.1109/IGARSS.2018.8519215.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision . Springer, 2016.
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset
and deep learning benchmark for land use and land cover classiﬁcation. IEEE Journal of Selected
Topics in Applied Earth Observations and Remote Sensing , 12(7):22172226, July 2019. ISSN
2151-1535. doi: 10.1109/jstars.2019.2918242. URL http://dx.doi.org/10.1109/
JSTARS.2019.2918242 .
Simon Kornblith, Jonathon Shlens, and Quoc V . Le. Do better imagenet models transfer better? In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. URL
http://arxiv.org/abs/1805.08974 .
9
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 25 , pp. 1097–
1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.
pdf.
Daoyu Lin, Kun Fu, Yang Wang, Guangluan Xu, and Xian Sun. MARTA GANs: Unsupervised
Representation Learning for Remote Sensing Image Classiﬁcation. IEEE Geoscience and Remote
Sensing Letters , 14(11):20922096, November 2017. ISSN 1558-0571. doi: 10.1109/lgrs.2017.
2752750. URL http://dx.doi.org/10.1109/LGRS.2017.2752750 .
F. Luus, B.P. Salmon, F. Bergh, and B. Maharaj. Multiview deep learning for land-use classiﬁca-
tion. IEEE Geoscience and Remote Sensing Letters , 12:1–5, 10 2015. doi: 10.1109/LGRS.2015.
2483680.
Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan
Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised
pretraining. CoRR , abs/1805.00932, 2018. URL http://arxiv.org/abs/1805.00932 .
Dimitrios Marmanis, Mihai Datcu, Thomas Esch, and Uwe Stilla. Deep Learning Earth Observa-
tion Classiﬁcation Using ImageNet Pretrained Networks. IEEE Geoscience and Remote Sensing
Letters , 13(1):105–109, jan 2016. ISSN 1545-598X. doi: 10.1109/LGRS.2015.2499239. URL
http://ieeexplore.ieee.org/document/7342907/ .
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784 , 2014.
Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Kornblith, Quoc V . Le, and Ruoming Pang.
Domain adaptive transfer learning with specialist models. CoRR , abs/1811.07056, 2018. URL
http://arxiv.org/abs/1811.07056 .
Keiller Nogueira, Otvio A.B. Penatti, and Jefersson A. dos Santos. Towards better exploiting
convolutional neural networks for remote sensing scene classiﬁcation. Pattern Recogn. , 61
(C):539–556, January 2017. ISSN 0031-3203. doi: 10.1016/j.patcog.2016.07.001. URL
https://doi.org/10.1016/j.patcog.2016.07.001 .
Maithra Raghu, Chiyuan Zhang, Jon M. Kleinberg, and Samy Bengio. Transfusion: Understanding
transfer learning with applications to medical imaging. CoRR , abs/1902.07208, 2019. URL
http://arxiv.org/abs/1902.07208 .
Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features
off-the-shelf: an astounding baseline for recognition. CoRR , abs/1403.6382, 2014. URL http:
//arxiv.org/abs/1403.6382 .
David Rolnick, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran,
Andrew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexan-
dra Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Karthik Mukkavilli, Konrad P. Kording, Carla
Gomes, Andrew Y . Ng, Demis Hassabis, John C. Platt, Felix Creutzig, Jennifer Chayes, and
Yoshua Bengio. Tackling climate change with machine learning, 2019.
Pierre Sermanet, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. Over-
feat: Integrated recognition, localization and detection using convolutional networks, 2013.
I.D. Stewart and T. Oke. Local climate zones for urban temperature studies. Bulletin of the American
Meteorological Society , 93:1879–1900, 12 2012. doi: 10.1175/BAMS-D-11-00019.1.
Gencer Sumbul, Marcela Charfuelan, Begm Demir, and V olker Markl. Bigearthnet: A large-scale
benchmark archive for remote sensing image understanding, 2019.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. pp.
1–9, 06 2015. doi: 10.1109/CVPR.2015.7298594.
10
Michael Xie, Neal Jean, Marshall Burke, David Lobell, and Stefano Ermon. Transfer Learning
from Deep Features for Remote Sensing and Poverty Mapping. September 2015. URL http:
//arxiv.org/abs/1510.00098 .
Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classiﬁca-
tion. In Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geo-
graphic Information Systems - GIS ’10 , pp. 270, New York, New York, USA, 2010. ACM Press.
ISBN 9781450304283. doi: 10.1145/1869790.1869829. URL http://portal.acm.org/
citation.cfm?doid=1869790.1869829 .
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features
in deep neural networks? In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,
and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 27 , pp.
3320–3328. Curran Associates, Inc., 2014. URL http://papers.nips.cc/paper/
5347-how-transferable-are-features-in-deep-neural-networks.pdf .
Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-supervised semi-
supervised learning. CoRR , abs/1905.03670, 2019a. URL http://arxiv.org/abs/1905.
03670 .
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario
Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer,
Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and
Neil Houlsby. The visual task adaptation benchmark, 2019b.
Ming Zhu, Yongning He, and Qingyu He. A Review of Researches on Deep Learning in Remote
Sensing Application. International Journal of Geosciences , 10(01):1–11, 2019. ISSN 2156-
8359. doi: 10.4236/ijg.2019.101001. URL http://www.scirp.org/journal/doi.
aspx?DOI=10.4236/ijg.2019.101001 .
Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, and Friedrich
Fraundorfer. Deep Learning in Remote Sensing: A Comprehensive Review and List of Re-
sources. IEEE Geoscience and Remote Sensing Magazine , 5(4):8–36, dec 2017. ISSN 2168-6831.
doi: 10.1109/MGRS.2017.2762307. URL http://ieeexplore.ieee.org/document/
8113128/ .
Xiaoxiang Zhu, Jingliang Hu, Chunping Qiu, Yilei Shi, Hossein Bagheri, Jian Kang, Hao Li,
Lichao Mou, Guicheng Zhang, Matthias Hberle, Shiyao Han, Yuansheng Hua, Rong Huang,
Lloyd Hughes, Yao Sun, Michael Schmitt, and Yuanyuan Wang. So2Sat LCZ42, 2018. URL
https://mediatum.ub.tum.de/1454690 .
11
A A DDITIONAL DATASET INFORMATION
This section provides additional information on the used datasets and data sources.
A.1 S ENTINEL -2 S ATELLITE
Sentinel-2 is a multi-spectral satellite constellation from the European Space Agency (ESA). Since
2017 two satellites are in operation delivering a 5-days revisit at equator and 2-3 days at high lati-
tudes. The characteristics of the 13 bands are presented in Table 5.
Table 5: Sentinel-2 channel characteristics (Abbreviations: NIR: Near Infra-Red, SWIR: Short-
Wavelength Infra-Red).
Band and Highest Sensitivity Target Spatial Resolution [m] Central Wavelength [nm]
B01 - Aerosols 60 443
B02 - Blue 10 490
B03 - Green 10 560
B04 - Red 10 665
B05 - Red edge 1 20 705
B06 - Red edge 2 20 740
B07 - Red edge 3 20 783
B08 - NIR 10 842
B08A - Red edge 4 20 865
B09 - Water vapor 60 945
B10 - Cirrus 60 1375
B11 - SWIR 1 20 1610
B12 - SWIR 2 20 2190
12
A.2 B IGEARTH NET
The following ﬁgures show some example images and label distribution for the BigEarthNet dataset.
Figure 3: BigEarthNet image examples. Some images might be affected by seasonal snow, clouds
or cloud shadows, which is not reﬂected in the land cover labels of this dataset. Note that some of
the images (for example samples 4, 6, 21) could be affected by seasonal snow and cloud coverage,
which is not reﬂected in the labels.
Labels for the examples in Fig. 3:
1. Vineyards, Broad-leaved forest (2 labels)
2. Coniferous forest, Mixed forest, Transitional woodland/shrub (3 labels)
3. Sea and ocean (1 label)
4. Land principally occupied by agriculture, with signiﬁcant areas of natural vegetation,
Coniferous forest, Mixed forest, Water bodies (4 labels)
5. Non-irrigated arable land, Mixed forest (2 labels)
6. Land principally occupied by agriculture, with signiﬁcant areas of natural vegetation,
Coniferous forest, Mixed forest, Transitional woodland/shrub (4 labels)
7. Sea and ocean (1 labels)
13
8. Coniferous forest, Mixed forest (2 labels)
9. Discontinuous urban fabric, Industrial or commercial units, Coniferous forest, Mixed for-
est, Transitional woodland/shrub (5 labels)
10. Non-irrigated arable land, Pastures (2 labels)
11. Coniferous forest, Mixed forest, Water bodies (3 labels)
12. Sea and ocean (1 labels)
13. Sparsely vegetated areas, Peatbogs (2 labels)
14. Coniferous forest, Mixed forest, Transitional woodland/shrub (3 labels)
15. Continuous urban fabric (1 label)
16. Complex cultivation patterns, Land principally occupied by agriculture, with signiﬁcant
areas of natural vegetation, Broad-leaved forest (3 labels)
17. Land principally occupied by agriculture, with signiﬁcant areas of natural vegetation,
Broad-leaved forest, Sclerophyllous vegetation, Transitional woodland/shrub (4 labels)
18. Agro-forestry areas, Broad-leaved forest, Transitional woodland/shrub (3 labels)
19. Non-irrigated arable land, Land principally occupied by agriculture, with signiﬁcant areas
of natural vegetation, Coniferous forest, Mixed forest (4 labels)
20. Non-irrigated arable land (1 label)
21. Coniferous forest, Mixed forest, Transitional woodland/shrub, Water bodies (4 labels)
22. Coniferous forest, Mixed forest (2 labels)
23. Non-irrigated arable land, Land principally occupied by agriculture, with signiﬁcant areas
of natural vegetation, Agro-forestry areas, Sclerophyllous vegetation, Transitional wood-
land/shrub, Water bodies (6 labels)
24. Discontinuous urban fabric, Non-irrigated arable land, Inland marshes (3 labels)
25. Land principally occupied by agriculture, with signiﬁcant areas of natural vegetation,
Broad-leaved forest, Coniferous forest (3 labels)
14
Figure 4: BigEarthNet labels distribution counts.
15
A.3 E UROSAT
The following ﬁgures show some example images and label distribution for the EuroSAT dataset.
Figure 5: EuroSAT image examples.
Figure 6: EuroSAT class distribution counts.
16
A.4 RESISC–45
The following ﬁgures show some example images and label distribution for the RESISC–45 dataset.
Figure 7: RESISC–45 image examples.
Figure 8: RESISC–45 class distribution counts.
17
A.5 S O2SAT
The following ﬁgures show some example images and label distribution for the So2Sat dataset.
Figure 9: So2Sat image examples.
Figure 10: So2Sat class distribution counts.
18
A.6 UC M ERCED
The following ﬁgures show some example images and label distribution for the UC Merced dataset.
Figure 11: UC Merced image examples.
Figure 12: UC Merced class distribution counts.
19
B T RAINING SETUP
B.1 A RCHITECTURE AND HYPER -PARAMETERS
All the models trained in this work use the ResNet50 v2 architecture (He et al., 2016) and were
trained using SGD with momentum set to 0.9 on large batch sizes 512or1024 . In total we conﬁgure
and sweep only a ﬁxed set of hyper-parameters per model, namely:
•learning rate:{0.1,0.01},
•weight decay:{0.01,0.0001},
•training schedules: {short, medium, long },
•preprocessing:{resize ,crop & rot}(described in Appendix B.3).
All the 3 training schedules use a linear warm-up for learning rate over the ﬁrst wsteps/epochs
and decrease the learning rate by 10per each learning phase pin the schedule. The learning rate
schedules are given by:
•short:p={750,1500,2250,2500}steps,w= 200 steps.
•medium:p={3000,6000,9000,10000}steps,w= 500 steps.
•long:p={30,60,80,90}epochs,w= 5epochs.
These hyperparameter settings follow approximately the setup in (Zhai et al., 2019b) with minor
modiﬁcations for the schedule and preprocessing. In in inital phase, more extensive hyperparam-
eter sets were tried out, but with not much effect on the best performance and therefore for the
experiments presented in this paper we limit the conﬁgurations to the ones described above.
B.2 D ETAILED SWEEPS PER EXPERIMENT
The models for experiments in Table 2 and Table 3 were trained sweeping over all hyper param-
eters. The best performing models obtained from ﬁne-tuning ImageNet were used as in-domain
representations for all experiments (excluding same up- and down-stream dataset conﬁgurations).
For Fig. 2, the models were trained by sweeping over the learning rate and using only the short and
medium training schedules. The weight decay was set to 0.0001 , and the preprocessing was set to
resize .
B.3 P RE-PROCESSING AND DATA AUGMENTATION
Data pre-processing and augmentation can have a signiﬁcant impact on performance. Therefore, in
the reported results we used 2 pre-processing settings described below.
•resize - resize the original RGB input to 224x224 both at training and evaluation time.
•crop & rot - during training resize the original RGB input to 256x256, perform a random
crop of 224x224 and apply one of 8 random rotations (90 degrees and horizontal ﬂip).
During evaluation resize to 256x256 and perform a central crop of 224x224.
Table 6 shows the difference of each strategy when ﬁne-tune from ImageNet.
Table 6: Accuracy of each pre-processing strategy when ﬁne-tuning an ImageNet representation.
BigEarthNet EuroSAT RESISC–45 So2Sat UC Merced
100 1k Full 100 1k Full 100 1k Full 100 1k Full 100 1k Full
resize 17.3 24.5 75.4 85.2 96.0 98.9 37.5 78.6 95.8 44.9 51.8 63.1 69.1 98.2 99.2
crop & rot 17.8 25.1 73.4 87.3 96.8 99.1 44.9 84.9 96.6 43.8 53.7 59.6 79.9 99.0 99.2
20
Learning When and Where to Zoom with Deep Reinforcement Learning
Burak Uzkent
Department of Computer Science
Stanford University
buzkent@cs.stanford.eduStefano Ermon
Department of Computer Science
Stanford University
ermon@cs.stanford.edu
Abstract
While high resolution images contain semantically more
useful information than their lower resolution counterparts,
processing them is computationally more expensive, and in
some applications, e.g. remote sensing, they can be much
more expensive to acquire. For these reasons, it is desirable
to develop an automatic method to selectively use high res-
olution data when necessary while maintaining accuracy
and reducing acquisition/run-time cost. In this direction,
we propose PatchDrop a reinforcement learning approach
to dynamically identify when and where to use/acquire high
resolution data conditioned on the paired, cheap, low res-
olution images. We conduct experiments on CIFAR10, CI-
FAR100, ImageNet and fMoW datasets where we use signif-
icantly less high resolution data while maintaining similar
accuracy to models which use full high resolution images.
1. Introduction
Deep Neural Networks now achieve state-of-the-art per-
formance in many computer vision tasks, including im-
age recognition [4], object detection [23], and object track-
ing [18, 46, 48]. However, one drawback is that they require
high quality input data to perform well, and their perfor-
mance drops signiﬁcantly on degraded inputs, e.g., lower
resolution images [59], lower frame rate videos [28], or un-
der distortions [41]. For example, [55] studied the effect of
image resolution, and reported a 14% performance drop on
CIFAR10 after downsampling images by a factor of 4.
Nevertheless, downsampling is often performed for com-
putational and statistical reasons [61]. Reducing the resolu-
tion of the inputs decreases the number of parameters, re-
sulting in reduced computational and memory cost and mit-
igating overﬁtting [2]. Therefore, downsampling is often
applied to trade off computational and memory gains with
accuracy loss [25]. However, the same downsampling level
is applied to allthe inputs. This strategy can be suboptimal
because the amount of information loss (e.g., about a label)depends on the input [7]. Therefore, it would be desirable
to build an adaptive system to utilize a minimal amount of
high resolution data while preserving accuracy.
In addition to computational and memory savings, an
adaptive framework can also beneﬁt application domains
where acquiring high resolution data is particularly expen-
sive. A prime example is remote sensing, where acquiring
a high resolution (HR) satellite image is signiﬁcantly more
expensive than acquiring its low resolution (LR) counter-
part [26, 32, 9]. For example, LR images with 10m-30m
spatial resolution captured by Sentinel-1 satellites [8, 36]
are publicly and freely available whereas an HR image with
0.3m spatial resolution captured by DigitalGlobe satellites
can cost in the order of 1,000 dollars [6]. This way, we can
reduce the cost of deep learning models trained on satellite
images for a variety of tasks, i.e., poverty prediction [37],
image recognition [49, 38], object tracking [45, 44, 47].
Similar examples arise in medical and scientiﬁc imaging,
where acquiring higher quality images can be more expen-
sive or even more harmful to patients [16, 15].
In all these settings, it would be desirable to be able to
adaptively acquire only speciﬁc parts of the HR quality in-
put. The challenge, however, is how to perform this selec-
tion automatically and efﬁciently, i.e., minimizing the num-
ber of acquired HR patches while retaining accuracy . As
expected, naive strategies can be highly suboptimal. For
example, randomly dropping patches of HR satellite images
from the functional Map of the World (fMoW) [3] dataset
will signiﬁcantly reduce accuracy of a trained network as
seen in Fig. 1a. As such, an adaptive strategy must learn
to identify and acquire useful patches [27] to preserve the
accuracy of the network.
To address this challenges, we propose PatchDrop , an
adaptive data sampling-acquisition scheme which only sam-
ples patches of the full HR image that are required for in-
ferring correct decisions, as shown in Fig. 1b. PatchDrop
uses LR versions of input images to train an agent in a
reinforcement learning setting to sample HR patches only
if necessary. This way, the agent learns when andwhere
to zoom in the parts of the image to sample HR patches.
1arXiv:2003.00425v2  [cs.CV]  20 Apr 2020
67.3% 66.1% 64.0% 60.8%
28.8% 59.5% 46.8% 39.1%(a)
ShipHorseT ruckClassiﬁer
Classiﬁer
ClassiﬁerAgent
Agent
AgentLow Resolution 
ImageSampled High 
Resolution Patches
x (b)
Figure 1: Left: shows the performance of the ResNet34 model trained on the fMoW original images and tested on images
with dropped patches. The accuracy of the model goes down with the increased number of dropped patches. Right: shows
the proposed framework which dynamically drops image patches conditioned on the low resolution images.
PatchDrop is extremely effective on the functional Map of
the World (fMoW) [3] dataset. Surprisingly, we show that
we can use only about 40% of full HR images without any
signiﬁcant loss of accuracy. Considering this number, we
can save in the order of 100,000 dollars when performing a
computer vision task using expensive HR satellite images
at global scale. We also show that PatchDrop performs
well on traditional computer vision benchmarks. On Ima-
geNet, it samples about 50% of HR images on average with
a minimal loss in the accuracy. On a different task, we then
increase the run-time performance of patch-based CNNs,
BagNets [1], by 2×by reducing the number of patches that
need to be processed using PatchDrop. Finally, leveraging
the learned patch sampling policies, we generate hard pos-
itive training examples to boost the accuracy of CNNs on
ImageNet and fMoW by 2-3 %.
2. Related Work
Dynamic Inference with Residual Networks Similarly
to DropOut [39], [13] proposed a stochastic layer dropping
method when training the Residual Networks [11]. The
probability of survival linearly decays in the deeper lay-
ers following the hypothesis that low-level features play key
roles in correct inference. Similarly, we can decay the like-
lihood of survival for a patch w.r.t its distance from image
center based on the assumption that objects will be dom-
inantly located in the central part of the image. Stochas-
tic layer dropping provides only training time compression.
On the other hand, [53, 57] proposes reinforcement learning
settings to drop the blocks of ResNet in both training and
test time conditionally on the input image. Similarly, by re-
placing layers with patches , we can drop more patches from
easy samples while keeping more from ambiguous ones.Attention Networks Attention methods have been ex-
plored to localize semantically important parts of im-
ages [52, 31, 51, 42]. [52] proposes a Residual Atten-
tion network that replaces the residual identity connections
from [11] with residual attention connections. By residu-
ally learning feature guiding, they can improve recognition
accuracy on different benchmarks. Similarly, [31] proposes
a differentiable saliency-based distortion layer to spatially
sample input data given a task. They use LR images in
the saliency network that generates a grid highlighting se-
mantically important parts of the image space. The grid is
then applied to HR images to magnify the important parts of
the image. [21] proposes a perspective-aware scene parsing
network that locates small and distant objects. With a two
branch (coarse and fovea) network, they produce coarse and
ﬁne level segmentations maps and fuse them to generate ﬁ-
nal map. [60] adaptively resizes the convolutional patches
to improve segmentation of large and small size objects.
[24] improves object detectors using pre-determined ﬁxed
anchors with adaptive ones. They divide a region into a
ﬁxed number of sub-regions recursively whenever the zoom
indicator given by the network is high. Finally, [30] pro-
poses a sequential region proposal network (RPN) to learn
object-centric and less scattered proposal boxes for the sec-
ond stage of the Faster R-CNN [33]. These methods are tai-
lored for certain tasks and condition the attention modules
on HR images. On the other hand, we present a general
framework and condition it on LR images.
Analyzing Degraded Quality Input Signal There has
been a relatively small volume of work on improving
CNNs’ performance using degraded quality input sig-
nal [20]. [40] uses knowledge distillation to train a student
network using degraded input signal and the predictions of
a teacher network trained on the paired higher quality sig-
nal. Another set of studies [55, 58] propose a novel method
to perform domain adaptation from the HR network to a
LR network. [29] pre-trains the LR network using the HR
data and ﬁnetunes it using the LR data. Other domain adap-
tation methods focus on person re-identiﬁcation with LR
images [14, 22, 54]. All these methods boost the accuracy
of the networks on LR input data, however, they make the
assumption that the quality of the input signal is ﬁxed.
3. Problem statement
Low 
Resolution
ImageAgent
Action
Sampled High
Resolution
ImageClassiﬁer
ActionSampling
RewardHR - 1 HR - 2 HR - P
Label
Optional
Figure 2: Our Bayesian Decision inﬂuence diagram. The
LR images are observed by the agent to sample HR patches.
The classiﬁer then observes the agent-sampled HR image
together with the LR image to perform prediction. The ul-
timate goal is to choose an action to sample a masked HR
image to maximize the expected utilities considering the ac-
curacy and the cost of using/sampling HR image.
We formulate the PatchDrop framework as a two step
episodic Markov Decision Process (MDP), as shown in the
inﬂuence diagram in Fig. 2. In the diagram, we represent
the random variables with a circle, actions with a square,
and utilities with a diamond. A high spatial resolution im-
age,xh, is formed by equal size patches with zero over-
lapxh= (x1
h,x2
h,···,xP
h), wherePrepresents the num-
ber of patches. In contrast with traditional computer vision
settings,xhis latent, i.e., it is not observed by the agent .
y∈{1,···,N}is a categorical random variable represent-
ing the (unobserved) label associated with xh, whereNis
the number of classes. The random variable low spatial res-
olution image ,xl, is the lower resolution version of xh.xl
is initially observed by the agent in order to choose the bi-
nary action array, a1∈{0,1}P, where ap
1= 1 means that
the agent would like to sample the p-th HR patch xp
h. We
deﬁne the patch sampling policy model parameterized by
θp, as
π1(a1|xl;θp) =p(a1|xl;θp), (1)
whereπ1(xl;θp)is a function mapping the observed LR
image to a probability distribution over the patch sampling
action a1. Next, the random variable masked HR image ,
xm
h, is formed using ap
1andxp
h, with the masking operation
formulated as xm
h=xh⨀a1. The ﬁrst step of the MDP
can be modeled with a joint probability distribution over therandom variables, xh,y,xm
h, andxl, and action a1, as
p(xh,xm
h,xl,y,a1) =p(xh)p(y|xh)p(xl|xh)
·p(a1|xl;θp)p(xm
h|a1,xh).(2)
In the second step of the MDP, the agent observes the
random variables, xm
handxl, and chooses an action a2∈
{1,···,N}. We then deﬁne the class prediction policy as
follows:
π2(a2|xm
h,xl;θcl) =p(a2|xm
h,xl;θcl), (3)
whereπ2represents a classiﬁer network parameterized by
θcl. The overall objective, J, is then deﬁned as maximizing
the expected utility, Rrepresented by
max
θp,θclJ(θp,θcl) =Ep[R(a1,a2,y)], (4)
where the utility depends on a1,a2, andy. The reward
penalizes the agent for selecting a large number of high-
resolution patches (e.g., based on the norm of a1) and in-
cludes a classiﬁcation loss evaluating the accuracy of a2
given the true label y(e.g., cross-entropy or 0-1 loss).
4. Proposed Solution
4.1. Modeling the Policy Network and Classiﬁer
In the previous section, we formulated the task of Patch-
Drop as a two step episodic MDP similarly to [50]. Here,
we detail the action space and how the policy distributions
fora1and a2are modelled. To represent our discrete ac-
tion space for a1, we divide the image space into equal size
patches with no overlaps, resulting in Ppatches, as shown
in Fig. 3. In this study, we use P= 16 regardless of the size
of the input image and leave the task of choosing variable
size bounding boxes as a future work. In the ﬁrst step of the
two step MDP, the policy network, fp,outputs the probabil-
ities for all the actions at once after observing xl. An alter-
native approach could be in the form of a Bayesian frame-
work where ap
1is conditioned on a1:p−1
1 [7, 30]. However,
the proposed concept of outputting all the actions at once
provides a more efﬁcient decision making process for patch
sampling.
In this study, we model the action likelihood function of
the policy network, fp, by multiplying the probabilities of
the individual high-resolution patch selections, represented
by patch-speciﬁc Bernoulli distributions as follows:
π1(a1|xl,θp) =P∏
p=1sap
1p(1−sp)(1−ap
1), (5)
wheresprepresents the prediction vector formulated as
sp=fp(xl;θp). (6)
150
1
2
3
RewardLow Resolution 
Image
High Resolution 
Sampled Patchesdrop
drop
drop
drop
sample
sample4
5
6
dropdropcat
horse
carship
04812
1 13
261014
37111595Policy NetworkHR Classiﬁer
LR Classiﬁer2
OptionalFigure 3: The workﬂow of the PatchDrop formulated as a two step episodic MDP. The agent chooses actions conditioned
on the LR image, and only agent sampled HR patches together with LR images are jointly used by the two-stream classiﬁer
network. We note that the LR network can be disconnected from the pipeline to only rely on selected HR patches to perform
classiﬁcation. When disconnecting LR network, the policy network samples more patches to maintain accuracy.
To get probabilistic values, sp∈[0,1], we use a sigmoid
function on the ﬁnal layer of the policy network.
The next set of actions, a2, is chosen by the classiﬁer,
fcl, using the sampled HR image xm
hand the LR input xl.
The upper stream of the classiﬁer, fcl, uses the sampled HR
images,xm
h, whereas the bottom stream uses the LR im-
ages,xl, as shown in Fig. 3. Each one outputs probability
distributions, sl
clandshm
cl, for class labels using a softmax
layer. We then compute the weighted sum of predictions via
scl= (S/P)shm
cl+ (1−S/P)sl
cl, (7)
whereSrepresents the number of sampled patches. To form
a2, we use the maximally probable class label: i.e.,aj
2= 1
ifsj
cl= max(scl)and aj
2= 0otherwise where jrepresents
the class index. In this set up, if the policy network samples
no HR patch , we completely rely on the LR classiﬁer, and
the impact of the HR classiﬁer increases linearly with the
number of sampled patches.
4.2. Training the PatchDrop Network
After deﬁning the two step MDP and modeling the pol-
icy and classiﬁer networks, we detail the training procedure
ofPatchDrop . The goal of training is to learn the optimal
parameters of θpandθcl. Because the actions are discrete,
we cannot use the reparameterization trick to optimize the
objective w.r.t. θp. To optimize the parameters θpoffp, we
need to use model-free reinforcement learning algorithms
such as Q-learning [56] and policy gradient [43]. Policy
gradient is more suitable in our scenario since the number
of unique actions the policy network can choose is 2Pand
increases exponentially with P. For this reason, we use
theREINFORCE method [43] to optimize the objective
w.r.tθpusing
∇θpJ=E[R(a1,a2,y)∇θplogπθp(a1|xl)]. (8)
Averaging across a mini-batch via Monte-Carlo sampling
produces an unbiased estimate of the expected value, but
with potentially large variance. Since this can lead to anunstable training process [43], we replace R(a1,a2,y)in
Eq. 8 with the advantage function to reduce the variance:
∇θpJ=E[AP∑
p=1∇θplog(spap
1+ (1−sp)(1−ap
1))],(9)
A(a1,ˆ a1,a2,ˆ a2) =R(a1,a2,y)−R(ˆ a1,ˆ a2,y),(10)
where ˆ a1and ˆ a2represent the baseline action vectors. To
getˆ a1, we use the most likely action vector proposed by the
policy network: i.e.,ai
1= 1 ifsi
p>0.5andsi
p= 0 other-
wise. The classiﬁer, fcl, then observes xlandˆxm
hsampled
using ˆ a1, on two branches and outputs the predictions, ˆscl,
from which we get ˆ aj
2:i.e.,ˆ aj
2= 1 ifˆsj
cl= max(ˆscl)and
ˆ aj
2= 0otherwise where jrepresent the class index. The ad-
vantage function assigns the policy network a positive value
only when the action vector sampled from Eq. 5 produces
higher reward than the action vector with maximum likeli-
hood, which is known as a self-critical baseline [34].
Finally, in this study we use the temperature scaling
method [43] to encourage exploration during training time
by bounding the probabilities of the policy network as
sp=αsp+ (1−α)(1−sp), (11)
whereα∈[0,1].
Pre-training the Classiﬁer After formulating our rein-
forcement learning setting for training the policy network,
we ﬁrst pre-train the two branches of fcl,fh
clandfl
cl, on
xh∈Xhandxl∈Xl. We assume that Xhis observable
in the training time. The network trained on Xhcan per-
form reasonably (Fig. 1a) when the patches are dropped at
test time with a ﬁxed policy , formingxm
h. We then use this
observation to pre-train the policy network, fp, to dynami-
cally learn to drop patches while keeping the parameters of
fh
clandfl
clﬁxed.
Pre-training the Policy Network (Pt) After training the
two streams of the classiﬁer, fcl, we pre-train the policy net-
work,fp, using the proposed reinforcement learning setting
while ﬁxing the parameters of fcl. In this step, we only use
fh
clto estimate the expected reward when learning θp. This
is because we want to train the policy network to understand
which patches contribute most to correct decisions made by
the HR image classiﬁer, as shown in Fig. 1a.
Input: Input(Xl,Y,C)Xl={x1
l,x2
l,...,xN
l}
fork←0toK 1do
sp←fp(xl;θp)
sp←α+ (1−sp)(1−α)
a1∼π1(a1|sp)
xm
h=xh⨀a1
a2←fh
cl(xm
h;θh
cl)
Evaluate Reward R(a1,a2,y)
θp←θp+∇θp
end
fork←0toK 2do
Jointly Finetune θh
clandθpusingfh
cl
end
fork←0toK 3do
Jointly Finetune θh
clandθpusingfh
clandfl
cl
end
Algorithm 1: PatchDrop Pseudocode
Finetuning the Agent and HR Classiﬁer (Ft-1) To fur-
ther boost the accuracy of the policy network, fp, we jointly
ﬁnetune the policy network and HR classiﬁer, fh
cl. This
way, the HR classiﬁer can adapt to the sampled images, xm
h,
while the policy network learns new policies in line with it.
The LR classiﬁer, fl
cl, is not included in this step.
Finetuning the Agent and HR Classiﬁer (Ft-2) In the
ﬁnal step of the training stage, we jointly ﬁnetune the policy
network,fp, andfh
clwith the addition offl
clinto the classi-
ﬁerfcl. This way, the policy network can learn policies to
drop further patches with the existence of the LR classiﬁer.
We combine the HR and LR classiﬁers using Eq. 7. Since
the input tofl
cldoes not change, we keep θl
clﬁxed and only
updateθh
clandθp. The algorithm for the PatchDrop training
stage is shown in Alg. 1. Upon publication, we will release
the code to train and test PatchDrop.
5. Experiments
5.1. Experimental Setup
Datasets and Metrics We evaluate PatchDrop on the
following datasets: (1) CIFAR10, (2) CIFAR100, (3) Im-
ageNet [4] and (4) functional map of the world (fMoW) [3].
To measure its performance, we use image recognition ac-
curacy and the number of dropped patches (cost).
Implementation Details In CIFAR10/CIFAR100 exper-
iments, we use a ResNet8 for the policy and ResNet32 for
the classiﬁer networks. The policy and classiﬁer networks
use 8×8px and 32×32px images. In ImageNet/fMoW, weuse a ResNet10 for the policy network and ResNet50 for
the classiﬁer. The policy network uses 56 ×56px images
whereas the classiﬁer uses 224 ×224px images. We initial-
ize the weights of the LR classiﬁer with HR classiﬁer [29]
and use Adam optimizer in all our experiments [17]. Fi-
nally, initially we set the exploration/exploitation param-
eter,α, to 0.7 and increase it to 0.95 linearly over time.
Our PatchDrop implementation can be found in our GitHub
repository1.
Reward Function We chooseR= 1−(|a1|1
P)2
if
y= ˆy(a2)and -σotherwise as a reward. Here, ˆyandyrep-
resent the predicted class by the classiﬁer after the observa-
tion ofxm
handxland the true class, respectively. The pro-
posed reward function quadratically increases the reward
w.r.t the number of dropped patches. To adjust the trade-
off between accuracy and the number of sampled patches,
we introduce σand setting it to a large value encourages the
agent to sample more patches to preserve accuracy.
5.2. Baseline and State-of-The-Art Models
No Patch Sampling/No Patch Dropping In this case,
we simply train a CNN on LR or HR images with cross-
entropy loss without any domain adaptation and test it on
LR or HR images. We call them LR-CNN and HR-CNN.
Fixed and Stochastic Patch Dropping We propose two
baselines that sample central patches along the horizontal
and vertical axes of the image space and call them Fixed-H
and Fixed-V . We list the sampling priorities for the patches
in this order 5,6,9,10,13,14,1,2,0,3,4,7,8,11,15 for Fixed-H ,
and 4,5,6,7,8,9,10,11,12,13,14,15,0,1,2,3 for Fixed-V . The
patch IDs are shown in Fig. 3. Using a similar hypothesis,
we then design a stochastic method that decays the survival
likelihood of a patch w.r.t the euclidean distance from the
center of the patch pto the image center.
Super-resolution We use SRGAN [19] to learn to up-
sample LR images and use the SR images in the down-
stream tasks. This method only improves accuracy and in-
creases computational complexity since SR images have the
same number of pixels with HR images.
Attention-based Patch Dropping In terms of the state-
of-the art models, we ﬁrst compare our method to the Spa-
tial Transformer Network (STN) by [31]. We treat their
saliency network as the policy network and sample the top
Sactivated patches to form masked images for classiﬁer.
Domain Adaptation Finally, we use two of the state-of-
the-art domain adaptation methods by [55, 40] to improve
recognition accuracy on LR images. These methods are
based on Partially Coupled Networks (PCN), and Knowl-
edge Distillation (KD) [12].
The LR-CNN, HR-CNN, PCN, KD, and SRGAN are
standalone models and always use full LR or HR image.
1https://github.com/ermongroup/PatchDrop
Figure 4: Policies learned on the fMoW dataset. In columns 5 and 8, Ft-2 model does not sample any HR patches and the LR
classiﬁer is used. Ft-1 model samples more patches as it does not utilize LR classiﬁer.
For this reason, we have same values for them in Pt, Ft-1,
and Ft-2 steps and show them in the upper part of the tables.
5.3. Experiments on fMoW
One application domain of the PatchDrop is remote sens-
ing where LR images are signiﬁcantly cheaper than HR im-
ages. In this direction, we test the PatchDrop on functional
Map of the World [3] consisting of HR satellite images. We
use 350,000, 50,000 and 50,000 images as training, valida-
tion and test images. After training the classiﬁers, we pre-
train the policy network for 63 epochs with a learning rate
of 1e-4 and batch size of 1024. Next, we ﬁnetune (Ft-1 and
Ft-2) the policy network and HR classiﬁers with the learn-
ing rate of 1e-4 and batch size of 128. Finally, we set σto
0.5, 20, and 20 in the pre-training, and ﬁne-tuning steps.
As seen in Table 1, PatchDrop samples only about 40%
of each HR image on average while increasing the accuracy
of the network using the full HR images to 68.3%. Fig. 4
shows some examples of how the policy network chooses
actions conditioned on the LR images. When the image
contains a ﬁeld with uniform texture, the agent samples a
Acc. (%)
(Pt)SAcc. (%)
(Ft-1)SAcc. (%)
(Ft-2)S
LR-CNN 61.4 0 61.4 0 61.4 0
SRGAN [19] 62.3 0 62.3 0 62.3 0
KD [40] 63.1 0 63.1 0 63.1 0
PCN [55] 63.5 0 63.5 0 63.5 0
HR-CNN 67.3 16 67.3 16 67.3 16
Fixed-H 47.7 7 63.3 6 64.9 6
Fixed-V 48.3 7 63.2 6 64.7 6
Stochastic 29.1 7 57.1 6 63.6 6
STN [31] 46.5 7 61.8 6 64.8 6
PatchDrop 53.4 7 67.1 5.9 68.3 5.2
Table 1: The performance of the proposed PatchDrop and
baseline models on the fMoW dataset. Srepresents the av-
erage number of sampled patches. Ft-1 and Ft-2 represent
the ﬁnetuning steps with single and two stream classiﬁers.small number of patches, as seen in columns 5, 8, 9 and 10.
On the other hand, it samples patches from the buildings
when the ground truth class represents a building, as seen in
columns 1, 6, 12, and 13.
Also, we perform experiments with different downsam-
pling ratios and σvalues in the reward function. This way,
we can observe the trade-off between the number of sam-
pled patches and accuracy. As seen in Fig. 5, as we in-
crease the downsampling ratio we zoom into more patches
to maintain accuracy. On the other hand, with increasing σ,
we zoom into more patches as larger σvalue penalizes the
policies resulting in unsuccessful classiﬁcation.
Experiments on CIFAR10/CIFAR100 Although CI-
FAR datasets already consists of LR images, we believe that
conducting experiments on standard benchmarks is useful
to characterize the model. For CIFAR10, after training the
classiﬁers, we pre-train the policy network with a batch size
of 1024 and learning rate of 1e-4 for 3400 epochs. In the
joint ﬁnetuning stages, we keep the learning rate, reduce the
batch size to 256, and train the policy and HR classiﬁer net-
works for 1680 and 990 epochs, respectively. σis set to -0.5
in the pre-training stage and -5 in the joint ﬁnetuning stages
whereasαis tuned to 0.8. Our CIFAR100 methods are sim-
2.0 4.0 6.07.2
Number of Sampled Patches68.1068.1568.2068.2568.3068.3568.40Accuracy (%)ds=2
ds=3
ds=4
ds=5
2.03.04.05.06.07.07.5
Number of Sampled Patches64.565.066.067.068.069.0Accuracy (%)
σ=2σ=5σ=8σ=11σ=14σ=17σ=20σ=23σ=26
Figure 5: Left: The accuracy and number of sampled
patches by the policy network w.r.t downsampling ratio
used to get LR images for the policy network and classiﬁer.
Right: The accuracy and number of sampled patches w.r.t
toσparameter in the reward function in the joint ﬁnetuning
steps (ds=4). It is set to 0.5 in the pre-training step.
LRHRFt-1Ft-2
Figure 6: Policies learned on ImageNet. In columns 3 and 8, Ft-2 model does not sample any HR patches and the LR
classiﬁer is used. Ft-1 model samples more patches as it does not use the LR classiﬁer.
CIFAR10 CIFAR100 ImageNet
Acc. (%)
(Pt)Acc. (%)
(Ft-1)Acc. (%)
(Ft-2)S
(Pt,Ft-1,Ft-2)Acc. (%)
(Pt)Acc. (%)
(Ft-1)Acc. (%)
(Ft-2)S
(Pt,Ft-1,Ft-2)Acc. (%)
(Pt)Acc. (%)
(Ft-1)Acc. (%)
(Ft-2)S
(Pt,Ft-1,Ft-2)
LR-CNN 75.8 75.8 75.8 0,0,0 55.1 55.1 55.1 0,0,0 58.1 58.1 58.1 0,0,0
SRGAN [19] 78.8 78.8 78.8 0,0,0 56.1 56.1 56.1 0,0,0 63.1 63.1 63.1 0,0,0
KD [40] 81.8 81.8 81.8 0,0,0 61.1 61.1 61.1 0,0,0 62.4 62.4 62.4 0,0,0
PCN [40] 83.3 83.3 83.3 0,0,0 62.6 62.6 62.6 0,0,0 63.9 63.9 63.9 0,0,0
HR-CNN 92.3 92.3 92.3 16,16,16 69.3 69.3 69.3 16,16,16 76.5 76.5 76.5 16,16,16
Fixed-H 71.2 83.8 85.2 9,8,7 48.5 65.8 67.0 9,10,10 48.8 68.6 70.4 10,9,8
Fixed-V 64.7 83.4 85.1 9,8,7 46.2 65.5 67.2 9,10,10 48.4 68.4 70.8 10,9,8
Stochastic 40.6 82.1 83.7 9,8,7 27.6 63.2 64.8 9,10,10 38.6 66.2 68.4 10,9,8
STN [31] 66.9 85.2 87.1 9,8,7 41.1 64.3 66.4 9,10,10 58.6 69.4 71.4 10,9,8
PatchDrop 80.6 91.9 91.5 8.5,7.9,6.9 57.3 69.3 70.4 9,9.9,9.1 60.2 74.9 76.0 10.1 ,9.1,7.9
Table 2: The results on CIFAR10, CIFAR100 and ImageNet datasets. Srepresents the average number of sampled patches
per image. The Pt, Ft-1 and Ft-2 represent the pre-training and ﬁnetuning steps with single and two stream classiﬁers.
ilar to the CIFAR10 ones, including hyper-parameters.
As seen in Table 2, PatchDrop drops about 56% of the
patches in the original image space in CIFAR10, all the
while with minimal loss in the overall accuracy. In the case
of CIFAR100, we observe that it samples 2.2 patches more
than the CIFAR10 experiment, on average, which might be
due to higher complexity of the CIFAR100 dataset.
Experiments on ImageNet Next, we test the Patch-
Drop on ImageNet Large Scale Visual Recognition Chal-
lenge 2012 (ILSVRC2012) [35]. It contains 1.2 million,
50,000, and 150,000 training, validation and test images.
For augmentation, we use randomly cropping 224 ×224px
area from the 256×256px images and perform horizontal
ﬂip augmentation. After training the classiﬁers, we pre-
train the policy network for 95 epochs with a learning rate
of 1e-4 and batch size of 1024. We then perform the ﬁrst
ﬁne-tuning stage and jointly ﬁnetune the HR classiﬁer and
policy network for 51 epochs with the learning rate of 1e-4
and batch size of 128. Finally, we add the LR classiﬁer and
jointly ﬁnetune the policy network and HR classiﬁer for 10
epochs with the same learning rate and batch size. We set σ
to 0.1, 10, and 10 for pre-training and ﬁne-tuning steps.
As seen in Table 2, we can maintain the accuracy of theHR classiﬁer while dropping 56% and50% of the patches
with the Ft-1 and Ft-2 model. Also, we show the learned
policies on ImageNet in Fig. 6. The policy network decides
to sample no patch when the input is relatively easier as in
column 3, and 8.
Analyzing Policy Network’s Actions To better under-
stand the sampling actions of policy network, we visualize
the accuracy of the classiﬁer w.r.t the number of sampled
patches as shown in Fig. 7 (left). Interestingly, the accuracy
of the classiﬁer is inversely proportional to the number of
sampled patches. We believe that this occurs because the
policy network samples more patches from the challenging
and ambiguous cases to ensure that the classiﬁer success-
fully predicts the label. On the other hand, it successfully
learns when to sample no patches . However, it samples no
patch (S=0)7%of the time on average in comparison to
sampling 4≤S≤750% of the time. Increasing the ratio for
S=0 is a future work of this study. Finally, Fig. 7 (right) dis-
plays the probability of sampling a patch given its position.
We see that the policy network learns to sample the central
patches more than the peripheral patches as expected.
051015
Number of Sampled Patches (<)0.650.700.750.800.850.900.951.00Accuracy (%) CIFAR10
CIFAR100
ImageNet
fMoW
0 5 10 15
Patch ID0.000.020.040.060.080.100.120.14Sampling ProbabilityCIFAR10
CIFAR100
ImageNet
fMoWFigure 7: Left: The accuracy w.r.t the average number of
sampled patches by the policy network. Right: Sampling
probability of the patch IDs (See Fig. 3 for IDs).
6. Improving Run-time Complexity of BagNets
150123Low Resolution Imagedropdropdropdropsamplesample456dropdropcathorsecarshipPolicy Network
CNNCNNCNNCNN562
10--- Shared Weights ---High Resolution Sampled Patches9
0481215132610143711159
Figure 8: Dynamic BagNet. The policy network processes
LR image and sample HR patches to be processed indepen-
dently by CNN. More details on BagNet can be found in [1].
Previously, we tested PatchDrop on fMoW satellite im-
age recognition task to reduce the ﬁnancial cost of ana-
lyzing satellite images by reducing dependency on HR im-
ages while preserving accuracy. Next, we propose the use
of PatchDrop to decrease the run-time complexity of local
CNNs, such as BagNets. They have recently been proposed
as a novel image recognition architecture [1]. They run a
CNN on image patches independently and sum up class-
speciﬁc spatial probabilities. Surprisingly, the BagNets per-
form similarly to CNNs that process the full image in one
shot. This concept ﬁts perfectly to PatchDrop as it learns
to select semantically useful local patches which can be fed
to a BagNet. This way, the BagNet is not trained on all
Acc. (%)
(Pt)SAcc. (%)
(Ft-1)SRun-time. (%)
(ms)
BagNet (No Patch Drop) [1] 85.6 16 85.6 16 192
CNN (No Patch Drop) 92.3 16 92.3 16 77
Fixed-H 67.7 10 86.3 9 98
Fixed-V 68.3 10 86.2 9 98
Stochastic 49.1 10 83.1 9 98
STN [19] 67.5 10 86.8 9 112
BagNet (PatchDrop) 77.4 9.5 92.7 8.5 98
Table 3: The performance of the PatchDrop and other mod-
els on improving BagNet on CIFAR10 dataset. We use a
similar set up to our previous CIFAR10 experiments.the patches from the image but only on useful patches . By
dropping redundant patches, we can then speed it up and
improve its accuracy. In this case, we ﬁrst train the Bag-
Net on all the patches and pre-train the policy network on
LR images (4×) to learn patches important for BagNet. Us-
ing LR images and a shallow network (ResNet8), we reduce
the run-time overhead introduced by the agent to 3%of the
CNN (ResNet32) using HR images. Finally, we jointly ﬁne-
tune (Ft-1) the policy network and BagNet. We illustrate the
proposed conditional BagNet in Fig. 8.
We perform experiments on CIFAR10 and show the re-
sults in Table 3. The proposed Conditional BagNet using
PatchDrop improves the accuracy of BagNet by 7%clos-
ing the gap between global CNNs and local CNNs. Addi-
tionally, it decreases the run-time complexity by 50%, sig-
niﬁcantly reducing the gap between local CNNs and global
CNNs in terms of run-time complexity2. The increase in the
speed can be further improved by running different GPUs
on the selected patches in parallel at test time.
Finally, utilizing learned masks to avoid convolutional
operations in the layers of global CNN is another promis-
ing direction of our work. [10] drops spatial blocks of the
feature maps of CNNs in training time to perform stronger
regularization than DropOut [39]. Our method, on the other
hand, can drop blocks of the feature maps dynamically in
both training and test time.
CIFAR10 (%)
(ResNet32)CIFAR100 (%)
(ResNet32)ImageNet (%)
(ResNet50)fMoW (%)
(ResNet34)
No Augment. 92.3 69.3 76.5 67.3
CutOut [5] 93.5 70.4 76.5 67.6
PatchDrop 93.9 71.0 78.1 69.6
Table 4: Results with different augmentation methods.
7. Conditional Hard Positive Sampling
PatchDrop can also be used to generate hard positives for
data augmentation. In this direction, we utilize the masked
images,Xm
h, learned by the policy network (Ft-1) to gen-
erate hard positive examples to better train classiﬁers. To
generate conditional hard positive examples, we choose the
number of patches to be masked, M, from a uniform dis-
tribution with minimum and maximum values of 1 and 4.
Next, givenspby the policy network, we choose Mpatches
with the highest probabilities and mask them and use the
masked images to train the classiﬁer. Finally, we compare
our approach to CutOut [5] which randomly cuts/masks im-
age patches for data augmentation. As shown in Table 4, our
approach leads to higher accuracy in all the datasets when
using original images, Xh, in test time. This shows that the
policy network learns to select informative patches.
2The run-times are measured on Intel i7-7700K CPU@4.20GHz
8. Conclusion
In this study, we proposed a novel reinforcement learn-
ing setting to train a policy network to learn when andwhere
to sample high resolution patches conditionally on the low
resolution images. Our method can be highly beneﬁcial in
domains such as remote sensing where high quality data is
signiﬁcantly more expensive than the low resolution coun-
terpart. In our experiments, on average, we drop a 40-60 %
portion of each high resolution image while preserving sim-
ilar accuracy to networks which use full high resolution
images in ImageNet and fMoW. Also, our method signif-
icantly improves the run-time efﬁciency and accuracy of
BagNet, a patch-based CNNs. Finally, we used the learned
policies to generate hard positives to boost classiﬁers’ accu-
racy on CIFAR, ImageNet and fMoW datasets.
Acknowledgements
This research was supported by Stanfords Data for De-
velopment Initiative and NSF grants 1651565 and 1733686.
References
[1] Wieland Brendel and Matthias Bethge. Approximating cnns
with bag-of-local-features models works surprisingly well on
imagenet. arXiv preprint arXiv:1904.00760 , 2019. 2, 8
[2] Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A
downsampled variant of imagenet as an alternative to the ci-
far datasets. arXiv preprint arXiv:1707.08819 , 2017. 1
[3] Gordon Christie, Neil Fendley, James Wilson, and Ryan
Mukherjee. Functional map of the world. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 6172–6180, 2018. 1, 2, 5, 6
[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 1, 5
[5] Terrance DeVries and Graham W Taylor. Improved regular-
ization of convolutional neural networks with cutout. arXiv
preprint arXiv:1708.04552 , 2017. 8
[6] Jonathan RB Fisher, Eileen A Acosta, P James Dennedy-
Frank, Timm Kroeger, and Timothy M Boucher. Impact
of satellite imagery spatial resolution on land use classiﬁ-
cation accuracy and modeled water quality. Remote Sensing
in Ecology and Conservation , 4(2):137–149, 2018. 1
[7] Mingfei Gao, Ruichi Yu, Ang Li, Vlad I Morariu, and
Larry S Davis. Dynamic zoom-in network for fast object
detection in large images. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
6926–6935, 2018. 1, 3
[8] Dirk Geudtner, Ram ´on Torres, Paul Snoeij, Malcolm David-
son, and Bj ¨orn Rommen. Sentinel-1 system capabilities and
applications. In 2014 IEEE Geoscience and Remote Sensing
Symposium , pages 1457–1460. IEEE, 2014. 1
[9] Pedram Ghamisi and Naoto Yokoya. Img2dsm: Height sim-
ulation from single imagery using conditional generative ad-versarial net. IEEE Geoscience and Remote Sensing Letters ,
15(5):794–798, 2018. 1
[10] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock:
A regularization method for convolutional networks. In
Advances in Neural Information Processing Systems , pages
10727–10737, 2018. 8
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 2
[12] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 5
[13] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-
ian Q Weinberger. Deep networks with stochastic depth. In
European conference on computer vision , pages 646–661.
Springer, 2016. 2
[14] Jiening Jiao, Wei-Shi Zheng, Ancong Wu, Xiatian Zhu,
and Shaogang Gong. Deep low-resolution person re-
identiﬁcation. In Thirty-Second AAAI Conference on Arti-
ﬁcial Intelligence , 2018. 3
[15] Eunhee Kang, Junhong Min, and Jong Chul Ye. A deep con-
volutional neural network using directional wavelets for low-
dose x-ray ct reconstruction. Medical physics , 44(10):e360–
e375, 2017. 1
[16] Justin Ker, Lipo Wang, Jai Rao, and Tchoyoson Lim. Deep
learning applications in medical image analysis. Ieee Access ,
6:9375–9389, 2017. 1
[17] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[18] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Fels-
berg, Roman Pﬂugfelder, Luka Cehovin Zajc, Tomas V ojir,
Goutam Bhat, Alan Lukezic, Abdelrahman Eldesokey, et al.
The sixth visual object tracking vot2018 challenge results.
InProceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 0–0, 2018. 1
[19] Christian Ledig, Lucas Theis, Ferenc Husz ´ar, Jose Caballero,
Andrew Cunningham, Alejandro Acosta, Andrew Aitken,
Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-
realistic single image super-resolution using a generative ad-
versarial network. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 4681–4690,
2017. 5, 6, 7, 8
[20] Pei Li, Loreto Prieto, Domingo Mery, and Patrick J Flynn.
On low-resolution face recognition in the wild: Compar-
isons and new techniques. IEEE Transactions on Informa-
tion Forensics and Security , 14(8):2000–2012, 2019. 2
[21] Xin Li, Zequn Jie, Wei Wang, Changsong Liu, Jimei Yang,
Xiaohui Shen, Zhe Lin, Qiang Chen, Shuicheng Yan, and
Jiashi Feng. Foveanet: Perspective-aware urban scene pars-
ing. In Proceedings of the IEEE International Conference on
Computer Vision , pages 784–792, 2017. 2
[22] Xiang Li, Wei-Shi Zheng, Xiaojuan Wang, Tao Xiang, and
Shaogang Gong. Multi-scale learning for low-resolution per-
son re-identiﬁcation. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision , pages 3765–3773,
2015. 3
[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision , pages 740–755.
Springer, 2014. 1
[24] Yongxi Lu, Tara Javidi, and Svetlana Lazebnik. Adaptive ob-
ject detection using adjacency and zoom prediction. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 2351–2359, 2016. 2
[25] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
Shufﬂenet v2: Practical guidelines for efﬁcient cnn architec-
ture design. In Proceedings of the European Conference on
Computer Vision (ECCV) , pages 116–131, 2018. 1
[26] K Malarvizhi, S Vasantha Kumar, and P Porchelvan. Use of
high resolution google earth satellite imagery in landuse map
preparation for urban related applications. Procedia Technol-
ogy, 24:1835–1842, 2016. 1
[27] Silviu Minut and Sridhar Mahadevan. A reinforcement learn-
ing model of selective visual attention. In Proceedings of the
ﬁfth international conference on Autonomous agents , pages
457–464. ACM, 2001. 1
[28] Matthias Mueller, Neil Smith, and Bernard Ghanem.
Context-aware correlation ﬁlter tracking. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 1396–1404, 2017. 1
[29] Xingchao Peng, Judy Hoffman, X Yu Stella, and Kate
Saenko. Fine-to-coarse knowledge transfer for low-res im-
age classiﬁcation. In 2016 IEEE International Conference
on Image Processing (ICIP) , pages 3683–3687. IEEE, 2016.
3, 5
[30] Aleksis Pirinen and Cristian Sminchisescu. Deep reinforce-
ment learning of region proposal networks for object detec-
tion. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 6945–6954, 2018. 2,
3
[31] Adria Recasens, Petr Kellnhofer, Simon Stent, Wojciech Ma-
tusik, and Antonio Torralba. Learning to zoom: a saliency-
based sampling layer for neural networks. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
pages 51–66, 2018. 2, 5, 6, 7
[32] Felix Rembold, Clement Atzberger, Igor Savin, and Os-
car Rojas. Using low resolution satellite imagery for yield
prediction and yield anomaly detection. Remote Sensing ,
5(4):1704–1733, 2013. 1
[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
Faster r-cnn: Towards real-time object detection with region
proposal networks. In Advances in neural information pro-
cessing systems , pages 91–99, 2015. 2
[34] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret
Ross, and Vaibhava Goel. Self-critical sequence training for
image captioning. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 7008–
7024, 2017. 4
[35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpa-
thy, Aditya Khosla, Michael Bernstein, Alexander C. Berg,
and Li Fei-Fei. ImageNet Large Scale Visual Recogni-tion Challenge. International Journal of Computer Vision ,
115(3):211–252, Dec. 2015. 7
[36] Vishnu Sarukkai, Anirudh Jain, Burak Uzkent, and Stefano
Ermon. Cloud removal from satellite images using spa-
tiotemporal generator networks. In The IEEE Winter Confer-
ence on Applications of Computer Vision , pages 1796–1805,
2020. 1
[37] Evan Sheehan, Chenlin Meng, Matthew Tan, Burak Uzkent,
Neal Jean, Marshall Burke, David Lobell, and Stefano Er-
mon. Predicting economic development using geolocated
wikipedia articles. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data
Mining , pages 2698–2706. ACM, 2019. 1
[38] Evan Sheehan, Burak Uzkent, Chenlin Meng, Zhongyi Tang,
Marshall Burke, David Lobell, and Stefano Ermon. Learning
to interpret satellite images using wikipedia. arXiv preprint
arXiv:1809.10236 , 2018. 1
[39] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way
to prevent neural networks from overﬁtting. The Journal of
Machine Learning Research , 15(1):1929–1958, 2014. 2, 8
[40] Jong-Chyi Su and Subhransu Maji. Adapting models
to signal degradation using distillation. arXiv preprint
arXiv:1604.00433 , 2016. 2, 5, 6, 7
[41] Jian Sun, Wenfei Cao, Zongben Xu, and Jean Ponce. Learn-
ing a convolutional neural network for non-uniform motion
blur removal. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 769–777,
2015. 1
[42] Ming Sun, Yuchen Yuan, Feng Zhou, and Errui Ding. Multi-
attention multi-class constraint for ﬁne-grained image recog-
nition. In Proceedings of the European Conference on Com-
puter Vision (ECCV) , pages 805–821, 2018. 2
[43] Richard S Sutton and Andrew G Barto. Reinforcement learn-
ing: An introduction . MIT press, 2018. 4
[44] Burak Uzkent, Matthew J Hoffman, and Anthony V odacek.
Integrating hyperspectral likelihoods in a multidimensional
assignment algorithm for aerial vehicle tracking. IEEE Jour-
nal of Selected Topics in Applied Earth Observations and
Remote Sensing , 9(9):4325–4333, 2016. 1
[45] Burak Uzkent, Matthew J Hoffman, and Anthony V odacek.
Real-time vehicle tracking in aerial video using hyperspec-
tral features. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops , pages
36–44, 2016. 1
[46] Burak Uzkent, Matthew J Hoffman, Anthony V odacek, and
Bin Chen. Feature matching with an adaptive optical sensor
in a ground target tracking system. IEEE Sensors Journal ,
15(1):510–519, 2014. 1
[47] Burak Uzkent, Aneesh Rangnekar, and Matthew Hoffman.
Aerial vehicle tracking by adaptive fusion of hyperspectral
likelihood maps. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops , pages
39–48, 2017. 1
[48] Burak Uzkent, Aneesh Rangnekar, and Matthew J Hoffman.
Tracking in aerial hyperspectral videos using deep kernelized
correlation ﬁlters. IEEE Transactions on Geoscience and
Remote Sensing , 57(1):449–461, 2018. 1
[49] Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang,
Marshall Burke, David Lobell, and Stefano Ermon. Learning
to interpret satellite images in global scale using wikipedia.
arXiv preprint arXiv:1905.02506 , 2019. 1
[50] Burak Uzkent, Christopher Yeh, and Stefano Ermon. Efﬁ-
cient object detection in large images using deep reinforce-
ment learning. In The IEEE Winter Conference on Applica-
tions of Computer Vision , pages 1824–1833, 2020. 3
[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural
information processing systems , pages 5998–6008, 2017. 2
[52] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng
Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.
Residual attention network for image classiﬁcation. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 3156–3164, 2017. 2
[53] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and
Joseph E Gonzalez. Skipnet: Learning dynamic routing in
convolutional networks. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 409–424,
2018. 2
[54] Yan Wang, Lequn Wang, Yurong You, Xu Zou, Vincent
Chen, Serena Li, Gao Huang, Bharath Hariharan, and Kil-
ian Q Weinberger. Resource aware person re-identiﬁcation
across multiple resolutions. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
8042–8051, 2018. 3
[55] Zhangyang Wang, Shiyu Chang, Yingzhen Yang, Ding Liu,
and Thomas S Huang. Studying very low resolution recogni-
tion using deep networks. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
4792–4800, 2016. 1, 3, 5, 6
[56] Christopher J. C. H. Watkins and Peter Dayan. Q-learning.
InMachine Learning , pages 279–292, 1992. 4
[57] Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven
Rennie, Larry S Davis, Kristen Grauman, and Rogerio Feris.
Blockdrop: Dynamic inference paths in residual networks.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 8817–8826, 2018. 2
[58] Yuan Yao, Xutao Li, Yunming Ye, Feng Liu, Michael K
Ng, Zhichao Huang, and Yu Zhang. Low-resolution im-
age categorization via heterogeneous domain adaptation.
Knowledge-Based Systems , 163:656–665, 2019. 3
[59] Linwei Yue, Huanfeng Shen, Jie Li, Qiangqiang Yuan,
Hongyan Zhang, and Liangpei Zhang. Image super-
resolution: The techniques, applications, and future. Signal
Processing , 128:389–408, 2016. 1
[60] Rui Zhang, Sheng Tang, Yongdong Zhang, Jintao Li, and
Shuicheng Yan. Scale-adaptive convolutions for scene pars-
ing. In Proceedings of the IEEE International Conference on
Computer Vision , pages 2031–2039, 2017. 2
[61] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
Shufﬂenet: An extremely efﬁcient convolutional neural net-
work for mobile devices. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition , pages
6848–6856, 2018. 1
GeoCLIP: Clip-Inspired Alignment between Locations
and Images for Effective Worldwide Geo-localization
Vicente Vivanco Cepeda, Gaurav Kumar Nayak, Mubarak Shah
Center for Research in Computer Vision, University of Central Florida, USA
{vicente.vivancocepeda, gauravkumar.nayak}@ucf.edu; shah@crcv.ucf.edu
Abstract
Worldwide Geo-localization aims to pinpoint the precise location of images taken
anywhere on Earth. This task has considerable challenges due to immense variation
in geographic landscapes. The image-to-image retrieval-based approaches fail to
solve this problem on a global scale as it is not feasible to construct a large gallery
of images covering the entire world. Instead, existing approaches divide the globe
into discrete geographic cells, transforming the problem into a classification task.
However, their performance is limited by the predefined classes and often results
in inaccurate localizations when an image’s location significantly deviates from
its class center. To overcome these limitations, we propose GeoCLIP, a novel
CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between
the image and its corresponding GPS locations. GeoCLIP’s location encoder
models the Earth as a continuous function by employing positional encoding
through random Fourier features and constructing a hierarchical representation
that captures information at varying resolutions to yield a semantically rich high-
dimensional feature suitable to use even beyond geo-localization. To the best of our
knowledge, this is the first work employing GPS encoding for geo-localization. We
demonstrate the efficacy of our method via extensive experiments and ablations on
benchmark datasets. We achieve competitive performance with just 20% of training
data, highlighting its effectiveness even in limited-data settings. Furthermore, we
qualitatively demonstrate geo-localization using a text query by leveraging CLIP
backbone of our image encoder.
1 Introduction
Image geo-localization refers to determining the geographical location where a photograph is taken
from. This problem is of great significance in a wide variety of applications, such as navigation,
tourism, and security. However, this task poses several challenges, especially for locations that lack
distinctive landmarks or are outside tourist hotspots. Typically, image-based geo-localization mainly
focuses on identifying the GPS locations of images within a specified region of interest (e.g., within a
city). However, worldwide geo-localization [ 12,14,5] is not limited to any specific area but to the
entire globe, thus making it significantly more challenging, and hasn’t been explored extensively.
Geo-localization at the global level requires finding the locations (i.e., latitude and longitude) of
images from different parts of the world. Many retrieval-based methods [ 28,11,34,29,32,24,20,33]
which perform image-to-image retrieval limit their search space to specific cities. Extending such
approaches for worldwide geo-localization demands maintaining a gallery of all possible images of
the entire globe to perform a match with the query image. However, creating such a large image
gallery is infeasible; thus, these methods are inappropriate for retrieval at the global scale. Existing
works instead use classification-based approaches [ 14,26,12,5,27,8,18] using scenes, hierarchy,
queries, and segmentation. These methods divide the Earth into geographic classes (covering a
particular area) and apply a linear classifier for prediction. If an image belonging to a class is far from
its center, it would result in a localization error even though the classification prediction is correct.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2309.16020v1  [cs.CV]  27 Sep 2023
Feature 
Similarity Matrix 
 Image Encoder 
Location Encoder 
GPS Gallery 
(B)  Approach 
Query Image Image 
Pairs of (A) Problem Setup 
GPS Gallery 
Retrieved GPS 
Coordinates Query 
Retrieve 
Pairs of 
CLIP 
Figure 1: The objective of image-based geo-localization is to find the GPS coordinate of a given image. (A)
We formulate this problem as an image-to-GPS retrieval approach where the image is a query, and the gallery
is a collection of GPS coordinates. (B) We propose a novel method, GeoCLIP, having two major components:
a location encoder L(·)to obtain high-dimensional features from the 2D GPS coordinates and a CLIP-based
image encoder V(·)to obtain the image features. We perform the matching of features of a query image against
a gallery of GPS embeddings, and the most similar GPS embedding is the predicted GPS coordinates.
Also, the predefined classes limit the prediction to ≈21k possible locations, which is not dense
enough to predict any location in the world. Moreover, these methods often need large amounts of
training data which takes days to train even with high computational resources. Thus, there is a need
for a new effective and efficient approach that can better model the relationships between images and
their corresponding geographic locations. We summarize the problem setup and compare the major
differences between existing works and our proposed method in Figure 1.
To address the above limitations of existing approaches, we propose a CLIP[ 15]-inspired image-to-
GPS retrieval approach (named ‘GeoCLIP’) where we retrieve the GPS coordinates of an unseen
query image by matching it with the gallery of GPS coordinates. To facilitate matching, we design
location and image encoders to project GPS and image in a shared embedding space. Our location
encoder transforms a given 2D GPS coordinate into a high-dimensional feature space. The traditional
latitude and longitude systems have inherent distortions in representing certain areas (e.g., countries
near to poles and equator have inconsistent representations). Hence, we first convert the GPS
locations using equal earth projection [ 17] to minimize such distortion. Another challenge is to obtain
a high-dimensional representation of a 2D GPS coordinate. The direct use of standard MLP leads
to spectral bias, as they approximate low-frequency functions and fail to capture high-frequency
details with low-dimensional inputs. Previous studies [ 21] have demonstrated that such spectral bias
can significantly degrade the performance in various applications, particularly when capturing these
details is crucial for the task. To alleviate this, we employ a positional encoding technique using
Random Fourier Features (RFF) [21] before processing it through a feedforward network.
To enrich the location features, we further enforce hierarchical representation in our location encoder
by capturing features at different granularities (ranging from coarse to fine-grained resolutions) and
combining them to obtain a rich high-dimensional feature. Specifically, we vary the frequencies in
RFF by changing sigma values using our exponential assignment strategy, enabling us to process
the 2D GPS coordinates at different resolutions. We employ contrastive loss between features of the
image and GPS. It is well established that the incorporation of additional negative samples enhances
the performance of contrastive learning. Our gallery is a collection of GPS coordinates instead of
images. Hence it is easy for us to fetch negatives by uniformly sampling the GPS coordinates from
the globe. To this end, we implement a queue of GPS values to serve as artificial negatives. Our
location encoder learns generic features as we find them suitable for use even beyond geo-localization.
Moreover, using the CLIP [ 15] backbone in our image encoder helps extract meaningful features and
allows us to compare GPS coordinates with text descriptions, which is not feasible in existing works.
Besides this, our image-to-GPS retrieval method facilitates a much finer evaluation than previous
techniques, resulting in better localization performance.
Below we summarize our major contributions as follows:
•To the best of our knowledge, we are the first to solve worldwide geo-localization task via
image-to-GPS retrieval approach (GeoCLIP) by explicitly aligning the features of images to
corresponding GPS locations.
2
•Our location encoder incorporates positional encoding with random Fourier features to
efficiently encode GPS coordinates and mitigate spectral bias in MLPs. In addition, we
use an exponential sigma assignment strategy to facilitate learning hierarchical features at
different resolutions (Sec. 3.1.1).
•We validate the efficacy of our location encoder in an additional application, highlighting its
versatility beyond geo-localization tasks (Sec. 4.5).
•Unlike existing works that only use an image as query, we also qualitatively demonstrate
global geo-localization with text query using GeoCLIP (Sec. 4.4).
• We also demonstrate the effectiveness of our approach in limited data settings (Sec. 4.2).
2 Related Works
This section briefly discusses the relevant works related to our approach.
Global Image Prediction: Many previous works [ 27,18,26,12,14], have tackled the problem of
global image geo-localization. While the most common localization papers use an image-to-image
retrieval technique [16, 19, 20, 25, 34, 32], this is not yet feasible on the global scale because of the
data collection that would be necessary for a global reference dataset to compare against. Instead,
Weyand et al. [27] proposed to split the earth into individual classes to predict the location of an
image. Through constructing these classes, the prediction can be relatively coarse or fine-grained
based on the geographic size of the classes. V o et al. [26] utilized multiple levels of geographic
classes to learn different granularities, and Seo et al. [18] was the first to combine these levels together
for an enhanced prediction. Müller et al. [12] additionally uses the scene labels of images to learn by
having separate encoders designated to learn each scene’s features. Recently Clark et al. [5] made use
of even more hierarchies and scenes than the previous works, they also use a specialized transformer
decoder to learn different features from every input image that represents each hierarchy and scene.
The use of semantic segmentation was introduced by [ 14] in an attempt to provide additional context
and understanding for the model to better geo-localize. They use two transformer encoders, one for
the RGB image and the other for the segmented representation of that image, and share information
between the encoders to get a better location prediction. Unlike existing global image geo-localization
works, we instead take an image-to-GPS retrieval approach by directly matching the query image
features to the features of GPS data.
Learning from GPS Data: The GPS coordinates of an image are used in [ 22,10,30,31,3] to
provide additional context when performing classification. They construct GPS embeddings by
leveraging image tags, textual descriptors, and information from geospatial databases, which are
paired with image embeddings to improve the classification accuracy. We differ from this work
by utilizing Random Fourier Features (RFF) [ 21] with with MLPs. RFF is often used in NeRFs to
represent the location or viewing angle for reconstructing the image or scene at that location or from
that view. However, our location encoder uses RFF to represent a specific location on the earth and
aligns them with the corresponding image features. We later also show that GPS embeddings of our
location encoder is generic and can even be utilized for classification tasks as well, leading to further
performance gains and showing its utility beyond geo-localization tasks.
Contrastive Learning: It is used in a wide variety of tasks in multiple deep learning-based disciplines.
The basic idea is to match the features from a positive pair while pushing away from negative example
features. Our method uses two different contrastive learning strategies, SimCLR[ 2] and CLIP[ 15].
SimCLR uses a base image and creates two separately augmented versions of it. It then attempts to
match these two versions of the image together and push away the other images that are not from
the same base. They show that this creates a robust feature representation of images. On the other
hand, CLIP utilizes a large image-text database to learn a shared feature space between its image
and text encoders. To establish a meaningful and distinct representation for both the text and image,
they enforce similarity between the image features and the corresponding text caption. CLIP has
been widely used in many tasks since its inception. We utilize the pretrained model from CLIP as a
backbone to our image encoder, while our augmentation strategy is similar to SimCLR. However,
instead of matching the image to another image, as in SimCLR, or to text, as in CLIP, we match it to
a gallery of GPS features. The additional benefit of using CLIP backbone is the ability to compare
GPS coordinates through our location encoder and text from CLIP’s text encoder, which provides
a very intriguing qualitative analysis that we show later (Sec. 4.4). Now, we explain our proposed
approach in detail in the next section.
3
Location Encoder 
 (A) (B) 
Figure 2: (A) Given a batch of GPS coordinates ( GB), we generate additional negatives Qusing our dynamic
queue strategy. Each image in batch ( IB) is processed using our image encoder V(·)with CLIP[ 15] backbone.
Similarly, we process the GPS coordinates using our location encoder L(·). We train our model to align image
features with corresponding GPS embeddings using contrastive loss over feature similarity matrix. (B) L(·)
transforms 2D coordinates GiintoG′
iusing equal earth projection (EEP)[ 17]. Then we obtain hierarchical
representations of G′
iusing RFF [ 21] and MLPs, aggregated to obtain a rich high-dimensional representation Li.
3 Proposed Approach
Worldwide Image Geo-localization Setup : Given the access to training dataset Dtrain =
{(In, Gn)}N
n=1containing pairs of image In, and GPS coordinate Gn. Our goal is to train a world-
wide model Wusing samples from Dtrain and then use the trained model Wto predict the location
of unseen query images of test dataset Dtest. Specifically, our objective is to accurately predict the
latitude and longitude of query images, GQ
k=W(IQ
k),∀k∈[1..K]where IQ
k∈Dtest. The images
in the query set IQcan belong to any geographical location of Earth, making it a challenging setup.
We formulate this problem as a retrieval problem. Traditionally, retrieval involves matching a query
image with images from the gallery, and the GPS coordinates of the closest gallery image would
be the desired output. However, this approach is not practical as it becomes infeasible to construct
a gallery containing images covering the entire world. Instead, it is much easier to build a gallery
of GPS coordinates. Hence, geo-localization can be framed as an image-to-GPS matching problem
where the closest GPS match would be the location of the query image.
In order to perform matching, the features of the image and GPS location need to be extracted.
Hence, we have two major components in our architecture, i.e., Location Encoder L(·)and Vision
Transformer V(·)as Image Encoder. Now, we explain the model architecture containing details of
both the encoders, followed by the training and evaluation strategies.
3.1 Model architecture
The architecture of our method GeoCLIP is shown in Figure 2.
Image Encoder : The CLIP [ 15] model has demonstrated strong generalization performance across
a wide range of computer vision tasks. Motivated by it, our image encoder contains a Vision
Transformer, which is a pre-trained CLIP ViT[ 6]-L/14 model. We use it as a backbone and keep
it frozen. To adapt this backbone for our geo-localization task, we add two linear trainable layers
with dimensions of h1andh2, respectively. These additional layers are employed to fine-tune the
learned representation for our task. Hence, our overall image encoder V(·)extracts the features
Vi=V(Ii),∀i∈[1. . . N ]andIi∈Dtrain .
Next, we explicitly encode the GPS coordinates, which has not been tried for the geo-localization task.
We now explain our location encoder in detail, which encodes the GPS coordinates of a location.
3.1.1 Location Encoder
Encoding 2D GPS coordinates (i.e., Gi∈R2,∀i∈[1. . . N ]) to a high-dimensional representation
(RD) where D≫2, is a challenging task. A simple use of standard MLPs for such encoding suffers
from spectral bias [ 21] (unable to capture high-frequency details). Hence, to handle this challenge and
effectively perform this encoding, we employ different components in our encoder, i.e., representing
GPS coordinates using equal earth projection[ 17], using positional encoding through random Fourier
features, and enforcing hierarchical representation at various scales. We explain each of them below:
4
Equal Earth Projection (EEP) : To accurately represent 2D GPS coordinates and minimize distor-
tions inherent in standard coordinate systems (i.e., countries closer to the poles are overrepresented
in traditional latitude and longitude systems), we employ the Equal Earth Projection (EEP)[ 17]. By
utilizing EEP, we counterbalance the distortions and ensure a more accurate representation of the
Earth’s surface. Given Gi∈R2(in radians), we transform it into G′
iby applying the EEP as below:
G′lat
i=2√
3Glon
icosθ
3(9P4θ8+ 7P3θ6+ 3P2θ2+P1);G′lon
i=P4θ9+P3θ7+P2θ3+P1θ (1)
where sinθ=√
3
2sinGlat
i,P1= 1.340264 ,P2=−0.081106 ,P3= 0.000893 ,P4= 0.003796
Random Fourier Features (RFF) : To capture rich high-frequency details from low-dimensional GPS
input, we apply a sinusoidal-based positional encoding technique to G′
i. Specifically, we leverage
random Fourier features (RFF)[ 21] for this positional encoding. We construct an encoding layer that
determines the range of frequencies to be encoded. We limit the frequencies using a fixed matrix R,
whose entries are sampled from a Gaussian distribution with the standard deviation ( σ). The matrix
Ris set at the beginning of training and remains unchanged throughout the training process. The
RFF operation γ(·)encodes GPS coordinate G′
iasγ(G′
i) = [cos(2 πRG′
i),sin(2πRG′
i)]T, where
the entries of a mthrow and nthcolumn of matrix Rarerm,n∼ N(0, σ).
Hierarchical Representation : To construct a hierarchical representation capable of capturing
information at varying scales, we vary the frequency in RFF to obtain multiple high-dimensional
representations spanning from coarse to fine-grained. As the frequency range in RFF depends on the
σvalue, we propose an exponential assignment strategy for choosing the σvalues in our location
encoder. Specifically, for a given a range of σvalues (i.e., [ σmin,σmax]), we obtain the σvalues for
Mlevel hierarchy using the following:
σi= 2log2(σmin)+(i−1)(log2(σmax)−log2(σmin))/(M−1),∀i∈ {1. . . M}. (2)
This exponential assignment of σvalues enables our location encoders L(·)to effectively process
the input 2D coordinates at different resolutions. Consequently, this hierarchical setting allows L(·)
to specialize in capturing features of a particular location at various scales. It enforces our model to
efficiently learn a rich understanding of spatial information, making it suitable for a wide range of
geospatial deep learning tasks.
Next, we pass the encoded hierarchical features through a feedforward MLP. Each MLP fiindepen-
dently processes the outputs of RFF and yields a vector vi∈Rd. We then perform element-wise
addition of these resulting vectors to obtain a joint representation v=PM
i=1vi.
Below we summarize the overall operations performed by our location encoder L(·)for obtaining
rich high-dimensional encoded features ( Li) of a GPS 2D coordinate Gi:
Li=L(Gi) =MX
i=1fi(γ(EEP (Gi), σi)) (3)
3.2 Model Training
We train our model using a contrastive learning scheme (similar to [ 15]) where we maximize the
similarity of features of an image Ii(i.e., Vi) with its corresponding location features of GPS
coordinate Gi(i.e.,Li) and minimize its similarity with other GPS coordinates in a batch. To further
improve our learned representation, we also adopt a training scheme similar to SimCLR [2].
We apply a set of random augmentations ( A) to each image in a batch and create Pviews of an image
to improve the quality of learned representations and promote better generalization. We employ
the same set of augmentations used in the SimCLR. For each augmented image Iij(ithsample of
Dtrain andjthview, we also create variation in its corresponding GPS coordinates Giby injecting
random noise ηjto it (denoting it by Gij) which helps to enforce spatial smoothness in the location
encoder. The noise ηjis sampled from a Gaussian distribution with a standard deviation of ση(i.e.,
η∼ N(0, σ2
η). This added noise encourages the location encoder to learn more robust representations
that are less sensitive to small variations in the GPS coordinates.
At any point during training, our location encoder L(·)can obtain the embedding of any GPS
location g(i.e.,Lg) without requiring a corresponding image. We exploit this capability to generate
additional negative samples during training to improve the learned representation with contrastive
5
learning. We append a queue Qof GPS locations with a fixed length Sto every batch of training
dataDtrain . During training, we generate embeddings for these GPS coordinates ( ˜L). We also inject
noise ( η′∼ N(0, σ2
η′)) to˜L, such that ση′≥ση. This additional queue of GPS embeddings notably
improves the performance of our model, particularly at smaller scales (refer Table 2(a)).
Hence, for an ithsample of a batch ( IB
i, GB
i∈Dtrain ) having Paugmented views with temperature
τ, our overall training objective is to minimize the following loss:
Li=−PX
j=1logexp (Vij·Lij/τ)
P|B|
i=0exp(Vij·Lij/τ) +PS
i=0exp(Vij·˜Li/τ). (4)
After the model parameters are updated on a batch of Dtrain with batch size |B|, we update the
queue Qby replacing its oldest coordinates with GPS coordinates from the current batch GBwhere
|B| ≪S=|Q|, ensuring an up-to-date reflection of the most recent GPS coordinates encountered
(naming it as ‘dynamic queue’ strategy).
Evaluation : By modeling the Earth as a continuous function, our location encoder can generate an
embedding for any GPS coordinate on the planet. Hence, our method allows us to directly compare
the embedding of a query image with the embeddings of any chosen GPS location, effectively
eliminating the constraint of working within a predetermined set of classes.
Unlike existing works that create classes using the knowledge of the training set, we create a gallery
of possible GPS coordinates during the evaluation phase by randomly sampling from the training
set. To obtain the GPS predictions ( GQ
k) for a kthimage in the unseen query set ( IQ
k), we compare
the query image’s embedding with the embeddings of GPS coordinates in our gallery. The GPS
coordinate with the highest similarity to the query image’s embedding indicates the most probable
location for the image (i.e., GQ
kisarg maxGQ
i∈DtestV(IQ
k)·L(GQ
i)).
4 Experiments
Datasets and evaluation details : We perform experiments on publicly available benchmark datasets
to have a fair comparison with existing works. For training, we use the MediaEval Placing Tasks
2016 (MP- 16) dataset [ 9], which consists of 4.72million geotagged images from Flickr [ 1]. We test
our trained model on several different datasets: Im2GPS3k [ 7], the recently introduced Google World
Streets 15K dataset (GWS15k) [ 5], and YFCC26k [ 23]. During testing, we conduct image-to-GPS
retrieval where the query image from the test datasets is matched against a GPS gallery of 100K
(Im2GPS3k) and 500K (GWS15k) coordinates. The performance of GeoCLIP on different gallery
sizes is put in supplementary. In our evaluation, similar to [ 5], we also uses a Ten Crop ( 5different
crops of an image and their flipped counterparts) method where the predictions on these crops are
averaged to make a joint prediction. We report our results using a threshold metric, similar to the
protocol followed in previous works. Specifically, for each query image, we compute the Geodesic
distance between GPS coordinates predicted by our model and the respective ground truth. We
calculate how many of them (in %) fall within the distance thresholds ( 1km,25km,200km,750km,
and2500 km) and report average performance of model over three runs.
Implementation details : Each MLP fiused in our location encoder L(·)is composed of an input
layer with 512dimensions, four hidden layers (each with ReLU activation and 1024 dimensions), and
an output layer of 512dimensions. The RFF of the positional encoding layers have an input size of 2
dimensions and an output size of 512dimensions. The dynamic queue ( |Q|as4096 ) is initialized
randomly with values drawn from a uniform distribution covering the range of latitude and longitude.
We add gaussian noise to GPS coordinates with σηandση′as150and1000 respectively. Further
ablations on the noise and queue length are put in supplementary. We use three hierarchies ( Mas
3) with σminas20, and σmax as28respectively (refer Sec. 4.3). Our image encoder V(·)utilizes
OpenAI’s pre-trained CLIP ViT-L/14 as backbone with two trainable linear layers h1andh2having
dimensions of 768and512respectively. We perform image augmentations similar to SimCLR.
Training details : Our GeoCLIP model is trained using Adam optimizer with a learning rate of
3×10−5and a weight decay of 1×10−6. We use a learning rate of 3×10−4to train classifiers with
the CLIP backbone. We employ a step decay learning rate scheduler with a gamma of 0.87and a step
size of one epoch. The model is trained until convergence, which typically occurs around tenth epoch.
We use batch size |B|as512when training on full dataset. For data-efficient settings with 20%,
10%, and 5%of the data, we use |B|as256,256, and 128respectively. The learnable temperature
6
parameter τis initialized to 0.07(refer eq. 4). The CLIP backbone is frozen during training while
location encoder and h1andh2layers are fully trainable. We train our models on an NVIDIA Ampere
GPU along with 12CPUs. Our code is in PyTorch [13] and is shared in supplementary.
4.1 Comparison with state-of-the-art methods
Table 1: We compare the performance of GeoCLIP with the state-of-the-art methods on (a) Im2GPS3k [ 7] and
(b) GWS15k [ 5] datasets. Our method yields consistent gains across datasets and different distance thresholds.
(a) Results on the Im2GPS3k [7] dataset
MethodStreet City Region Country Continent
1km 25km200km 750km 2500 km
[L]kNN, σ=4[26] 7.2 19.4 26.9 38.9 55.9
PlaNet [27] 8.5 24.8 34.3 48.4 64.6
CPlaNet [18] 10.2 26.5 34.6 48.6 64.6
ISNs [12] 10.5 28.0 36.6 49.7 66.0
Translocator [14] 11.8 31.1 46.7 58.9 80.1
GeoDecoder [5] 12.8 33.5 45.9 61.0 76.1
Ours 14.11 34.47 50.65 69.67 83.82(b) Results on the recent GWS15k [5] dataset
MethodStreet City Region Country Continent
1km 25km200km 750km 2500 km
ISNs [12] 0.05 0.6 4.2 15.5 38.5
Translocator [14] 0.5 1.1 8.0 25.5 48.3
GeoDecoder [5] 0.7 1.5 8.7 26.9 50.5
Ours 0.6 3.1 16.9 45.7 74.1
We perform a comparative analysis of GeoCLIP against worldwide Geo-Localization benchmarks,
Im2GPS3k and Google World Streets 15k (GWS15k). The results are shown in Table 1. In all
the metrics, our method surpasses the previous state-of-the-art (SOTA) performance on Im2GPS3k,
achieving improvements of +1.31%,+0.97%,+3.95%, and +8.67% in the 1km,25km,200km,
750km, and 2500 km thresholds respectively. The results on another dataset (YFCC26k) are put in
supplementary, where we also observe a similar trend.
Notably, our approach exhibits a large gain on the more challenging GWS15k dataset, surpassing
the previous SOTA model with significant accuracy improvements of +1.6%,+8.2%,+18.8%, and
+23.6%in the 25km,200km,750km, and 2500 km thresholds respectively. Our performance is nearly
double the accuracy of the previous state-of-the-art model. The GWS15k contains samples that are
uniformly sampled across the Earth and are not biased towards any specific geographic location.
Moreover, the images in this dataset have a large distribution shift compared to the training set, making
the geo-localization task tough and challenging. Our substantial improvement can be attributed to
the better embeddings achieved by our encoders. Specifically, the hierarchical nature of the location
encoder helps focus on multiple resolutions of a location simultaneously, and its alignment with
corresponding out-of-distribution images gets further enhanced with the robust features obtained by
the CLIP backbone of our image encoder.
4.2 Performance of GeoCLIP in limited data settings
100% 20% 10% 5%
Percentage of Available Data0102030405060708090100Accuracy (%)
14.113.1 12.110.834.532.6 31.9 31.150.649.0 48.9 48.769.7 69.2 69.167.683.8 83.8 83.6 83.2GeoCLIP: Data Efficiency
1 km (Street) 25 km (City) 200 km (Region) 750 km (Country) 2500 km (Continent)
Figure 3: We vary the amount of training data from 100% to as low as 5%. GeoCLIP achieves respectable
accuracy even with only 5%of the training data, demonstrating its utility in limited data scenarios.
The worldwide geo-localization methods assume access to a training set containing millions of data
samples. Training on such large scale datasets is often highly time-consuming, even with substantial
computational resources at hand, taking days to complete. Moreover, practical limitations may arise,
such as restricted access to data due to privacy concerns or proprietary rights held by companies.
Hence, we also investigate the potential of our GeoCLIP method in limited-data settings. In the
7
figure 1, we present a comprehensive analysis of our method’s performance across various data
settings: 20%,10%,5%of the full dataset. We can observe that our method maintains a high level of
performance, closely approximating the results achieved with the full dataset, even as the available
data decreases exponentially. Notably, even with a mere 20% of the data, the performance of GeoCLIP
drops by only 0.0%,0.5%,1.6%,1.9%, and 1.0%on2500 km , 750km,200km,25km, and 1km
distance thresholds respectively. These results highlight the efficiency of GeoCLIP in limited-data
scenarios, emphasizing its potential to address practical challenges in worldwide geo-localization.
4.3 Ablations
Effective encoding of raw GPS coordinates : The direct use of an individual MLP to encode GPS
coordinates into feature vectors serves as a baseline. As shown in Table 2(a), the MLP performs well
at larger scales, but it stumbles at finer scales due to distortions in the standard coordinate system
and its own spectral bias. To handle the distortions, we apply the Equal Earth Projection (EEP),
which boosts performance at 1km,25km, and 200km thresholds by +0.1%,+6.07%, and +3.93%,
respectively. However, the encoding of fine-grained details remains a challenge. To address this,
we introduce an additional encoding layer using Random Fourier Features ( σas24). This layer
enables the MLP to learn high-frequency functions necessary to distinguish features at finer scales,
like streets and cities. This modification nearly doubles the accuracy at 1km and 25km, with increases
of+5.14% and+14.95%, respectively. When we include harder negatives via our dynamic queue,
improving performance across all scales, with a notable increase of +4.47% at the 1km threshold.
Importance of hierarchical learning : We experiment with different σ’s in eq. 2 ( 20,24,28) to
investigate their influence across various scales. With a single accuracy metric, one might typically
seek an optimal sigma. However, we’re optimizing for five scale metrics ( 1km,25km,200km,750km,
and2500 km), making the task more difficult. Our findings, highlighted in table 2(b) and further
elaborated in the supplementary material, reveal an interesting trade-off: smaller σexcels at larger
scales but has issues with smaller scales, while larger σincreases fine-grained accuracy but decreases
on larger scales. Building upon this observation, we combine the representations from the encoders
with varying σs. By merging the outputs from the different encoders, we create a joint hierarchical
representation that maximizes the location encoder’s capabilities across all scales. This method
outperforms the use of individual σat all scales, effectively addressing the varying performance issue
related to different σs, thereby endorsing the need for hierarchical representation in our architecture.
Table 2: We perform ablations to show the importance of different components of our location encoder. (a)
Our encoding of GPS coordinates using Equal Earth Projection (EEP), Random Fourier Features(RFF), and
Dynamic Queue (DQ) strategy led to significant gains compared to the MLP alone, especially in challenging
localization within 1km and 25 km. (b) Our GeoCLIP approach with hierarchies (GeoCLIP w/ H) leads to further
gains and even performs much better than the respective classification with hierarchies C (w/ H) approach. The
improvements with respect to baseline in (a) and C (w/ H) in (b) are shown in brackets and highlighted in blue.
(a) Encoding GPS coordinates
MethodStreet City Region Country Continent
1km 25km 200km 750km 2500 km
MLP (Baseline) 0.13 11.01 41.68 67.83 83.28
+EEP 0.23 17.08 45.61 68.8 82.98
+EEP +RFF 5.37 32.03 49.42 68.4 82.98
+EEP +RFF +DQ 9.84 33.1 49.28 69.07 83.35
(+9.71) (+22.09) (+7.6) (+1.24) (+0.07)(b) Benefits of hierarchical learning
MethodStreet City Region Country Continent
1km 25km 200km 750km 2500 km
C (w/ H) 11.4 34.3 49.4 67.4 80.8
GeoCLIP (Coarse) 1.07 24.66 49.18 69.47 83.75
GeoCLIP (Medium) 9.84 33.1 49.28 69.07 83.35
GeoCLIP (Fine) 13.61 33.43 47.78 65.1 80.95
GeoCLIP (w/ H) 14.11 34.47 50.65 69.67 83.82
(+2.71) (+0.17) (+1.25) (+2.27) (+2.02)
4.4 Qualitative Results: Geo-localization using Text-based query
We demonstrate GeoCLIP’s ability to process text queries, highlighting the semantic richness of our
model. As our image encoder has a pre-trained CLIP backbone (trained to match visual and text
features), our location encoder gets inherent alignment with CLIP’s text features, enabling us to map
textual descriptions to geographical coordinates.
This capability allows us to assign a geographical context to any given text. Specifically, we can take
any text, generate its embedding using CLIP text encoder, process it through our layers h1andh2, and
calculate its similarity with a set of GPS embeddings. This results in a similarity map, demonstrating
the geographical distribution of the text query’s concept. For instance, using "Desert" as a text query,
we compute the cosine similarity between the text query’s embedding and the GPS embeddings. The
resulting similarity map, (Figure 4), reveals the spatial distribution of desert regions worldwide.
8
4.5 Utility of our location encoder beyond geo-localization
We investigate the utility of GeoCLIP’s location encoder in improving multi-class image classification
task. We perform experiments on the geo-tagged NUS-WIDE dataset [ 4] with the same train and
test splits as used in [ 31]. We conduct two experiments: 1) Classifying an image using only its GPS
location features, and 2) Classifying an image using a combination of its GPS location features and
visual features.
We obtain GPS features from our pretrained location encoder (which is kept frozen), which are
normalized and fed through a Multilayer Perceptron (MLP) with a single hidden layer of 2048
dimensions, followed by a dropout layer for regularization. For classification incorporating visual
context, we adopt the Bag-of-Visual-Words (BovW) representation based on SIFT descriptors as the
visual feature for a fair comparison with existing works. We normalize location and visual features,
concatenate them, and then pass them through the MLP.
Our results in Table 3 indicate that GeoCLIP achieves state-of-the-art performance on both approaches.
This is particularly significant, as it highlights that our location encoder has learned a semantically rich
representation across the Earth, being able to generalize beyond geo-localization and can therefore
be extended to assist other location-aware methods to improve performance without any significant
changes to their architecture.
Text Query: “Desert” 
Figure 4: Qualitative demonstration: Geo-localization
with text query using GeoCLIP. Our location can differ-
entiate between geographical coordinates and associate
semantic meaning from text to geographical space, pro-
viding a geographical context to textual descriptions.Method Classifier mAP
Visual (V) visual 0.234
OneHot [3] geo 0.066
HashTag [22] geo 0.163
TagFeature [10] geo 0.161
GPS2Vec onehot [30] geo 0.130
GPS2Vec [30] geo 0.182
GPS2Vec+ [31] geo 0.210
GeoCLIP (Ours) geo 0.249
OneHot [3] + V fusion 0.238
HashTag [22] + V fusion 0.261
TagFeature [10] + V fusion 0.260
GPS2Vec Onehot [30] + V fusion 0.277
GPS2Vec [30] + V fusion 0.300
Liao et al. [10] fusion 0.347
GPS2Vec+ [31] + V fusion 0.348
GeoCLIP (Ours) + V fusion 0.362
Table 3: Comparison of methods utilizing GPS data
for image classification on the geotagged NUSWIDE
Dataset. Our method outperforms existing methods
using our GPS features with or without visual features.
5 Conclusion
We solved worldwide geo-localization by formulating it as an image-to-GPS retrieval problem. We
evaluated the efficacy of our method (GeoCLIP) not only in full training data scenarios but also
in limited data settings. The performance of GeoCLIP was competitive even when the training
data was reduced significantly. GeoCLIP could even perform geo-localization using text queries by
leveraging the CLIP[ 15] backbone of our image encoder. Moreover, our location encoder (using
equal earth projection[ 17], RFF[ 21], and MLPs) better encoded GPS coordinates compared to the
direct use of MLPs. However, coarse σor a fine-grained σused in RFF performed better in localizing
either large or small areas. Our hierarchical learning overcame this issue by capturing the location
features at various scales, leading to a further performance boost. Also, our location encoder is
not tied to geo-localization and helps to aid in classification problems. In the future, we plan to
further investigate the utility of our location encoder in assisting other computer vision problems.
Limitations : As we use a CLIP-based backbone in our image encoder, precomputing the image
features on the large training dataset (each image of dimension 224×224×3) is time-consuming.
But once the features are precomputed, the training of our GeoCLIP model is relatively fast. We also
observed that a single σvalue used in RFF is insufficient to perform well on both small and large
areas. However, our hierarchical learning is a promising direction to overcome this issue.
Broader Impacts : Our image-based geo-localization can act as a substitute for GPS-based localiza-
tion in scenarios where GPS signals are unreliable, blocked, or inaccurate. Combining image-based
geo-tagging with GPS-based positioning systems can help to improve stability and safety in appli-
cations like autonomous driving and navigation. Also, it can benefit digital forensics, but avoiding
unethical use that could mislead location detection systems or violate privacy is important.
9
Acknowledgments and Disclosure of Funding
This work was supported by the US Army contract W911NF-2120192. We would like to extend our
gratitude to all the reviewers for their valuable suggestions. We would also like to thank Brandon
Clark for his helpful discussions on this project.
References
[1] Flickr : https://www.flickr.com.
[2]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In International conference on machine
learning , pages 1597–1607. PMLR, 2020.
[3]Gordon A. Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of
the world. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
6172–6180, 2017.
[4]Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng.
Nus-wide: a real-world web image database from national university of singapore. In ACM
International Conference on Image and Video Retrieval , 2009.
[5]Brandon Clark, Alec Kerrigan, Parth Parag Kulkarni, Vicente Vivanco Cepeda, and Mubarak
Shah. Where we are and what we’re looking at: Query based worldwide image geo-localization
using hierarchies and scenes. arXiv preprint arXiv:2303.04249 , 2023.
[6]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
[7]James Hays and Alexei A. Efros. Im2gps: estimating geographic information from a single
image. In 2008 IEEE Conference on Computer Vision and Pattern Recognition , pages 1–8,
2008.
[8]Mike Izbicki, Evangelos E Papalexakis, and Vassilis J Tsotras. Exploiting the earth’s spherical
geometry to geolocate images. In Machine Learning and Knowledge Discovery in Databases:
European Conference, ECML PKDD 2019, Würzburg, Germany, September 16–20, 2019,
Proceedings, Part II , pages 3–19. Springer, 2020.
[9]Martha Larson, Mohammad Soleymani, Guillaume Gravier, Bogdan Ionescu, and Gareth J.F.
Jones. The benchmarking initiative for multimedia evaluation: Mediaeval 2016. IEEE MultiMe-
dia, 24(1):93–96, 2017.
[10] Shuai Liao, Xirong Li, Heng Tao Shen, Yang Yang, and Xiaoyong Du. Tag features for
geo-aware image classification. IEEE Transactions on Multimedia , 17(7):1058–1067, 2015.
[11] Liu Liu and Hongdong Li. Lending orientation to neural networks for cross-view geo-
localization. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pages 5624–5633, 2019.
[12] Eric Muller-Budack, Kader Pustu-Iren, and Ralph Ewerth. Geolocation estimation of photos
using a hierarchical model and scene classification. In Proceedings of the European conference
on computer vision (ECCV) , pages 563–579, 2018.
[13] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. In Advances in Neural Information Processing Systems
32, pages 8024–8035. Curran Associates, Inc., 2019.
[14] Shraman Pramanick, Ewa M Nowara, Joshua Gleason, Carlos D Castillo, and Rama Chellappa.
Where in the world is this image? transformer-based geo-localization in the wild. In Com-
puter Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
Proceedings, Part XXXVIII , pages 196–215. Springer, 2022.
[15] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning ,
pages 8748–8763. PMLR, 2021.
[16] Krishna Regmi and Mubarak Shah. Bridging the domain gap for ground-to-aerial image
matching. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
pages 470–479, 2019.
10
[17] Bojan Šavri ˇc, Tom Patterson, and Bernhard Jenny. The equal earth map projection. International
Journal of Geographical Information Science , 33(3):454–465, 2019.
[18] Paul Hongsuck Seo, Tobias Weyand, Jack Sim, and Bohyung Han. Cplanet: Enhancing
image geolocalization by combinatorial partitioning of maps. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 536–551, 2018.
[19] Yujiao Shi, Liu Liu, Xin Yu, and Hongdong Li. Spatial-aware feature aggregation for image
based cross-view geo-localization. Advances in Neural Information Processing Systems , 32,
2019.
[20] Yujiao Shi, Xin Yu, Dylan Campbell, and Hongdong Li. Where am i looking at? joint location
and orientation estimation by cross-view matching. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 4064–4072, 2020.
[21] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,
Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let
networks learn high frequency functions in low dimensional domains. Advances in Neural
Information Processing Systems , 33:7537–7547, 2020.
[22] K. Tang, M. Paluri, L. Fei-Fei, R. Fergus, and L. Bourdev. Improving image classification with
location context. In 2015 IEEE International Conference on Computer Vision (ICCV) , pages
1008–1016, Los Alamitos, CA, USA, dec 2015. IEEE Computer Society.
[23] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland,
Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications
of the ACM , 59(2):64–73, 2016.
[24] Yicong Tian, Chen Chen, and Mubarak Shah. Cross-view image matching for geo-localization
in urban environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 3608–3616, 2017.
[25] Aysim Toker, Qunjie Zhou, Maxim Maximov, and Laura Leal-Taixé. Coming down to earth:
Satellite-to-street view synthesis for geo-localization. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , pages 6488–6497, 2021.
[26] Nam V o, Nathan Jacobs, and James Hays. Revisiting im2gps in the deep learning era. In
Proceedings of the IEEE international conference on computer vision , pages 2621–2630, 2017.
[27] Tobias Weyand, Ilya Kostrikov, and James Philbin. Planet-photo geolocation with convolutional
neural networks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam,
The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14 , pages 37–55. Springer, 2016.
[28] Scott Workman, Richard Souvenir, and Nathan Jacobs. Wide-area image geolocalization with
aerial reference imagery. In Proceedings of the IEEE International Conference on Computer
Vision , pages 3961–3969, 2015.
[29] Hongji Yang, Xiufan Lu, and Yingying Zhu. Cross-view geo-localization with layer-to-layer
transformer. Advances in Neural Information Processing Systems , 34:29009–29020, 2021.
[30] Yifang Yin, Zhenguang Liu, Ying Zhang, Sheng Wang, Rajiv Ratn Shah, and Roger Zimmer-
mann. Gps2vec: Towards generating worldwide gps embeddings. Proceedings of the 27th ACM
SIGSPATIAL International Conference on Advances in Geographic Information Systems , 2019.
[31] Yifang Yin, Ying Zhang, Zhenguang Liu, Yuxuan Liang, Sheng Wang, Rajiv Ratn Shah, and
Roger Zimmermann. Learning multi-context aware location representations from large-scale
geotagged images. In Proceedings of the 29th ACM International Conference on Multimedia ,
MM ’21, page 899–907, New York, NY , USA, 2021. Association for Computing Machinery.
[32] Sijie Zhu, Mubarak Shah, and Chen Chen. Transgeo: Transformer is all you need for cross-view
image geo-localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 1162–1171, 2022.
[33] Sijie Zhu, Linjie Yang, Chen Chen, Mubarak Shah, Xiaohui Shen, and Heng Wang. r2
former: Unified retrieval and reranking transformer for place recognition. arXiv preprint
arXiv:2304.03410 , 2023.
[34] Sijie Zhu, Taojiannan Yang, and Chen Chen. Vigor: Cross-view image geo-localization beyond
one-to-one retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 3640–3649, 2021.
11
Supplementary for: “GeoCLIP: Clip-Inspired
Alignment between Locations and Images
for Effective Worldwide Geo-localization”
We organize our supplementary document as follows:
1. Results on additional dataset
2. Results for limited data settings on YFCC26k and GWS15k datasets
3. Additional Ablations
(a) Gallery Size
(b) Queue Length
(c)σηfor Batch GPS noise
(d)ση′for Queue GPS noise
(e)σfor Random Fourier Features
(f) Number of hierarchies ( M)
4. Qualitative Demonstration
(a) Hierarchical learning in our location encoder L(·)
(b) GeoCLIP with Image Query
(c) GeoCLIP with Text Query
(d) Distribution of correct predictions of GeoCLIP on different datasets
12
1 Results on additional dataset
In section 4.1of the main paper, we demonstrated the performance of our GeoCLIP method on
Im2GPS3k [ 7] and GWS15k [ 5] datasets and compared them with the state-of-the-art methods. Here,
we perform experiments on another dataset YFCC26k [23]. The results are provided in Table 1.
Table 1: Results on YFCC26k [23] dataset
MethodStreet City Region Country Continent
1km 25km 200km 750km 2500 km
PlaNet [27] 4.4 11.0 16.9 28.5 47.7
ISNs [12] 5.3 12.3 19.0 31.9 50.7
Translocator [14] 7.2 17.8 28.0 41.3 60.6
GeoDecoder [5] 10.1 23.9 34.1 49.6 69.0
Ours 11.61 22.19 36.69 57.47 76.02
We can observe that GeoCLIP achieves state-of-the-art performance on the YFCC26k dataset in the
majority of distance threshold metrics, with +1.51%,+2.59%,+7.87%, and +7.02% improvements
in accuracy on the 1km,200km,750km, and 2500 km respectively. This result highlights that
GeoCLIP performs well across datasets, being useful across different data distributions.
2 Results for limited data settings on YFCC26k and GWS15k datasets
(B)    Data Efficiency of GeoCLIP on GWS15k (A)    Data Efficiency of GeoCLIP on YFCC26k 
Figure 1: Performance of GeoCLIP in limited data scenarios on (A) YFCC26k dataset and (B) GWS15k
datasets. GeoCLIP achieves decent performance across datasets even when the training data is significantly
reduced.
13
We show the efficacy of GeoCLIP on limited training samples of Im2GPS3k in section 4.2of the
main paper. Now, we further investigate the performance of GeoCLIP for limited data settings on
other datasets (YFCC26k and GWS15k).
As shown in Figure 1, we observe a similar trend of GeoCLIP on YFCC26k and GWS15k datasets
as in the Figure 3of the main paper. The performance of GeoCLIP is not affected considerably,
even when the amount of data is limited. This observation is consistent and holds across datasets.
Surprisingly, even if the training data is reduced exponentially (getting as low as 5%), GeoCLIP still
achieves competitive performance.
3 Additional Ablations
We had discussed a few important ablations (benefits of our GPS encoding and hierarchical learning)
in Sec. 4.3of the paper draft. In this section, we present and discuss other ablations on different
components of our method GeoCLIP.
3.1 Gallery size
The existing methods, which perform worldwide geo-localization, limit themselves to matching a
query image to 21k GPS coordinates (at the finest resolution). They are restricted due to their design
choice of predefined classes. However, our method, GeoCLIP, can perform matching against a gallery
of any arbitrary length. hence, we vary the gallery size and evaluate GeoCLIP performance on them.
The results for the datasets Im2GPS3k [ 7], YFCC26k [ 23], and GWS15k [ 5] are reported in Table 2.
Table 2: Results on the Im2GPS3k [ 7], YFCC26k [ 23], and GWS15k [ 5] datasets when the gallery size is
varied.
Dataset Gallery SizeStreet City Region Country Continent
1km 25km 200km 750km 2500 km
Im2GPS3k [ 7]21k GPS 11.88 33.10 48.75 68.70 83.18
100k GPS 14.11 34.47 50.65 69.67 83.82
500k GPS 13.98 33.80 50.95 69.80 84.38
1M GPS 13.98 33.47 51.48 69.14 82.85
YFCC26k [ 23]21k GPS 8.44 20.28 34.52 56.37 75.16
100k GPS 11.61 22.19 36.69 57.47 76.02
500k GPS 11.49 21.81 36.68 57.85 76.12
1M GPS 11.45 21.51 36.57 57.68 76.05
GWS 15k [5]21k GPS 0.18 2.4 14.84 42.13 73.05
100k GPS 0.53 3.2 15.94 44.55 72.79
500k GPS 0.6 3.12 16.92 45.69 74.06
1M GPS 0.6 2.95 16.96 46.15 74.19
The results demonstrate that expanding the GPS gallery improves performance, particularly at smaller
scales. For example, on the Im2GPS3k dataset, increasing the gallery size from 21K to100K leads
to an accuracy improvement from 11.88% to14.11% at the 1km scale. Similarly, augmenting the
gallery from 21K to100K GPS coordinates in the YFCC 26k dataset improves the accuracy at the
1km scale from 8.44% to11.61%. However, the 500K and 1M gallery sizes lead to a slight decrease
in performance compared to the 100K gallery. Hence, we used this gallery size to report performance
on these datasets.
On the GWS15k dataset, the 500K gallery yields better improvements, increasing the accuracy from
0.18% to0.6%at the 1km scale. However, further additions to the gallery do not provide more
gains. Even though increasing the gallery size to 1M, yields marginally beneficial at larger scales,
the performance at smaller scales reduces. Thus, using a 500k gallery size gives a better trade-off,
keeping low computational expense and performance on fine-grained scales as a priority.
3.2 Queue Length
Negative samples play a crucial role as we use contrastive learning to train our model (refer to eq. 4
of the paper draft). Our method performs image-to-GPS retrieval, thus, the negative samples in our
14
case are the GPS coordinates rather than images used in traditional retrieval methods, which are easy
to obtain. We have shown the benefit of additional negatives via our dynamic queue strategy in Table
2(a)in the main paper. Here, we do further analysis by evaluating the performance when the queue
length (S=|Q|) is varied. The results are presented in Table 3.
Table 3: Ablation on the length of the queue used for additional negatives during the training of GeoCLIP.
Queue LengthStreet City Region Country Continent
1km 25km 200km 750km 2500 km
512 7.57 32.03 48.76 67.42 82.35
1024 7.79 32.63 49.22 68.39 82.68
2048 8.73 32.65 49.14 67.11 82.22
4096 9.84 33.10 49.32 69.07 83.30
8192 9.64 32.00 49.01 68.12 82.82
16384 9.23 32.80 49.77 68.00 82.71
32768 8.45 32.83 49.20 68.20 82.70
65536 8.31 32.48 48.59 67.52 82.19
Among the tested queue lengths, |Q|= 4096 obtained 9.84%,33.10%,49.32%,69.07%, and 83.30%
accuracy on 1km,25km,200km,750km, and 2500 km thresholds, respectively. While other queue
lengths showed comparable results, the performance of the 4096 -length queue is better. Even though
the queue length 16384 showed a slight improvement in the 200km threshold, the 4096 -length queue
still outperformed it on the rest of the thresholds. Hence, we used |Q|as4096 in our experiments in
the main paper.
3.3 σηfor Batch GPS noise
As explained in section 3.2of the main paper, we create variation in GPS coordinates by adding
a small amount of noise η∼ N (0, σ2
η). Here, we perform an ablation where we vary the σηof
the Gaussian distribution and report their performance on Im 2GPS3k [ 7] dataset in Table 4. We
can observe that σηof150meters yields better performance. We used the same value of σηacross
different datasets for the experiments in the main paper.
Table 4: Ablation on different σηvalues. We observe the best performance when σηis150meters.
ση(in meters)Street City Region Country Continent
1km 25km 200km 750km 2500 km
0 8.43 32.06 47.76 65.49 81.84
10 8.89 32.17 47.66 67.01 81.59
50 8.56 31.82 47.14 66.19 81.57
150 8.95 32.61 47.96 67.09 82.08
300 8.05 31.32 46.89 65.40 81.54
500 7.54 31.78 47.27 65.34 81.39
3.4 ση′for Queue GPS noise
As described in the main paper (Sec. 3.2), we propose a dynamic queue strategy with GPS coordinates
and use them as additional negative samples n the contrastive learning of GeoCLIP. We also showed
the benefit of it in Table 2(a) in the paper draft. We now do further analysis on the queue and perform
ablations with respect to the static queue and the gaussian noise parameter ση′.
As shown in Table 5, compared to the static queue, we observe improvement in performance across
all the distance accuracy metrics while using the dynamic queue, especially on the finer scales ( 1km
and25km distance thresholds). Note that the performances reported are without hierarchical learning.
Moreover, adding noise sampled from gaussian distribution ( η′∼ N(0, σ2
η′)) to the GPS coordinates
of the dynamic queue leads to further gains in performance. ση′of1000 meters yields improvements
on all distance thresholds compared to dynamic queue performance, while for ση′>1000 m, the
performance on smaller scales improved at the cost of drop in accuracy on the large scales.
15
Table 5: Comparison of the performance of our dynamic queue strategy with the static queue. We also perform
an ablation on ση′used to add gaussian noise to the dynamic queue. The addition of noise to GPS coordinates
of dynamic queue yields further gains in performance.
MethodStreet City Region Country Continent
1km 25km 200km 750km 2500 km
Static Queue 5.93 31.8 49.02 67.42 82.22
Dynamic Queue 7.55 32.91 49.28 67.46 82.48
Dynamic Queue + noise ( ση′= 1000 m) 9.84 33.10 49.32 69.07 83.35
Dynamic Queue + noise ( ση′= 5000 m) 11.85 33.07 48.90 67.60 82.87
Dynamic Queue + noise ( ση′= 10000 m) 11.53 33.51 48.63 67.55 82.66
Dynamic Queue + noise ( ση′= 25000 m) 10.01 32.85 47.76 67.22 82.46
3.5 σfor Random Fourier Features
We employed positional encoding with Random Fourier features (RFF) in our location encoder
(discussed in Sec. 3.1.1) to overcome the spectral bias (favoring low-frequency) of direct usage of
MLPs. In RFF, the σvalue plays an important role in determining the range of frequencies. In an
ideal scenario where a single metric is being optimized, a specific optimal sigma value could be
determined through a hyperparameter search. However, in our task, we evaluate our method on five
different distance metrics. Based on our extensive experiments when searching for an optimal σvalue
on a particular metric, we found a particular pattern.
In Figure 2, we train our GeoCLIP model by varying the σvalue on a wide range (from 20to212), and
evaluate their performance on different threshold metrics. We observe an interesting trend. A single
σvalue does not perform best for all the metrics. We observe that higher σvalues are preferable for
fine-grained scales ( 1km and 25km). Conversely, for coarser scales ( 200km,750km, and 2500 km),
lower σvalues perform better than their higher σcounterparts.
Based on the above observations, a simple approach would be to select intermediate σvalues to
achieve reasonably good performance across multiple scales. Hence, we showed results of our method
on the intermediate σvalue in Table 2(a) in the main paper. However, this strategy is suboptimal.
Our proposed hierarchical strategy outperforms the single σvalue approach when optimizing metrics
at different scales (Table 2(b) in the main paper), emphasizing the combination of hierarchies being
better than their individual parts.
3.6 Number of hierarchies ( M)
In Table 2(b) of the main paper, we demonstrate the importance of hierarchical learning to perform
well across all the distance metrics. In Table 6, we further conduct experiments with varying numbers
of hierarchies ( M) to identify the optimal Mthat maximizes performance across all metrics. We
observe that increasing the number of hierarchies leads to improved model performance up to three
hierarchies. However, beyond that, increasing the hierarchies leads to a slight decrease in performance.
Hence, in the main paper, we utilized three hierarchies in our GeoCLIP model.
Table 6: Performance of GeoCLIP when the number of hierarchies is varied.
Number of hierarchiesStreet City Region Country Continent
1km 25km 200km 750km 2500 km
1 9.84 33.1 49.28 69.07 83.35
2 13.85 32.9 49.3 69.3 83.52
3 14.11 34.47 50.65 69.67 83.82
4 14.01 34.33 50.52 69.54 83.22
5 13.98 34.48 49.88 68.9 82.85
16
21 232527 29211
=2i
246810121416Accuracy (%)
Accuracy at 1 km
21 232527 29211
=2i
2628303234Accuracy (%)
Accuracy at 25 km
21 232527 29211
=2i
434445464748495051Accuracy (%)
Accuracy at 200 km
21 232527 29211
=2i
6264666870Accuracy (%)
Accuracy at 750 km
21 232527 29211
=2i
798081828384Accuracy (%)
Accuracy at 2500 kmFigure 2: Ablation on different σvalues (single hierarchy) used to generate the Random Fourier Features. We
train our model by varying σfrom 20to212and measure the performance across different distance thresholds.
The trend is shown by dashed lines. Higher σvalues are preferred on smaller scales ( 1km and 25km) while
larger scales ( 200km,750km and 2500 km) performs well with smaller σvalues.
17
4 Qualitative Demonstration
In this section, we qualitatively demonstrate the effectiveness of our method. We first visually
demonstrate the geo-localization by different hierarchies of our location encoder. In the subsequent
subsections, we show the results of GeoCLIP on both the image query and text query. Finally, we
provide a visualization of the distribution of correct predictions of our model and compare them with
the actual test dataset distributions.
4.1 Hierarchical learning in our location encoder L(·)
3 Level Hierarchy 
Query: “Thailand” 2 Level Hierarchy 1 Level Hierarchy 
Query: “India” 
Query: “South Africa” 
Figure 3: Visualization of the geo-localization of different places mentioned in the query. For each query, we
show the heatmap of predictions using 1-level, 2-level, and 3-level hierarchies. The 1-level hierarchy performs
localization at a coarse resolution while increasing the hierarchies helps in localizing more precisely.
18
4.2 GeoCLIP with Image Query
Images Error ≦
1km 
25km 
200km 
750km 
2500km 
Figure 4: Sample query images from Im2GPS3k dataset [ 7] on which GeoCLIP localizes within an error of
1km (1st row), 25km (2nd row), 200km (3rd row), 750km (4th row) and 2500 km (last row).
19
4.3 Distribution of correct predictions of GeoCLIP on different datasets
Im2GPS3k YFCC26k 
2500 km 
750 km 
200 km 
25 km 
1 km 
GWS15k 
Dataset 
Distribution 
Accuracy 
Figure 5: Demonstration of the distribution of correct predictions of GeoCLIP at different threshold metrics on
various benchmark datasets. Our predictions match closely with the dataset distribution.
4.4 GeoCLIP with Text Query
Specific Landmarks and Places:
Query: “Pyramids of Giza” Query: “Burj Khalifa” Query: “Great Wall of China” 
Figure 6: Geo-localization using GeoCLIP with text query as specific landmarks.
20
(a)Text Query: “Boston, Massachusetts"
(b)Text Query: “Colosseum, Rome"
(c)Text Query: “Niagara Falls"
Figure 7: Across different text queries ranging from city to specific places, we demonstrate the geo-localization
using our method GeoCLIP on the left, while the original region is shown on the right.
21
GEO-Bench:
Toward Foundation Models for Earth Monitoring
Alexandre Lacoste∗1Nils Lehmann∗2Pau Rodriguez1Evan David Sherwin3
Hannah Kerner4Bj¨orn L ¨utjens5Jeremy Irvin3David Dao6
Hamed Alemohammad7Alexandre Drouin1,8Mehmet Gunturkun1Gabriel Huang1,9
David V azquez1Dava Newman5Y oshua Bengio8,9Stefano Ermon3
Xiao Xiang Zhu2
1ServiceNow Research2Technical University of Munich3Stanford University
4Arizona State University5MIT6ETH Zurich7Clark University
8Mila-Quebec9University of Montreal
Abstract
Recent progress in self-supervision has shown that pre-training large neural networks
on vast amounts of unsupervised data can lead to substantial increases in generalization
to downstream tasks. Such models, recently coined foundation models , have been
transformational to the field of natural language processing. V ariants have also been
proposed for image data, but their applicability to remote sensing tasks is limited. To
stimulate the development of foundation models for Earth monitoring, we propose
a benchmark comprised of six classification and six segmentation tasks, which were
carefully curated and adapted to be both relevant to the field and well-suited for model
evaluation. We accompany this benchmark with a robust methodology for evaluating
models and reporting aggregated results to enable a reliable assessment of progress.
Finally, we report results for 20 baselines to gain information about the performance
of existing models. We believe that this benchmark will be a driver of progress across
a variety of Earth monitoring tasks.
1 Introduction
Earth monitoring with machine learning-based methods plays an increasing role in climate change
mitigation and adaptation as well as climate science Rolnick et al. [2019]. Related applications include
methane source detection Sheng et al. [2020], Dileep et al. [2020], forest carbon quantification L ¨utjens
et al. [2019], extreme weather prediction McGovern et al. [2017], and crop monitoring Kerner et al. [2020],
Dado et al. [2020]. Across many of these applications, pre-trained models (e.g., a ResNet trained on
ImageNet) have been used to increase generalisation performance. Improvement of the pre-trained models
has been shown to reduce the need for large labelled datasets in some contexts Chen et al. [2020] and
can improve model generalisation outside of the training distribution Hendrycks et al. [2019]. Recent
studies exploring the scaling of such pre-trained models found that increasing the size of an unsupervised
(or weakly supervised) dataset as well as properly scaling the model led to an even greater increase in
performance under various metrics Kaplan et al. [2020], Radford et al. [2021].
While the training of such large-scale models is usually reserved for industrial research groups with very
large computer clusters, the publication of pre-trained models creates vast opportunities for the entire
research and technology community (including communities of domain experts outside of machine
learning). These large pre-trained models were recently coined as foundation models Bommasani et al.
[2021] as they might serve as foundations for sub-fields of machine learning. Specifically, the publication
of large pre-trained models like BERT Devlin et al. [2018] and GPT-3 Brown et al. [2020] led to a
37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.arXiv:2306.03831v2  [cs.LG]  23 Dec 2023
paradigm shift in the field of natural language processing (NLP). This inspired a similar shift in the field of
computer vision with the release of models like CLIP Radford et al. [2021] and DINO Caron et al. [2021].
While CLIP performs well on various types of vision tasks, it still under-performs on Earth monitoring
tasks Radford et al. [2021]. This is not surprising as it is trained mainly on RGB images taken from a
ground perspective at a single point in time.
While there are many similarities between Earth observation datasets and typical ML image datasets, there
are also many important differences to consider when designing effective ML models. Earth observation
images are taken from an overhead rather than ground perspective, usually from a fixed distance from
the Earth’s surface (defined by a satellite’s orbit). The satellite revisits provide a temporal axis that is
sometimes irregular (e.g., a few times per year) or regular (e.g., every five days) with cloud coverage
causing spurious occlusions. Images are acquired with sensors containing multiple spectral bands (e.g.,
thirteen for Sentinel-2), or even with different kinds of sensors, e.g., synthetic aperture radar (SAR), which
can penetrate cloud coverage. Moreover, the GPS coordinates and timestamp of each acquisition offer
the opportunity to combine data from multiple sources, e.g., weather data, semantic maps, and elevation.
This leads to a rich multi-modal signal with potentially missing information that can be inferred from other
elements of the signal. There are currently petabytes of accessible satellite datasets containing images of the
Earth under various modalities from the present day to as far back as the 1960s. Distilling this large amount
of information into pre-trained models of various sizes offers the opportunity to redistribute this information
and make it accessible to various labs for increasing the performances on a large range of downstream tasks.
The fundamental goal of these large pre-trained models is to improve generalization performance on
downstream tasks. Hence, to support the machine learning community in producing better pre-trained
models, it is crucial to provide a benchmark with a wide variety of downstream tasks, covering a range
of modalities and dataset shapes that are likely to be encountered in practice. At the moment, existing
works on pre-training models from earth observations e.g., Cong et al. [2022], Manas et al. [2021],
Wang et al. [2022], evaluate on different sets of downstream tasks, making it impossible to directly
compare performance. Moreover, the set of tasks is often narrow in terms of diversity and the statistical
methodologies do not adequately report the uncertainties in the evaluation.
The present work aims to fill this void by providing a wide range of tasks across various countries with vari-
ous modalities of sensors. Also, the transformed versions of the datasets are smaller than their original form,
and all results can be replicated on single GPUs. This increases accessibility to research labs with limited
resources and reduces overall energy consumption. Our proposed benchmark, GEO-Bench1, is composed
of six image classification and six semantic segmentation tasks, which were curated by domain experts
to ensure their diversity and relevance toward sustainable development. We expect this contribution to:
• Stimulate and facilitate the development of foundation models for Earth monitoring
• Provide a systematic way of measuring the quality of models for better scientific progress
• Provide insights into which pre-trained models work best
•Potentially reduces negative impacts of foundation models through an open evaluation procedure.
In what follows, we start by discussing sources of data that can serve to train foundation models for
earth monitoring (Sec. 2). We then present the details of GEO-Bench (Sec. 3) and how it can be used
for the evaluation of foundation models (Sec. 4). Further, we review existing benchmark datasets for earth
monitoring and discuss why GEO-Bench is complementary (Sec. 5). Finally, we present an extensive
set of experiments, showing the performance of 20 state-of-the-art models on the benchmark to lay down
reference points and to gain valuable information on existing pre-trained models (Sec. 6).
2 Remote sensing data for self-supervision
The development of foundation models does not typically rely on a specific dataset for the pre-training phase.
The choice of data source is part of the design of the model, e.g., a very large corpus of text from the internet
Mitchell et al. [2018] or pairs of text associated with images from the web Radford et al. [2021]. As such,
we do not provide data for training foundation models with this benchmark. However, for completeness, we
outline potential sources of Earth observation data that could be used for pre-training foundation models.
Multispectral images with revisits Satellite data sources such as Sentinel-2 Drusch et al. [2012], ESA
[2021] and Landsat 8 USGS [2021] provide images in multiple spectral bands with periodic revisits. This
1https://zenodo.org/communities/geo-bench
2
Change
Semanticfine-tuneself-supervise
Forest carbon quantification
Cattle counting
Filling data gaps
Cloud segmentation
Land cover classification
Methane plume detectionLargePretrainedModel
Visual
SAR
MultispectralFigure 1: Foundation models encapsulate multimodal data streams through self-supervised training.
The trained models can then be fine-tuned for a variety of climate-related remote sensing tasks. Image
sources: quantification L ¨utjens et al. [2019], detection Jongaramrungruang et al. [2021], generation L ¨utjens
et al. [2021], counting Laradji et al. [2020], segmentation Zantedeschi et al. [2019], and multi-class
classification Pallai and Wesson [2017].
yields a four-dimensional array of structured data (longitude, latitude, wavelength, time) which can be used
to perform various forms of self-supervision, e.g., predicting adjacent tiles Jean et al. [2019] or contrasting
the different seasons for the same region Manas et al. [2021].
Other sensors Synthetic Aperture Radar (SAR) and terrain elevation are also frequently available and can
be matched to other sources of data through geolocalisation Pepin et al. [2020]. Such data are complemen-
tary to optical spectral bands and may encourage the model to learn higher-level semantic representations.
Semantic data Through georeferencing, text-based data such as Wikipedia articles can be linked to
satellite images Uzkent et al. [2019]. It is also possible to join content from non-image data layers like
OpenStreetMap Li et al. [2020]. By predicting or contrasting information from these sources, the model
may learn useful and transferable semantic representations.
3 GEO-Bench
GEO-Bench is composed of 6 classification tasks and 6 segmentation tasks. Detailed characteristics are
presented in Table 1, examples are depicted in Figure 2 and 3, and the spatial coverage on the world map is
presented in Figure 8 (supplementary material). In what follows, we describe the procedure for collecting
and transforming the datasets.
3.1 Design Principles
GEO-Bench was established by modifying and gathering geospatial datasets, adhering to principles that
secure accessibility, usability, and effective model performance assessment across tasks.
Ease of Use A fundamental goal was to create an accessible, simple-to-use benchmark, and a compact
dataset assortment with code for loading the data in a consistent schema. A key aim was to harmonize
data to reduce the engineering work needed to tailor pre-trained architectures, while maintaining sensor
type and resolution diversity.
Sector Experts and Steering Committee To align GEO-Bench with practical use-cases, we assembled
a team of six sector experts from fields such as forestry and climate science. A steering committee of
respected scientists guides high-level benchmark decisions, assuring relevance and impact.
Diversity of Modalities The objective is to evaluate model adaptability to varied geospatial sensors.
Thus, the benchmark encompasses multispectral, SAR, hyperspectral, elevation, and cloud probability
modalities, with spatial resolutions from 0.1 to 30 m/pixel.
Diversity of Tasks We ventured beyond image classification, incorporating object detection and
semantic segmentation. To maintain ease of use , detection and counting tasks were transformed into
semantic segmentation. This led to two task sets: six image classification tasks, and six semantic
segmentation tasks Felzenszwalb et al. [2010], Lempitsky and Zisserman [2010].
Original Train, Validation, and Test Splits Original dataset splits were preserved when available;
otherwise, we generated validation and test sets from the train set while ensuring no spatial overlap.
3
Permissive License Most datasets needed to be adapted from their original form to satisfy the above
criteria and be included in the benchmark. Hence, we include only datasets with permissive licenses.
Classification
Name Image Size # Classes Train V al Test # Bands RGB res Sensors Cite License
m-bigearthnet 120 x 120 43 20000 1000 1000 12 10.0 Sentinel-2 Sumbul et al. [2021] CDLA-P-1.0
m-so2sat 32 x 32 17 19992 986 986 18 10.0 Sentinel-2
+ Sentinel-1Zhu et al. [2019] CC-BY -4.0
m-brick-kiln 64 x 64 2 15063 999 999 13 10.0 Sentinel-2 Lee et al. [2021] CC-BY -SA 4.0
m-forestnet 332 x 332 12 6464 989 993 6 15.0 Landsat-8 Irvin et al. [2020] CC-BY -4.0
m-eurosat 64 x 64 10 2000 1000 1000 13 10.0 Sentinel-2 Helber et al. [2019] MIT
m-pv4ger 320 x 320 2 11814 999 999 3 0.1 RGB Mayer et al. [2022] MIT
Segmentation
Name Image Size # Classes Train V al Test # Bands RGB res Sensors Cite License
m-pv4ger-seg 320 x 320 2 3000 403 403 3 0.1 RGB Mayer et al. [2022] MIT
m-chesapeake-landcover 256 x 256 7 3000 1000 1000 4 1.0 RGBN Robinson et al. [2019] CDLA-P-1.0
m-cashew-plantation 256 x 256 7 1350 400 50 13 10.0 Sentinel-2 Z. et al. [2021] CC-BY -4.0
m-SA-crop-type 256 x 256 10 3000 1000 1000 13 10.0 Sentinel-2 link CC-BY -4.0
m-nz-cattle 500 x 500 2 524 66 65 3 0.1 RGB Abuaiadah and Switzer [2022] CC-BY -4.0
m-NeonTree 400 x 400 2 270 94 93 5 0.1 RGB
+ Hyperspectral
+ ElevationWeinstein et al. [2021] CC0 1.0
Table 1: GEO-Bench: Characteristics of datasets in the benchmark. Since datasets are modified , we
prepend their name with “m-” to distinguish them from the original dataset.
Figure 2: Representative samples of the classification benchmark .
Figure 3: Representative samples of the segmentation benchmark .
3.2 Dataset Transformations
To produce a benchmark that complies with the design choices of Section 3.1, we applied the following
transformations to each dataset. The procedure that was used to download and transform each dataset
is fully documented and open-sourced in the GEO-Bench GitHub repository2.
2https://github.com/ServiceNow/geo-bench
4
Subsampling Large Datasets To be more representative of typical downstream tasks, where data is
usually scarce, datasets larger than 20000 samples were randomly subsampled. Avoiding large downstream
tasks also comes with other benefits:
•In Appendix A, we show that larger downstream datasets can decrease the ability to discriminate
between two models that are similar in performance.
•Downstream tasks with very large training sets will not usually benefit from pre-training3. Hence
they are less useful for our evaluation purpose.
•A smaller benchmark is faster to download, yields results quicker and requires less energy for
computation.
•We can increase the variety of experiments and the number of seeds to improve the knowledge
gained from experiments.
Removing Class Imbalance We randomly subsampled large classes to have near-uniform class
sizes across datasets. This was done to prevent users of the benchmark from increasing their score by
using clever class imbalance techniques instead of making progress on better pre-trained models. While
good performance on highly imbalanced (long tail of classes) datasets would be a desired property of
a pre-trained model, we have not found a good dataset containing a large number of classes.
4 Using The Benchmark
Fine Tuning In the self-supervised learning literature, it is common to use the pre-trained model to
encode a fixed representation of each image in the dataset and learn to classify images based on this
representation Jean et al. [2019]. While this works relatively well, this method highly depends on the
pre-training task as it may not learn to encode information that is important for the downstream task Tian
et al. [2020], Penatti et al. [2015]. In practice, fine-tuning the pre-trained model often mitigates this issue
and is known to frequently yield a much higher generalization performance than a model trained from
random weights Manas et al. [2021], Chen et al. [2020]. Since this is more representative of practical
usage, we encourage users of the benchmark to report the results of fine-tuned models. On the other hand,
we do not discourage users from also reporting results with fixed backbones (pre-trained weights) as this
can provide valuable information about the pre-trained model. In all cases, we ask users to report their
fine-tuning methodology with enough details for reproducibility.
Hyperparameter Tuning Deep learning algorithms often require the adjustment of hyperparameters,
especially when an architecture is fine-tuned on a small dataset. For this reason, we recommend adjusting
hyperparameters, but within a maximum budget of 16 trials per task4. Early stopping based on validation
metrics is also recommended.
Data Augmentation Data augmentation plays a crucial role in the training of deep learning models,
especially with small training datasets. Hence, we consider it to be part of the fine-tuning process. As a
guideline, we propose limiting the augmentations to 90◦rotations and vertical and horizontal flips5. On the
other hand, we also encourage users to study what are the best data augmentations for remote sensing as this
could lead to useful findings for practitioners and the benchmark is well-suited for evaluating such findings.
Toolbox To facilitate the usage of the benchmark, we provide a collection of tools for various parts
of the experimental pipeline as part of the open-sourced codebase6. This includes tools for loading datasets
and visualising results. We also provide tools based on PyTorch-Lightning Falcon and The PyTorch
Lightning team [2019] to facilitate model training.
3From Bayes rule, we know that the influence of the prior (pre-trained model) decreases as the size of the training
data increases.
4While 16 is fairly small, we believe it’s enough to adjust sensitive hyperparameters such as learning rate. Also,
this favours models that are less sensitive to hyperparameter tuning.
5Random crop and resize are also common in vision, but in remote sensing, this reduces the spatial resolution,
which is often crucial for high performances.
6https://github.com/ServiceNow/geo-bench
5
4.1 Reporting Results
For reliable and comparable results across different publications, we recommend that users follow this
procedure to report results. The aim is to report results on individual tasks as well as aggregated across all
tasks, with reliable confidence intervals (inspired by Agarwal et al. [2021]). Code is provided to generate
figures based on raw results.
Random Seeds As demonstrated in Agarwal et al. [2021], 3-5 seeds are not enough to obtain reliable con-
fidence intervals. Since pre-training and hyperparameter search are usually the computational bottlenecks,
we recommend retraining the selected hyperparameter configuration for at least 10 different seeds.
Interquartile Mean (IQM) We recommend using IQM. This metric removes the outliers by trimming
the 25% highest values as well as the 25% lowest value and computing the average of the remaining values.
The resulting finite sample estimator is less biased than the median and has less variance than the mean,
often resulting in smaller confidence intervals Agarwal et al. [2021].
Normalising Results To aggregate performance metrics across multiple tasks, one must first normalise
their values. A common approach consists of applying a linear transformation based on reference points
Bellemare et al. [2013]. As such, we propose to use the lowest and highest metric values achieved by
a set of strong baselines (see Sec. 6) as official reference points . For each individual task, we scale the
results such that the maximum score is 1 and the lowest one is 0. Hence, if a future model were to achieve
a score superior to 1, it would imply that progress is being made on the benchmark. All reference points
will be published alongside the benchmark.
Bootstrapping To quantify uncertainty over observed IQMs, we use bootstrapping Eforn [1979]. That
is, we sample ntimes, with replacement, the results from training with ndifferent seeds, and we compute
IQM. Repeating this procedure n= 1000 times provides a distribution over IQM results, from which
confidence intervals can be extracted.
Aggregated Results After normalizing the results we simply compute IQM across all datasets and
all results of a given model. For confidence intervals, we use stratified bootstrap , where seeds are sampled
with replacement individually for each dataset, but IQM is computed across all datasets.
Displaying the results In Figure 4, we show how to compactly display results from a wide range
of baselines across the benchmark as well as aggregated results and statistical uncertainties. In Figure 5,
we display the results for a growing training set size (with fixed validation and test set). This compactly
reports the results of thousands of experiments.
Publishing the results We ask experimenters to publish the results of all seeds on all datasets for all
models as a CSV file along with the open-sourced code of their experiments. This will allow future authors
to incorporate existing results in their comparison figures.
5 Related Works
SustainBench consists of 15 public datasets covering 7 sustainable development goals Y eh et al. [2021].
Seven of these datasets are two-dimensional remote sensing. It includes a public leaderboard for tracking
model performance. A featured task is the Brick Kiln classification, selected for its georeferenced, high-
quality ground truth labels. SustainBench’s purpose is monitoring progress in specified tasks, thus compris-
ing a diverse set of datasets. It doesn’t aim for solution under a single framework or aggregate result tracking.
TorchGeo is a Python library designed to streamline the integration of remote sensing datasets into
the PyTorch deep learning ecosystem Stewart et al. [2021]. TorchGeo currently features data loaders for
52 publicly available datasets of satellite, aerial, and drone imagery for classification, regression, change
detection, semantic segmentation, instance segmentation, and object detection tasks. Our benchmark
directly interfaces with TorchGeo and uses its data loaders for several datasets included in the benchmark.
EarthNets is a concurrently developed platform to evaluate deep learning methods on remote sensing
datasets Xiong et al. [2022]. In their methodology, they analyse the metadata of 400 publicly available
remote sensing datasets. Using meta-information such as the number of samples, the size of each sample,
and the type of annotations, they analyse the correlation between each dataset and identify a variety of
6
clusters. Based on this analysis, they recommend two classification, two segmentation, and two detection
datasets for benchmarking. In contrast, we provide a collection of 12 datasets and we propose a robust
methodology for aggregating results and reporting statistical uncertainties of the evaluation process.
AiTLAS recently proposed a benchmark of 22 classification datasetsDimitrovski et al. [2023], 3 of
which intersect with our classification benchmark. They proposed a standardised version of train, valid,
test splits for existing datasets as well as a fine-tuning procedure. By leveraging the overlap of labels
across datasets, they also provide a more accurate test metric for real-world applications. Experiments
are conducted using 10 different model families across the 22 datasets, using RGB images as input.
6 Experiments
aggregated0.000.200.400.600.801.00normalized test metric
m-bigearthnet m-brick-kiln m-eurosatResNet18-Rnd
ResNet18-timmResNet18-MoCo-S2
ResNet50-SECO-S2ResNet50-MoCo-S2
ResNet50-timmConvNeXt-B-timm
ViT-T-timmViT-S-timm
SwinV2-T-timm
m-forestnet m-pv4ger m-so2sat
Figure 4: Classification Benchmark RGB Only: Normalised accuracies of various baselines (higher
is better). Violin plots are obtained from bootstrap samples of normalized IQM (Section 4.1). The left
plot reports the average across all tasks.
102
101
100
aggregated0.5
0.00.51.0normalized test metric
102
101
100
m-bigearthnet102
101
100
m-brick-kiln102
101
100
m-eurosatResNet18-Rnd
ResNet50-timmConvNeXt-B-timm
ViT-T-timmViT-S-timm
SwinV2-T-timm
102
101
100
m-forestnet102
101
100
m-pv4ger102
101
100
m-so2sat
Figure 5: Classification vs Train Size: Normalised accuracies of a subset of the baselines on Classification
benchmark with a growing size of the training set. The shaded region represents an 80% confidence
interval, obtained from bootstrap samples of normalized IQM (Section 4.1). The left plot reports the
average across all tasks.
aggregated0.400.500.600.700.800.90normalized test metric
m-bigearthnet m-brick-kilnResNet50-timm
ResNet50-timm+R-MultiResNet50-MoCo-S2-multi
ResNet50-DINO-S2-multi
m-eurosat m-so2sat
Figure 6: Effect of Multispectral with
ResNet50 . Only Sentinel-2 tasks are reported.
Normalised accuracies (Sec 4.1).
aggregated-0.200.000.200.400.600.80normalized test metric
m-bigearthnet m-brick-kilnViT-S-timm
ViT-S-timm+R-MultiViT-S-MoCo-S2-multi
ViT-S-DINO-S2-multi
m-eurosat m-so2satFigure 7: Effect of Multispectral with ViT-S .
Only Sentinel-2 tasks are reported. Normalised
accuracies (Sec 4.1).
7
In this section, we provide a range of baselines for the classification and segmentation benchmarks. These
will serve as reference points for future evaluation7. We also seek to answer the following questions:
• Which new architecture performs best on remote sensing data (Section 6.2.2)?
• What is the effect of training set size on the performance of each model (Section 6.2.3)?
• Can we leverage multispectral channels to improve performance (Section 6.2.4)?
•Are smaller datasets better at discriminating the performance of different models (Section A.5)?
6.1 Protocol
For each model, we replaced the last layer with a randomly initialised layer of the appropriate shape for the
task at hand. We use different learning rates for the last layer (which starts from random weights) and for the
backbone (which starts from pre-trained weights). The best learning rates were selected using the highest ac-
curacy or Intersection over Union (IoU) on the validation set over 16 trials8. After choosing the hyperparam-
eters, we repeated the training for 10 seeds. To minimize overfitting, we selected the best time step using ac-
curacy (or IoU) on the validation set and we reported the test metrics at the chosen time step. We use AdamW
Loshchilov and Hutter [2017] to train convolution architectures and SGD to train transformer architectures.
6.2 Classification
6.2.1 Baselines Naming Schema
Each baseline name starts with the corresponding architecture: ResNet18 and ResNet50: standard ResNet
architectures He et al. [2016]; ConvNeXt-B: the base architecture of ConvNeXt Liu et al. [2022b]; ViT-T
and ViT-S: ViT architectures Dosovitskiy et al. [2020] of size tiny and small respectively; SwinV2-T:
a SwinV2-tiny architecture Liu et al. [2022a];
Then, keywords provide details about the training procedure: SeCo: a ResNet50 model trained on Sentinel
2 data with temporal contrastive loss across seasons Manas et al. [2021]; MoCo-S2 and DINO-S2:
model trained with self-supervision on Sentinel data Wang et al. (RGB and Multispectral pre-trained
weights); Rnd: weights are randomly initialised; timm: pre-trained weights are obtained from the timm
library, usually from training on ImageNet; +R-Multi: we manually augment an RGB architecture by
randomly initialising the weights of the missing channels in the 1st layer; multi: the pre-trained model
has multispectral channels.
6.2.2 Comparing Baselines on RGB only
In Figure 4, we report bootstrapped IQM of the normalized accuracy (Sec 4.1) for the six datasets of the
classification benchmark, as well as aggregated results9. In this first experiment, all models can only see
the RGB channels.
These results offer valuable information across 10 common baselines in the literature. We denote the
outstanding performance of ConvNext and SwinV2 compared to other models. It is by a large margin the
best models in aggregated results and almost systematically outperforms all models on all datasets. We can
also observe the large difference between Scratch ResNet18 and ResNet18 on all datasets. This highlights
the importance of using a pre-trained model. Also, perhaps disappointingly, the existing model pre-trained
on remote sensing data does not exhibit any improvement compared to their timm pre-trained weights,
i.e., ResNet18-MoCo-S2, ResNet50-MoCo-S2, and ResNet50-SeCo-S2 are all comparable to ResNet18
on the aggregated performance. On the other hand, in Section 6.2.4, we see that ResNet50-MoCo-S2-multi
can leverage multispectral data to slightly surpass ResNet50-timm.
Another insight that can be gained from these results is how useful a dataset is at discriminating baselines,
i.e., a dataset where most baselines perform equally would have limited utility in our benchmark. To this
end, we had to discard GeoLifeClef 2022 Cole et al. [2020] as all models were performing equally badly10.
7We recall that all datasets have been modified from their original version. Hence, our results are not directly
comparable to other published results.
8The range of selected learning rates is different for each model and is selected based on early experiments, see
appendix for details.
9We note that the variance of the results represents the uncertainty of the mean (IQM) which is significantly
smaller than the variance of the raw seeds presented in Figure 9 in Appendix.
10We suspect this dataset to have high aleatoric uncertainties.
8
m-eurosat also offers limited discriminativity as most models obtain very high accuracy (see Figure 9).
To make this dataset harder, we subsample down to 2000 training samples. We can now see that smaller
models tend to perform better on this dataset, but the discriminativity remains fairly low.
6.2.3 Accuracy vs training set size
As part of the benchmark, we also provide official subsets of the training sets with train ratios of (0.01,
0.02, 0.05, 0.1, 0.2, 0.5, 1)11.
Figure 5 depicts a different perspective on the models. First, we can observe the noise due to the hyperparam-
eter selection process that is not accounted for by repeating 10 seeds with fixed hyperparameters. Also, we
see that ConvNeXt often becomes better than SwinV2 as the training set decreases. This coincides with the
common observations that transformer architectures tend to be more data-hungry, but also tend to outperform
convolution architectures in the high data regime Dosovitskiy et al. [2010]. We note also, that ConvNeXt-
B-timm only requires 2% of the training set to obtain aggregated performances comparable to that of
ResNet18-Rnd. This impressive factor of 50x on data efficiency highlights the importance of developing
new architectures and new pre-training methods. Finally, we can observe an increase in the discriminativity
of the datasets as the training set decreases, specifically for m-eurosat, when the task becomes more difficult,
the strong baselines stand out even more. The discriminativity of datasets is further studied in Section A.5.
6.2.4 Leveraging Multispectral Information
We now study the effect of leveraging multispectral information during the pre-training phase and during
the fine-tuning phase. We do so by fixing the backbone to either ResNet50 (Fig. 6) or ViT-S (Fig. 7) and
exploring various weight initialisation schema. Since we could only find pre-trained models for Sentinel-2,
we limit this experiment to the four datasets satisfying this criterion.
We found that using a model pre-trained on RGB-only (timm pre-trained) and augmenting the architecture
by randomly initialising the weights of the missing channels in the first layer (+RMulti) does not lead
to systematic improvement. Moreover, the fine-tuning time is largely extended since we have to wait until
the newly initialised weights on the first layer fully converge. On the other hand, the ResNet50 pre-trained
on Sentinel-2 using DINO or MoCo Wang et al. leads to a modest performance increase on average. When
looking at ViT-S (Fig. 7), incorporating multi-spectral only leads to a systematic performance decrease.
6.3 Segmentation
We defer experiments on the Segmentation benchmark to Appendix A.3, where we provide experiments on
six baselines (ResNet18, ResNet50, ResNet101) ×(U-Net, DeepLabV3) with pre-trained weights provided
by the timm library. While ResNet101-DeepLabV3 performs best in aggregate, it still underperforms
on some datasets.
6.4 Resource Usage
See Appendix A.6 for detailed resource usage of each algorithm evaluated in this section. We report the
number of parameters, memory usage, the time required for a forward pass, and the convergence time
for fine-tuning on downstream tasks. While memory footprint can increase by a factor of 4x for a model
like SwinV2 and ConvNeXt-B compared to ResNet50, their forward pass is only twice as slow.
7 Conclusion
We developed a new benchmark for evaluating pre-trained models on remote sensing downstream tasks.
This involves adapting a variety of remote sensing datasets to a more conventional machine learning
pipeline and providing code for fine-tuning and evaluating individual tasks. We expect that this benchmark
will stimulate the development of new foundation models that could lead to better generalization on a
variety of earth monitoring downstream tasks and could open up opportunities for new applications.
11Reporting results on all 7 subsets increases the number of experiments by 7x. However, in Figure 13 (see
Appendix), we show that the convergence time is proportional to the training set size. This means that training on
all seven subsets takes on average about 1.88 times longer than just training on the full training set.
9
Limitations Our benchmark does not extensively evaluate all desired features of a pre-trained model
for earth monitoring. For example, it does not evaluate its ability to fine-tune temporal data nor perform
fusion with other types of data such as text or weather. The spatial coverage of the benchmark covers
most continents and improves coverage over individual datasets. However, the spatial coverage could
still be largely improved to include a much wider range of countries and biomes. Finally, as pre-trained
models become stronger, they will get closer to the theoretical limit of generalization performance, i.e.
approaching the aleatoric uncertainty of the dataset. Under such a regime, we expect a bigger overlap
between error bars when comparing 2 different models.
References
Diab Abuaiadah and Alexander Switzer. Remote sensing dataset for detecting cows from high resolution aerial images.
2022.
Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement
learning at the edge of the statistical precipice. Advances in neural information processing systems , 34:29304–29320,
2021.
Hamed Alemohammad. The case for open-access ML-ready geospatial training data. In International Geoscience
and Remote Sensing Symposium . IEEE, 2021.
Marc G Bellemare, Y avar Naddaf, Joel V eness, and Michael Bowling. The arcade learning environment: An evaluation
platform for general agents. Journal of Artificial Intelligence Research , 47:253–279, 2013.
Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic
parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability,
and Transparency , pages 610–623, 2021.
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,
Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models.
arXiv preprint arXiv:2108.07258 , 2021.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 , 2020.
Marshall Burke, Anne Driscoll, David B Lobell, and Stefano Ermon. Using satellite imagery to understand and
promote sustainable development. Science , 371(6535), 2021.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.
Emerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294 , 2021.
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for
semantic image segmentation. arXiv preprint arXiv:1706.05587 , 2017.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on machine learning , pages 1597–1607. PMLR, 2020.
Elijah Cole, Benjamin Deneu, Titouan Lorieul, Maximilien Servajean, Christophe Botella, Dan Morris, Nebojsa Jojic,
Pierre Bonnet, and Alexis Joly. The geolifeclef 2020 dataset. arXiv preprint arXiv:2004.04192 , 2020.
Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Y utong He, Marshall Burke, David B Lobell,
and Stefano Ermon. Satmae: Pre-training transformers for temporal and multi-spectral satellite imagery. arXiv
preprint arXiv:2207.08051 , 2022.
Walter T Dado, Jillian M Deines, Rinkal Patel, Sang-Zi Liang, and David B Lobell. High-resolution soybean yield
mapping across the us midwest using subfield harvester data. Remote Sensing , 12(21):3471, 2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Sonu Dileep, Daniel Zimmerle, J Ross Beveridge, and Timothy V aughn. Automated identification of oil field features
using cnns. 2020.
Ivica Dimitrovski, Ivan Kitanovski, Dragi Kocev, and Nikola Simidjievski. Current trends in deep learning for earth
observation: An open-source benchmark arena for image classification. ISPRS Journal of Photogrammetry and
Remote Sensing , 197:18–35, 2023.
10
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arxiv 2020. arXiv preprint arXiv:2010.11929 , 2010.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Matthias Drusch, Umberto Del Bello, S ´ebastien Carlier, Olivier Colin, V eronica Fernandez, Ferran Gascon, Bianca
Hoersch, Claudia Isola, Paolo Laberinti, Philippe Martimort, et al. Sentinel-2: Esa’s optical high-resolution mission
for gmes operational services. Remote sensing of Environment , 120:25–36, 2012.
B Eforn. Bootstrap methods: another look at the jackknife. The Annals of Statistics , 7:1–26, 1979.
EPA. Greenhouse Gas Emissions: Understanding Global Warming Potentials. Technical report, US
Environmental Protection Agency, February 2017. URL https://www.epa.gov/ghgemissions/
understanding-global-warming-potentials .
ESA. Sentinel-2. Technical report, European Space Agency, Paris, France, 2021. URL https:
//sentinel.esa.int/web/sentinel/missions/sentinel-2 .
William Falcon and The PyTorch Lightning team. PyTorch Lightning, 3 2019. URL https:
//github.com/Lightning-AI/lightning .
Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively
trained part-based models. IEEE transactions on pattern analysis and machine intelligence , 32(9):1627–1645, 2010.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth
Observations and Remote Sensing , 2019.
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can improve
model robustness and uncertainty. arXiv preprint arXiv:1906.12340 , 2019.
Jeremy Irvin, Hao Sheng, Neel Ramachandran, Sonja Johnson-Y u, Sharon Zhou, Kyle Story, Rose Rustowicz, Cooper
Elsworth, Kemen Austin, and Andrew Y Ng. Forestnet: Classifying drivers of deforestation in indonesia using
deep learning on satellite imagery. arXiv preprint arXiv:2011.05479 , 2020.
Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, and Stefano Ermon. Tile2vec: Unsupervised
representation learning for spatially distributed data. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 33, pages 3967–3974, 2019.
Forrest Johnson, Andrew Wlazlo, Ryan Keys, Viren Desai, Erin Wetherley, Ryan Calvert, and Elena Berman. Airborne
methane surveys pay for themselves: An economic case study of increased revenue from emissions control. preprint,
Environmental Monitoring, July 2021. URL http://eartharxiv.org/repository/view/2532/ .
Siraput Jongaramrungruang, Christian Frankenberg, Andrew K. Thorpe, and Georgios Matheou. Methanet - an
ai-driven approach to quantifying methane point-source emission from high-resolution 2-d plume imagery. ICML
Workshop on Tackling Climate Change with AI , 2021.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 , 2020.
Hannah Kerner, Gabriel Tseng, Inbal Becker-Reshef, Catherine Nakalembe, Brian Barker, Blake Munshell, Madhava
Paliyam, and Mehdi Hosseini. Rapid response crop maps in data sparse regions. arXiv preprint arXiv:2006.16866 ,
2020.
Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions
of machine learning. arXiv preprint arXiv:1910.09700 , 2019.
Issam Laradji, Pau Rodriguez, Freddie Kalaitzis, David V azquez, Ross Y oung, Ed Davey, and Alexandre
Lacoste. Counting cows: Tracking illegal cattle ranching from high-resolution satellite imagery. arXiv preprint
arXiv:2011.07369 , 2020.
11
Jihyeon Lee, Nina R. Brooks, Fahim Tajwar, Marshall Burke, Stefano Ermon, David B. Lobell, Debashish Biswas,
and Stephen P . Luby. Scalable deep learning to identify brick kilns and aid regulatory capacity. Proceedings
of the National Academy of Sciences , 118(17), 2021. ISSN 0027-8424. doi: 10.1073/pnas.2018863118. URL
https://www.pnas.org/content/118/17/e2018863118 .
Victor Lempitsky and Andrew Zisserman. Learning to count objects in images. Advances in neural information
processing systems , 23, 2010.
Haifeng Li, Xin Dou, Chao Tao, Zhixiang Wu, Jie Chen, Jian Peng, Min Deng, and Ling Zhao. Rsi-cb: A large-scale
remote sensing image classification benchmark using crowdsourced data. Sensors , 20(6):1594, 2020.
Ze Liu, Han Hu, Y utong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Y ue Cao, Zheng Zhang, Li Dong,
et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 12009–12019, 2022a.
Zhuang Liu, Hanzi Mao, Chao-Y uan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet
for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
11976–11986, 2022b.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017.
Bj¨orn L ¨utjens, Lucas Liebenwein, and Katharina Kramer. Machine learning-based estimation of forest carbon stocks
to increase transparency of forest preservation efforts. 2019 NeurIPS Workshop on Tackling Climate Change with
AI (CCAI) , 2019.
Bj¨orn L ¨utjens, Brandon Leshchinskiy, Christian Requena-Mesa, Farrukh Chishtie, Natalia D ´ıaz-Rodr ´ıguez, Oc ´eane
Boulais, Aruna Sankaranarayanan, Aaron Pina, Yarin Gal, Chedy Raissi, Alexander Lavin, and Dava Newman.
Physically-consistent generative adversarial networks for coastal flood visualization. ICML Workshop on AI for
Modeling Oceans and Climate Change (AIMOCC) , 2021.
Lei Ma, Y u Liu, Xueliang Zhang, Y uanxin Y e, Gaofei Yin, and Brian Alan Johnson. Deep learning in remote sensing ap-
plications: A meta-analysis and review. ISPRS journal of photogrammetry and remote sensing , 152:166–177, 2019.
Oscar Manas, Alexandre Lacoste, Xavier Gir ´o-i Nieto, David V azquez, and Pau Rodriguez. Seasonal contrast:
Unsupervised pre-training from uncurated remote sensing data. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 9414–9423, 2021.
M Maskey, H Alemohammad, KJ Murphy, and R Ramachandran. Advancing ai for earth science: A data systems
perspective. Eos, 101, 2020.
Kevin Mayer, Benjamin Rausch, Marie-Louise Arlt, Gunther Gust, Zhecheng Wang, Dirk Neumann, and Ram
Rajagopal. 3d-pv-locator: Large-scale detection of rooftop-mounted photovoltaic systems in 3d. Applied
Energy , 310:118469, 2022. ISSN 0306-2619. doi: https://doi.org/10.1016/j.apenergy.2021.118469. URL
https://www.sciencedirect.com/science/article/pii/S0306261921016937 .
Amy McGovern, Kimberly L. Elmore, David John Gagne, Sue Ellen Haupt, Christopher D. Karstens, Ryan Lagerquist,
Travis Smith, and John K. Williams. Using artificial intelligence to improve real-time decision-making for
high-impact weather. Bulletin of the American Meteorological Society , 98(10), 2017.
T. Mitchell, W. Cohen, E. Hruschka, P . Talukdar, B. Yang, J. Betteridge, A. Carlson, B. Dalvi, M. Gardner, B. Kisiel,
J. Krishnamurthy, N. Lao, K. Mazaitis, T. Mohamed, N. Nakashole, E. Platanios, A. Ritter, M. Samadi, B. Settles,
R. Wang, D. Wijaya, A. Gupta, X. Chen, A. Saparov, M. Greaves, and J. Welling. Never-ending learning.
Communications of the ACM , 61(5):103–115, April 2018. ISSN 0001-0782, 1557-7317. doi: 10.1145/3191513.
URL https://dl.acm.org/doi/10.1145/3191513 .
Cassandra Pallai and Kathryn Wesson. Chesapeake bay program partnership high-resolution land cover classification
accuracy assessment methodology, 2017. URL https://chesapeakeconservancy.org/wp-content/
uploads/2017/01/Chesapeake_Conservancy_Accuracy_Assessment_Methodology.pdf .
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud
Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350 , 2021.
Ot´avio AB Penatti, Keiller Nogueira, and Jefersson A Dos Santos. Do deep features generalize from everyday objects
to remote sensing and aerial scenes domains? In Proceedings of the IEEE conference on computer vision and
pattern recognition workshops , pages 44–51, 2015.
12
Karissa Pepin, Howard A. Zebker, and William Ellsworth. High-Pass Filters to Reduce the Effects of Broad
Atmospheric Contributions in Sbas Inversions: A Case Study in the Delaware Basin. In IGARSS 2020 -
2020 IEEE International Geoscience and Remote Sensing Symposium , pages 1030–1033, Waikoloa, HI,
USA, September 2020. IEEE. ISBN 978-1-72816-374-1. doi: 10.1109/IGARSS39084.2020.9324656. URL
https://ieeexplore.ieee.org/document/9324656/ .
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language
supervision. arXiv preprint arXiv:2103.00020 , 2021.
Caleb Robinson, Le Hou, Kolya Malkin, Rachel Soobitsky, Jacob Czawlytko, Bistra Dilkina, and Nebojsa Jojic. Large
scale high-resolution land cover mapping with multi-resolution data. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition , pages 12726–12735, 2019.
David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, Andrew Slavin
Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, et al. Tackling climate change with
machine learning. arXiv preprint arXiv:1906.05433 , 2019.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In International Conference on Medical image computing and computer-assisted intervention , pages
234–241. Springer, 2015.
Victor Schmidt, Kamal Goyal, Aditya Joshi, Boris Feld, Liam Conell, Nikolas Laskaris, Doug Blank, Jonathan Wilson,
Sorelle Friedler, and Sasha Luccioni. CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning
Computing. 2021. doi: 10.5281/zenodo.4658424.
Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. Communications of the ACM , 63(12):54–63,
2020.
Hao Sheng, Jeremy Irvin, Sasankh Munukutla, Shawn Zhang, Christopher Cross, Kyle Story, Rose Rustowicz, Cooper
Elsworth, Zutao Y ang, Mark Omara, et al. Ognet: Towards a global oil and gas infrastructure database using deep
learning on remotely sensed imagery. arXiv preprint arXiv:2011.07227 , 2020.
Adam J Stewart, Caleb Robinson, Isaac A Corley, Anthony Ortiz, Juan M Lavista Ferres, and Arindam Banerjee.
Torchgeo: deep learning with geospatial data. arXiv preprint arXiv:2111.08872 , 2021.
Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 3645–3650, 2019.
Gencer Sumbul, Arne De Wall, Tristan Kreuziger, Filipe Marcelino, Hugo Costa, Pedro Benevides, Mario Caetano,
Beg¨um Demir, and V olker Markl. Bigearthnet-mm: A large-scale, multimodal, multilabel benchmark archive for
remote sensing image classification and retrieval [software and data sets]. IEEE Geoscience and Remote Sensing
Magazine , 9(3):174–180, 2021.
Y onglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good
views for contrastive learning? Advances in Neural Information Processing Systems , 33:6827–6839, 2020.
USGS. Landsat 8. Technical report, United States Geological Survey, Reston, Virginia, USA, 2021. URL
https://www.usgs.gov/core-science-systems/nli/landsat/landsat-8?qt-science_support_
page_related_con=0#qt-science_support_page_related_con .
Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang, Marshall Burke, David Lobell, and Stefano Ermon.
Learning to interpret satellite images using wikipedia. In Proceedings of the Twenty-Eighth International Joint
Conference on Artificial Intelligence , 2019.
Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and
Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.
Di Wang, Jing Zhang, Bo Du, Gui-Song Xia, and Dacheng Tao. An empirical study of remote sensing pretraining.
IEEE Transactions on Geoscience and Remote Sensing , pages 1–1, 2022. doi: 10.1109/TGRS.2022.3176603.
Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad M Albrecht, and Xiao Xiang Zhu.
Ssl4eo-s12: A large-scale multi-modal, multi-temporal dataset for self-supervised learning in earth observation.
Ben G Weinstein, Sarah J Graves, Sergio Marconi, Aditya Singh, Alina Zare, Dylan Stewart, Stephanie A Bohlman,
and Ethan P White. A benchmark dataset for canopy crown detection and delineation in co-registered airborne rgb,
lidar and hyperspectral imagery from the national ecological observation network. PLoS computational biology ,
17(7):e1009180, 2021.
13
Zhitong Xiong, Fahong Zhang, Yi Wang, Yilei Shi, and Xiao Xiang Zhu. Earthnets: Empowering ai in earth
observation. arXiv preprint arXiv:2210.04936 , 2022.
Christopher Yeh, Chenlin Meng, Sherrie Wang, Anne Driscoll, Erik Rozi, Patrick Liu, Jihyeon Lee, Marshall Burke,
David B Lobell, and Stefano Ermon. Sustainbench: Benchmarks for monitoring the sustainable development goals
with machine learning. arXiv preprint arXiv:2111.04724 , 2021.
Jin Z., Lin C., Weigl C., Obarowski J., and Hale D. Smallholder cashew plantations in benin, 2021.
V alentina Zantedeschi, Fabrizio Falasca, Alyson Douglas, Richard Strange, Matt J Kusner, and Duncan Watson-Parris.
Cumulo: A dataset for learning cloud classes. arXiv preprint arXiv:1911.04227 , 2019.
Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, and Friedrich Fraundorfer. Deep
learning in remote sensing: A comprehensive review and list of resources. IEEE Geoscience and Remote Sensing
Magazine , 5(4):8–36, 2017.
Xiao Xiang Zhu, Jingliang Hu, Chunping Qiu, Yilei Shi, Jian Kang, Lichao Mou, Hossein Bagheri, Matthias H ¨aberle,
Y uansheng Hua, Rong Huang, et al. So2sat lcz42: A benchmark dataset for global local climate zones classification.
arXiv preprint arXiv:1912.12171 , 2019.
Checklist
1. For all authors...
(a)Do the main claims made in the abstract and introduction accurately reflect the paper’s
contributions and scope? [Y es]
(b) Did you describe the limitations of your work? [Y es]
(c)Did you discuss any potential negative societal impacts of your work? [Y es] See Appendix C
(d)Have you read the ethics review guidelines and ensured that your paper conforms to them?
[Y es]
2. If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)...
(a)Did you include the code, data, and instructions needed to reproduce the main experimental
results (either in the supplemental material or as a URL)? [Y es]
(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they were
chosen)? [Y es]
(c)Did you report error bars (e.g., with respect to the random seed after running experiments
multiple times)? [Y es]
(d)Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Y es] See Appendix A.6.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Y es]
(b) Did you mention the license of the assets? [Y es]
(c) Did you include any new assets either in the supplemental material or as a URL? [Y es]
(d)Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [No] They are creative common datasets
(e)Did you discuss whether the data you are using/curating contains personally identifiable
information or offensive content? [Yes] Let’s discuss it here. There is no text data and
remote sensing data is unlikely to contain PII. The only high-resolution dataset is NeonTree
and it is collected over protected forests in the United States.
5. If you used crowdsourcing or conducted research with human subjects...
(a)Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b)Did you describe any potential participant risks, with links to Institutional Review Board
(IRB) approvals, if applicable? [N/A]
(c)Did you include the estimated hourly wage paid to participants and the total amount spent
on participant compensation? [N/A]
14
A Extended Results
In this section, we continue the experiment sections to include other results, that were deferred due to
space limitations.
A.1 Benchmark Coverage
In Figure 8, we depict the coverage of the benchmark on the world map. While there are still large uncovered
areas such as China, Russia, North Africa, and South America, the benchmark covers all continents except
Antarctica. Most importantly, the coverage of the benchmark is greater than that of individual datasets.
m-brick-kiln
m-forestnet
m-pv4ger-seg
m-nz-cattle
m-seasonet
m-NeonTree
m-pv4ger
m-SA-crop-type
m-eurosat
m-bigearthnet
Figure 8: World coverage of the different datasets.
A.2 Classification
m-bigearthnet0.600.620.640.660.680.70test metric
m-brick-kiln0.970.970.980.980.99
m-eurosat0.900.920.940.960.98
ResNet18-Rnd
ResNet18-timmResNet18-MoCo-S2
ResNet50-SECO-S2ResNet50-MoCo-S2
ResNet50-timmConvNeXt-B-timm
ViT-T-timmViT-S-timm
SwinV2-T-timm
m-forestnet0.450.500.550.600.65
m-pv4ger0.960.960.970.970.980.980.990.99
m-so2sat0.430.450.480.500.530.550.580.600.62
Figure 9: Classification Benchmark: Raw accuracies of all seeds of various baselines (higher is better).
Violin plots represents the distribution of seeds.
In Figure 9, we report the raw accuracies, before normalisation and IQM. This different perspective, gives
a sense of the variance of the results as well as how close it is to the maximum. We recall that uncertainty
of the mean expressed in Figure 4 is lower than the variance of the results in Figure 9. This follows from
the central limit theorem.
A.3 Segmentation
In this section, we report results for six baselines on the Segmentation benchmark. First, we introduce
the baselines that are evaluated.
A.3.1 Baselines
ResNet18 U-Net - ResNet101 U-Net ResNet augmented with the U-Net architecture Ronneberger
et al. [2015] with pre-trained weights from the timm library.
ResNet18 DeepLabV3 - ResNet101 DeepLabV3 ResNet augmented with the DeepLabV3 architecture
Chen et al. [2017] with pre-trained weights from the timm library.
15
A.3.2 Comparing Baselines on RGB only
In Figure 10, we report the bootstrapped IQM of the normalized Intersection over Union (IoU) (Sec. 4.1)
for the 6 segmentation datasets. In Figure 11, we report the seeds for the 10 experiments.
From the results, we can observe a stronger performance with U-Net architecture and the larger backbone
ResNet101 performs a bit less than ResNet50 in general but has less stable performance, leading to slightly
lower performance than ResNet50 backbone.
In Figure 12, we observe the behaviour of the baselines as the size of the training set grows from 1% to 100%.
aggregated0.000.250.500.751.00normalized test metric
m-NeonTree m-SA-crop-type m-cashew-plantResNet18-U-Net-timm
ResNet50-U-Net-timmResNet101-U-Net-timm
ResNet18 DeepLabV3-timmResNet50 DeepLabV3-timm
ResNet101 DeepLabV3-timm
m-chesapeake m-nz-cattle m-pv4ger-seg
Figure 10: Segmentation Benchmark: Normalised intersection over union (IoU) of various baselines
(higher is better). Violin plots are obtained from bootstrap samples of normalized IQM (Section 4.1). Left
plot reports average across all tasks.
m-NeonTree0.400.450.500.550.600.65test metric
m-SA-crop-type0.260.270.280.290.300.310.32
m-cashew-plant0.360.380.400.420.440.460.480.50
ResNet18-U-Net-timm
ResNet50-U-Net-timmResNet101-U-Net-timm
ResNet18 DeepLabV3-timmResNet50 DeepLabV3-timm
ResNet101 DeepLabV3-timm
m-chesapeake0.530.550.570.600.620.650.680.700.72
m-nz-cattle0.650.680.700.720.750.780.800.82
m-pv4ger-seg0.930.930.930.940.940.940.940.94
Figure 11: Segmentation Benchmark: Raw IoU of all seeds of various baselines (higher is better). Violin
plots represents the distribution of seeds.
102
101
100
aggregated0.5
0.00.51.0normalized test metric
102
101
100
m-NeonTree102
101
100
m-SA-crop-type102
101
100
m-cashew-plantResNet18-U-Net-timm
ResNet50-U-Net-timmResNet101-U-Net-timm
ResNet18 DeepLabV3-timmResNet50 DeepLabV3-timm
ResNet101 DeepLabV3-timm
102
101
100
m-chesapeake102
101
100
m-nz-cattle102
101
100
m-pv4ger-seg
Figure 12: Segmentation vs Train Size: Normalised IoU (higher is better) on the Segmentation benchmark
with a growing size of the training set. Shaded region represents 80% confidence interval, obtained from
bootstrap samples of normalized IQM (Section 4.1). Left plot reports average across all tasks.
A.4 Convergence Time
As seen in Section 6.2.3, conducting experiments with a growing training set size brings a different and
important perspective on the performances of the models. In our experiments, this comes with a seven
16
fold increase in number of experiments. On the surface, this may seem like an excessive amount of
computation, but most experiments will run much faster with smaller training set. Indeed, if we decrease
the training size at an exponential pace, and we assume that the training time is proportional to the size
of the training set, we get a more modest increase in computational need. In GEO-Bench, we generate
pre-defined subsets using the following ratios of the training set: ( 1×, 0.5×, 0.2×, 0.1×, 0.05×, 0.02×,
0.01×). With the proportional training time assumption, this leads to a cumulative 1.88 ×, which is less
costly than repeating the experiment twice, and far from the seven fold increase that one could assume.
To confirm the assumption that the training time is proportional to size of the training set, we conduct the
following experiment. As a measure of the training time, we use the convergence time i.e., how many train-
ing steps12are required to achieve the peak performance on the validation set. Let τr
i,j,kbe the convergence
time of model i, on dataset j, with hyperparameter trial k, trained with a ratio rof the training set. Let the
reference convergence time be defined as the average convergence time when using 10% of the training set:
Ti,j=1
nknkX
k=1τ0.1
i,j,k.
Then, the average relative convergence time is:
ρr
j=1
ni,nknkX
k=1niX
i=1τr
i,j,k
Ti,j.
In Figure 13, we plot ρr
jfor all datasets and all partition size on a log-log plot. We can observe a mild
behaviour change near 1% of the training size, but overall we can conclude that the convergence time
is proportional the training set size.
102
101
100
Train Size Ratio101
100101Normalized Training Time
dataset
m-bigearthnet
m-brick-kiln
m-eurosat
m-forestnet
m-pv4ger
m-so2sat
Figure 13: Convergence Time: Average time for the training to reach convergence as the training size
increase.
A.5 Discriminativity of Datasets
It is common knowledge in machine learning that larger datasets yields better generalization performances.
However, when comes the time to compare algorithms against each other, we hypothesises that less is
more i.e. smaller datasets are more likely to exhibit statistical difference between a given pair of models.
In this section we provide experimental evidences on this question. Let p(Al
i> Al
j)be the probability
that algorithm iis better than algorithm jon dataset l. More concretely, Al
iandAl
jare random variables
corresponding to the accuracies on the validation set of task l. We estimate this probability by repeating
the training procedure for 10 random seeds, as recommended in Section 4.1, and comparing all pair of
results. Using this quantity, we propose the following discriminativity metric
dl
ij:=1−H2[p(Al
i>Al
j)],
whereH2corresponds to entropy in bits i.e., using log2. This measures how good dataset lis good at
telling if algorithm iis better than algorithm j. For example, if algorithm iis always better than jor vice
12We also multiply by the batch size
17
versa, then we have: dl
ij= 1. On the other end if they perform equally well, then dl
ij= 0. Next, to get
an estimate at the dataset level, we average discriminativity across all pairs of mmodels to obtain
Dl:=1
m2X
ijdl
ij.
Finally, to obtain en estimate of the uncertainty of this measure, we use stratified bootstrap of all
experiments, and repeat this procedure 100 times.
In this experiment, we consider a smaller training set as a different Dlvalue. Hence, with seven different par-
titions on the 6 tasks of GEO-Bench classification, this leads to a total of 42 different datasets. In Figure 14,
we analyse the influence of the training set size on the discriminativity of a dataset, and we conclude:
•Reducing the dataset size almost systematically improve its ability to discriminate between
models, but only by a small value.
• BigEarthNet is the most discriminative dataset
• The full size of pv4ger offers poor discriminativity but could be improved if reduced.
• Our estimation of discriminativity is quite noisy and sensitive to the random seeds.
0.00.20.40.60.81.0m-bigearthnet
0.00.20.40.60.81.0m-so2sat
0.00.20.40.60.81.0m-brick-kiln
0.010.020.05 0.10.20.51.0
train ratio0.00.20.40.60.81.0m-forestnet
0.010.020.05 0.10.20.51.0
train ratio0.00.20.40.60.81.0m-eurosat
0.010.020.05 0.10.20.51.0
train ratio0.00.20.40.60.81.0m-pv4gerEffect of train size on discriminativity
Figure 14: Discriminativity of datasets: We report how discriminative are each datasets as we vary the
training set size.
A.6 Resources Usage
For convenience, we also provide resource usage of the different models in Figure 15.
Number of Parameters: The number of parameters of a model gives a hint on the overall memory
usage, but most importantly on the learning capacity of the model. In Figure 15-left, we see that ConvNeXt
is by far the one with the highest capacity. While ViT-T has less parameters than ResNet18. We note
that SwinV2 has up to 3 billion parameters with SwinV2-G, but we focus on models that could be run
on a single 32 GB GPUs without extra work.
Memory Usage: While the number of parameters gives a hint about the memory usage, to capture
the memory usage of all hidden states we need to measure the memory usage in action with nvidia-smi .
This is reported in the second plot of Figure 15.
Forward time: We measure the time for a forward pass of the network with a batch size of 32. This
gives a sense of the efficiency of the network deployed in production. V alues are reported in the third
plot of Figure 15. Violin-plot report the distribution of several measurements during one epoch of training
on BigEarthNet, and the average value is reported as a solid line.
Convergence time: In Figure 15 right, we report the number of training step required to reach peak
generalization performance on validation. These values are reported on from the 100% training set, and
the violin-plot report the distribution of values across the different tasks of GEO-Bench-classification and
the different trials of the hyperparameter search.
B Remote Sensing Data Schema
Band Earth monitoring data comes with a challenging amount of heterogeneity. Fortunately, the
transformer architecture offers the opportunity to mix various modalities using encodings such as temporal
18
0 2 4 6 8
1e7ResNet18-timmResNet18-RndResNet18-MoCo-S2ResNet50-timmResNet50-MillionAIDResNet50-MoCo-S2ViT-T-timmViT-S-timmSwinV2-T-timmConvNeXt-B-timmNumber of parameters
0 2 4 6 8Memory usage (GB)
with batch_size=32
0 100 200 300Forward time (ms)
with batch_size=32
0 50 100 150 200Convergence time
 in training steps / 1000Figure 15: Resources Usage: Resources reported for various models. Violin-plots report distribution over
several tests and its average as a solid line. See text for more details.
encoding V aswani et al. [2017] and positional encoding Dosovitskiy et al. [2020]. Similarly, band encoding
can be leveraged to communicate the source of the data. To this end, we define Band as the core class
in our schema. It consists of an array of data with spatial extent accompanied with BandInfo , providing
information such as the band name, spatial resolution, and spectral range. Through a hierarchy of classes,
we also provide the type of sensors (see Figure 16). This provides further information for introspection
and flexible information for users to define a Band Encoding that could be required for transformer
architectures. Finally, a Sample is a set of Band accompanied by a label which can also be a Band .
BandInfo 
SpectralBand 
Sentinel2 Landsat Radar 
Sentinel1 Elevation MultiBand 
Hyperspectral Segmentation 
Figure 16: Class hierarchy of BandInfo enabling introspection on the type of data.
Task Specifications Each dataset is accompanied by a TaskSpecifications object describing the
schema of a particular dataset without having to load any samples. It contains the dataset name, the type of
labels, the BandInfo of each band and their shapes. The aim of this data structure is to let users procedu-
rally generate a machine learning model that is suitable for the given dataset at the beginning of the training.
Band Statistics We also provide band statistics (minimum, maximum, mean, variance, and percentiles).
This lets users transform the input data in various possible ways to fit the statistics expected by the
pre-trained model.
C Societal Impact of Foundation Models for Earth Monitoring
Remote sensing and Earth monitoring have been transformational in the past decades. Applications include
military, insurance, market forecasting, climate science, and more. Much of this impact is not directly
attributed to deep learning nor large pre-trained networks and its review extends beyond the scope of this
section. In this section, our focus is on the impact of bringing foundation models to Earth monitoring.
C.1 Climate mitigation and adaptation
Machine learning on remote sensing data is widely used to develop solutions for a variety of problems
relevant to climate change Burke et al. [2021], Rolnick et al. [2019], Zhu et al. [2017], Ma et al. [2019].
The vast majority of these solutions are built by curating datasets for a specific task and require significant
resources to develop. Furthermore, the solutions are often tailored to specific regions as extending
approaches to new geographies remains a significant challenge, primarily due to the lack of labeled
data Zhu et al. [2017]. Less-economically developed regions of the world are no less susceptible to the
impacts of climate change, yet suffer from the lack of effective remote sensing-based solutions Burke et al.
19
[2021]. Foundation models for Earth monitoring have the potential to address many of these issues and
substantially accelerate and enable the development of new remote sensing solutions for climate change.
C.2 Increased accessibility
Reducing the need for curating a large labeled dataset for each task could democratise access to the
development of machine learning models for remote sensing, specifically for groups or organisations
with limited budgets Maskey et al. [2020], Alemohammad [2021]. In particular, foundation models may
especially benefit non-profit organisations, academic universities, startups, and developing countries. It
may also open opportunities for applications that were not previously profitable. Although we believe
that increased accessibility to these models will have a largely net positive impact, we acknowledge that
this accessibility may lead to unexpected applications with potentially negative impacts Bommasani et al.
[2021]. We also note that such models may have dual-use applications, where, for example, they may
help oil and gas industries in their operations in ways that increase (or reduce) overall emissions.
C.3 Emissions of large pre-trained models
Recent work has investigated emissions of large neural networks Strubell et al. [2019], Schwartz et al.
[2020], Schmidt et al. [2021], Lacoste et al. [2019], Patterson et al. [2021]. Specifically, training a large
transformer can emit 284 tCO 2e when trained on computers using largely fossil fuel energy (US national
average) Strubell et al. [2019]. When put in perspective with individual actions, such emissions are
large—e.g., a roundtrip passenger flight from San Francisco to London is 2.8 tCO 2e , about 100 ×smaller.
However, the extensive reusability of pre-trained models and their potential for helping efforts to mitigate
climate change Rolnick et al. [2019] calls for a different perspective.
When evaluating new tools and systems, it is important to consider the likely net impact on emissions
of both the creation and testing of the tool and its eventual deployment. For example, evaluating the
performance of airborne methane sensing tools at emission levels commonly found in oil and gas operations
can emit about 7 metric tonnes of methane, roughly 600 tCO 2e equivalent using a 20-year global warming
potential EPA [2017]. However, in a single day of flying, such a single instrument can survey hundreds
of sites, often identifying leaks for repair that emit well over 7 metric tonnes of methane per day Johnson
et al. [2021]. Similarly, foundation models may significantly advance our ability to leverage enormous
quantities of passively collected satellite data to massively reduce emissions, qualitatively advance our
understanding of climate science, or improve our ability to adapt to climate change.
In sum, the potential benefits for climate change mitigation with improved Earth monitoring methods
likely outweigh the emissions associated with foundation models. Moreover, various actions can be taken
to reduce and mitigate emissions related to the training of your model Lacoste et al. [2019]:
•Select data centers that are certified carbon neutral or largely powered by renewable energy, with
good power usage effectiveness (PUE). Such measures can reduce emissions dramatically 50 ×
reduction in emissions Lacoste et al. [2019].
•Design your code development pipeline to minimize the number of computationally-intensive
runs required, e.g. employ modular development and testing when possible.
•Make your code more efficient and sparsify your network when possible Patterson et al. [2021].
This can reduce emissions up to 10-fold.
• Favour more energy-efficient hardware, e.g., TPUs or GPUs.
•Monitor Schmidt et al. [2021] and report your emissions Lacoste et al. [2019]. Better communi-
cation about climate change is fundamental for systemic changes. Better documentation will help
other coders pick up where you left off, potentially bypassing some computationally intensive runs.
• Offset the cumulative emissions of your projects.
C.4 Fairness and biases
Large language models are known to amplify and perpetuate biases Bender et al. [2021]. While this can
lead to serious societal issues, we believe that biases in remote sensing models are likely to have much
less impact. We do however anticipate potential biases and fairness issues.
20
Data coverage and resolution Some satellites cover the whole Earth with standard spatial resolution
and revisit rate (e.g., Sentinel-2 covers the whole Earth at 10-60 m/pixel resolution every 5 days). This
makes imagery freely available uniformly across the planet. Other satellite data providers such as Maxar
acquire images on-demand and have higher spatial resolution (up to 0.3m per pixel), but also have lower
revisit rates and high costs. Some countries, such as New Zealand, freely provide aerial imagery with
resolution up to 0.1m per pixel13. Finally, it is worth noting that cloudy seasons in some climates may
limit data availability for some countries. Overall, while the coverage is fairly uniform, some regions have
much higher coverage than others and money can be a limiting factor to access the data. This can lead
to some level of biases and fairness issues.
13https://data.linz.govt.nz/
21
Revisiting pre-trained remote sensing model
benchmarks: resizing and normalization matters
Isaac Corley∗
University of Texas at San Antonio
San Antonio, TX, USA
isaac.corley@my.utsa.eduCaleb Robinson∗
Microsoft AI for Good Research Lab
Redmond, WA, USA
caleb.robinson@microsoft.org
Rahul Dodhia
Microsoft AI for Good Research Lab
Redmond, WA, USA
rahul.dodhia@microsoft.comJuan M. Lavista Ferres
Microsoft AI for Good Research Lab
Redmond, WA, USA
jlavista@microsoft.com
Peyman Najafirad
University of Texas at San Antonio
San Antonio, TX, USA
peyman.najafirad@utsa.edu
Abstract
Research in self-supervised learning (SSL) with natural images has progressed
rapidly in recent years and is now increasingly being applied to and benchmarked
with datasets containing remotely sensed imagery. A common benchmark case is to
evaluate SSL pre-trained model embeddings on datasets of remotely sensed imagery
with small patch sizes, e.g., 32 ×32 pixels, whereas standard SSL pre-training takes
place with larger patch sizes, e.g., 224 ×224. Furthermore, pre-training methods
tend to use different image normalization preprocessing steps depending on the
dataset. In this paper, we show, across seven satellite and aerial imagery datasets
of varying resolution, that by simply following the preprocessing steps used in
pre-training (precisely, image sizing and normalization methods), one can achieve
significant performance improvements when evaluating the extracted features on
downstream tasks – an important detail overlooked in previous work in this space.
We show that by following these steps, ImageNet pre-training remains a competitive
baseline for satellite imagery based transfer learning tasks – for example we find
that these steps give +32.28 to overall accuracy on the So2Sat random split dataset
and +11.16 on the EuroSAT dataset. Finally, we report comprehensive benchmark
results with a variety of simple baseline methods for each of the seven datasets,
forming an initial benchmark suite for remote sensing imagery.2
1 Introduction
With increasing frequency, self-supervised learning (SSL) models, foundation models, and transfer
learning methods have been applied to remotely sensed imagery [ 31,33,19,40,6,53,18,11,35,
∗Equal contribution
2Experimental code, datasets, and model checkpoints will be made available in the TorchGeo library at https:
//github.com/microsoft/torchgeo and are currently hosted at https://github.com/isaaccorley/
resize-is-all-you-need
Preprint. Under review.arXiv:2305.13456v1  [cs.CV]  22 May 2023
Figure 1: Difference in downstream task metrics, Overall Accuracy (OA) (multiclass) or mean
Average Precision (mAP) (multilabel), after resizing images to 224 ×224 from the original, smaller,
image size. ImageNet pre-trained models, for example, often are trained with 224 x 224 inputs and
therefore do not produce useful embeddings with smaller image patches.
42,10,55,56,25,49,36,39]. As such, rigorous benchmarks are needed to identify the strengths and
weaknesses in the proposed methods.
A commonly used benchmark in any transfer learning setup is the use of embeddings from a
model that is pretrained on the ImageNet (ILSVRC2012) dataset [ 13] – due to both the ease of
implementation [ 9,34] and strong performance when generalizing to unseen data [ 27]. However,
even with fully convolutional neural networks, the size of image inputs to the model is an important
factor that should be controlled for at test/inference time. Common large-scale benchmarks libraries
like PyTorch Image Models (timm) [ 57] and OpenCLIP [ 28] provide benchmark results trained at
varying image sizes and evaluate at the same sizes as opposed to the original dataset size. Plainly put,
models that are pretrained on ImageNet images that have been resized and cropped to a fixed image
size (traditionally 224 ×224 or 256 ×256), will produce the most relevant embeddings for transfer
learning when they are given the same image size at test time.
Satellite missions such as Sentinel-2 [ 15] and Landsat-8 [ 45] capture imagery over the Earth’s surface
at relatively low spatial resolutions, e.g. 10-60 meters/pixel, compared to the resolution of objects in
natural imagery. Because of this, it is common for labeled datasets of remotely sensed imagery to
contain images of smaller sizes, e.g. 32 ×32 [59], than traditional image classification datasets. Thus,
if images from these datasets are used as-is with ImageNet pretrained models, then the results will be
sub-optimal.
A similar story can be told with image normalization methods. A standard preprocessing method for
ImageNet pre-trained models is to normalize all values in an image to a [0,1]range then perform
channel-wise standardization with ImageNet statistics. However, as remotely sensed imagery usually
has a higher bit-depth (or color-depth) than images in standard vision datasets (12 or 16-bit depth vs.
8-bit depth), different image normalizations methods are usually applied. For example, a common
method used with Sentinel-2 imagery is to divide all values by 10,000 (to convert the raw sensor values
to reflectance values) then use these as inputs in a network [ 35,56]. If images that are normalized
with one method are used with a network that is pre-trained under a different normalization method,
then the results will also be sub-optimal.
We demonstrate that it is vital to consider how an embedding model was trained when using it
for transfer learning on downstream remote sensing tasks. For example, through simple bilinear
upsampling of input images from 64 ×64 to 224 ×224 on the EuroSAT RGB dataset [ 26], we find that
accuracy of the embeddings generated by a ImageNet pretrained ResNet-50 [ 22] increases from 0.82
2
to 0.91. Similarly, performing a channel-wise standardization instead of re-scaling the image values
to represent reflectance results in a performance increase from 0.66 to 0.91 (when combined with
resizing to 224 ×224). Performing these steps correctly gives simple baselines, like ImageNet
pre-training, results that are competitive with previously published methods. Additionally, we
benchmark several simple methods, including MOSAIKS [ 44] and a simple image statistic based
feature extraction method, and find that they beat ImageNet and/or remote sensing SSL pretraining
methods on several datasets.
While not particularly surprising, our results form a set of strong baselines that can be used to
benchmark future methods for self-supervised learning with remotely sensed imagery against. Further,
our experimental setup is open-sourced and can be easily appended to as the community focuses on
different geospatial machine learning tasks.
Figure 2: The effect of input image size on EuroSAT downstream performance (overall accuracy)
across different ResNet models. By default, EuroSAT images are 64 ×64 pixels, however resizing
to larger image sizes before embedding increases downstream accuracy under a KNN ( k= 5)
classification model in all cases.
Our main contributions are as follows:
•We propose a set of strong baseline methods for remote sensing scene classification –
including an ImageNet pretrained ResNet-50, random convolutional features (RCF), and a
simple image statistic feature extraction method – that outperform self-supervised pretrained
models on several datasets. We have implemented these methods into the open source
TorchGeo library [46] (see Appendix A).
•We present a set of benchmark results across seven geospatial machine learning datasets
commonly used as downstream tasks for testing pre-trained model performance with our
baseline methods.
•We demonstrate the importance of proper resizing and normalization of images for optimal
performance and fair comparisons in geospatial machine learning benchmarks.
1.1 Related Work
Recent works have shown that while many new deep learning architectures claim to achieve state-
of-the-art performance due to their proposed novel model design, they in fact only do so because of
inconsistencies in training strategies and hyperparameters when comparing to baselines and prior
methods. Bello et al. [ 4] explored that by simply retraining with recent training techniques and
tricks, the original ResNet [ 22] architecture significantly outperforms its own previous baselines and
reaches a competitive top-1 ImageNet accuracy. Du et al. [ 16] concluded the same findings for 3D
ResNets [ 52] for video recognition tasks. Goyal et al. [ 21] examined the similar effects for numerous
architectures in the 3D point cloud classification field. Finally, Musgrave et al. [37] repeat the same
idea for metric learning methods. In other words, when all models are on the same playing field,
performance gains from past methods over strong baselines tend to become insignificant.
Previous papers that explore the effect of resizing inputs on performance in convolutional neural
networks include Richter et al. [ 43] and Touvron et al. [ 51]. Both papers investigate different
3
Table 1: Results on the EuroSAT dataset [ 26]
for multiclass classification using KNN ( k= 5).
We report Overall Accuracy (OA) for both RGB
and all MSI bands. We compare to fine-tuned
performance of several SSL methods taken from
their respective papers. *The Scale-MAE result
uses a KNN-5 and is comparable to the other
KNN results.
Model Weights Size RGB MSI
ResNet50 MoCo64 94.11 81.85
224 95.76 93.65
ResNet50 ImageNet64 82.09 78.65
224 91.17 89.81
ResNet50 Random64 59.92 ±0.34 75.10 ±0.23
224 73.76 ±0.53 87.19 ±0.81
RCF Random64 78.85 ±0.33 87.56 ±0.35
224 76.90 ±0.33 87.41 ±0.12
RCF Empirical64 81.47 ±0.08 91.10±0.11
224 77.88 ±0.08 90.14 ±0.15
Image Stat. - 64 76.94 89.56
ViT-L Scale-MAE [42] 64 96.00* -
ResNet18 GASSL [2] 64 89.51 -
ResNet18 SeCo [35] 64 93.14 -
ViT-L SatMAE [10] 224 98.94 -Table 2: Results on the SAT-6 dataset [ 3] for mul-
ticlass classification using KNN ( k= 5). We re-
port Overall Accuracy (OA) and compare to the
fully-supervised performance of DeepSAT and
DeepSATv2 models taken from their respective
papers.
Model Weights Size OA
ResNet50 MoCo34 98.15
224 99.86
ResNet50 ImageNet34 96.55
224 99.89
ResNet50 Random34 91.64 ±0.66
224 98.57 ±0.08
RCF Random34 99.40 ±0.06
224 99.29 ±0.07
RCF Empirical34 99.65 ±0.02
224 98.85 ±0.06
Image Stat. - 28 99.60
DeepSat [3] Sup. 28 93.92
DeepSatv2 [32] Sup. 28 99.84
experimental setups by varying training and testing at different image sizes and empirically show that
increasing the image size during inference improves performance which begins to saturate around an
image size of 256 ×256. However, both works strictly explore natural images only with ImageNet
pretraining as opposed to remotely sensed imagery, as is the objective of this paper. Wang et al. [ 56]
provide the closest evidence of this case for remote sensing data by performing a short experiment
reporting linear probing results showing a boost in performance while increasing the input image
size.
2 Methods
In this study we extract feature representations (or embeddings) from remotely sensed image datasets
using a variety of methods (described below) while varying the image preprocessing steps. Specif-
ically, we vary the image size that is passed through to the feature extractor using Pytorch’s [ 41]
torch.nn.functional.interpolate implementation with bilinear interpolation, and we vary the
image normalization method between channel-wise standardization (i.e. the default practice for most
ImageNet pretrained models), converting the input image values into a reflectance value (i.e. the
default practice for most Sentinel-2 pretrained models), min-max normalization, or method specific
normalizations (e.g. the percentile normalization from [ 35]). In datasets that have multispectral
information we run experiments using only the RGB channels, as well as all the channels (MSI)3.
We extract feature representations using the following methods:
ResNet-50 Random init. [22] A vanilla ResNet-50 with random weight initialization (following the
default torchvision settings). The features generated by this and the following two ResNet-50
models are produced by the final global average pool operation and are 2048-dimensional.
ResNet-50 ImageNet [13] A ResNet-50 that is pretrained on ImageNet with images of size 224x224
(default torchvision pretrained weights).
ResNet-50 SSL4EO [56] A ResNet-50 that is pretrained using the MoCo-v2 [ 23,7] self-supervised
learning method on the SSL4EO dataset with 224x224 images.
RCF (Random) [44] A feature extraction method that consists of projecting the input to a lower
dimensional space using random convolutional features (RCF). We use the implementation
3Note that for processing multispectral (MSI) imagery through ImageNet pretrained ResNets, we repeat
the RGB weights in the first convolutional layer to account for the additional input bands. For SSL4EO MSI
pretrained ResNets, we zero-pad channels to account for any bands not made available in datasets.
4
from TorchGeo with 512 convolutional filters and a 3x3 kernel size. In the results we refer
to this method as RCF with random weights.
MOSAIKS / RCF (Empirical) [44] A feature extraction method similar to RCF but that initializes
the weights using ZCA whitened patches sampled randomly from the training set. We use
the implementation from TorchGeo with 512 convolutional filters and a 3x3 kernel size. In
the results we refer to this method as RCF with empirical weights.
Image Statistics A hand crafted baseline method that consists of simply computing per-channel
pixel statistics from the imagery. Given an image we compute the mean, standard deviation,
minimum, and maximum value for each band and concatenate these into a simple 4c-
dimensional feature representation, where cis the number of input channels.
2.1 Evaluation
For evaluating the representation performance of a pretrained model it is common to perform “linear
probing” on a given downstream task by training a linear model on the representations generated by
the pre-trained model and measuring the performance of this linear model. However, this method
is implemented very differently between papers – some papers use data augmentation [ 56] while
others don’t, and others use a variety of different optimizers (SGD, Adam, LARS), regularization
methods4, and learning rates / learning rate schedules. Therefore, for fair evaluation we fit a K-
Nearest-Neighbors (KNN) model [ 12] to extracted features from various datasets, setting k= 5, as
performed similarly in [42, 53].
3 Datasets
The datasets used throughout our experiments were selected particularly due to their original image
sizes being small to show the effects of resizing. These datasets are commonly benchmarked without
resizing which makes them perfect candidates for quantifying the effects of size vs performance. We
also select datasets which are from both low-resolution satellite sources as well as high resolution
aerial imagery.
EuroSAT The EuroSAT dataset [ 26] is a land cover classification dataset of patches extracted from
multispectral Sentinel-2 [ 15] imagery. The dataset contains 27,000 64 ×64 10m spatial
resolution images with 13 bands and labels for 10 land cover categories. We use the dataset
splits defined in Neumann et al. [38].
SAT-6 The SAT-6 dataset [ 3] is a land cover classification dataset of patches extracted from aerial
imagery from the National Agriculture Imagery Program (NAIP) [ 17]. The dataset contains
405,000 28 ×28 RGBN patches at 1m spatial resolution and labels for 6 land cover categories.
We use the train and test splits provided with the dataset.
So2Sat The So2Sat dataset [ 59] is a local climate zone (LCZ) classification dataset of patches
extracted from Sentinel-1 and Sentinel-2 imagery. For our experiments we only utilize the
Sentinel-2 bands. The dataset contains 400,673 multispectral patches with 10 bands and at
10m spatial resolution. Each patch is of size 32 ×32 and contains a single label from 17
total LCZ categories. We use the train and test splits from the Random and Culture-10 sets
provided with the dataset.
BigEarthNet The BigEarthNet dataset [ 47] is a multi-label land cover classification dataset of
patches extracted from multispectral Sentinel-2 imagery. The dataset contains 590,326 120
×120 10m spatial resolution images with 12 bands and labels for 19 land cover categories.
We use the splits provided with the dataset and defined in [48].
TreeSatAI The TreeSatAI dataset [ 1] is a multi-sensor, multilabel tree species classification dataset
of patches extracted from aerial and multispectral Sentinel-1 [ 50] and Sentinel-2 imagery.
For our experiments we only utilize the Sentinel-2 bands. The dataset contains 50,381 10m
spatial resolution images with 12 spectral bands, which are available in 6 ×6 or 20 ×20
sizes, and labels for 20 tree species categories. We use the train and test splits provided with
the dataset.
4For example, by default the Adam optimizer in PyTorch will not apply L2 regularization on the weights of
the model (weight decay), while scikit-learn linear models are trained with L2 regularization by default.
5
Table 3: Results on the So2Sat dataset [ 59] for multiclass classification using KNN ( k= 5). We report
Overall Accuracy (OA) for both RGB and all MSI bands and for both the Random andCulture-10
splits. We compare to both fully-supervised and linear probing results for several SSL methods.
Random Culture-10
Model Weights Size RGB MSI RGB MSI
ResNet50 MoCo34 75.07 72.51 51.45 49.36
224 93.93 96.15 56.03 53.54
ResNet50 ImageNet34 66.21 56.18 47.76 42.11
224 92.99 88.46 54.53 50.32
ResNet50 Random34 46.19 ±0.19 55.06 ±0.35 29.10 ±0.30 35.47 ±0.18
224 71.74 ±1.87 84.10 ±0.32 34.16 ±0.23 45.68 ±0.50
RCF Random34 72.67 ±0.45 89.40 ±0.14 30.92 ±0.11 45.23 ±0.33
224 74.22 ±0.44 89.72 ±0.11 31.19 ±0.21 45.36 ±0.36
RCF Empirical34 71.00 ±0.32 95.37±0.06 35.32±0.45 47.63 ±0.10
224 51.66 ±0.46 95.20 ±0.02 27.36 ±0.24 44.98 ±0.16
Image Stat. - 32 83.84 91.09 38.36 47.93
ResNet50 MoCo [56] 224 - - - 61.80
ResNet50 DINO [5] 224 - - - 57.00
ViT-S DINO [5] 224 - - - 62.50
ViT-S MAE [24] 224 - - - 60.00
ResNet50 Sup. [56] 224 - - - 57.50
ViT-S Sup. [56] 224 - - - 59.30
UC Merced The UC Merced (UCM) dataset [ 58] is a land use classification dataset that consists of
2,100 256 ×256 pixel aerial RGB images over 21 target classes. We use the train/val/test
splits defined in Neumann et al. [38].
RESISC45 The RESISC45 dataset [ 8] is a scene classification dataset that consists of 45 scene
classes and 31,500 256 ×256 pixel aerial RGB images extracted from Google Earth. We
use the dataset splits defined in Neumann et al. [38].
4 Results and Discussion
4.1 Fair Comparisons to ImageNet Pretraining
As stated in Section 1.1, prior research has shown the significance of resizing images during testing for
ImageNet pretrained models. To emphasize this, we perform a short experiment comparing features
extracted from the EuroSAT [ 26] dataset using a ResNet-18 pretrained with both the Seasonal Contrast
(SeCo) method [ 35] and ImageNet. For fair evaluation, we compute downstream task results at the
original image size 64 ×64 and resized to 224 ×224 with KNN and linear probe methods. For linear
probing we utilize the exact same experimental setup and script as in [ 35] while only adding a resize
transformation. As seen in Table 8, depending on the model used for evaluation, one pretraining
method can appear better than another. Furthermore, while increasing the image size improves
performance for both methods, it does not improve equally. When reading the linear probing results
in [35], one would assume that the SSL pretrained model clearly outperforms ImageNet pretraining.
However, as we can see, this is not the case, and further investigation are needed. Further, in Table 7,
we observe that an ImageNet pretrained model outperforms the best reported results in SatMAE [ 10]
in the same experimental setup.
4.2 Image Size vs. Performance
Figure 2 shows how the performance of a variety ResNet-50 models varies with input image size on
the EuroSAT dataset when using just the RGB bands vs. all spectral bands as input. We observe in
all cases that the default dataset image size (64 ×64 pixels) does not result in optimal performance.
For example, resizing from 64 ×64 to 256 ×256 results in a 10 point increase in accuracy in a
ResNet-50 that is pretrained on ImageNet. In Tables 1-5 we report performance from each method at
6
Figure 3: t-SNE [ 54] plots of EuroSAT test set embeddings extracted using a ResNet50 pretrained on
ImageNet with different preprocessing. (left to right: 32 ×32 with normalization, 224 ×224 without
normalization, 224 × 224 with normalization)
the native resolution of the dataset and after resizing each image to 224x224 and observe performance
improvements across all methods in nearly all cases.
To visualize the effects of resizing (and standard normalization), in Figure 3 we show t-SNE [ 54]
plots of EuroSAT RGB features extracted using a ResNet-50 pretrained on ImageNet. The the plot
shows that EuroSAT classes are clearly separable at an input size of 224 x 224 while only partially
separable at 32 x 32. Additionally, when resizing but not using any normalization, there are no clear
clusters corresponding to the dataset classes. While we use a NVIDIA DGX server with 2x A100
GPUs to increase the speed of our benchmarks, we note that none of these methods actually require a
GPU to perform inference or KNN classification on extracted features.
Table 4: Results on the BigEarthNet dataset [ 47] for 19-class multilabel classification using KNN
(k= 5). We report overall F1 score, and overall mean average precision (mAP). For reference, we
compare to the fully supervised S-CNN as well as fine-tuned results from the GASSL, SeCo, and
SatMAE SSL methods.
RGB MSI
Model Weights Size F1 mAP F1 mAP
ResNet50 MoCo120 68.99 70.65 63.61 64.64
224 72.56 74.81 68.33 70.17
ResNet50 ImageNet120 65.38 66.62 62.61 62.96
224 67.47 69.07 65.04 65.88
ResNet50 Random120 52.34 ±0.22 52.63 ±0.19 60.48 ±0.34 61.17 ±0.50
224 57.05 ±1.02 57.61 ±1.13 64.94 ±0.25 66.31 ±0.32
RCF Random120 54.48 ±0.26 53.94 ±0.26 69.98 ±0.20 72.01 ±0.28
224 54.37 ±0.28 53.74 ±0.23 70.06 ±0.21 72.12 ±0.29
RCF Empirical120 57.40 ±0.22 57.22 ±0.23 73.31±0.14 76.18 ±0.19
224 53.36 ±0.23 52.90 ±0.22 73.41±0.13 76.29 ±0.15
Image Stat. - 120 61.67 62.00 69.42 71.29
S-CNN BigEarthNet [47] 120 67.59 - 70.98 -
ResNet50 GASSL [2] 120 - 80.20 - -
ResNet50 SeCo [35] 120 - 82.62 - -
ViT-L SatMAE [10] 224 - 82.13 - -
4.3 Benchmarks
We perform thorough benchmarks using the methods described in Section 2 on each dataset from
Section 3, using the evaluation metric common to that dataset, in Tables 1 through 7. In each
experiment we fit a non-parametric k-nearest neighbor model with k= 5 to the train set. For
deterministic methods we report a single value calculated over the test set for each dataset, while
for stochastic methods we report the average ±the standard deviation of the metric calculated over
the test set over 5 runs with different random seeds. We bold the best performing of the baseline
methods by column and italicize the second best performing method. Additionally, we show several
fine-tuning, linear probing, and fully-supervised baselines from original dataset papers or other SSL
remote sensing papers. Note that we perform these comparisons not with the goal of outperforming
7
Table 5: Results on the TreeSatAI dataset [ 1] for multilabel classification using KNN ( k= 5).
We report overall F1 score and mean average precision mAP. We compare to the fully-supervised
LightGBM performance and fine-tuned Presto SSL method.
RGB MSI
Model Weights Size F1 mAP F1 mAP
ResNet50 MoCo34 29.21 29.93 37.65 36.24
224 37.68 37.57 45.18 44.14
ResNet50 ImageNet34 27.69 27.30 32.07 30.69
224 40.37 40.58 42.00 41.33
ResNet50 Random34 29.37 ±0.42 29.08 ±0.18 36.47 ±0.34 34.73 ±0.15
224 35.42 ±0.33 34.75 ±0.43 49.09 ±0.83 48.48 ±0.89
RCF Random34 33.15 ±0.21 32.15 ±0.09 52.24 ±0.35 51.83 ±0.33
224 32.37 ±0.20 31.29 ±0.18 52.49 ±0.17 51.99 ±0.43
RCF Empirical34 31.70 ±0.06 31.13 ±0.17 56.00±0.04 56.08 ±0.25
224 28.93 ±0.47 28.50 ±0.23 55.60±0.13 55.77 ±0.29
Image Stat. - 20 38.39 37.19 51.97 51.56
LightGBM [29] - 20 - - 52.52 61.66
ViT Presto [53] 9 - - 50.32 67.78
Table 6: Results on the RESISC45 dataset [ 8] for
multiclass classification using KNN ( k= 5). We
report Overall Accuracy (OA) and compare to
performance metrics of various remote sensing
SSL methods taken from their respective papers.
*The Scale-MAE result uses a KNN-5 and is
comparable to the other KNN results.
Model Weights Size OA
ResNet-50 MoCo 256 73.24
ResNet-50 ImageNet 256 77.48
ResNet-50 Random 256 36.30 ±0.25
RCF Random 256 42.29 ±0.12
RCF Empirical 256 36.15 ±0.36
Image Stat. - 256 34.03
ViT-L Scale-MAE [42] 256 85.0 *
ViT-L SatMAE [10] 256 77.1*
ViT-L ConvMAE [20] 256 78.8*Table 7: Results on the UC Merced dataset [ 58]
for multiclass classification using KNN ( k= 5).
We report Overall Accuracy (OA) and compare
to the linear probing performance of the Scale-
MAE, SatMAE, and ConvMAE methods taken
from their respective papers. *The Scale-MAE
result uses a KNN-5 and is comparable to the
other KNN results.
Model Weights Size OA
ResNet50 MoCo 256 85.50
ResNet50 ImageNet 256 90.70
ResNet50 Random 256 47.94 ±1.07
RCF Random 256 52.14 ±0.24
RCF Empirical 256 56.90 ±0.63
Image Stat. - 256 47.90
ViT-L Scale-MAE [42] 256 85.1*
ViT-L SatMAE [10] 256 84.2*
ViT-L ConvMAE [20] 256 81.7*
them but for transparency of the difference in performance in representation ability to the state-of-the-
art. Finally, we note that our evaluation method is the same as that of Reed et al. [ 42] and indicate
this with an asterisk where appropriate.
For the EuroSAT experiments we show results from GASSL [ 2], SeCo [ 35], and SatMAE [ 10] self-
supervised methods that use fine-tuning on top of the pretrained network (as reported by SatMAE).
We note that methods which use a (ViT) [ 14] model are unable to accept input images with varying
sizes and therefore we only report performance from their original training image size.
For the SAT-6 experiments we compare to the performance of the DeepSat [ 3] model proposed in the
original SAT-6 dataset paper as well as the DeepSatv2 [32] model from a follow-up paper.
For the UC Merced experiments, we compare to the performance of SatMAE [ 10], Scale-MAE [ 42],
and ConvMAE [20] as reported in the Scale-MAE paper.
Our results show the following:
8
•SSL4EO MoCo-v2 pretrained weights have the best overall performance across downstream
tasks. They rank in the top-2 methods by performance for 6 out of the 7 RGB datasets, and
3 out of 5 multispectral datasets.
•The Scale-MAE pretrained model performs the best in the EuroSAT and RESISC45 datasets,
however is outperformed by ImageNet pretraining in the UCM dataset.
•MOSAIKS (i.e. RCF with empirical weights) is a very strong baseline on the multispectral
datasets and ranks in the top 2 methods by performance for 4 out of the 5 multispectral
datasets (counting the Random and Culture-10 splits of So2Sat as seperate datasets).
•The image statistic baseline outperforms ImageNet pretrained models on all but one of the
multispectral datasets (and it is 0.25% lower than ImageNet in this case).
•In SAT-6 experiments, all methods except for the randomly initialized ResNet-50 achieve
greater than 99% accuracy. Even the image statistic baseline achieves a 99.6% overall
accuracy. This suggests that the dataset is too simple to be used as a benchmark for
comparing models as it will be difficult to observe statistically significant changes in
accuracy between 99.6% (any result worse than this would suggest a model that is less
expressive than simply extracting image statistics) and 100%. Nevertheless, future work
could explore this dataset in other settings, such as few-shot learning.
•Resizing images does not result in significantly changed downstream performance with the
RCF methods (as compared to the ResNet based models). We hypothesize that this method
is largely scale invariant – however leave further experiments (such as varying convolutional
size with input size, etc.) to future work.
•Out of the five datasets with multispectral information, adding the additional multispectral
bands to the RGB bands degrades ResNet-50 ImageNet pretrained performance in two cases.
However, in all cases, adding multispectral information increases the ResNet-50 random
initialized performance. This further highlights the difference in distributions between
ImageNet, natural imagery, and remotely sensed imagery.
•In the So2Sat dataset, switching from the Random set to the Culture-10 set decreases the
accuracy of RCF methods more than the pre-trained models. We hypothesize that this is
because the Culture-10 set tests geographic generalization, and RCF will only be able to use
color/texture from the train set while the pre-trained models could potentially group similar
patches across sets to similar feature representations.
Table 8: Comparison of SeCo [ 35] vs. ImageNet pretraining on the EuroSAT validation set. We show
Overall Accuracy results for both KNN and linear probe at different image sizes.
Size Weights KNN ( k= 3) KNN ( k= 10 ) Linear Probe
64SeCo 84.04 84.11 93.14
ImageNet 85.39 85.20 86.44
224SeCo 86.57 85.63 96.30
ImageNet 90.54 90.63 93.13
5 Best Practices
To recap, below is a list of best practices we believe all remote sensing pre-training research should
include in their analyses. While these may seem obvious, it is critical to follow these guidelines to
produce accurate and transparent benchmarks for understanding the strengths and weaknesses of
proposed methods to the community.
1.Always compare to simple baseline : Performance across datasets can be misleading,
therefore always compare a simple and effective baseline. We recommend an ImageNet
pretrained model, random convolutional features, and image statistics.
2.Resize & Normalize : Resize and normalize inputs to the same parameters as during training,
for all methods being compared. For example, when comparing to ImageNet pretrained
models perform min/max normalization on inputs to the range [0,1], perform channel-wise
standardization to scale inputs to µ= 0andσ= 1, and resize inputs to 224 × 224.
9
3.Prefer KNN over Linear Probing. Prefer KNN and Linear Probing over Fine-tuning :
Linear probing has the potential to overstate feature representation ability due to the numer-
ous hyperparameters and ways to perform linear probing experiments. Additionally, while
fine-tuning compares pretrained weights as an initialization, this tends to not be the purest in-
dicator for representation ability and has been shown to underperform for out-of-distribution
downstream tasks [30].
References
[1]Steve Ahlswede, Christian Schulz, Christiano Gava, Patrick Helber, Benjamin Bischke, Michael
Förster, Florencia Arias, Jörn Hees, Begüm Demir, and Birgit Kleinschmit. Treesatai benchmark
archive: A multi-sensor, multi-label dataset for tree species classification in remote sensing.
Earth System Science Data , 15(2):681–695, 2023.
[2]Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lobell,
and Stefano Ermon. Geography-aware self-supervised learning. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 10181–10190, 2021.
[3]Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert DiBiano, Manohar Karki, and
Ramakrishna Nemani. Deepsat: a learning framework for satellite imagery. In Proceedings of
the 23rd SIGSPATIAL international conference on advances in geographic information systems ,
pages 1–10, 2015.
[4]Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus Cubuk, Aravind Srinivas, Tsung-Yi Lin,
Jonathon Shlens, and Barret Zoph. Revisiting resnets: Improved training and scaling strategies.
Advances in Neural Information Processing Systems , 34:22614–22627, 2021.
[5]Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski,
and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings
of the IEEE/CVF international conference on computer vision , pages 9650–9660, 2021.
[6]Keumgang Cha, Junghoon Seo, and Taekyung Lee. A billion-scale foundation model for remote
sensing images. arXiv preprint arXiv:2304.05215 , 2023.
[7]Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297 , 2020.
[8]Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification:
Benchmark and state of the art. Proceedings of the IEEE , 105(10):1865–1883, 2017.
[9] François Chollet et al. Keras. https://keras.io , 2015.
[10] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall
Burke, David Lobell, and Stefano Ermon. Satmae: Pre-training transformers for temporal
and multi-spectral satellite imagery. Advances in Neural Information Processing Systems , 35:
197–211, 2022.
[11] Isaac Corley and Peyman Najafirad. Supervising remote sensing change detection models with
3d surface semantics. In 2022 IEEE International Conference on Image Processing (ICIP) ,
pages 3753–3757. IEEE, 2022.
[12] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions on
information theory , 13(1):21–27, 1967.
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition , pages 248–255. Ieee, 2009.
[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020.
10
[15] Matthias Drusch, Umberto Del Bello, Sébastien Carlier, Olivier Colin, Veronica Fernandez,
Ferran Gascon, Bianca Hoersch, Claudia Isola, Paolo Laberinti, Philippe Martimort, et al.
Sentinel-2: Esa’s optical high-resolution mission for gmes operational services. Remote sensing
of Environment , 120:25–36, 2012.
[16] Xianzhi Du, Yeqing Li, Yin Cui, Rui Qian, Jing Li, and Irwan Bello. Revisiting 3d resnets for
video recognition. arXiv preprint arXiv:2109.01696 , 2021.
[17] USDA Farm Service Agency (FSA). National Agriculture Imagery Program (NAIP). USDA
Geospatial Data Gateway, 2015.
[18] Anthony Fuller, Koreen Millard, and James R Green. Satvit: Pretraining transformers for earth
observation. IEEE Geoscience and Remote Sensing Letters , 19:1–5, 2022.
[19] Anthony Fuller, Koreen Millard, and James R Green. Transfer learning with pretrained remote
sensing transformers. arXiv preprint arXiv:2209.14969 , 2022.
[20] Peng Gao, Teli Ma, Hongsheng Li, Jifeng Dai, and Yu Qiao. Convmae: Masked convolution
meets masked autoencoders. arXiv preprint arXiv:2205.03892 , 2022.
[21] Ankit Goyal, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. Revisiting point cloud shape
classification with a simple and effective baseline. In International Conference on Machine
Learning , pages 3809–3820. PMLR, 2021.
[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770–778, 2016.
[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 9729–9738, 2020.
[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked
autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 16000–16009, 2022.
[25] Konrad Heidler, Lichao Mou, Di Hu, Pu Jin, Guangyao Li, Chuang Gan, Ji-Rong Wen, and
Xiao Xiang Zhu. Self-supervised audiovisual representation learning for remote sensing data.
International Journal of Applied Earth Observation and Geoinformation , 116:103130, 2023.
[26] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel
dataset and deep learning benchmark for land use and land cover classification. IEEE Journal
of Selected Topics in Applied Earth Observations and Remote Sensing , 12(7):2217–2226, 2019.
[27] Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer
learning? arXiv preprint arXiv:1608.08614 , 2016.
[28] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan
Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi,
Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/
zenodo.5143773 . If you use this software, please cite it as below.
[29] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and
Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural
information processing systems , 30, 2017.
[30] Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang. Fine-
tuning can distort pretrained features and underperform out-of-distribution. arXiv preprint
arXiv:2202.10054 , 2022.
[31] Alexandre Lacoste, Evan David Sherwin, Hannah Kerner, Hamed Alemohammad, Björn Lütjens,
Jeremy Irvin, David Dao, Alex Chang, Mehmet Gunturkun, Alexandre Drouin, et al. Toward
foundation models for earth monitoring: Proposal for a climate change benchmark. arXiv
preprint arXiv:2112.00570 , 2021.
11
[32] Qun Liu, Saikat Basu, Sangram Ganguly, Supratik Mukhopadhyay, Robert DiBiano, Manohar
Karki, and Ramakrishna Nemani. Deepsat v2: feature augmented convolutional neural nets for
satellite image classification. Remote Sensing Letters , 11(2):156–165, 2020.
[33] Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao Liu, Song
Gao, Tianming Liu, Gao Cong, Yingjie Hu, et al. On the opportunities and challenges of
foundation models for geospatial artificial intelligence. arXiv preprint arXiv:2304.06798 , 2023.
[34] TorchVision maintainers and contributors. Torchvision: Pytorch’s computer vision library.
https://github.com/pytorch/vision , 2016.
[35] Oscar Manas, Alexandre Lacoste, Xavier Giró-i Nieto, David Vazquez, and Pau Rodriguez. Sea-
sonal contrast: Unsupervised pre-training from uncurated remote sensing data. In Proceedings
of the IEEE/CVF International Conference on Computer Vision , pages 9414–9423, 2021.
[36] Georgii Mikriukov, Mahdyar Ravanbakhsh, and Begüm Demir. Deep unsupervised contrastive
hashing for large-scale cross-modal text-image retrieval in remote sensing. arXiv preprint
arXiv:2201.08125 , 2022.
[37] Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. A metric learning reality check. In
Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part XXV 16 , pages 681–699. Springer, 2020.
[38] Maxim Neumann, Andre Susano Pinto, Xiaohua Zhai, and Neil Houlsby. In-domain representa-
tion learning for remote sensing. arXiv preprint arXiv:1911.06721 , 2019.
[39] Maxim Neumann, André Susano Pinto, Xiaohua Zhai, and Neil Houlsby. Training general
representations for remote sensing using in-domain knowledge. In IGARSS 2020-2020 IEEE
International Geoscience and Remote Sensing Symposium , pages 6730–6733. IEEE, 2020.
[40] Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, and Aditya Grover.
Climax: A foundation model for weather and climate. arXiv preprint arXiv:2301.10343 , 2023.
[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-
Performance Deep Learning Library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché
Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32 ,
pages 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.
[42] Colorado J Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian Clipp,
Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scale-mae: A scale-aware masked
autoencoder for multiscale geospatial representation learning. arXiv preprint arXiv:2212.14532 ,
2022.
[43] Mats L Richter, Wolf Byttner, Ulf Krumnack, Anna Wiedenroth, Ludwig Schallner, and
Justin Shenk. (input) size matters for cnn classifiers. In Artificial Neural Networks and
Machine Learning–ICANN 2021: 30th International Conference on Artificial Neural Networks,
Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part II 30 , pages 133–144. Springer,
2021.
[44] Esther Rolf, Jonathan Proctor, Tamma Carleton, Ian Bolliger, Vaishaal Shankar, Miyabi Ishihara,
Benjamin Recht, and Solomon Hsiang. A generalizable and accessible approach to machine
learning with global satellite imagery. Nature communications , 12(1):4392, 2021.
[45] David P Roy, Michael A Wulder, Thomas R Loveland, Curtis E Woodcock, Richard G Allen,
Martha C Anderson, Dennis Helder, James R Irons, David M Johnson, Robert Kennedy, et al.
Landsat-8: Science and product vision for terrestrial global change research. Remote sensing of
Environment , 145:154–172, 2014.
12
[46] Adam J Stewart, Caleb Robinson, Isaac A Corley, Anthony Ortiz, Juan M Lavista Ferres, and
Arindam Banerjee. Torchgeo: deep learning with geospatial data. In Proceedings of the 30th
International Conference on Advances in Geographic Information Systems , pages 1–12, 2022.
[47] Gencer Sumbul, Marcela Charfuelan, Begüm Demir, and V olker Markl. Bigearthnet: A large-
scale benchmark archive for remote sensing image understanding. In IGARSS 2019-2019 IEEE
International Geoscience and Remote Sensing Symposium , pages 5901–5904. IEEE, 2019.
[48] Gencer Sumbul, Arne De Wall, Tristan Kreuziger, Filipe Marcelino, Hugo Costa, Pedro Bene-
vides, Mario Caetano, Begüm Demir, and V olker Markl. Bigearthnet-mm: A large-scale,
multimodal, multilabel benchmark archive for remote sensing image classification and retrieval
[software and data sets]. IEEE Geoscience and Remote Sensing Magazine , 9(3):174–180, 2021.
[49] Xian Sun, Peijin Wang, Wanxuan Lu, Zicong Zhu, Xiaonan Lu, Qibin He, Junxi Li, Xuee Rong,
Zhujun Yang, Hao Chang, et al. Ringmo: A remote sensing foundation model with masked
image modeling. IEEE Transactions on Geoscience and Remote Sensing , 2022.
[50] Ramon Torres, Paul Snoeij, Dirk Geudtner, David Bibby, Malcolm Davidson, Evert Attema,
Pierre Potin, BjÖrn Rommen, Nicolas Floury, Mike Brown, et al. Gmes sentinel-1 mission.
Remote sensing of environment , 120:9–24, 2012.
[51] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Hervé Jégou. Fixing the train-test
resolution discrepancy. Advances in neural information processing systems , 32, 2019.
[52] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spa-
tiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international
conference on computer vision , pages 4489–4497, 2015.
[53] Gabriel Tseng, Ivan Zvonkov, Mirali Purohit, David Rolnick, and Hannah Kerner. Lightweight,
pre-trained transformers for remote sensing timeseries. arXiv preprint arXiv:2304.14065 , 2023.
[54] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research , 9(11), 2008.
[55] Di Wang, Jing Zhang, Bo Du, Gui-Song Xia, and Dacheng Tao. An empirical study of remote
sensing pretraining. IEEE Transactions on Geoscience and Remote Sensing , 2022.
[56] Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad M Albrecht, and
Xiao Xiang Zhu. Ssl4eo-s12: A large-scale multi-modal, multi-temporal dataset for self-
supervised learning in earth observation. arXiv preprint arXiv:2211.07044 , 2022.
[57] Ross Wightman, Nathan Raw, Alexander Soare, Aman Arora, Chris Ha, Christoph Reich,
Fredo Guan, Jakub Kaczmarzyk, mrT23, Mike, SeeFun, contrastive, Mohammed Rizin,
Hyeongchan Kim, Csaba Kertész, Dushyant Mehta, Guillem Cucurull, Kushajveer Singh,
hankyul, Yuki Tatsunami, Andrew Lavin, Juntang Zhuang, Matthijs Hollemans, Mohamed
Rashad, Sepehr Sameni, Vyacheslav Shults, Lucain, Xiao Wang, Yonghye Kwon, and Yusuke
Uchida. rwightman/pytorch-image-models: v0.8.10dev0 Release, February 2023. URL
https://doi.org/10.5281/zenodo.7618837 .
[58] Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use clas-
sification. In Proceedings of the 18th SIGSPATIAL international conference on advances in
geographic information systems , pages 270–279, 2010.
[59] Xiao Xiang Zhu, Jingliang Hu, Chunping Qiu, Yilei Shi, Jian Kang, Lichao Mou, Hossein
Bagheri, Matthias Haberle, Yuansheng Hua, Rong Huang, et al. So2sat lcz42: A benchmark
data set for the classification of global local climate zones [software and data sets]. IEEE
Geoscience and Remote Sensing Magazine , 8(3):76–89, 2020.
13
A Benchmarks
To illustrate the simplicity of loading the models used in our benchmarks we show examples of how
to initialize all the models used in our paper in Python on this page and an example of how to apply
one of these models to a dataset on the next page. We hope that this low-overhead will encourage the
use of simple baselines in future work.
A.1 Model Loading
import timm
from torchgeo.models import (
ResNet50_Weights,
RCF,
MOSAIKS,
ImageStatistics,
get_model
)
from torchvision.models.feature_extraction import create_feature_extractor
train_ds = ...
channels = ...
seed = ...
image_stat = ImageStatistics()
rcf = RCF(
in_channels=channels,
features=512,
kernel_size=3,
seed=seed
)
mosaiks = MOSAIKS(
dataset=train_ds,
in_channels=channels,
features=512,
kernel_size=3,
seed=seed
)
imagenet = timm.create_model(
"resnet50",
in_chans=channels,
pretrained=True,
num_classes=0
)
random = timm.create_model(
"resnet50",
in_chans=channels,
pretrained=False,
num_classes=0
)
moco = get_model(
"resnet50",
weights=ResNet50_Weights.SENTINEL2_ALL_MOCO
)
moco = create_feature_extractor(
moco,
return_nodes=["global_pool"]
)
14
A.2 Model Benchmarking
Below is an example of how to benchmark a K-Nearest Neighbors classifier on MOSAIKS features
from the EuroSAT multispectral dataset.
from torchgeo.datamodules import EuroSATDataModule
from torchgeo.models import MOSAIKS
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
channels = 13
seed = 0
dm = EuroSATDataModule("data/eurosat/", download=True)
dm.setup(stage="fit")
model = MOSAIKS(
dataset=dm.train_dataset,
in_channels=channels,
features=512,
kernel_size=3,
seed=seed
)
model = model.eval().cuda()
x_train, y_train = [], []
for batch in dm.train_dataloader():
with torch.no_grad():
x = batch["image"].cuda()
x_train.append(model(x).cpu())
y_train.append(batch["label"])
dm.setup(stage="test")
x_test, y_test = [], []
for batch in dm.test_dataloader():
with torch.no_grad():
x = batch["image"].cuda()
x_test.append(model(x).cpu())
y_test.append(batch["label"])
x_train = torch.cat(x_train).numpy()
x_test = torch.cat(x_test).numpy()
y_train = torch.cat(y_train).numpy()
y_test = torch.cat(y_test).numpy()
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)
knn = KNeighborsClassifier(n_neighbors=5, algorithm="brute")
knn.fit(X=x_train, y=y_train)
y_pred = knn.predict(x_test)
print(accuracy_score(y_test, y_pred)) * 100.0
# 91.0740
15
Temporal Cluster Matching for Change Detection of Structures
from Satellite Imagery
Caleb Robinson
caleb.robinson@microsoft.com
Microsoft AI for Good Research LabAnthony Ortiz
anthony.ortiz@microsoft.com
Microsoft AI for Good Research LabJuan M. Lavista Ferres
jlavista@microsoft.com
Microsoft AI for Good Research Lab
Brandon Anderson
branders@law.stanford.edu
Stanford RegLabDaniel E. Ho
dho@law.stanford.edu
Stanford RegLab
ABSTRACT
Longitudinal studies are vital to understanding dynamic changes of
the planet, but labels (e.g., buildings, facilities, roads) are often avail-
able only for a single point in time. We propose a general model,
Temporal Cluster Matching (TCM), for detecting building changes
in time series of remotely sensed imagery when footprint labels are
observed only once. The intuition behind the model is that the rela-
tionship between spectral values inside and outside of building’s
footprint will change when a building is constructed (or demol-
ished). For instance, in rural settings, the pre-construction area may
look similar to the surrounding environment until the building is
constructed. Similarly, in urban settings, the pre-construction areas
will look different from the surrounding environment until con-
struction. We further propose a heuristic method for selecting the
parameters of our model which allows it to be applied in novel set-
tings without requiring data labeling efforts (to fit the parameters).
We apply our model over a dataset of poultry barns from 2016/2017
high-resolution aerial imagery in the Delmarva Peninsula and a
dataset of solar farms from a 2020 mosaic of Sentinel 2 imagery
in India. Our results show that our model performs as well when
fit using the proposed heuristic as it does when fit with labeled
data, and further, that supervised versions of our model perform
the best among all the baselines we test against. Finally, we show
that our proposed approach can act as an effective data augmenta-
tion strategy – it enables researchers to augment existing structure
footprint labels along the time dimension and thus use imagery
from multiple points in time to train deep learning models. We
show that this improves the spatial generalization of such models
when evaluated on the same change detection task.
CCS CONCEPTS
•Computing methodologies →Machine learning ;Unsuper-
vised learning ;Computer vision representations .
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
COMPASS ’21, June 28-July 2, 2021, Virtual Event, Australia
©2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8453-7/21/06. . . $15.00
https://doi.org/10.1145/3460112.3471952KEYWORDS
change detection, building footprints, deep learning, clustering
ACM Reference Format:
Caleb Robinson, Anthony Ortiz, Juan M. Lavista Ferres, Brandon Anderson,
and Daniel E. Ho. 2021. Temporal Cluster Matching for Change Detection of
Structures from Satellite Imagery. In ACM SIGCAS Conference on Computing
and Sustainable Societies (COMPASS) (COMPASS ’21), June 28-July 2, 2021,
Virtual Event, Australia. ACM, New York, NY, USA, 9 pages. https://doi.org/
10.1145/3460112.3471952
1 INTRODUCTION
Catalogs of high-resolution remotely sensed imagery have become
increasingly available to the scientific community. The availability
of such imagery has revolutionized scientific fields and society
at large. For example, 1m resolution aerial imagery from the US
Department of Agriculture (NAIP imagery) has been released on
a 2-year rolling basis over the entire US for over a decade and
the commercial satellite imagery provider, Planet, recently started
to release 5m satellite imagery covering the whole tropical forest
region of the world on a monthly basis. One estimate is that the
opening of Landsat imagery in 2008 led to the creation of $3.45B
in economic value in 2017 alone [ 27]. The accumulation of such
data facilitates an entirely new branch of longitudinal studies –
analyzing the Earth and how it has changed over time.
As the climate, technology, and human population change on
an ever more rapid timescale, such longitudinal studies become
particularly vital to understanding the past, present, and future of
the environment. Despite the usefulness of time series data, such
research faces two important practical challenges. First, the large
labeled datasets that have fueled advances in computer vision are
much more limited in the satellite imagery context [ 5,6,25]. Sec-
ond, efforts in creating labeled data from remotely sensed imagery
are typically focused on a single point in time [ 6,25,36]. Project
requirements may only call for a single layer of labels, or budget
constraints may limit the number of labels that can be generated.
This has the effect of creating labeled datasets that are “frozen” in
time. Expanding such “frozen” datasets to multiple points in time in
independent follow-up work can be resource-intensive and difficult,
as the same image-preprocessing and labeling methodology steps
used in the original work need to be precisely reproduced in order
to generate comparable data.
Going beyond “frozen” datasets would enable a wide range of
temporal inferences from satellite imagery, with significant social,
economic, and policy implications. Previous studies include thearXiv:2103.09787v2  [cs.CV]  29 Jun 2021
COMPASS ’21, June 28-July 2, 2021, Virtual Event, Australia Robinson, et al.
detection of urban expansion [ 35], zoning violations [ 23], habitat
modification [ 7], compliance with agricultural subsidies [ 20], con-
struction on wetlands [ 12], and damage assessments from natural
disasters [9, 10, 19].
Algorithmic approaches for expanding “frozen” datasets can
thus be useful in facilitating ecological and policy-based analy-
sis. In this work we propose a simple model, Temporal Cluster
Matching (TCM), for determining when structures were previously
constructed given a labeled dataset of structure footprints generated
from imagery captured at a particular point in time. This model,
importantly, does not rely on the differences in spectral values
between layers of remotely sensed imagery as there can be consid-
erable variance in these values depending on imaging conditions,
the type of sensor used, etc. Instead, it compares a representation
of the spectral values inside a building footprint to a representation
of the spectral values in the surrounding area for each point in the
time series. Whenever the distribution of spectral values within
the footprint becomes dissimilar to that of its surroundings then
the footprint is likely to have been developed. We further propose
a method for fitting the parameters of this model which does not
rely on additional labeled footprint data over time and show that
this “semi-supervised TCM” performs comparably to supervised
methods.
Specifically, we demonstrate the performance of this algorithm
in two distinct settings:
(1)Poultry barns from concentrated animal feeding operations
(CAFOs) in the United States, using high-resolution aerial
imagery from the National Agricultural Imagery Program
(NAIP), and
(2)Solar farm footprints in the Indian state of Karnataka, using
Sentinel 2 annual mosaics.
Both settings are of significant environmental importance and
are ripe for longitudinal study. First, CAFOs can have profound
effects on water quality and human health in their proximity [ 22].
Nitrates and other potentially harmful chemicals can, for exam-
ple, make their way into the groundwater, spreading to adjacent
wells and bodies of water over timescales that range into decades.
Usage of antibiotics for growth promotion can lead to resistant
bacterial infections in nearby populations [ 2]. Effective regulation
in either scenario requires differentiation of these contaminant
sources, which, in turn, depends on accurate historical labels and
spatio-temporal modeling.
Second, understanding the growth of solar systems is increas-
ingly important in the transition toward clean energy. India is an
important example of this, as it has set ambitious goals of generat-
ing 450 GW of renewable energy by 2030 with 175 GW deployment
by 2022 [ 8]. Achieving this goal will require an expansion of solar
farm installations throughout the country and policy makers will
be able to determine better the effects of country-wide efforts with
solar farm change data that can be updated year-over-year in a
consistent manner. Understanding such solar expansion may also
enable more targeted investments for solar potential [21].
To summarize, our contributions include:
•A lightweight model, Temporal Cluster Matching, for de-
tecting when structures are developed in a time-series of
remotely sensed imagery, as well as a heuristic method forfitting the parameters of the model. Combined, this results
in a proposed approach that only relies on labeled building
footprints for a single point in time.
•A series of baseline methods, both supervised and semi-
supervised, to evaluate our proposed approach against.
•Experiments comparing our model to the baseline models
in two datasets: poultry barn footprints with aerial imagery,
and solar farm footprints with satellite imagery.
•A code release that allows users to run the model in novel
settings, as well as scripts for reproducing our experiments:
https://github.com/microsoft/temporal-cluster-matching
2 RELATED WORK
Our work pertains to several different literatures. First, much work
has focused on methods for detection of building footprints. For
instance, [ 39] uses Mask R-CNN to train a model to detect buildings
while [ 37] uses a semantic segmentation model (U-Net [ 24]) to
segment buildings in imagery. While deep learning approaches
have made rapid advances, they have largely been focused on static
inferences. Moving to a different domain (spatially or temporally)
can prove challenging due to shifts in the input distribution (what
the input images look like), co-registration errors, and shifts in the
target (what buildings looks like) [ 31]. Zhang et al., for instance,
note these as some of the challenges faced when detecting structures
at a global scale [38].
Second, other research has focused on detecting changes in satel-
lite imagery. Historically, the remote sensing literature has started
from pixel-wise change-point detection – detecting when change
happens in a time series of repeated observations of the same lo-
cation in space. Much work has focused on how to model the
characteristics of these time series such as seasonality, changes in
illumination, atmospheric conditions, etc [ 1]. A popular method
for performing this task, BFAST, models a time series of remotely
sensed observations with trend, seasonal, and remainder compo-
nents, then uses an unsupervised iterative method to detect change
points based on the model [ 34]. This method relies on observing a
relatively long time series, e.g. that shows seasonal components, and
thus is not applicable to time series with few data points. Change-
point detection can also be performed on other units of analysis.
[30] review the literature and organize methods based on their
unit of analysis, e.g. pixel-based, object-based, kernel-based, and
based on their method for comparing scenes, e.g. based on differ-
encing, transformation, or modeling. Within this organization, our
work is the most similar to those that operate on image-object
overlays [ 14,16,29] whereby a single segmentation is applied to all
imagery in a time series and change at an object level is computed.
We use similar ideas in designing our baseline approaches (Section
3.4).
The computer vision and machine learning literature also ad-
dresses similar problems. Several methods use a fully supervised
approach to detect changes in known building locations: [ 15] use
a supervised approach with decision trees to classify whether a
building change occurred from pairs of imagery and [ 17] use sup-
port vector machines to provide estimates for which buildings have
changed. Other methods perform a superpixel segmentation step
to create objects in pairs of imagery, then model change over these
Temporal Cluster Matching for Change Detection of Structures from Satellite Imagery COMPASS ’21, June 28-July 2, 2021, Virtual Event, Australia
Figure 1: (A and B) Examples of two poultry barn footprints over 5 years of NAIP imagery. We observe inter-year variability
of NAIP imagery and the change in the relation of color/texture between the footprint and neighborhood when a footprint
is “developed”. (C and D) Examples of two solar farm footprints over 5 years of Sentinel 2 imagery. Note, in A we outline the
building footprint location in yellow through the entire series of imagery, but omit this outline in remaining rows.
objects with a Markov random field [ 18]. Most recently, advances in
deep learning have driven end-to-end pipelines in building change
detection. [ 3], for instance, uses a Generative Adversarial Network
(GAN) to overcome the limitations of pixel-level inferences. These
methods all rely on having existing labeled data on either when
changes have occurred, or on unchanged areas in consecutive pairs
of imagery.
Finally, numerous research teams have provided benchmark
datasets for evaluating models at a single point in time [ 6,25,36].
Few datasets provide longitudinal information about the same lo-
cation over time, so a typical research approach has been to train a
model on labeled imagery from one period. [ 11], for instance, assess
the growth of intensive livestock farms in North Carolina, but do
so using a model trained on images of such facilities for a single
period of time. A notable exception to this is the recent SpaceNet
7 dataset/challenge [ 33]. This dataset includes 24 multi-spectral
images at a 4m/px spatial resolution as well as building footprint
labels over time for over 100 unique locations around the world.
It is particularly challenging for object based change detection ap-
proaches as the median building size is 12.1 pixels [ 33] and theimagery is not perfectly co-registered. Pixel based segmentation
models followed by in-depth post-processing methods achieved the
top performance in the competition [ 32], however it is not yet clear
how to adapt such methodology to general change detection tasks.
Our approach contributes to this body of work by providing a
semi-supervised approach for detecting changes in structures that
easily enables researchers to expand a dataset beyond a single time
period, hence enabling domain adaptation by efficient sampling
of images across time. The approach we propose can be seen as a
lightweight, data-driven method to expand “frozen” imagery lon-
gitudinally, enabling researchers to address a rich set of dynamic
questions.
3 METHODS
3.1 Problem statement
Formally, we would like to find when a structure was developed (or,
more generally, changed) given a time series of remotely sensed
imagery of it and the surrounding area,
X1,...,X𝑡
and its foot-
print ,𝑃, at time𝑡. We represent this footprint as a mask, Y𝑡. Here,
COMPASS ’21, June 28-July 2, 2021, Virtual Event, Australia Robinson, et al.
Figure 2: Schematic description of Temporal Cluster Matching: (A) the input imagery and query building footprint; (B) 𝑘-means
clustering of the input imagery (C, D) discrete distributions of clusters created from the pixels within the footprint polygon
and from the pixels outside of the footprint (i.e. the neighborhood); (E) the KL-divergence between the two distributions
indicates the similarity of the footprint to its neighborhood.
the𝑖thimage in the time series X𝑖∈Z𝑤×ℎ×𝑐is a georeferenced
image with width, 𝑤, height,ℎ, and number of spectral bands, 𝑐.
Similarly, Y𝑡∈{0,1}𝑤×ℎis a georeferenced binary mask with the
same dimensions that contains a 1in every spatial location that
the given structure covers at time 𝑡and a 0elsewhere. We want to
estimate the point in time that the structure was created, i.e. find
𝑙∈[1,𝑡], where X𝑙contains the structure, for the smallest such 𝑙.
Note that we assume the structure to exist at 𝑡.
3.2 Temporal Cluster Matching
Our proposed model, Temporal Cluster Matching, relies on the
assumption that when a structure is built its footprint will have a
different set of colors and textures than its immediate surroundings
compared to when the structure was not built. For example, an
undeveloped piece of land in a rural setting will likely contain some
sort of vegetation, and that vegetation will probably look similar
(in color/texture) to some of its surroundings. When a structure is
built on this land, then it will likely look dissimilar to its surround-
ings (unless e.g. its entire surroundings are also developed at the
same time). The same intuition holds in urban environments – an
undeveloped piece of land will look dissimilar to its surroundings,
however, when it is developed, it will look similar.
We assume that we are given a footprint ,𝑃, that outlines a structure
that has been labeled as developed at time 𝑡. Now, we formally
define the neighborhood of this footprint . This neighborhood should
be larger than the extent of the footprint in order to observe a rep-
resentative set of colors/textures, so we let 𝑟be a radius that servesas a buffer to the building footprint polygon. We then create Y𝑡
by rasterizing the polygon within this buffered extent and create
X0,...,X𝑡
by cropping the same buffered extent from each layer
of remotely sensed imagery.
Next, we define a method for comparing the set of colors/textures
within the footprint to those in the surrounding neighborhood . Given
a single layer of remotely sensed image from the time series, X,
we run𝑘-means to partition the pixels into 𝑘clusters. Each pixel
can be represented by a set of features that encodes color and tex-
ture at its location, for example: the spectral values at the pixel’s
location, a texture descriptor (such as a local binary pattern) at
the location, the spectral values in a window around the location,
or some combination of the previous representations. Regardless,
the cluster model will assign a cluster index to each pixel in X
which we call C. We then represent an area by the discrete distri-
bution of cluster indices observed in that area. Specifically, we let
𝐷footprint be the distribution of cluster indices from C[Y𝑡=1]and
𝐷neighborhood be the distribution of cluster indices from C[Y𝑡=0]1.
Now, we can compare the set of colors/textures within a foot-
print to those in its surrounding neighborhood by calculating the
KL-divergence between the two distributions of cluster indices,
𝑑=𝐷𝐾𝐿(𝐷footprint||𝐷neighborhood). Larger KL-divergence values
mean that the color/texture of a footprint is dissimilar to that of its
surrounding neighborhood and that it is likely to be developed. We
1We use the notation C[Y𝑡=1]to mean all the cluster indices of pixels where
Y𝑡=1. We build the discrete distribution by counting the number of pixels assigned
to each cluster and normalizing the vector of counts by its sum.
Temporal Cluster Matching for Change Detection of Structures from Satellite Imagery COMPASS ’21, June 28-July 2, 2021, Virtual Event, Australia
perform this comparison method for each image in the time series
to create a list of KL-divergence values [𝑑1,...,𝑑𝑡]
Finally, we let 𝜃be a threshold value to determine the smallest
KL-divergence value that we will consider to indicate a “developed”
footprint. More specifically, our model will estimate 𝑙as the time
that a footprint is first developed for the first 𝑙where𝑑𝑙>𝜃. This
parameter can be found by experimentation using labeled data, or
with the heuristic method we describe in Section 3.3. See Algorithm
1 and Figure 2 for an overview of this proposed approach.
Algorithm 1: Temporal Cluster Matching
Input: Time series of remotely sensed imagery, 𝑃,𝑘,𝑟, and
𝜃
Output:𝑙, the first point in time that the footprint
described by 𝑃was developed
1
X1,X1,...,X𝑡
←crop the imagery according to the
buffered extent of 𝑃by a radius𝑟
2Y𝑡←rasterize𝑃in the same buffered extent
3for𝑙←1to𝑡do
4 C←cluster indices from a 𝑘-means clustering of X𝑙
into𝑘clusters
5𝐷footprint←distribution of cluster indices C[Y𝑡=1]
6𝐷neighborhood←distribution of cluster indices
C[Y𝑡=0]
7𝑑←𝐷𝐾𝐿(𝐷footprint||𝐷neighborhood)
8 if𝑑>𝜃then
9 return𝑙
10 end
11end
12return𝑡
Section 3.4 explores more complex decision models than the
single threshold described above, but we note that these require
labeled data to fit.
3.3 A heuristic for semi-supervised Temporal
Cluster Matching
In application scenarios we would like to use our model, given a
dataset of (a) known structure footprints at time 𝑡and (b) a time
series of remotely sensed imagery over a certain study area, to find
when each structure was constructed. Here we propose a method for
determining reasonable parameter values for the number of clusters,
𝑘, buffer radius, 𝑟, and decision threshold, 𝜃,without assuming that
we have prior labeled data on construction dates .
This heuristic compares the distribution of KL-divergence values
calculated by our algorithm for given hyperparameters, 𝑘and𝑟,
over all footprints at time 𝑡(when we assume that structures exist)
to the distribution of KL-divergence values over a set of randomly
generated polygons over the study area. The intuition is that the
relationship between random polygons and their neighborhoods
is similar to the relationship between undeveloped structure foot-
prints and their neighborhoods . In other words, this distribution
of KL divergence values between color distributions from random
polygons and their surroundings will represent what we wouldexpect to observe by chance – i.e. notthe relationship between the
colors in a building footprint and its surroundings. We want to find
parameter settings for our algorithm that minimize the overlap be-
tween these two distributions because it will make it easier to iden-
tify change (see Figure 3 for an illustration of this for poultry barn
footprints). Formally, we let 𝑝be the distribution of KL-divergence
values over footprints at 𝑡and𝑞be the distribution of KL-divergence
values over random polygons sampled from the study area (over all
points in time). These are discrete distributions (e.g after binning
KL divergence values) and we can measure the overlap with the
Bhattacharyya coefficient, 𝐵𝐶(𝑝,𝑞)=Í
𝑥∈𝑋√︁
𝑝(𝑥)𝑞(𝑥). Choosing
𝑘and𝑟thus becomes a search min𝑘,𝑟𝐵𝐶(𝑝,𝑞).
Finally, after choosing 𝑘and𝑟, we can simply choose 𝜃as a value
representing the 98th percentile (or similar) of the resulting distri-
bution of random polygons, 𝑞. Practically, this value simply needs
to separate 𝑝and𝑞and visualization of these two distributions
should suggest appropriate values.
We test this heuristic in Section 5.1 by comparing change de-
tection performance from fitting our proposed model with this
heuristic versus with labeled data.
Figure 3: We show the distribution of KL divergence values
generated by our proposed approach for (1) poultry barn
footprints in aerial imagery where there is guaranteed to
be a structure and (2) randomly generated footprints over
similar aerial imagery. We observe that (1) is unimodal with
a large mean value, as the color distributions of footprints
that contain buildings are dissimilar to the color distribu-
tion of their surroundings and (2) is unimodal with a small
mean value, as random patches are highly likely to have a
color distribution that is similar to their neighborhood. Our
proposed heuristic method looks to find hyperparameters
for the model that minimize the overlap in these two distri-
butions such that a simple threshold can identify changes in
building footprints.
3.4 Baseline approaches
Here we propose a series of baselines and variants of our model
to compare against. We refer to our proposed model / heuristic
for fitting the model as “Semi-supervised TCM” as it only depends
on labeled building footprints from a single point in time. This is
specifically in contrast to variant approaches like “Supervised TCM”
COMPASS ’21, June 28-July 2, 2021, Virtual Event, Australia Robinson, et al.
(see below) that use labeled building footprints over time to fit the
model parameters.
Supervised TCM Here, we fit the parameter 𝜃in our proposed
model using labeled data instead of our proposed heuristic.
We can do this by searching over values of 𝜃and measuring
performance on the labeled data. In this case, 𝑘and𝑟are
model hyperparameters that can be searched over using
validation data.
Supervised TCM with LR We use the series of KL-divergence
values computed by TCM as a feature representation in a
logistic regression (LR) model that directly predicts which
point in time a structure is first observed. This is a supervised
method as it requires a sample of labeled footprint data over
time to fit.
Average-color with threshold This baseline uses the same
structure as TCM with two changes: instead of clustering col-
ors we compute average colors representations (over space
for each spectral band) and instead of computing KL-divergence
between distributions of cluster indices we compute the Eu-
clidean distance between the average colors representations.
Specifically, we compute the average color in a footprint
and the average color of its neighborhood, then take the
Euclidean distance between them and treat this distance in
the same way we have previously treated the KL-divergence
values. This has the effect of removing 𝑘as a hyperparame-
ter, however the rest of the algorithm stays the same. Similar
to the KL with threshold method we fit 𝜃using labeled data.
Average-color with LR This method is identical to Super-
vised TCM with LR, but using the technique from Average-
color with threshold to compute Euclidean distances between
average color representations.
Color-over-time In this baseline we compute features from a
time series of imagery by averaging the colors (over space
for each spectral band) in the given footprint at each point
in time, then taking the Euclidean distance between these
average representations in subsequent pairs of imagery. For
example, a time series of 5 images would result in an overall
feature representation of 4 distances: the distance between
the average colors at time 1 and average colors at time 2, the
distance between the average colors at time 2 and the average
colors at time 3, etc. We use this overall representation in a
logistic regression model that predicts which point in time
the structure is first observed.
CNN-over-time In this baseline we use the given structure
footprints and satellite imagery at time 𝑡to train a U-Net
based semantic segmentation model to predict whether or
not each pixel in an image contains a developed structure.
We then use this trained model to score the imagery from
each point and time and determine the first layer in which a
building is constructed. For simplicity, if the network predicts
that over 50% of a footprint is constructed, then we count it
as constructed.
Mode predictions This baseline is simply predicting the most
frequent time point that we first observe constructed build-
ings based on the labels in the dataset of interest. For example,if ‘2011’ was the most frequent year that we observed build-
ings to be constructed in a dataset, then this approach would
predict every building was constructed in 2011 regardless of
input. This serves as a lower bound on the performance of
the supervised methods.
4 DATASETS
4.1 Poultry barn dataset
We use the Soroka and Duren dataset of 6,013 labeled poultry barn
polygons, Poultry barn , created from aerial imagery from the
National Agriculture Imagery Program (NAIP) from 2016/2017 over
the Delmarva Peninsula (containing portions of Virginia, Maryland,
and Delaware) [ 26]. NAIP imagery is 4 channel (red, green, blue,
NIR) high-resolution ( ≤1m/px ) aerial imagery and is collected
independently by each state in the US at least once every three years
on a rolling basis. Because of this, the availability and quality of
the imagery varies between states. For instance, the NAIP imagery
from 2011 in Delaware and Maryland are collected on different days
of the year, at different times of day, etc. See Figure 1 for example
images of the NAIP imagery over time overlayed with the barn
footprints. Additionally, we have manually labeled the earliest year
(out of the years shown in Table 1) that a poultry barn can be seen
for a random subset of 1,000 of the poultry barn footprints.
State Years of NAIP data
Delaware 2011, 2013, 2015, 2017, 2018
Maryland 2011, 2013, 2015, 2017, 2018
Virginia 2011, 2012, 2014, 2016, 2018
Table 1: NAIP data availability over states covering the Del-
marva Peninsula.
4.2 Solar farm dataset
We also use a solar farm dataset, Solar farm , containing poly-
gons delineating solar installations in the Indian state of Karnataka
for the year 2020. The dataset includes 935 individual polygons
covering a total area or 25.7 km2. The polygons were created by
manually filtering the results of a model run on an annual median
composite of Sentinel 2 multispectral surface reflectance imagery.
We collect additional median composites of Sentinel 2 imagery for
2016 through 20192to use for change detection. See Figure 1 for
examples of the imagery overlayed with the solar farm footprints.
For each of the 935 footprints we have manually labeled the earliest
year (between 2016 and 2020) that a solar farm can be seen in the
imagery.
5 EXPERIMENTS AND RESULTS
We experiment with different configurations of our algorithm on
thePoultry barn andSolar farm datasets. In all experiments we
measure the accuracy (ACC) – the percentage of labeled footprints
2The data from 2016, 2017 and 2018 are composites of the Sentinel 2 top of atmosphere
products (Level 1C), while the 2019 and 2020 data are additionally corrected for surface
reflectance (Level 2A). All data was processed with Google Earth Engine using the
COPERNICUS/S2 andCOPERNICUS/S2_SR collections respectively.
Temporal Cluster Matching for Change Detection of Structures from Satellite Imagery COMPASS ’21, June 28-July 2, 2021, Virtual Event, Australia
Method Semi-Supervised ACC MAE
Poultry barn Semi-supervised TCM ✓ 0.94 0.15
Supervised TCM 0.93 +/- 0.01 0.17 +/- 0.04
Supervised TCM with LR 0.96 +/- 0.01 0.12 +/- 0.03
CNN over time ✓ 0.37 1.36
Average-color with LR 0.95 +/- 0.01 0.15 +/- 0.05
Average-color with threshold 0.91 +/- 0.02 0.24 +/- 0.06
Color-over-time 0.90 +/- 0.02 0.41 +/- 0.08
Mode predictions 0.84 0.80
Solar farm Semi-supervised TCM ✓ 0.71 0.49
Supervised TCM 0.70 +/- 0.04 0.51 +/- 0.08
Supervised TCM with LR 0.78 +/- 0.03 0.29 +/- 0.05
CNN over time ✓ 0.64 0.68
Average-color with LR 0.65 +/- 0.03 0.49 +/- 0.05
Average-color with threshold 0.50 +/- 0.04 0.93 +/- 0.08
Color-over-time 0.79 +/- 0.01 0.29 +/- 0.02
Mode predictions 0.42 0.81
Table 2: Comparison of our proposed semi-supervised model (“Semi-supervised TCM”) to other baseline methods for detecting
change in structures over time series of imagery. Note that the semi-supervised methods only have access to building footprint
labels at time 𝑡, while the other methods are “supervised” and additionally have access to labels on when buildings were
constructed over time. We observe that our semi-supervised approach achieves identical performance to a supervised variant
where the model parameters are learned. We further observe the proposed approach outperforms supervised baseline methods
for detecting change. Reported values are shown as averages (+/-) a standard deviation over 50 random train/test splits where
appropriate. Single values are reported for the semi-supervised methods as they are evaluated on the entire labeled data set.
for which we correctly identify the first “developed” year and mean
absolute error (MAE) – the average of absolute differences between
the predicted year and labeled year.
5.1 Semi-supervised TCM
We first test how parameters chosen with the proposed heuristic
correlate with performance of the model. The benefit of the heuris-
tic method is that it does not require labeled temporal data to fit the
model, but we need to show that the parameters it selects actually
result in good performance. Here, we search over buffer sizes in
{100,200,400}meters and{0.016,0.024}degrees for the Poultry
barn andSolar farm datasets, respectively, and number of clusters
in{16,32,64}for both datasets. For each configuration combina-
tion we create 𝑝and𝑞as described in Section 3.3, compute the
Bhattacharyya coefficient, estimate 𝜃, then evaluate the predicted
change years on the labeled data. We find that the Bhattacharyya
coefficient is correlated with the result; there is -0.77 rank order cor-
relation between the coefficient and accuracy (p=0.01) in Poultry
barn and a -0.94 rank order correlation (p=0.004) in Solar farm .
In both datasets, the smallest Bhattacharyya coefficient was paired
with the best performing algorithm configuration.
Second, we compare the performance of the model with heuristic
estimated parameters to that with learned parameters. To learn the
parameters for our proposed model, “Supervised TCM”, we ran-
domly partition the labeled time series data into 80/20 train/test
splits. We find the values of 𝑘,𝑟, and𝜃(with a grid search over
the same space for 𝑘and𝑟as mentioned above) using the trainingsplit, then evaluate this model on the test split. We repeat this pro-
cess for 50 random partitions and report the average and standard
deviation metrics for the best combination Table 2. We observe
that the heuristic method produces results that are equivalent to
those of the learned model. In Poultry barn our proposed method
achieves a 94% accuracy with a mean absolute error of 0.15 years
which suggests it will be effective for driving longitudinal studies
of the growth of poultry CAFOs.
Finally, we observe that our method significantly outperforms
the other semi-supervised baseline, CNN over time. In both Poul-
try barn andSolar farm we observe considerable covariate shift.
For example, in the Poultry barn dataset there is a large shift in
input distribution over time due to the fact that the aerial imagery
is collected at different days of the year, at different times of day,
etc. The deep learning model is trained solely on imagery from
the last point in each time series where we can confirm that there
exists buildings in each footprint, however is unable to reliably gen-
eralize over time. We did not experiment with domain adaptation
techniques to attempt to fix this, however we explore the use of our
proposed method in this capacity in Section 6. We note that our
proposed model is unaffected by shifts in the input distributions
year-over-year as it never compares imagery from different years.
5.2 Supervised models
In the previous section we showed that we can estimate the param-
eters of our model without labeled time series data. Not requiring
additional labeling is a major benefit of the TCM approach. In
this section we explore the performance of our proposed approach
COMPASS ’21, June 28-July 2, 2021, Virtual Event, Australia Robinson, et al.
against supervised baseline approaches, using labels generated go-
ing back in time. We find that logistic regression models are effective
at predicting the building construction date from the series of KL
divergence values produced by our proposed approach (KL with
LR). In both datasets this method is overall the top performing
method with 96% accuracy in the Poultry barn dataset and 78%
accuracy in the Solar farm dataset. In the Solar farm dataset the
Color-over-time baseline has tied for top performance (within a
standard deviation), but the same features are not as effective in
thePoultry barn dataset where the color shifts are more dramatic
year-over-year. Even so, the performance of the color-over-time
baseline was much better than we originally hypothesized and
should be compared to in future work regardless of perceived co-
variate shifts.
We also observe that our proposed method (that computes clus-
tered representations) dominates the family of average-color base-
lines. This, along with the fact that we observe that more clusters in
the𝑘-means model usually results in better performance, suggests
that the clustered representation is an important component of our
approach. We hypothesize that more rich feature representations
will prove even more effective as both the colors and textures of
a footprint will change when it becomes developed. This is a triv-
ial addition to the existing model and we hope to test it in future
studies.
Finally, we observe that our heuristic method performs very well
overall. In both datasets there are only two supervised methods
that achieve stronger results than the semi-supervised proposed
approach.
6 TEMPORAL CLUSTER MATCHING AS A
DATA AUGMENTATION STRATEGY
In Table 2 we show that training semantic segmentation models
on structure footprint masks at a time 𝑡does notresult in a model
that can generalize well over time (and thus cannot detect change).
Previously (in Section 5.1) we hypothesized that this is due to the
covariate shift in the time series imagery in the two datasets that
we test on – the Poultry barn dataset uses NAIP aerial imagery
that is collected at different times of day and different days of the
year on a rolling three year basis and the Solar farm dataset uses
Sentinel 2 mosaics created from TOA corrected imagery in 2016
through 2018 and surface reflectance corrected imagery in 2019
and 2020. Thus, a model trained with data from a single period in
both of these cases is unlikely to perform well in other layers.
Here we test this hypothesis by using our proposed method to
augment the data used to train the CNN over time method for the
Poultry barn dataset. Specifically, we run Semi-supervised TCM
over the Poultry barn dataset to create predictions as to when
each footprint was constructed. We then use these estimates to
create an expanded training set that contains pairs of imagery over
all time points with footprint masks that are predicted to have a
building. For example, if our model believes that there was a build-
ing in a given footprint at 2011 in the NAIP imagery, then we can
train the segmentation model with (NAIP 2011, footprint), (NAIP
2013, footprint), etc. We find that this increases the performance of
the model on the change detection task in all cases that we tested.
For example, we apply this augmentation step to the same modelconfiguration used in the results from Table 2 and achieve a 56%
accuracy and 0.97 MAE (a 19% improvement in ACC and 0.39 im-
provement in MAE). These results are not competitive with the
other methods we test in the change detection task. This likely
stems from the fact that the segmentation model, in contrast to the
other methods, is not specialized to change detection. On the other
hand, the segmentation model can be run over new imagery to find
novel instances of poultry barns (e.g., barns destroyed prior to the
date of original data labeling), and is thus necessary to improve the
performance of such models for more general applications.
While a more rigorous evaluation of how to improve the deep
learning segmentation baseline is outside the scope of this paper,
we hypothesize that more data augmentation strategies (e.g. Ran-
dAugment [ 4] and AugMix [ 13]), unsupervised domain adaptation
methods [ 28], and a hyperparmeter search over dimensions such
as class balancing strategies, temporal balancing strategies, learn-
ing rates, architecture, etc. would all improve performance. These
types of experimentation will be critical for any future work that
attempts to create general purpose models for detecting building
construction at scale. That said, one of the main benefits of our
proposed TCM is that it provides a lightweight approach to detect
construction.
7 CONCLUSION AND FUTURE WORK
We have proposed Temporal Cluster Matching (TCM) for detecting
change in building footprints from time series of remotely sensed
imagery. This model is based on the intuition that the relationship
between the distribution of colors inside and outside of a build-
ing footprint will change upon construction. We further propose
a heuristic based method for fitting the parameters of our model
and show that this approach effectively detects poultry barn and
solar farm construction. TCM does not depend on having labels
over time, yet it can outperform similar models that have such
labels available. Further, we show that the feature representation
from TCM – a sequence of KL-divergence values between the dis-
tribution of color clusters inside and outside of a building footprint
– can be used in supervised models to improve change detection
performance. Finally, we show how TCM can be used as a data
augmentation technique for training deep learning models to detect
building footprints from remotely sensed imagery.
This work motivates several future directions. First, the per-pixel
representation of TCM will affect detectable changes. We used sim-
ple color representations, but more elaborate representations could
be promising (e.g. texture descriptors or higher dimensional image
embeddings). Second, future work should explore other applica-
tions of TCM. Here we experimented with imagery where the size
of the footprints were relatively large compared to the spatial reso-
lution of the imagery. However, our model may not perform as well
when the footprint is relatively smaller. For example, we briefly
experimented with detecting changes using general building foot-
prints in the US and NAIP imagery and found that the relationship
between the color distributions of small residential buildings and
their surroundings was very noisy, although we did not attempt to
investigate further. The top performing methods from the recent
SpaceNet7 challenge run their building detection algorithms on
upsampled imagery and a similar strategy may be useful with TCM.
Temporal Cluster Matching for Change Detection of Structures from Satellite Imagery COMPASS ’21, June 28-July 2, 2021, Virtual Event, Australia
Finally, we hypothesize that TCM would work with time series of
imagery from multiple remote sensing sensor modalities. A bene-
fit of our model is that it does not consider inter-year differences
and thus is not affected by shifts in the color distributions of the
imagery, but our experimental results do not explore the extent in
which this is a useful property. Practically, there may be problems
(such as shifts in geolocation accuracy) when applying TCM over
stacks of imagery from different sources.
In summary, we hope TCM approach illustrated here will enable
researchers to overcome the “frozen” labels of many emerging
earth imagery datasets. Our lightweight approach to augment labels
temporally should foster richer exploration of time series of satellite
imagery and help us to understand the earth as it was, is, and will
be.
ACKNOWLEDGMENTS
We thank Microsoft Azure for support in cloud computing, and
Schmidt Futures, Stanford Impact Labs, and the GRACE Communi-
cations Foundation for research support.
REFERENCES
[1]Samaneh Aminikhanghahi and Diane J Cook. 2017. A survey of methods for time
series change point detection. Knowledge and information systems 51, 2 (2017),
339–367.
[2]Jonathan Anomaly. 2015. What’s Wrong with Factory Farming? Public Health
Ethics (2015). https://ssrn.com/abstract=2392453
[3]Ying Chen, Xu Ouyang, and Gady Agam. 2019. ChangeNet: Learning to detect
changes in satellite images. In Proceedings of the 3rd ACM SIGSPATIAL Interna-
tional Workshop on AI for Geographic Knowledge Discovery . 24–31.
[4]Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. 2020. Randaugment:
Practical automated data augmentation with a reduced search space. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops . 702–703.
[5]Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan Pang, Jing Huang,
Saikat Basu, Forest Hughes, Devis Tuia, and Ramesh Raskar. 2018. Deepglobe
2018: A challenge to parse the earth through satellite images. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition Workshops . 172–181.
[6]Adam Van Etten, Dave Lindenbaum, and Todd M. Bacastow. 2018. SpaceNet:
A Remote Sensing Dataset and Challenge Series. CoRR abs/1807.01232 (2018).
arXiv:1807.01232 http://arxiv.org/abs/1807.01232
[7]Michael J Evans and Jacob W Malcom. 2020. Automated Change Detection Meth-
ods for Satellite Data that can Improve Conservation Implementation. bioRxiv
(2020), 611459.
[8]Anmar Frangoul. 2020. India has some huge renewable energy goals. But can
they be achieved? CNBC (2020). https://www.cnbc.com/2020/03/03/india-has-
some-huge-renewable-energy-goals-but-can-they-be-achieved.html
[9]Ananya Gupta, Elisabeth Welburn, Simon Watson, and Hujun Yin. 2019. CNN-
Based Semantic Change Detection in Satellite Imagery. In International Conference
on Artificial Neural Networks . Springer, 669–684.
[10] Rohit Gupta and Mubarak Shah. 2020. Rescuenet: Joint building segmentation
and damage assessment from satellite imagery. arXiv preprint arXiv:2004.07312
(2020).
[11] Cassandra Handan-Nader and Daniel E Ho. 2019. Deep learning to map concen-
trated animal feeding operations. Nature Sustainability 2, 4 (2019), 298–306.
[12] Cassandra Handan-Nader, Daniel E Ho, and Larry Y Liu. 2020. Deep Learning
with Satellite Imagery to Enhance Environmental Enforcement. Data-Driven
Insights and Decisions: A Sustainability Perspective. Elsevier (2020).
[13] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and
Balaji Lakshminarayanan. 2019. Augmix: A simple data processing method to
improve robustness and uncertainty. arXiv preprint arXiv:1912.02781 (2019).
[14] Xin Huang, Yinxia Cao, and Jiayi Li. 2020. An automatic change detection method
for monitoring newly constructed building areas using time-series multi-view
high-resolution optical satellite images. Remote Sensing of Environment 244 (2020),
111802.
[15] Franck Jung. 2004. Detecting building changes from multitemporal aerial stere-
opairs. ISPRS Journal of Photogrammetry and Remote Sensing 58, 3-4 (2004),
187–201.
[16] Clemens Listner and Irmgard Niemeyer. 2011. Recent advances in object-based
change detection. In 2011 IEEE International Geoscience and Remote Sensing Sym-
posium . IEEE, 110–113.[17] José A Malpica, María C Alonso, Francisco Papí, Antonio Arozarena, and Alex
Martínez De Agirre. 2013. Change detection of buildings from satellite imagery
and lidar data. International Journal of Remote Sensing 34, 5 (2013), 1652–1675.
[18] Diego Marcos, Raffay Hamid, and Devis Tuia. 2016. Geospatial correspondences
for multimodal registration. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition . 5091–5100.
[19] Masashi Matsuoka and Fumio Yamazaki. 2004. Use of satellite SAR intensity
imagery for detecting building areas damaged due to earthquakes. Earthquake
Spectra 20, 3 (2004), 975–994.
[20] Alex Moltzau. 2020. Estonia’s National Strategy for Artificial Intelligence. Medium
(2020). https://medium.com/swlh/estonias-national-strategy-for-artificial-
intelligence-2623259ddf4c
[21] Sheila Moynihan. 2016. Mapping Solar Potential in India. Office of Energy
Efficiency and Renewable Energy (2016). https://www.energy.gov/eere/articles/
mapping-solar-potential-india
[22] David Osterberg and David Wallinga. 2004. Addressing Externalities From Swine
Production to Reduce Public Health and Environmental Impacts. American
Journal of Public Health 94, 10 (2004), 1703–1708. https://doi.org/10.2105/AJPH.
94.10.1703 arXiv:https://doi.org/10.2105/AJPH.94.10.1703 PMID: 15451736.
[23] Ray Purdy. 2010. Using Earth observation technologies for better regulatory
compliance and enforcement of environmental laws. Journal of Environmental
Law 22, 1 (2010), 59–87.
[24] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional
networks for biomedical image segmentation. In International Conference on
Medical image computing and computer-assisted intervention . Springer, 234–241.
[25] Ribana Roscher, Michele Volpi, Clément Mallet, Lukas Drees, and Jan Dirk Wegner.
2020. SemCity Toulouse: A benchmark for building instance segmentation in
satellite images. ISPRS Annals of Photogrammetry, Remote Sensing and Spatial
Information Sciences 5 (2020), 109–116.
[26] A.M. Soroka and Z. Duren. 2020. Poultry feeding operations on the Delaware,
Maryland, and Virginia Peninsula from 2016 to 2017: U.S. Geological Survey data
release. https://doi.org/10.5066/P9MO25Z7.
[27] Crista L. Straub, Stephen R. Koontz, and John B. Loomis. 2019. Economic valuation
of landsat imagery . Technical Report. U.S. Geological Survey, Reston, VA. https:
//doi.org/10.3133/ofr20191112 Report.
[28] Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. 2019. Unsupervised
domain adaptation through self-supervision. arXiv preprint arXiv:1909.11825
(2019).
[29] A Tewkesbury. 2011. Mapping the extent of urban creep in Exeter using OBIA.
InProceedings of RSPSoc Annual Conference . 163.
[30] Andrew P Tewkesbury, Alexis J Comber, Nicholas J Tate, Alistair Lamb, and
Peter F Fisher. 2015. A critical synthesis of remotely sensed optical image change
detection techniques. Remote Sensing of Environment 160 (2015), 1–14.
[31] Devis Tuia, Claudio Persello, and Lorenzo Bruzzone. 2016. Domain adaptation
for the classification of remote sensing data: An overview of recent advances.
IEEE geoscience and remote sensing magazine 4, 2 (2016), 41–57.
[32] Adam Van Etten and Daniel Hogan. 2021. The SpaceNet Multi-Temporal Urban
Development Challenge. arXiv preprint arXiv:2102.11958 (2021).
[33] Adam Van Etten, Daniel Hogan, Jesus Martinez-Manso, Jacob Shermeyer,
Nicholas Weir, and Ryan Lewis. 2021. The Multi-Temporal Urban Development
SpaceNet Dataset. arXiv preprint arXiv:2102.04420 (2021).
[34] Jan Verbesselt, Rob Hyndman, Glenn Newnham, and Darius Culvenor. 2010.
Detecting trend and seasonal changes in satellite image time series. Remote
sensing of Environment 114, 1 (2010), 106–115.
[35] Lei Wang, Congcong Li, Qing Ying, Xiao Cheng, Xiaoyi Wang, Xueyan Li, Lu-
anyun Hu, Lu Liang, Le Yu, HuaBing Huang, et al .2012. China’s urban expansion
from 1990 to 2010 determined with satellite remote sensing. Chinese Science
Bulletin 57, 22 (2012), 2802–2812.
[36] Nicholas Weir, David Lindenbaum, Alexei Bastidas, Adam Van Etten, Sean
McPherson, Jacob Shermeyer, Varun Kumar, and Hanlin Tang. 2019. Spacenet
MVOI: a multi-view overhead imagery dataset. In Proceedings of the IEEE/CVF
International Conference on Computer Vision . 992–1001.
[37] Siyu Yang. 2018. How to extract building footprints from satellite images using
deep learning. Microsoft (2018). https://azure.microsoft.com/en-us/blog/how-to-
extract-building-footprints-from-satellite-images-using-deep-learning/
[38] Amy Zhang, Xianming Liu, Andreas Gros, and Tobias Tiecke. 2017. Building
detection from satellite images on a global scale. arXiv preprint arXiv:1707.08952
(2017).
[39] Kang Zhao, Jungwon Kang, Jaewook Jung, and Gunho Sohn. 2018. Building
extraction from satellite images using mask R-CNN with building boundary
regularization. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops . 247–251.
Contents lists available at ScienceDirect
Remote Sensing of Environment
journal homepage: www.elsevier.com/locate/rse
Key issues in rigorous accuracy assessment of land cover products
Stephen V. Stehmana,⁎, Giles M. Foodyb
aDepartment of Forest and Natural Resources Management, SUNY College of Environmental Science and Forestry, Syracuse, NY 13210, United States
bSchool of Geography, University of Nottingham, Sir Clive Granger Building, University Park, Nottingham NG7 2RD, United Kingdom
ARTICLE INFO
Edited by Emilio Chuvieco
Keywords:
Probability sampling
Remote sensing
Inference
UncertaintyArea estimation
Error matrixABSTRACT
Accuracy assessment and land cover mapping have been inexorably linked throughout the first 50years of
publication of Remote Sensing of Environment. The earliest developers of land-cover maps recognized the im-
portance of evaluating the quality of their maps, and the methods and reporting format of these early accuracy
assessments included features that would be familiar to practitioners today. Specifically, practitioners have
consistently recognized the importance of obtaining high quality reference data to which the map is compared,the need for sampling to collect these reference data, and the role of an error matrix and accuracy measures
derived from the error matrix to summarize the accuracy information. Over the past half century these tech-
niques have undergone refinements to place accuracy assessment on a more scientifically credible footing. Wedescribethecurrentstatusofaccuracyassessmentthathasemergedfromnearly50yearsofpracticeandidentify
opportunities for future advances. The article is organized by the three major components of accuracy assess-
ment, the sampling design, response design, and analysis, focusing on good practice methodology that con-tributes to a rigorous, informative, and honest assessment. The long history of research and applications un-derlying the current practice of accuracy assessment has advanced the field to a mature state. However,
documentation of accuracy assessment methods needs to be improved to enhance reproducibility and trans-
parency,andimprovedmethodsarerequiredtoaddressnewchallengescreatedbyadvancedtechnologythathasexpanded the capacity to map land cover extensively in space and intensively in time.
1. Introduction
Land cover and land cover change are fundamental environmental
variables of broad impact. Land cover is an essential climate variable
(Hollmann et al., 2013) that impacts numerous environmental pro-
cesses and patterns ranging from influencing albedo and hence climate
to zoogeographic distributions and hence patterns of biodiversity. The
effects of land cover may be relatively direct, such as by influencing
albedo, or indirect, such as by impacting environmental processes that
determine aspects of ecosystem services valuations which is why land
cover can be used as a surrogate variable in these valuations (Costanza
et al., 1997). Land cover also greatly influences human use of the land
(Turner et al., 2007) and can greatly impact health, wealth and well-
being.Changesinlandcovercanbeparticularlyimportant.Takinginto
consideration the impacts on just one essential variable, water, a land-
cover conversion such as deforestation might impact processes such as
interception, infiltration, evapotranspiration, ground water recharge
and flood frequency with consequent impacts on variables such as
erosion and sedimentation. Indeed, it has been recognized for many
years that land cover change is one of the most importantenvironmental variables impacting major societal concerns such asclimate change and biodiversity conservation (Vitousek et al., 1997;
Chapin et al., 2000; Foley et al., 2005). Given the importance of land
coverandlandcoverchange,thereisconsiderableneedforhighqualityand up-to-date information on them. Because the remotely sensed re-
sponse typically measured is a function of Earth surface properties,
remotesensinghasconsiderablepotentialasasourceofinformationon
land cover and land cover change at a range of spatial and temporal
scales.
From a wide range of methods (Lu and Weng, 2007; Mather and
Tso, 2016), information on land cover is commonly extracted from
remotelysenseddataviaaclassificationanalysisandoftenpresentedin
the form of a thematic map. Similarly, comparisons of mapped re-
presentations for different time periods typically provide the basis for
monitoring land cover change. Consequently, maps depicting land
cover are central to numerous scientific and practical applications.
Maps provide a generalised view of the world. As such, while a high
quality map depicting the theme of interest can be used successfully to
provide desired information it must be recognized that it can be im-
perfect and require careful interpretation. A fundamental feature of
https://doi.org/10.1016/j.rse.2019.05.018
Received 6 December 2018; Received in revised form 18 April 2019; Accepted 14 May 2019⁎Corresponding author.
E-mail addresses: svstehma@esf.edu (S.V. Stehman), giles.foody@nottingham.ac.uk (G.M. Foody).Remote Sensing of Environment 231 (2019) 111199
Available online 12 June 2019
0034-4257/ Crown Copyright © 2019 Published by Elsevier Inc. All rights reserved.
T
significance to the user of the land cover data is its quality. Quality has
many dimensions (e.g., positional accuracy and image segmentation
accuracy) but the focus of this article is limited to the critical issue of
thematic accuracy. The latter is simple in concept, relating to how
closely the map shows reality in terms of the land cover mosaic it re-
presents, but has been surprisingly poorly addressed despite its sig-
nificance (Olofsson et al., 2013).
Land cover maps can be easily generated from remotely sensed
imagery. Using standard image classifiers available in popular software
it is trivially easy to generate a land cover map. But the map produced
will be a function of the data and classifier used (Foody and Mathur,
2006;Pal and Foody, 2012; Foody et al., 2016). Different training sets,
classification algorithms, and remotely sensed images could be used toproduce very dissimilar representations of the same geographic region.
Regional to global maps of land cover, for example, provide different
representationsofthesamephenomenawhichtypicallydisagreeonthe
label for much of the region mapped (Herold et al., 2008; Tchuenté
et al., 2011). Which map to use as the closest representation of reality
requires an ability to rigorously assess map accuracy; otherwise, each
map remains nothing more than a pretty picture representing simply
one possible untested hypothesised land cover scenario (Strahler et al.,
2006;McRoberts, 2011). Good scientific practice demands that the
quality of the measurements made by remote sensing be evaluated andintegrated into studies. High quality measurement is often seen as a
hallmark ofscience.Withoutitthereisthedangerofmis-interpretation
andmis-understanding.Theassessmentmust,however,berigorousand
defensible. Poor or weak accuracy assessments offer a veneer of sci-
entific respectability but are a façade that can impede understanding
and successful practical application.
Accuracy assessment has a long history in remote sensing as
Congalton and Green (1999, Chapter 2) trace the origins back to ap-
proximately 1975. The subject has developed greatly and there is alarge literature that explores the many dimensions and issues of im-portance. This article does not aim to revisit the whole subject but is
focused on appropriate methodology for a useful and credible accuracy
assessment in typical scenarios. For the most part we assume that the
map and reference labels represent a hard classification and that ac-
curacyassessmentisconductedindependentlyofclassifiertraining.The
focus is explicitly on the correct use of design-based inference for the
estimation of accuracy expressed on an overall and per-class basis. In
this, the accuracy assessment is founded on the comparison of the
predicted map class label with the actual label observed on the ground
forasampleofselectedsitesinthemappedarea.Althoughtheneedfor
accuracy assessment has long been recognized by the remote sensing
community and there has been considerable research on the topic,
many land cover maps produced have not been subjected to rigorous
evaluation (Olofsson et al., 2013) limiting their scientific and practical
value. An accuracy assessment does more than simply describe the
quality of a map, it provides a means to enhance its usefulness. An
inaccurate map that is accompanied by a rigorous accuracy assessment
may, for example, provide highly valuable information.
Good practices have been promoted to help the production of
credible accuracy assessments (Olofsson et al., 2014). These good
practice recommendations also extend to area estimation, cautioning
against the common practice of using “pixel counting” for area esti-
mation (i.e., summing the number of pixels allocated to a class in the
map and multiplying by pixel areal extent). Instead the good practice
guidelines advocate estimation of area based on the reference classifi-
cation and using information contained in the error matrix to improve
precision of the estimator (McRoberts, 2011; Olofsson et al., 2013,
2014). Although we emphasize elements of good practice, it is also
necessary to point out aspects of bad practice such as neglecting to use
estimation formulas that correspond to the sampling design, normal-
izing the error matrix (which equalises user's and producer's accuracy
even though they may be meaningfully different and can be a large
source of bias) or using the kappa coefficient (which unnecessarilyadjustsforchanceagreementinamannerinwhichchanceagreementis
mis-estimated).
The organization of the article is as follows. Section 1 provides an
overviewofthehistoricaldevelopmentofaccuracyassessment(Section
1.1) and articulates general criteria to guide planning and im-
plementation of accuracy assessment of land cover maps (Section 1.2).
We then proceed to review the current state of good practice for the
three main components of accuracy assessment (Stehman and
Czaplewski, 1998), sampling design (Section 2), response design
(Section 3), and analysis (Section 4). Recognizing the critical role of
statistical inference in accuracy assessment, we describe the commonly
used design-based inference approach in Section 5 along with alter-
native options of model-based and Bayesian inference. Practitionershavelongbeenawareofthedifficultiesofobtaininggoldstandard(i.e.,
perfect) reference classifications motivating the discussion in Section 6
of the problems and impacts of imperfect reference data. Section 7 is
devoted to future needs and directions of accuracy assessment, and
conclusions stated in Section 8 complete the article.
1.1. Historical context
TheScopusdatabase was used to search for relevant articles pub-
lished in the 50-year history of Remote Sensing of Environment (RSE).
Specifically, two searches were undertaken. The first search extracted
articleswiththeterm‘landcover’inthetitle,keywordsorabstract,and
the second search identified articles via the search phrase ‘land cover
AND accuracy’ to indicate the importance of accuracy assessments in
these land cover studies. Through the first 20years of RSEonly 6 ar-
ticles met the latter search phrase and comprised<1% of all articles
published in RSE(Table 1). In the most recent 15years of RSE
(2004–2018), the number of articles including both search terms ‘land
cover’and‘accuracy’hasexpandedto331comprisingabout6.5%ofall
articles published in RSEduring that 15year period. The percent of
articles in RSEwith ‘land cover’ has increased up until 1999 when it
reached a plateau of 15% of all articles. From 1994 to 2008 approxi-
mately 40% of all land cover articles also had ‘accuracy’ as a keyword
and this percent increased to about 50% for the past decade,
2009–2018, providing some evidence that accuracy has become more
prominently emphasized in land cover studies.
The earliest five publications in RSEidentified by the search phrase
of ‘land cover AND accuracy’ foreshadowed methods and challenges
that have persisted throughout the history of accuracy assessment of
land cover. The earliest article identified from the Scopus search was
Bryan(1975) inwhichindividualswithnoexperienceintheuseofside
lookingairborneradar(SLAR)imagerywereaskedtointerpretthelandcover class of 32 test case objects in Melbourne, FL, United States of
America (USA). Accuracy was reported simply as the proportion of
Table 1
Publications in Remote Sensing of Environment (RSE) in five-year intervals re-
sulting from two Scopussearches, one for ‘land cover’ and another for ‘land
cover’AND ‘accuracy’intitles, keywords,andabstracts(searches conducted25January 2019).
Time Interval Total
#RSELand Cover (%
of All)Accuracy AND Land Cover(% of Land Cover)
1969–1973 64 0 (0) 0
1974–1978 128 0 (0) 0
1979–1983 206 9 (4) 1 (11)
1984–1988 307 6 (2) 3 (50)
1989–1993 434 7 (2) 2 (29)
1994–1998 565 55 (10) 22 (40)
1999–2003 755 117 (15) 45 (38)
2004–2008 1263 185 (15) 72 (39)
2009–2013 1503 227 (15) 110 (48)
2014–2018 2187 286 (13) 149 (52)
Total 7412 892 (12) 404 (45)S.V. Stehman and G.M. Foody
Remote Sensing of Environment 231 (2019) 111199
2
theseobjectsclassifiedcorrectly. Walsh(1980) usedLandsatimageryto
map 12 land-cover classes in the vicinity of Crater Lake, Oregon, USA,
and implemented stratified random sampling of 5×5 pixel sampling
units to report user's accuracy (without accompanying standard error).
Jensen et al. (1984) used MSS to map vegetation types in a 4000ha
non-tidal wetland in South Carolina, USA. The accuracy assessmentconsisted of field sampling along two transects (not randomly located)
with a 1-m length of transect used as the assessment unit. Jensen et al.
(1984)summarized the accuracy information via an error matrix and
reported omission and commission error rates (without standard er-
rors).Gordon (1980) used Landsat imagery to map 21 classes within a
small area of Ohio, USA (<100km
2) for two dates, 1973 and 1975,
representing an early land-cover change application. User's accuracies
for each date were estimated based on photo-interpreted reference
classifications for the entire study area (no sampling conducted). Toll
(1985)compared accuracy of land cover classified by MSS versus TM
imagery for an 800km2test site near Washington, DC, USA. Stratified
systematic sampling was implemented with equal allocation of 75pixels (500m×500m) from each of the 7 classes, and the comparison
of MSS versus TM was reported using confidence intervals for overall
accuracy of each classifier. In addition to the examples identified from
the algorithmic Scopus search, we found accuracy assessments of crop
classifications reported in the very first two volumes of RSE.Haralick
et al. (1969-1970) reported error matrices along with overall and
producer's accuracies for a classification of crop types, and Anuta and
MacDonald (1971) presented an accuracy assessment of classifications
of crops in the Imperial Valley of California.
The initial forays into accuracy assessment of land cover included
commoncurrentpracticessuchasimplementingstratifiedsamplingand
reporting results in the form of an error matrix accompanied by user's
and producer's accuracies. We also observe in these early articles the
diversity of objectives of accuracy assessment, including basic de-
scriptionofmapquality(see Sections4.2and4.4 ofthisreviewarticle),
accuracy of land-cover change (Section 4.5), and comparison of maps
(Section4.6).Thesearticlesarealsoillustrativeofpresentdayproblems
such as: 1) absence of standard errors or confidence intervals quanti-
fying the uncertainty of accuracy and area estimates obtained (Section
4.4);2)lackofclarityinthedescriptionofthesamplingdesignsothatit
would be impossible to reproduce the sample selection protocol
(Section2.2);3)absenceofaprobabilitysamplingdesign(Section2.1);
4) misplaced concern regarding independence, as reflected in Toll's
(1985)statement that non-contiguous pixels were sampled “to increase
independence”(Section2.4);and5)notemployingstratifiedestimation
formulas for a stratified sampling design (Section 4.2). One major dif-
ferenceofcurrentpracticerelativetothesepioneerland-coverstudiesis
that present-day applications typically map vastly larger areas.
The late 1970s and early 1980s represented an era of emerging
methodological developments in accuracy assessment as articles were
published in RSEand other outlets. Sampling design and sample size
questions were commonly addressed (e.g., Hord and Brooner, 1976;
Fitzpatrick-Lins, 1981) with Hay (1979) proposing use of stratified
sampling with a minimum sample size of 50 per class, a practice stilloften followed today. Another focus of these early articles was the
question of whether maps were of acceptable quality, leading to an
emphasis on the decision-making framework of statistical hypothesis
testing as well as frequent reference to 85% accuracy as a benchmark
defining acceptable accuracy (Anderson, 1971; Aronoff, 1982a, 1982b;
Fitzpatrick-Lins, 1981) (see Section 7.5 for a critique of this 85%
benchmark). Among these early articles, Card (1982) warrants special
acclamation for recognizing the relevance of estimating proportion ofarea based on the reference classification, an idea reinvigorated by
McRoberts (2011) andOlofsson et al. (2013, 2014). Not surprisingly,
categorical data analyses developed in the statistical literature for two-
way contingency tables (Bishop et al., 1975) were proposed for use in
accuracy assessment (Congalton et al., 1983). These tables represent
sample counts for data cross-classified by two categorical variables andthus bear a strong resemblance to an error matrix. Two unfortunate
remnants of these contingency table analyses that still persist are the
normalized error matrix and kappa coefficient, both practices that are
now recommended to be avoided (see Sections 4.1, 4.8, and 7.5).
Congalton (1991) andCongalton and Green (1999) synthesized
muchofthemethodologydevelopedupto1990andmeritconsiderable
credit for putting accuracy assessment “on the map” of the remote
sensing community. A series of subsequent review articles further elu-
cidated the progression of accuracy assessment from the early 1990s to
the present (Janssen and van der Wel, 1994; Smits et al., 1999; Foody,
2002;Strahler et al., 2006; Wulder et al., 2006a; Olofsson et al., 2014).
Methodological developments and applications of accuracy assessment
have grown exponentially since the early 1990s (Table 1). Advances in
technology related to satellite imagery and classification methods have
surelyfuelledthisgrowthinlandcoverstudiesandaccuracyassessment
(Wulder et al., 2018). Land cover analyses have evolved from rather
limited studies of small geographic regions at a single point in time to
present day regional, national, and global studies at multiple time
periods, often with increasingly smaller spatial resolutions and in-
creasingly greater temporal densities. In this article, we discuss the
status of accuracy assessment of land cover highlighting advances in
concepts and methods developed primarily since Foody's (2002) re-
view, and focusing on issues relevant to land cover studies covering
largeareas.Weextendandapplythegoodpracticerecommendationsof
Strahler et al. (2006) andOlofsson et al. (2014) to critique current
practice.Inparticular,wediscusshowcommonlyusedprotocolssatisfyor fall short of good practice criteria (defined in the next section) spe-
cifiedtoensurearigorousandcredibleassessmentofaccuracyandarea
of land cover and land cover change. We also take a glimpse into the
future and posit new directions for needed developments to address
emerging challenges for accuracy assessment.
1.2. Good practice criteria to guide accuracy assessment methods and
reporting
If accuracy assessment is worth doing, then it most certainly is
worth doing well. A rigorous accuracy assessment should be viewed as
an essential part of a mapping project (Strahler et al., 2006). We thus
begin by articulating operational criteria that serve to guide the prac-
tice of accuracy assessment toward scientifically credible and in-
formativemethodsandresults.Inpractice,accuracyassessmentsshould
be designed and implemented to achieve these criteria. Criteria 1
through 4 address fundamental requirements for producing a map re-
levant, scientifically credible accuracy assessment that includes aspects
ofqualitycontrolandquantificationofuncertainty.Criteria5and6are
proposed to address the emerging concerns of transparency (openness)
and reproducibility of scientific research (Munafò et al., 2017). These
criteria are ideals to be aspired to and will be satisfied to a matter of
degree in any given application. Although the need for robust accuracy
assessments is often espoused, robustness is not included as a good
practice criterion because it lacks specific meaning relative to land
cover issues (and also because “robust” has a longstanding statistical
definition referring to resistance toimpacts of outliers and violationsof
assumptions). The six good practice criteria are:
1. Map relevant - accuracy assessments should address the reality of
the landscape mapped which is accomplished when the accuracyestimates and error matrix reflect the proportional areal re-
presentation of the study region in terms of both the map and re-
ference classifications (Section 4.1). In effect, the map relevant
criterion imposes the requirement that the accuracy estimates have
fidelity to the region of interest covered by the map. For example,
suppose that a map is produced with a binary legend of change and
no change, and it is known that the mapped region has experienced
approximately 3–5% change. If the error matrix shows roughly
equal proportions of area of change and no change (i.e., each classS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
3
occupies ~50% of the mapped region), the assessment is clearly not
map relevant. This criterion is most often violated by use of nor-
malized error matrices (Section 4.1) or reporting an error matrix
based on sample counts when the sampling design is stratified with
equal sample size allocated to each stratum (Section 4.4).
2. Statistically rigorous - the primary requirements for a statistically
rigorous assessment are to implement a probability sampling design
(assuming design-based inference will be used), to employ con-
sistent estimators (i.e., use estimation formulas that correspond to
thesamplingdesignimplemented),andtoquantifythevariabilityof
the accuracy and area estimates by reporting standard errors or
confidence intervals.
3. Quality assured – protocols should be in place to monitor and
evaluatethequalityofthereferencedataandresults.Anexampleof
qualityassurancewouldbetoevaluateconsistencyofreferenceclass
labels obtained by different interpreters by providing two or more
interpreters with a randomly chosen set of sample locations and
havingtheinterpretersindependentlylabelthelocationswhileblind
to the map category. These consistency data can be used to provide
feedback to enhance agreement among interpreters and also to ad-
dress the reliability criterion.
4. Reliable – a repetition of the accuracy assessment protocol should
produce approximately similar results. Reliability is tantamount to
controllingthosefeaturesoftheassessmentprotocolthataresubject
to variability. The two most prominent such features are associated
with variability attributable to sampling and variability attributable
to interpretation of the reference class labels. Reliability addresses
the question, if we had obtained a different sample and had used
differentinterpreters(alltrainedinthesamemannerandwithequal
expertise), would the estimates of accuracy and area be approxi-
mately the same? A reliable assessment would be reflected in small
standard errors for the accuracy and area estimates and small
variabilityamonginterpreterswhenassigningreferenceclasslabels.Reporting standard errors or confidence intervals provides an im-
portant assessment of reliability because these results quantify how
repeatable the estimates would be over different random outcomes
of the sample selection.
5. Transparent – provide all relevant details, positive or negative, that
would inform readers about the quality of the results (Castilla,
2016). For example, any problems with missing data due to cloudy
imagery or inability to access sample locations should be reported,
and any modifications to standard sampling design and analysis
protocols should also be documented. Synonyms for the transpar-
ency criterion would include “openness” and “honesty”. The reality
is that any accuracy assessment will be confronted with practical
difficulties that will interfere with perfect implementation. The
transparency criterion simply mandates an honest depiction of the
problems encountered and steps taken in response to these pro-
blems. Transparency is particularly critical for the response design,
and additional details are provided at the end of Section 3.
6. Reproducible – the methodology of the accuracy assessment should
be reproducible given the documentation provided for the sampling
design, response design, and analysis. The reproducibility criterion
places a premium on lucid description of the key details of each
protocol. Achieving the reproducibility criterion is critically de-
pendent on the transparency criterion because if documentation of
full details of implementation is lacking the assessment protocols
cannot be reproduced. A fully reproducible assessment would in-
clude any programming code and identification of software used to
produce the results as well as the sample data used in the analyses.
Although it may not be possible in all applications, providing the
sample data would allow users to obtain estimates for subregions of
particular interest (e.g., a state or province within a national as-
sessment)aswellastoconductsecondaryanalysessuchasexploring
how accuracyis associated with possible causes of misclassification.
Additional details necessary to satisfy the reproducibility criterionare presented in each of the sampling design, response design, and
analysis sections described below.
2. Sampling design
The sampling design is the protocol for selecting the subset of as-
sessment units for which the reference classification is obtained andthen compared to the map classification. Numerous sampling designs
have been used for accuracy assessment (Stehman and Czaplewski,
1998;Stehman,1999a; Brusetal.,2011)withthemostcommonlyused
designs being simple random, stratified random, systematic, and
cluster.Stehman (2009a) reviewed the advantages and disadvantages
of these basic designs relative to accuracy assessment objectives,practical considerations, and desirable design criteria. If design-based
inference is to be invoked, the most important criterion the sampling
design must satisfy is that it is a probability sampling design; in the
absence of probability sampling, it is necessary to implement model-
based inference or Bayesian inference (Section 5).
2.1. Probability versus representative sampling
Although the importance of sampling for collecting high quality
reference data was recognized in the early days of accuracy assessment
(Congalton,1991),itisonlyinthepast20orsoyearsthatsamplinghas
been placed in the rigorous statistical framework provided by prob-
ability sampling and design-based inference (Stehman and Czaplewski,
1998). The statistical rigor of accuracy assessment results produced
from a sample require that the sampling design satisfy the conditions
thatdefineaprobabilitysample.Theseconditionsarespecifiedinterms
of the inclusion probability of element u, denoted π
u. If a pixel is the
spatial unit of the accuracy assessment, the inclusion probability is theprobability that pixel u will be one of the pixels in the sample. The two
conditions required to satisfy the definition of probability sampling are
that π
uis known for all units selected in the sample, and π u> 0 for all
units in the population (i.e., all pixels in the region of interest). Thebasic sampling designs, simple random, stratified random, systematic,
and cluster, meet these conditions. Common exceptions to probability
sampling include purposely choosing sample locations because of
convenience or homogeneity of land cover and implementing a com-
plex selection protocol with numerous steps such that deriving inclu-
sion probabilities is intractable.
The focus on probability sampling is different from the common
admonition for representative sampling. By definition a probability
sample is representative because such a sample can be used to produce
unbiased estimators of the population of interest. Although inclusion
probabilities must be known, they are not required to be equal, so a
sample can be representative in the sense of yielding unbiased esti-
mators even if the sample elements are not selected with equal prob-
ability. Stratified sampling with equal allocation is a commonly used
probability sampling design for which the inclusion probabilities are
not equal for all elements of the population. In fact, a primary moti-
vation for stratified sampling is to create such unequal inclusion
probabilities for the purpose of increasing the sample size from rare
mapclasses.Itisthusamythtobelievethat“thesamplemustrepresent
the population” in the sense of the proportion of each class in the
samplematchingtheproportionofeachclassinthepopulation.Aslong
as the analysis takes into account the known unequal inclusion prob-
abilities, the estimates produced will reflect the actual parameters of
the population (i.e., the estimators are unbiased) even though the
sample itself may bear little resemblance to the population in terms of
proportion of area of each class. Implementing an equal probability
sampling design does offer the advantage of simplicity of analysis be-
cause the estimates are unweighted (i.e., weights are not needed to
accommodate different inclusion probabilities). Sampling designs that
have equal inclusion probabilities include simple random sampling,
systematic sampling and stratified sampling with proportionalS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
4
allocation (i.e., the sample size in each stratum is proportional to the
area of the stratum).
2.2. Choosing the sampling design
Deciding which sampling design to implement requires considering
the objectives of the accuracy assessment and prioritizing desirable
design criteria. The typical objectives for accuracy assessment focus on
description of overall accuracy, class-specific accuracy, and proportion
of area occupied by a class, and often these estimates are needed for
subregions (e.g., states or provinces would be subregions within a na-
tional assessment). Desirable sampling design criteria include that the
design: 1) satisfies the conditions defining a probability sampling de-
sign; 2) is easy to implement for both selecting the sample and pro-
ducing estimates of accuracy and area; 3) is cost effective; 4) readily
allows for increasing or decreasing the sample size; 5) is precise in the
sense that estimates have small standard errors; 6) has an unbiased
estimator of variance; and 7) is spatially well distributed across the
study area. Different sampling designs achieve these criteria to varying
degrees (Stehman, 2009a). Criterion 1 is critical to achieve a statisti-
cally rigorous assessment, whereas criteria 2, 3, and 4 address practical
concerns. In particular, criterion 4 addresses the common occurrence
thatthesamplesizemayneedtobeadjusted,as,forexample,whenthe
actual data collection cost is greater or less than the cost initially
budgeted for in planning. The ability to decrease the sample size (or, in
rare circumstances increase the sample size) while still maintaining the
probability sampling feature of the design is very desirable in some
applications. Criteria 5 through 7 pertain to the reliability of the as-
sessment as small standard errors (i.e., precise estimates) translate to
narrow confidence intervals thus reflecting greater reliability of the
accuracy assessment. Criterion 7 is relevant to precision because a
spatially well-distributed sample, such as results from systematic sam-
pling, often yields smaller standard errors (see Section 2.4).
The decision of what sampling design to use is not a matter of
choosing between whatis practical versuswhat is statisticallyrigorous,
but rather choosing what is practical and rigorous (Stehman and
Czaplewski, 1998; Stehman, 2001). The process for deciding which
sampling design to implement focuses on three major questions: 1)Should clusters be used? 2) Should strata be used? and 3) What selec-
tion protocol, simple random or systematic should be used? Regarding
the firsttwoquestions, therecommendation istostratifyforobjectives,
cluster for costs. Strata are generally used in the sampling design when
the objectives include precise estimates by class (e.g., user's accuracy)
orbyregion.Stratifyingbymapclassachievestheformerobjective,and
stratifying by region achieves the latter objective. Cluster sampling is
typically implemented when substantial savings in cost of imagery or
cost to travel to field sample locations are achieved by grouping sam-
pling units together (e.g., Mayaux et al., 2006; Stehman and Selkowitz,
2010;Zimmerman et al., 2013). The cost savings of cluster sampling
must be tempered by the fact that because of intracluster correlation
(i.e.,correlationamongsecondarysamplingunitswithinclusters),each
cluster may be equivalent to only a modest number of unclustered
sample points or units (Gallego, 2012; Gallego and Stibig, 2013). Yang
et al. (2018) provide a contrasting option to reduce cost by defining
strata constructed to represent cost of access and then increasing the
sample size in the low cost strata.
Theneedforreproducibility(Section1.2)dictatesthatthesampling
design must be clearly documented. Unfortunately the sample design is
often inadequately described. Ye et al. (2018) revealed the severity of
the problem as they found that only one-third of the 209 articles they
examinedintheirreviewofobject-basedaccuracyassessmentdescribed
how the sample was selected. Key features of the sampling design that
must be documented to satisfy the reproducibility criterion are: 1) de-
scribetherandomizationimplementedinthesampleselectionprotocol;
2) specify the inclusion probabilities or the information needed to de-
rive the inclusion probabilities; 3) if stratified sampling wasimplemented, describe how the strata were constructed, provide theproportion of area in each stratum, specify the sampling design im-
plemented within each stratum, and state the sample size allocated to
each stratum along with the rationale for this allocation; 4) if cluster
sampling was implemented, define the primary sampling unit (PSU)
andthesecondarysamplingunit(SSU),statewhetherone-stageortwo-
stagesamplingwasimplemented(inone-stagesamplingallSSUswithin
each sampled PSU are observed, whereas in two-stage sampling a
sample of SSUs is selected within each sampled PSU), and specify the
sampling design implemented at each stage.
Registering or archiving the original sample locations prior to col-
lecting the reference data would be a valuable contribution to re-
producibility. Gallego (2011) andWaldner et al. (2019) expressed the
concern that except for systematic sampling designs, it is impossible to
tracewhethertheoriginallyselectedsamplelocationswereactuallythe
ones visited to obtain reference data or whether some of the sample
locations had been moved or simply not used. This concern with “tra-
ceability” would be greatly diminished if the original sample locations
are registered or archived in the public domain prior to beginning data
collection. To address the transparency criterion, any problems related
to missing data (i.e., non-response due to difficult access on the ground
or due to clouds in the reference imagery) should be documented and
quantified (e.g., percent of sample units that were not obtainable). If
sample units are moved or replaced, these modifications of the original
sample selection should be clearly highlighted as part of an honest,
transparent portrayal of the actual implementation of the sampling
design.Practicalissuesoften mayimpactthesampleselectionprotocol,
but the remedies implemented to address these practical challenges
must still adhere to principles of statistical rigor (Stehman, 2001).
2.3. Sampling design for accuracy assessment and area estimation of land-
cover change
Applications mapping land cover change have become increasingly
common and there are many methods for change detection (Singh,1989;Mas, 1999). Here we focus on land-cover conversions and do not
addressaccuracyofland-covermodificationsthatarenotmanifestedas
a change in the land-cover class label. Choosing a sampling design in
land-cover change studies requires additional considerations not pre-
sent in single date assessments. Not all of these issues have been fully
resolved as the investigation of sampling designs for long-term, high
temporal frequency land-cover monitoring is still at a nascent stage.
Because change is typically a rare characteristic of the landscape,
stratifiedsamplingisoftenimplementedtoensureaspecifiedminimum
sample size from the change classes (Olofsson et al., 2013, 2014).
Stratification is commonly employed in single date land cover assess-
ments as well, but for land-cover change assessments the number of
from-to change types may be so large that defining every from-to
change possibility as a stratum would require an impractically large
sample size. In such cases, more general from-to categories can be de-
fined as strata,as for example cropland lossto include allchanges from
cropland to any other class and urban gain to include changes from all
other classes to urban (Wickham et al., 2013, 2017).
Multiple change periods may be of interest in land-change mon-
itoring. For example, suppose the objective is to monitor annual wet-
land change over a 20-year period using Landsat imagery. In such ap-
plications,andassumingastratifieddesignwillbeemployed,isitbetter
to have a single, permanent set of sample units observed for each year
throughoutthe20-yearperiod,orisitbettertoselectadifferentsample
for each annual change estimate? Permanent sample locations may
offer some efficiency in terms of the response design when using ar-
chived satellite imagery via software such as TimeSync (Cohen et al.,
2010)orCollectEarth(Beyetal.,2016).Further,theabilitytoviewthe
entire 20-year time series for each sample pixel may offer the inter-
preter greater temporal context for deciding whether change had oc-
curred in a particular year of the time series and thus a permanentS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
5
samplecouldconceivablyyieldamoreaccuratereferenceclassification.
If the change process is cyclical (e.g., forest management that produces
losses followed by gains in tree cover), then permanent sample loca-
tions may be preferred to capture this change dynamic. But if the dy-
namic is predominantly unidirectional change (e.g., conversion of
forest to developed), then a different sample for each time period of
interest may be preferred because it would be possible to stratify the
sampling design specific to that time period and this would likely yield
better precision than permanent sample units. Such an outcome was
observed in a study estimating forest change on a biannual basis in
whichArevalo et al. (2019) found that selecting a new sample for each
biannual change period (with strata defined specifically for eachbiannual period) was more precise than selecting a single, permanent
sample based on strata defined for the entire time period monitored.
Assessing the accuracy of a burned area map presents a special case
of change accuracy assessment. The objectives include assessing both
the spatial and temporal features of the burned area map (i.e., do the
map and reference data agree on whether the sample unit burned and
agreeonwhenitburned).Therarityofburnedareaexistsinbothspace
and time, so stratified sampling is motivated to intensify the sample in
thespaceandduringthetimeforwhichtheclassifiermapsburnedarea.
Boschetti etal.(2016) andPadillaet al.(2017) address thisproblemby
representing the population using voxels defined by a time interval(e.g., a 16-day Landsat acquisition interval) and a two-dimensional
spatial unit (e.g., a Thiessen Scene Area shown in Fig. 1 of Boschetti
etal.,2016).Thesevoxelscanthenbeassignedtostrataandthesample
intensified in strata for which burned area is more likely to be found.
The difference between this approach and a design in which a new
sample is selected for each time period is that the objective for the
burned area application does not include estimating accuracy for each
time period.
2.4. Spatial correlation and sampling design
Spatial correlation is frequently raised as a concern regarding
sampling design for accuracy assessment. The presence of spatial cor-
relation has no impact on design-based inference in the sense that es-
timators of accuracy and their associated variance estimators remain
unbiased regardlessofthe magnitudeofspatial correlation (De Gruijter
and Ter Braak, 1990; Stehman, 2000). Thus there is no need to space
sample units apart to legitimize design-based inference. The primary
impact of spatial correlation is that it may affect the precision of the
estimates. For example, positive spatial correlation will increase stan-
dard errors for cluster sampling because pixels within each cluster will
be more similar to each other. For systematic sampling, positive spatial
autocorrelation leads to smaller standard errors relative to simple
random sampling and this precision advantage is a primary motivation
for having a spatially well-distributed sample (Section 2.3).
The desire to avoid sampling spatially proximate units has led to
development of spatially balanced probability sampling designs that
spread sample units apart (Benedetti et al., 2017; Stevens and Olsen,
2004). Systematic sampling is the most familiar spatially balanced
sampling design. A simple but effective alternative to systematic sam-
pling for achieving spatial balance is to stratify the region spatially and
to sample one unit (e.g., one pixel or one cluster) per spatial stratum
(Fattorini et al., 2015). Two example applications of this design are
Fichet et al. (2014), in which the spatial strata were 20km×20km
cells, and Stehman and Selkowitz (2010), in which the spatial strata
were biomes in Alaska, USA. Benedetti et al. (2017) review a variety of
other spatially balanced probability sampling designs, some of which
are quite complex. The decision of whether to use these more complex
designs will require choosing between two desirable design criteria,
spatial balance versus ease of implementation and analysis.
A more problematic consequence of the desire for a spatially ba-
lanced sample is when ad hoc methods are used to avoid sampling
neighboringornearbyunits.Forexample,ifastratifiedrandomsampleisinitiallyselectedandthensampleunitsdiscardedormovediftheyarewithin a certain distance of another sample unit, the inclusion prob-
abilities of this design will be very complicated. Unless these compli-
cated inclusion probabilities are derived, the sampling design will not
satisfy the criteria defining a probability sample. Elaborate algorithms
based on minimizing the spatial correlation among sample locations
can be created but if such techniques are used it is incumbent on the
developers of the algorithm to derive the inclusion probabilities for the
design. If a spatially balanced sample is implemented for accuracy as-
sessment,we recommend usingoneoftheprobability samplingdesigns
currently documented in the peer-reviewed literature (Benedetti et al.,
2017).
2.5. Sample size planning and allocation to strata
The enduring question of “What sample size is needed?” must be
addressed when planning the sampling design. Although formulas for
sample size planning yield deterministic, objective outcomes, the sub-
jective component of choosing the sample size requires judicious se-
lection of values to input to these sample size formulas. The reality for
many applications is that the sample size chosen will be constrained by
availability of resources to collect the sample data. Pragmatically, a
priori sample size calculations are effectively an exercise in forecasting
whether the sample size afforded by the project's budget will meet the
desired precision requirements. Yet another difficulty is that typically
several objectives are of interest, and sample size planning becomes
more complex when attempting to meet precision requirements for
multiple estimates.
Acommonlyusedsamplesizeformulathatcanbeappliedtooverall
accuracy for simple random sampling and to user's accuracy for stra-
tified random sampling is:
=nz p p
d(1 )2
2
(1)
wherez=1.645 for a 90% confidence interval or z=1.96 for a 95%
confidence interval, dis the desired half-width of the confidence in-
terval (i.e., the confidence interval is estimated accuracy± d), andpis
theanticipatedoverallaccuracyinthecaseofsimplerandomsampling,orpis anticipated user's accuracy in the case of stratified sampling
wherewedeterminethesamplesize n
hforeachstratum.Forexample,if
the anticipated user's accuracy is p=0.80 and we would like a 95%
confidence interval (z =1.96) to have a half-width of d=0.05, Eq. (1)
yieldsn=246.Ifinsteadweanticipateauser'saccuracyof p=0.5,the
sample size formula yields n=384. The choice of dmay vary de-
pending on the importance of a class, so for example a high priority
class might have d=0.02 because we want a more narrow interval for
this important class, whereas a class regarded as less important to theobjectives might have d=0.05 reflecting tolerance of a wider interval.
Because the true overall accuracy or user's accuracy is unknown at thesample planning stage, pis subjectively chosen. Therefore, it is often
useful to try several values of pto assess the range of nthat might be
neededforvariouspotentialoutcomesofoveralloruser'saccuracy.Ifin
reality it turns out that pin Eq.(1)is different from the value used to
plan the sample size, the width of the confidence interval dspecified at
the planning stage will not be the width of the confidence interval es-
timated from the sample data. The maximum sample size from Eq. (1)
occurswhen p=0.5soitispossibletoestablishanupperboundonthe
sample size for a specified zandd. Eq.(1)does not normally apply to
estimatingproducer'saccuracybecauseinpracticethereferenceclassescannot be used for stratification (this would require complete coveragereference data).
In practice, the answer to the question, “Was the sample size large
enough?” is observable from the standard errors and width of the
confidence intervals obtained for the accuracy and area estimates.
Consequently,whenaddressingthequestionofwhetherthesamplesize
was adequate, the sample size planning details are not as important asS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
6
the magnitude of the standard errors or confidence intervals associated
with the estimates. For example, if the 95% confidence interval for
user's accuracy of cropland is 78% to 85%, would a user interested in
croplandhaveadifferentopinionofthequalityofthemapiftrueuser's
accuracy is 78% versus 85%? If not, the sample size was adequate to
generate a confidence interval that is sufficiently narrow for this user's
purpose.Conversely,iftheconfidenceintervalis60%to85%,wemight
expect a user to have a different opinion of map quality for cropland if
the user's accuracy is 60% (the lower bound of the confidence interval)
compared to if the user's accuracy is 85% (the upper bound). For this
latter interval, the range of values is likely too broad to meet the user's
needs.Ifthestandarderrorsareunacceptablylarge,theoptionexiststo
augment the sample size to reduce the standard errors and thereby
narrow the associated confidence interval (see Section 2.2).
The sample size allocation to strata is a critical feature of stratified
sampling designs. The three primary allocation options are equal, op-
timal, and proportional. Equal allocation is typically used when the
objectives specify precise estimation of user's accuracy for each class.
Equal allocation is justified if the classes are all considered equally
importantandallclasseshavethesameuser'saccuracy(i.e., pinEq.(1)
is the same for all classes). If classes differ in importance, priorityclasses should have more narrow confidence intervals which can be
achievedbydecreasing dinEq.(1).Theeffectofdecreasing dwillbeto
increase the sample size n. Optimal allocation is justified if the primary
objective is to minimize the standard error of a single estimate such as
overall accuracy or area of one class. If many estimates are of interest,
an algorithmto determine the optimal allocationfor multiple estimates
may not be available. For proportional allocation the sample size n
his
proportional topopulationsize Nhinstratumh.Proportional allocation
has the benefit of simplicity of analysis because it is an equal prob-
ability sampling design, so estimation formulas for accuracy and area
are the same as for simple random sampling (the standard error for-
mulas are different). However, the practical utility of proportional al-
locationislimitedbecausenearlythesameprecisionofestimatescanbe
achieved via poststratified estimation applied to a simple random or
systematic sample (see Section 4.2).
Thesampleallocationdecisionbecomescomplicatedwhenmultiple
objectives are of interest, as for example when estimating area and
accuracy for several classes and/or for multiple dates. The challenge is
thatdifferentsampleallocationsbenefitprecisionofdifferentestimates,
sochoosinganallocationthatisverybeneficialtooneobjectivemaybe
detrimentaltootherobjectivesintermsoftheresultingstandarderrors.
Olofsson et al. (2014) provide some ad hoc guidelines for sample al-
location when the objectives are to estimate accuracy and area, andWagner and Stehman (2015) provide a spreadsheet program for ob-
jectively determining the optimal allocation that simultaneously mini-mizes the sum of the variances of the estimates of user's accuracy,
producer's accuracy, and area (based on the reference classification) of
asingletargetclass.Additionalresearchonsampleallocationoptionsis
needed because multiple objectives are the norm for many present day
land cover studies. The sample size question is revisited in the context
of comparing accuracy in Section 4.6. A final important concept re-
garding sample size is that it is the absolute size of the sample not thepercent of the population sampled that drives the precision of the es-
timates. That is, the standard errors for the accuracy and area estima-
tors are determined by how large nandn
hare. Even if nandnhare a
verysmallpercentofthepixelspresentinthestudyregion,thestandarderrors of the estimators may be acceptably small (Stehman, 2001, p.
730).
2.6. Examples of sampling designs used in practice
A wide variety of sampling designs have been used to assess accu-
racy of large-area land-cover maps (Table 2). The examples listed in
Table 2have been purposely chosen to illustrate this diversity of op-
tions. One of the most commonly used sampling designs is stratifiedrandom sampling where the strata are the map classes. Olofsson et al.
(2014)recommend this design as a good practice option. Stratification
is a common feature of these examples with regional (geographic)
stratification employed for the objective of estimating accuracy by re-
gion and stratification by land-cover class implemented to ensure that
rare classes are represented by sufficient sample sizes to address the
objective of estimating user's accuracy. Cluster sampling is also rela-
tively prominent in these applications. The different sampling designs
implemented for the series of National Land Cover Datasets (NLCD) of
the United States (Wickham et al. 2004, 2010, 2013, and 2017) track
the evolution of advances in technology and mapping objectives
(Table 2). For example, the 1992 NLCD design (Wickham et al., 2004)
used cluster sampling because of the cost savings associated with col-
lecting the reference data which required obtaining hard copy aerial
photographs. The reference class labels for more recent NLCD assess-
ments were interpreted from freely available Google Earth and Landsat
imagery. Consequently, cluster sampling was not used because the cost
issue associated with purchase of aerial photographs was no longer a
consideration. Stratification was a prominent feature of all NLCD
sampling designs. However, the more recent NLCD products included
the objective of assessing accuracy of change so the number of strata
was increased and the strata re-defined to accommodate the objective
of estimating the accuracy of change. Sampling designs that use the
reference classification to define strata are conspicuously absent from
these examples (Table 2). This is because the usual implementation of
stratifiedsampling requiresthat allpixelsinthe regionbe assigned toa
stratum, and reference class labels are not available for all pixels in the
entire population. If complete coverage reference data were available
we would have no need for sampling to conduct the accuracy assess-
ment.
The high cost of obtaining accuracy reference data has motivated
development of methods to improve efficiency of reference data col-
lection. Having a common repository of reference data applicable toassessing accuracy of several map products would be a valuable addi-
tiontolandcoverscience(Lovelandetal.,2000; Wulderetal.,2018,p.
4266;Zhao et al., 2014). Olofsson et al. (2012) andStehman et al.
(2012)described a global sampling design tailored to this purpose, and
Tsendbazar et al. (2018) implemented a modified version of thisdesign
inAfrica.Effortstoimplementsamplingdesignsforcollectingreference
data that can be used for multiple maps should continue to be pursued
as the current practice of collecting reference data for every individual
mapping project is clearly an inefficient use of resources. Crowdsour-
cing and Volunteered Geographic Information (VGI) offer appealing
possibilities for decreasing the cost of obtaining the reference classifi-
cations.Stehman et al. (2018) elucidated the sampling design and in-
ference issues encountered when combining sample VGI and author-itative data, and Waldner et al. (2019) implemented a global stratified
systematic sample to evaluate methodology for augmenting expert in-terpretation with VGI. Extracting reference data from ongoing land-
cover monitoring sampling programs is another possibility to enhance
efficiency of reference data collection. For example, the Land Use/
Cover Area-frame sample (LUCAS) in Europe has been used for accu-
racy assessments (Gallego, 2011) and Kempeneers et al. (2013) used
sample data from several national forest inventories for accuracy as-sessment. Obtaining reference data from two or more sampling pro-
grams that were not specifically designed to provide accuracy assess-
ment information introduces several challenging issues for use of these
data (Stehman et al., 2000), but if these issues are addressed data from
these programs may enhance accuracy assessment and area estimation.
2.7. Sampling design for training and validation data
Using the same data for training and validation can lead to opti-
mistically biased accuracy assessments (Hammond and Verbyla, 1996).
Consequently,thetrainingsampleandthevalidationsampleneedtobe
independent of each other which can be achieved by appropriatelyS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
7
dividing a single sample of reference data or, perhaps more commonly,
by acquiring separate samples for training and testing. Moreover, the
goals and needs of training are different from those of accuracy as-
sessment and hence the optimal sample data for training could be
markedly different from that for accuracy assessment. For example,
purposive sampling and pragmatic site selection can be perfectly ac-
ceptable in training a classifier, whereas a probability sampling design
is a good practice requirement for accuracy assessment.
Independence of the accuracy assessment sample from the training
can be assured simply by implementing a separate probability sample
foraccuracyassessment.Thatis,allpixelsintheregionofinterest(ROI)
have a known and non-zero probability of being included in the re-
ference sample for accuracy assessment. Often the reference sample
usedforaccuracyassessmentisselectedafterthemaptobeassessedhas
been produced. This allows use of stratified sampling where the strata
are defined by the map classes. Constructing the strata from the map
does not violate the independence requirement as the strata affect only
precision of the estimates and using the map for stratification does not
introduce bias in the accuracy estimators.
In some applications training and validation data are collected si-
multaneously from a common sampling design followed by random
assignment of each sampled pixel to the training or validation set. In
such applications, a probability sampling design must still be im-
plemented to use design-based inference in a rigorous accuracy as-
sessment, even though probability sampling may not be a necessity for
training data. If cluster sampling is the probability sampling design
implemented, the random separation of sample units to training or
accuracy assessment should be done at the cluster level rather than the
pixel level (i.e., if a cluster is randomly assigned to training data, then
allsampledpixelswithinthatclusterwouldalsobeassignedtotraining
data). If pixels within a sampled cluster are randomly assigned to both
trainingandaccuracyassessment,theindependence ofthetrainingand
accuracy assessment samples is violated. Several studies have found
that optimistic estimates of accuracy are produced if individual pixels
rather than clusters are the units randomly separated to training and
validation (Friedl et al., 2000; Zhen et al., 2013; Lyons et al., 2018).3. Response design
The response design defines how a decision on the agreement be-
tween the predicted (map) class label and the reference class label is
made.Althoughthismayseemasimpletasktheresponsedesignentails
fundamental issues that greatly influence an accuracy assessment, in-
cluding specifying the spatial unit of the assessment, deciding what
ground data or imagery to collect, determining the labelling protocol
for the reference data, and deciding how thematic agreement will be
defined. Key general principles of the response design are identified in
this section, but because details of the response design are very specific
to each land-cover application, we do not delve into individual case
study examples. Congalton and Green (1999, Chapter 4) andOlofsson
et al. (2014) are good sources for additional description of response
design options.
A variety of reference data sources have been used in accuracy as-
sessments. Popular sources include data acquired directly in the field
and imagery with a spatial resolution considerably smaller than that
used to produce the land cover product being evaluated. The former
may include fieldwork undertaken specifically for the accuracy assess-
ment (e.g., Nusser and Klaas, 2003) and rigorous authoritative studies
suchasforestinventories(e.g., Wulderetal.,2006b; Kempeneersetal.,
2013).Thelatterdatasourcemaybeaerialphotographs(e.g., Wickham
et al., 2004), airborne video (e.g., Marsh et al., 1994) or fine spatial
resolution satellite sensor data such as are available via Google Earth
(Gorelick et al., 2017; Wickham et al., 2017; Badjana et al., 2017;
Tsendbazar et al., 2018).
In some applications, the same source of imagery used to determine
the accuracy assessment reference classification may have also been
used to produce the map classification. This typically occurs when it is
not possible to obtain better imagery for the accuracy assessment. For
example, land-cover change studies may be dependent on Landsat
imagerybothforaccuracyassessmentandforclassifiertraining.Insuch
applications, it is necessary to obtain the reference classification in a
manner that ensures better quality than can be obtained from the
classifier used to produce the map. Often better quality is achieved byTable 2
Example sampling designs used for accuracy assessment (listed in chronological order of publication). The three questions determining a sampling design (Section
2.2) are: 1) Are there strata? 2) Are there clusters (and if so, what are the primary sampling units (PSU) and secondary sampling units (SSU)? 3) What selectionprotocol was implemented?
Source Strata Clusters Selection protocol
Fuller et al. (1994) 32-class environmental stratification No Simple random within strata
Edwards Jr. et al. (1998) 1) 3 ecoregions
2) Road, off-road within each ecoregionPSU: USGS quadrangleSSU: 1ha block1st: Simple random within strata2nd: Simple random
Scepan (1999) Map land cover No Simple random within strata
Nusser and Klaas (2003) 1) 5 geographic strata
2) Land-cover classPSU: 7.5′ quadrangleSSU: pixel1st: Systematic within strata2nd: Systematic
Wickham et al. (2004) 1) 10 mapping regions
2) 60×30km cell3) map land coverPSU: aerial photographSSU: pixel1st: One random PSU per grid cell2nd: Simple random within strata
Wickham et al. (2010) 1) 10 mapping regions of US2) 120×120km cell
3) map land coverPSU: 12km×12km
SSU: pixel1st: One random PSU per grid cell2nd: Simple random within strata
Olofsson et al. (2012) andStehman et al. (2012) Regional related to biogeography PSU: 5km×5km
SSU: variableSimple random within strata
Wickham et al. (2013) 1) Mapping regions of US
2) Map land coverNo Simple random within strata
Gong et al. (2013) andZhao et al. (2014) 7000 hexagons globally, approximately equal area No Simple random within strata
Zimmerman et al. (2013) 1) Three regional strata (PSUs)2) Disturbance classes (SSUs)PSU: 1/8×1/8
oquadrangles
SSU: map pixel1st: Simple random within strata2nd: Stratified random
Fichet et al. (2014) 20×20km cell PSU: 2km×2km
SSU: point1st: One random PSU per stratum2nd: simple random
Linke et al. (2017) 1) 5 biomes
2) Change type within biomeNo Simple random within strata
Wickham et al. (2017) 1) East and west of US2) Change (e.g., forest loss) and stable classesNo Simple random within strata
Badjana et al. (2017) Stable and change classes No Simple random within strata
Waldner et al. (2019) Four strata representing mis-classification probabilities No Systematic (specialized)S.V. Stehman and G.M. Foody
Remote Sensing of Environment 231 (2019) 111199
8
focusing attention on a much smaller area (i.e., the reference sample)
and employing visual interpretation of the imagery to obtain the re-
ference classification (Cohen et al., 2010). Determining the reference
classification via a different protocol from that used to produce the
map, even if both employ common imagery, is acceptable to maintain
independence of the reference and map classifications. Moreover, in-
terpretersneednotbedifferentfromtheoperator(s)whodevelopedthe
classification as often the team that produced a classification has the
most incentive, the most resources, and the greatest expertise for ob-
taining the most accurate reference interpretation. A major benefit of
disclosing the sample locations, a feature of reproducibility of the
sampling design (Section 2.2), is that if there is any doubt regarding
impartialityorexpertiseoftheinterpreters,theopportunitywouldexist
to independently verify the reference class interpretations.
Even though gold standard reference data are rarely feasible to
obtain, it is still essential that the reference dataset be more accurate
than the map to be evaluated. The selection of reference data source,
which will vary with data availability and cost constraints, can have
important impacts on reference data quality. For example, the quality
of the data obtained from fine resolution imagery may vary in relation
to their date of acquisition relative to that of the data used to generate
the land cover map (the closer the two data sets are in time the better).
The quality of the data obtained via field work or image interpretation
may vary with the expertise of the staff involved. Interpreters trained
for a specific application are often used to obtain the reference class
labels, but reference data derived via crowdsourcing have become in-
creasinglyavailable(Fritzetal.,2009; Fonteetal.,2015).Regardlessof
the reference labelling protocol, the quality of the reference data set is
importantasitcanbeasourceofsubstantialmis-estimationofaccuracy
and land cover change (Foody, 2010, 2013).
The reference data labelling protocol addresses the conversion of
the reference data into a class label for comparison against the land
cover product to be evaluated. This includes issues such as theminimum mapping unit and rules to allocate a class label. The
minimum mapping unit is a fundamental variable that should be se-
lected with care; if the unit is too large it may not adequately represent
highlyfragmentedlandcovermosaics.Therulestolabelanobservation
also require careful selection. There are many definitions of land cover
classes (Ahlqvist, 2005; Comber et al., 2005). The map relevant cri-
terion (Section 1.2) requires that the definition of classes used for thereference data must be the same as those used in mapping, and an
implicit assumption in cross-tabulating map and reference data is that
the classes are exactly the same (Congalton and Green, 1999).
Accuracyassessmentbasedontheanalysisofanerrormatrixissite-
specificinthatthelabelinthemapiscomparedtoareferencelabeland
the spatial unit defines the fundamental geographical region upon
which the comparison is based. Commonly assessments are undertaken
onapixel,groupofpixels(e.g.block),oranobjectbasis(Radouxetal.,
2011;Stehman and Czaplewski, 1998, Table 1;Stehman and Wickham,
2011). Pixels are convenient as the basic building block of digitalimages but are artificial units whose boundaries need not relate in any
meaningful way to the land cover mosaic on the ground. Conversely,
objects are (potentially) natural spatial units that relate to real-world
land cover patches, but the quality of objects is often highly dependent
on the method to produce them. The analyses employed in a conven-
tional accuracy assessment (Section 4) are typically constructed based
on the assumption that the spatial unit represents homogeneous cover
of a single class. Unfortunately mixed pixels and mixed objects often
occur, representing a source of ambiguity and potential error in accu-
racy assessment, and alternative analyses must be considered in such
circumstances (Section 6).
The quantification of accuracy requires rules to define agreement
between the map and reference classifications. If each classification
provides a single label for anassessment unit(e.g., a pixel) and thetwo
labels agree the unit would be regarded as being correctly classified.
Should the labels disagree the map label is deemed to be incorrect, anassertion predicated on the assumption of perfect reference data.
Accordingly, all error in the assessment is placed upon the map which
may be unfair because issues such as geo-reference error in addition to
reference label error may be present (Foody, 2008; Verbyla and
Hammond, 1995). Sometimes the assessment is more difficult, for ex-
ample, if a spatial unit can belong to more than one class. Here,
agreement might focus on the dominant class or estimated class com-
position(e.g., LatifovicandOlthof,2004)inthemapandreferencedata
set.
Deviations from the common circumstances assumed in the dis-
cussion of the core good practice may occur such as when the map andreference data do not use the same set of classes or spatial units. It is
possible to adapt the accuracy assessment to accommodate such cir-
cumstances while maintaining the statistical rigor and credibility. For
example, when the set of classes differ an accuracy assessment can be
undertaken by bringing them to a common class scheme. Alternatively
methodsfornon-squarematricessuchasentropy-basedaveragemutual
information content (Finn, 1993; Foody, 2006) may be used. Similarly,
the use of different spatial units can be recognized in the response de-sign of the accuracy assessment allowing adaptation from typical per-
pixel based assessments for other scenarios such as the use of blocks or
objects (Stehman and Wickham, 2011; Ye et al., 2018).
Given that many options are available for each of these aspects of
protocol in the response design, it is critical that a detailed description
of the response design is provided to satisfy the reproducibility cri-
terion. Specifically, the following key features of the response design
should be documented: 1) definition of spatial assessment unit; 2) de-
finitionofclasses;3)sourcesofreferencedata(e.g.,imagery,fieldvisit,
date of imagery or field visit); 4) specific information collected from
each source (e.g., direct assignment of a reference class label versus
collection of an assortment of attributes that are later converted to a
reference class label); 5) rules for assigning reference class label(s)
based on the reference information collected; and 6) specification ofhow agreement between the map and reference classifications is de-
fined.
Documentation of the response design also requires considerable
information to satisfy the transparency criterion, such as: 1) if the re-
ference label was determined by human interpreters, describe char-
acteristics of the interpreters (i.e., their background, how they were
trained);2)statewhetherinterpreterswereunawareof(i.e.,“blind”to)
the map class of the sample units they were interpreting; 3) state
whether interpreters were oblivious to information regarding sample
allocation to strata that might consciously or unconsciously lead to
trying to produce reference interpretations that would correspond to
this known distribution of the sample by strata (e.g., if interpreters
know that the sample design was equally allocated to strata with 50
pixels per stratum, it is plausible that interpreters would produce re-
ference labels that were close to equal per class); and 4) provide in-
formation regarding the date of reference imagery or reference ground
data to allow evaluating whether discrepancies in time between data
for accuracy assessment and map production could be responsible for
some portion of classification error. Land-cover studies that provide
thorough explanations contribute to transparency and acquire cred-
ibility attributable to full disclosure and a self-critical evaluation.
Acknowledging potential limitations of the assessment provides map
users with a more informed understanding of the accuracy results and
this is preferred to less transparent reporting that would give the im-
pression of a stronger assessment than what took place in reality.
4. Analysis
The analysis component focuses on: 1) how to organize and sum-
marize information to quantify accuracy, and 2) how to estimate ac-
curacy and area from the sample data. Historically, the analysis pro-
tocol for accuracy assessment has depended on an error matrix and
accuracy measures derived from the error matrix, specifically overall,S.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
9
user's, and producer's accuracies. The error matrix also includes the
information needed to estimate the proportion of area of each class
based on the reference classification. Because of its ease of interpreta-
tion and valuable descriptive information, the error matrix should re-
main a cornerstone of the analysis protocol. In practice, the analysis
should be map relevant and statistically rigorous, focusing on easily
interpretable accuracy measures. To be map relevant, the error matrix
and accuracy estimates must reflect the proportional areal representa-tion of the study region (e.g., as shown in Table 3, the row totals of the
error matrix represent the proportion of area mapped of each class).The primary requirements for the analysis to satisfy the criterion of
statistical rigor are to employ consistent estimators and to quantify the
variability of the accuracy and area estimates by reporting standard
errors or confidence intervals (Sections 4.2 and 4.3). Deviating from
these exacting requirements for statistical rigor is sometimes justified
by an argument for pragmatism, but such deviations carry with them
the risk that the resulting accuracy and area estimates will have poor
credibility. The analysisshould also include provisions toassess quality
of reference data and to evaluate the potential impact that bias and
variability of the reference classification may have on the accuracy and
area estimates. Finally, it is important to note that the error matrix
supplies a global (entire map) level of assessment, but cannot provide
information regarding the spatial distribution of classification errors.
Consequently, a spatial display of classification error and/or un-
certainty may often be beneficial (Section 4.7).
4.1. Error matrix
The error matrix has traditionally provided an effective basis for
organizingandsummarizingagreementbetweenthemapandreference
classifications (Section 1.1). A population error matrix representing a
census of reference condition, and the corresponding notation defining
population parameters used to describe accuracy are displayed in
Table 3. In this error matrix, the rows represent the map classification
andthecolumnsrepresent thereference classification.Thecellentry p
ij
isthepopulationproportionofarea forwhichthemapclass is iandthe
referenceclassis j.Themaindiagonalcellsoftheerrormatrixrepresent
correct classifications, whereas the off-diagonal cells indicate whichclasses are confused, providing valuable information for improving the
map and for users to evaluate how specific types of misclassification
might affect their applications. The population error matrix simplifies
greatly to a 2×2 matrix for the common case of a binary legend (e.g.,
change and no change, burned area and not burned, or water and not
water).
The row and column totals of the population error matrix are im-
portant because they quantify the area distribution (e.g., fractionalcover) of the classes in the ROI. Given the layout of Table 3, the row
totals represent the proportion of area of each class according to the
map classification and the column totals represent the proportion of
area according to the reference classification. The estimated row andcolumn totals resulting from the analysis protocol must represent thereality of the ROI to satisfy the map relevant criterion (Section 1.2).
Normalizing an error matrix (Congalton et al., 1983) is an egregious
violation of the map relevant criterion as it results in an error matrix in
whichallrowandcolumntotalsareforcedtoequalto1/c (c=number
of classes). Thus a normalized matrix has no basis in the reality of themap or ground condition as rarely will all classes be present in equal
proportions (Stehman, 2004). Further, normalizing an error matrix
results in user's and producer's accuracies being equal and this also will
almost never be a real feature of a land-cover map.
The typical suite of accuracy measures computed from the popula-
tion error matrix includes overall, user's, and producer's accuracies.
Overall accuracy is the sum of the entries on the main diagonal of the
error matrix,
=
=O p
ic
ii
1
(2)
Overall accuracy has a direct interpretation in terms of area of the ROI
as it represents the proportion of area correctly classified. Overall ac-
curacy provides a very coarse assessment because it aggregates in-
formation over all map classes thereby obscuring important class-spe-
cificinformation.Forexample,ifthemapisabinarychange/nochange
classificationandchangeisrare(e.g.,1or2%ofthetotalarea),anaïve
map that shows no change will have very high overall accuracy simply
because most of the area is unchanged. In this example, class specific
information such as user's and producer's accuracies or quantity dis-
agreement and allocation agreement (described in subsequent equa-
tions) would be more informative than reporting just overall accuracy.
Overall accuracy does not “overweight” more prevalent classes or
“underweight” less common classes, but instead incorporates classes
proportionallytotheirarealrepresentationintheROI.Thelimitationof
overall accuracy is not how it weights or represents class-specific in-
formation, but rather that it simply provides no class-specific in-
formation. Consequently, it is strongly recommended to provide esti-
mates of user's and producer's accuracies or to provide the error matrix
when reporting an accuracy assessment so that these class-specific
measurescanbeestimatedbyothersifdesired.User'saccuracyforclass
i, defined as
=+ U p p / i ii i
(3)
quantifies accuracy conditional on the area mapped of each class, the
row total pi+. The complement of user's accuracy (1-U i) is the com-
mission error rate of class i. Producer's accuracy for class j, defined as
=+ P p p / j jj j
(4)
provides a view of accuracy conditional on the reference area of each
class,thecolumntotal p+j.Thecomplementofproducer'saccuracy(1–
Pj) is the omission error rate of class j.
Having two measures of class-specific accuracy has caused some
consternation when the objective is to compare accuracy or decide
which of two classifiers or maps is better, leading to omnibus measures
thatprovideasinglevaluetocharacterizeaccuracyofagivenclass. Liu
et al. (2007) reviewed a variety of such measures, including the Dice
coefficient, Jaccard's index of similarity, the average of user's and
producer's accuracy and the harmonic mean of user's and producer's
accuracy (also called Hellden's Index and F-score). These omnibus
measurestreattheoff-diagonalcellproportionsforagivenclass(p ijand
pji)asequallyimportant.Anexampleinwhichitwouldbereasonableto
make no distinction between the off-diagonal entries would be when
comparing two analysts who provide independent reference inter-
pretations for a common set of pixels in an evaluation of analyst con-
sistency. Because the identity of the two analysts would be inter-
changeable, it would be reasonable to create a single measure of
agreement per class between the two analysts. However, in accuracy
assessment the map and reference classifications are notTable 3
Population error matrix for a classification with four classes (1–4), where the
rows (i) represent the map classification and the columns (j) represent the re-ference classification; p
ijis the population proportion of area with map class i
and reference class j. The row (p i+) and column (p +j) marginal totals are the
sum of the pijvalues in each row and column.
Map 1 2 3 4 TotalUser's accuracy
1 p11 p12 p13 p14 p1+p11/p1+
2 p21 p22 p23 p24 p2+p22/p2+
3 p31 p32 p33 p34 p3+p33/p3+
4 p41 p42 p43 p44 p4+p44/p4+
Total p+1 p+2 p+3 p+4 1
Producer's
accuracyp11/p+1p22/p+2p33/p+3p44/p+4S.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
10
interchangeable as errors of omission and commission are not inter-
changeable. Consequently, combining omission error and commission
error into one measure obscures valuable information. Omission and
commission errors may not be equally problematic for the objectives of
agivenapplicationofthemapsohavingtheseparateestimatesofuser's
and producer's accuracies is often critical. If omnibus measures are
reported, they should be accompanied by the class-specific measures.
Non-site-specific accuracy is defined as the difference between the
row and column proportions of each class,
+ +p pk k
(5)
(Congalton and Green, 1999). Because omission and commission errors
may negate each other, non-site-specific accuracy may be close to 0
even if both omission and commission error rates are high. Non-site-
specific-accuracy has a direct connection to the objective of area esti-
mation (Section 4.3). For area estimation, the target parameter is p+k
because thereferenceclassificationis consideredthebest assessmentofgroundcondition.Consequently,non-site-specificaccuracymayalsobe
viewed as “map bias” in that the difference p
k+−p+krepresents the
degree to which the proportion of area mapped as class kdiffers from
the proportion of area of class kbased on the reference classification.
Mapbiasisthusthebiasattributabletousingpixelcountingtoestimatetheareaofeachclass(i.e.,countthenumberofpixelsforeachclassand
multiply by the area per pixel).
Pontius Jr. and Millones (2011) proposed partitioning the overall
differences between the map and reference classifications into two
mutually exclusive components, quantity and allocation differences.
Quantity disagreement of class kis defined as the absolute value of Eq.
(5),
=+ + q p pk k k
(6)
and overall quantity disagreement (aggregating the class-specific dis-
agreement) is defined as
=
=Q q /2.
kc
k
1
(7)
Allocation disagreement for class kis defined as
=+ + a min p p p p 2 [ , ] k k kk k kk
(8)
and overall allocation disagreement as
=
=A a /2.
kc
k
1
(9)
Lastly, if total disagreement is defined as
=
=D p 1
kc
kk
1
(10)
overall agreement can be partitioned as the sum of overall quantitydisagreement and overall allocation disagreement,
= +D Q A .
(11)
Pontius Jr. and Santacruz (2014) andPontius (2019) have further
developed this approach to describing map quality by partitioning the
allocation difference into two components, exchange in which a pair of
pixels swap class labels (e.g., a pixel classified as A by the map has
reference class B and the paired pixel is classified as B by the map but
hasreferenceclassA),andshiftinwhichtheallocationdifferenceisnot
exchange. These components of the difference between the map and
reference classifications may provide insights into map quality to sup-
plementtheunderstandinggleanedfromtheerrormatrixanduser'sand
producer's accuracies.
4.2. Estimating the error matrix and accuracy from a sample
In practice, the error matrix and associated accuracy measures areestimated from the sample of reference data. The formulas for esti-
mating the cell proportions of the error matrix and overall, user's, and
producer's accuracies depend on the sampling design used. Satisfying
the reproducibility criterion requires that the formulas used for esti-
mating accuracy and area and their associated standard errors must be
clearly documented. Pragmatically, the estimation formulas must in-
corporate a weight for each sampled pixel u, where the weight is 1/π u
(Stehman et al., 2018). In those cases for which the inclusion prob-
abilities are not the same for all sampled pixels, the estimator formulas
applied to the sample data do not necessarily match the formulas de-
finingthepopulationparameterspresentedin Section4.1.Forexample,
stratifiedsamplingformulas(Eqs. (12)–(18))accommodatethefactthat
pixels sampled from different strata may have different inclusion
probabilities, π u, and the sample data from different strata must be
weighted. The dependence of the estimator formulas on the samplingdesign is the essence of the property of consistent estimation (Särndal
et al., 1992, Section 5.3; Overton and Stehman, 1995) that serves as a
necessary ingredient of a statistically rigorous accuracy assessment.
Simplyput,theconsistencypropertyismetbyusingestimatorformulas
specific to the sampling design implemented.
Themajorityofaccuracyassessmentstudiesemploysimplerandom,
systematic, or stratified random sampling. Stratified estimation for-
mulas (Card, 1982) are particularly important because of their general
utilitytoallthreeofthesedesigns.Stratifiedestimationformulasare,of
course, always applied to a stratified sampling design. However, even
when the sampling design is simple random or systematic, stratified
estimation formulas can be applied, a protocol called poststratification
or poststratified estimation, where “post” indicates that the strata in-
formation is employed after the sample has been obtained at the esti-
mation (analysis) stage, but strata are not used in the sampling design.
Thus, poststratification uses stratified estimation formulas applied to
sampledatafromanunstratifieddesign.Thesameinformationrequired
to implement a stratified sampling design is required to implementpoststratified estimation. That is, we must know the stratum to which
eachpixelintheROIbelongs,andwemustknowtheproportionofarea
of the ROI each stratum occupies.
We present the stratified estimators because of their common use in
practice. For these sample estimators we assume that the strata are
defined as the classes mapped (e.g., a pixel with map class iis assigned
to stratum i), and that all pixels have the same area. The stratified es-
timator(estimatorsaredenotedbyplacinga^overtheparameterbeing
estimated) for the p
ij=proportion of area in cell (i, j) of the error
matrix is
=
+p Wn
nij iij
i
(12)
whereWiistheproportionofareamappedasclass i,nijisthenumberof
sample pixels in cell (i, j) of the error matrix, and ni+is the sample size
fromstratum i.Estimatorsforotherparameterscharacterizingtheerror
matrix can be produced by substituting
pij forpijin the formulas for
each parameter. The estimator of overall accuracy is then
= =
= =+O p Wn
nic
ii
ic
iii
i1 1
(13)
the estimator of user's accuracy of class iis
=+ U p p / i ii i
(14)
and the estimator of producer's accuracy of class jis
=+ P p p / j jj j
(15)
where
=+ =p pj ic
ij1 is the estimated proportion of area in reference
classj. The standard errors of the stratified estimators are obtained by
taking the square root of the estimated variance, where the variance
estimators are as follows:S.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
11
=
=+ V O W U U n( ) (1 )/( 1)
ic
i i i i
12(16)
= + V U U U n( ) (1 )/( 1)i i i i
(17)
= +
++
++
+ ++V P
NN P U U
nP Nn
nn
nn( )
1 (1 ) (1 )
11 /( 1)j
jj j j j
jj
i jc
iij
iij
ii 22 222
(18)
where
=+ =+
+N n j ic N
nij 1i
i is the estimated total number of pixels of re-
ference class j,Nj+is the total number of pixels of map class jin the
entire map, and nj+is the sample size of stratum j. For simple random
and systematic sampling these are poststratified variance estimators
following the recommendation of Särndal et al. (1992, Section 7.10.2)
to “condition” the estimators on the actual sample sizes ni+for each
stratum. Further, for systematic sampling the variance estimators pre-
sented above are approximations that usually result in overestimation
of variance.
In practice, situations arise in which the map class of a pixel is not
the same as the stratum to which it belongs, so in such cases the strata
donotcorrespondexactlytothemapclasses.Forexample,supposethat
astratifiedsampleisselectedusingtwostratadefinedbychangeandnochangeasdeterminedfromMapY.Afterthereferencesampledatahave
been obtained, the producers of Map Z want to use the same reference
data for their change/no change classification. However, some of the
pixels that were labelled as change by Map Y are not change in Map Z,
soforMapZthestratumtowhichapixelwasassignedwhenthesample
was selected is not necessarily the same as the Map Z class label of the
pixel. These reference sample data may still be used to assess accuracy
ofMapZ,buttheconventionalstratifiedformulas(Eqs. (12)–(18))must
be replaced by different formulas based on indicator functions
(Stehman,2014).Anothercommonsituationforwhichthestratadonot
correspond to the map classes is when a hierarchical classification le-
gend is employed. For example, suppose stratified sampling is im-
plemented and three of the strata are deciduous forest, coniferous
forest, and mixed forest. If a general forest class is defined as the
combinationofthesethreeclasses,thisforestclassisnotequivalenttoa
stratum for this sampling design so we cannot simply combine sample
counts from the three forest-related strata. Again an indicator function
approach provides the needed general framework for estimation that
can be applied to this situation (Stehman, 2014).
Estimation formulas for cluster sampling depend on whether one-
stage or two-stage cluster sampling is implemented and on the specific
sampling design employed at each stage. It is critically important that
the variance estimation formulas account for the fact that the sample
units are clustered because otherwise variances may be substantially
underestimated. Stehman (1997b) presents formulas relevant to one-
stage cluster sampling in which the clusters are selected using simple
random sampling. Estimation formulas for various two-stage cluster
sampling designs may be found in Zimmerman et al. (2013), Edwards
Jr. et al. (1998), Nusser and Klaas (2003), Stehman et al. (2003), and
Potapov et al. (2014). The variance estimation formulas may become
complex whenthecluster samplingdesignincludesstratification atone
or both stages.
4.3. Area estimation
Accuracy assessment and area estimation have been closely linked
dating back at least to Card (1982) as area estimation is a common
objective of land cover studies. Although the area of each class can be
readily computed from a map (i.e., “pixel counting”), classification
errorslikelyresultinbiasofthemaparea obtainedfrompixelcounting
(Eq.(5)). Consequently, Olofsson et al.'s (2014) good practice re-
commendation is that area estimation should be based on the referenceclassification because it is the best possible assessment of ground con-
dition (although see Waldner and Defourny (2017) for another per-
spective). The reference classification is available only for the pixels inthe sample so it is necessary to estimate area from the sample. In this
review, we limit attention to area estimation conducted in conjunction
with accuracy assessment. Gallego (2004) andStehman (2009b) pro-
vide expanded overviews of area estimation that encompass applica-tions in which accuracy assessment is not an objective.
The proportion of area of each class based on the reference classi-
fication can be estimated directly from the error matrix if the cells of
the error matrix are reported in terms of proportion of area (Table 3).
Forthecommonlyusedsamplingdesignsofsimplerandom,systematic,
or stratified random, an unbiased estimator of the proportion of area of
classk(p
+k) is the sum of the estimated error matrix cell entries for
columnk,
=+
=p pk
ic
ik
1
(19)
where
pik is given by Eq. (12)andc=number of classes. Note, as ex-
plained below, this calculation accounts for the effect of errors of
omissionandcommissionontheareaestimatesforaclass.Thevariance
estimator for
+pk is
=+
=+ V p W p p n( ) (1 )/k
ic
ii i i
12
(20)
where
=+ p p p /i ik i is the estimated proportion of area of class kin
stratumi(eqs.19 and 20 assume that the strata are equivalent to the
map classes). For simple random and systematic sampling designs, Eqs.
(19) and (20) represent poststratified estimation of area (Section 4.2).
4.3.1. Role of the map in area estimation
Although the fundamental data used for area estimation are the
reference classifications for the sampled pixels, the map still plays the
important role of reducing standard errors of these area estimates. In
the case of stratified sampling design or poststratified estimation, the
map provides the information to construct the strata (e.g., Gallego and
Bamps,2008).Asageneralrule,themoreaccuratethemap,thegreaterthereductioninthestandarderrorsoftheareaestimatescomparedtoa
strategy that does not use strata determined from the map.
Understanding that the role of the map is to reduce standard errors
is critical because the terminology used for area estimation is some-
times confusing in this regard. The terms “error-adjusted” (Olofsson
et al., 2013) or “bias-adjusted” estimator (Stehman, 2013) are a source
of this confusion. These phrases are based on the intuitively appealing
idea that the bias of the pixel counting estimate of area can be com-
pensatedforbyadjustingtheareausingasample-basedestimateofthis
bias.Fromthepopulationerrormatrix(Table3),theproportionofarea
of classkaccording to the reference classification can be expressed as
= ++ +p p p pk k
j kkj
i kik
(21)
whichuponinspectioncanbe viewedastheproportionofareamapped
as classkfrom pixel counting (p k+) and then adjusting this area by
subtracting the proportion of area of commission error and adding theproportionofarea ofomissionerrorofclass k.Ratherthanestimate the
three separate components of the righthand side of (21), we simply
estimatep
+kdirectly using eq. (19).
An alternative estimator of proportion of area directly originating
from an adjustment of the map proportion has been proposed(McRoberts and Walters, 2012). That is, the bias of the pixel counting
mapproportion p
k+isgivenby(p +k−pk+).Thisbiascanbeestimated
from the sample data, and then used to adjust the map proportion pk+
via.
= ++ + + +p p p p ( )k k k k
(22)S.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
12
where the term in parentheses is an estimator of the map bias given by
Eq.(5), also known as non site-specific accuracy. The estimator (22)
thus starts with the map proportion of area from pixel counting andadjusts that area for map bias estimated from the sample. Referring to
this estimator as bias-adjusted mis-directs the focus of the role of the
map in the process because the phrase bias-adjusted implies that the
mapsuppliesthefundamentaldatafortheareaestimator.Inrealitythe
reference sample data are the basis for the estimator of area, and the
role of the map is to provide the auxiliary information that contributes
to reducing the standard error of the area estimator via either stratified
sampling design or poststratified estimation. The key consideration
regardingthemapisnotwhetherpixelcountingleadstobiasbutrather
how much benefit does the map provide in terms of reducing the
standard error of the area estimators. An additional consideration is
that the so-called bias adjusted estimator (Eq. (22)) is not equivalent to
the stratified estimator (Eq. (19)) (unless the sampling design is stra-
tifiedrandom).Thestratifiedestimator(Eq. (19))isgenerallypreferred
to the estimator given by Eq. (22)because of smaller standard error
(Stehman, 2013).
Thesampleprovidesestimatesofthetotalareaorproportionofarea
of each class, whereas the map provides the spatially explicit in-
formation on how the area of each class is distributed across the
landscape. Consequently both the sample-based area estimates and the
map spatial depictionarecritically important.For consistency,the area
mapped for each class would ideallymatch the area estimated from the
sample. It would thus be desirable to adjust the map to create this
correspondence with class area sample estimates (Song et al., 2017;
Healey et al., 2018) or, for example, inventory data such as from
agriculture andforestrysurveys (Ramankuttyetal.,2008).Howbest to
make such area adjustments merits further study. The sample estimates
of area have associated uncertainty as quantified by the confidence
intervals, so techniques to adjust a map to match sample-based esti-
mates of area would also need to incorporate the inherent uncertaintyof the area estimates.
4.3.2. Pixel counting versus sample-based estimation of area
The good practice recommendation to estimate area using the re-
ference classification (Olofsson et al., 2013, 2014) is based on the
premise that the reference classification is obtained with negligible
error (Section 6.1) and variability (Section 6.3). The former may in-
troduce bias of the area estimator and the latter will increase the var-
iance of the area estimator. In reality, reference data error and varia-
bility thus affect whether estimating area based on a sample of
reference data is superior to the pixel counting approach to area esti-
mation.Meansquareerror(MSE)canbeusedtodecidewhichapproach
to area estimation, sample-based or pixel counting, is better, where
= + MSE Variance Bias2
(23)
and smaller MSE is preferable because it represents smaller uncertainty(Stehman,2005).Whencalculatingareafromthemap(pixelcounting),
the bias is attributable to map classification error (Eq. (5)), whereas
reference data error would be the only source of bias for the sample-based approach to estimating area. In terms of the variance contribu-
tion to MSE, the sample-based estimator would include contributions
from sampling variability, as quantified by the standard error, and
variability resulting from the reference classification (i.e., interpreter
variability, Section 6.3). It is less clear what contributions to variance
should be considered when calculating area from the map. The map
could be considered a census of the ROI in which case there would be
no variance contributed from sampling variability (Stehman, 2005).
However, it would seem reasonable to consider variability of the
mapping process as relevant, for example variability over different sets
of training data or variability over different technicians implementing
the classification.
Area estimation plays an important role in land cover science and a
variety of considerations impact the utility of area estimates producedfrom pixel counting versus sample-based estimation. Uncertainty of thearea estimators is one aspect of utility, and quality of reference data,
both accuracy and consistency, have received greater attention as
contributors to the overall uncertainty of area estimates. The con-
ceptual foundation of the comparison of uncertainty requires further
development to help resolve the question of which approach, pixel
counting or sample-based estimation, is better in a given application.
4.4. Reporting accuracy and area
Early presentations of the error matrix often focused on sample
countsbydisplayingthenumberofsamplepixelsthatfellineachcellof
the matrix (Congalton and Green, 1999, pp. 46–47; Congalton, 1991,
p.36). This practice is justifiable for equal probability sampling designs
such as simple random and systematic sampling. However, if the sam-
pling design is not equal probability, an error matrix comprised of
samplecountsdoes notprovidethecorrectrepresentation oftheROIin
terms of the area proportions for the cells of the error matrix, thereby
violating the map relevant criterion. Further, accuracy estimators pro-
duced from an errormatrix of samplecounts will be biased. Even when
the sampling design is equal probability, poststratified estimation
should typically be implemented and this will result in different esti-
mated proportions
pij
from those obtained using the sample counts (see
the numerical example in the following paragraph). The recommendedreporting format is consequently an error matrix expressed in terms of
percent or proportion of area (as in Table 3), with an error matrix of
sample counts provided as an appendix or supplemental information.
Anumericalexample illustratesthebasic principlesofa statistically
rigorous, map relevant analysis. In this example, the error matrix pre-sented as sample counts is provided in Table 4. Upon inspection of
Table 4, our attentionshould immediately be drawn to the fact that theclass percentages represented by the row totals are all equal, raising a
concern regarding the map relevant criterion. Is this mapped region a
highlyunusualcaseinwhichallclassesareequallycommon,oristhere
an alternate explanation? Given that there are four classes and all row
totals are 25%, the most likely explanation is that the sampling design
usedto collectthe reference datawasstratified bythe map classeswith
an equal sample size specified for each class. The accuracy assessment
documentation, as per the reproducibility criterion, should be explicit
regarding thesedetailsof thesamplingdesign toavoidthepotentialfor
mis-interpretation of the matrix or mis-calculation of accuracy mea-
sures. Assuming the sampling design was stratified with equal alloca-
tion, a simplediagnosticevaluation ofwhetherthe analysisof theerror
matrix was conducted appropriately can be obtained by examining the
overall accuracy reported from the error matrix (Table 4). If overall
accuracy is reported as 75%, based on summing the diagonal and di-
viding by the sample size of n=100, the analysis is likely wrong be-
cause this calculation of overall accuracy ignores the necessary
Table 4
Errormatrix presented intermsof sample counts(n ij).Becausethe totalsample
size is 100, the cell numbers can also be viewed as percent of area. Wkis the
percent of area mapped as class kin the ROI. Naïve producer's accuracy, cal-
culated as (n kk/n+k)*100%, and naïve overall accuracy, calculated as the sum
of the diagonal entries (75%), are the estimates obtained if the stratified esti-mation formulas are not used. User's accuracy computed as n
kk/nk+from the
sample counts is an unbiased estimator.
Map Reference
A B C D n k+User's(%) W k(%)
A 21 3 1 0 25 84 60
B 4 18 0 3 25 72 25
C 0 2 20 3 25 80 10
D 5 2 2 16 25 64 5
n+k 30 25 23 22 100
Naïve Prod(%) 70 72 87 73S.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
13
weighting of different strata (Eq. (13)) when equal allocation of the
sample size to strata is used. Similarly, if producer's accuracy is re-
portedasthediagonalentrydividedbythecorrespondingcolumntotal,
this naïve calculation of producer's accuracy is also likely incorrect
because again the proper stratified estimation formula (Eq. (15)) has
not been used.
The results of the analysis using the stratified estimation formulas
are provided in Table 5. The map relevant criterion specifies that the
row and column proportions should be consistent with expectations of
class composition for the ROI. In the case of stratified sampling or
poststratified estimation, the row totals should match closely the pro-
portion of each class computed from the entire map (i.e., the row totals
inTable 5are consistent with Wkspecified in Table 4). The Table 5
error matrix results based on using the statistically rigorous, consistentestimators may differ substantially from the results of Table 4. Because
user'saccuracyisestimatedonlyfromsampledatawithineachstratum,the unweighted estimates based on the sample counts from Table 4are
correct. However, overall and producer's accuracies are estimated fromsample data combined over all strata so employing the stratified esti-
mationformulas(i.e.,theconsistentestimatorsforthestratifieddesign)
is a necessity for correct estimation. In this example, the correct (i.e.,
consistent) estimator of producer's accuracy yields estimates (Table 5)
thatdifferconsiderablyfromthenaiveestimatesshownin Table4(e.g.,
for class A the correct estimate from Table 5is 91% versus 70% from
Table 4, and for class D the correct estimate is 43% from Table 5
compared to 73% from Table 4). This example demonstrates that ad-
herence to correct methodology is important as naïve estimation basedon the sample counts of Table 4mis-represents the reality that is ex-
pressed by the results in Table 5.
Table 5also illustrates a recommended format for reporting error
matrices.Cellentriesarereportedasproportion(orpercent)ofthetotalarea of the ROI which allows for computing estimates of overall, user's,
and producer's accuracies directly from the error matrix. Reporting
standard errors (SE) is important because an approximate 95% con-
fidence interval can be constructed as the estimated accuracy or
area ± 1.96*SEtoquantifytheuncertainty(precision)oftheestimates
thus providing a key indicator of reliability of the estimates. The row
and column totals provide direct representation of the composition of
the ROI in terms of proportion of area of the map classification and
reference classification allowing easy confirmation of the map relevant
criterion. Further, map bias (Eq. (5)) for each class is readily estimated
as the difference between the corresponding map and column propor-
tions (or percents). Row and column sample sizes, which strongly im-
pactthestandarderrors,areprovidedtoindicatethenumberofsample
observations that were used for estimating user's and producer's ac-
curaciesaswellasproportionofarea.Ifareaestimationisanobjective,
the standard error for the estimated proportion of area of each classbased on the reference classification should also be included (as it is in
Table5).Becausetheproportionofareabasedonthemapclassification
is known from the map and does not have to be estimated from the
reference sample, it is unnecessary to include a standard error in as-
sociation with the map proportion of area (row total).
4.5. Land-cover change
The analysis for change accuracy assessment has much in common
with single date map accuracy assessment as an error matrix and as-
sociated accuracy measures are still applicable (Congalton and Green,
1999). A starting point for describing accuracy of change is an error
matrixforthebinarychange/nochangeclassification.Ofcourse,afull
transition error matrix (Van Oort, 2007) that displays all class-specific
transitions would provide more complete information. However, for c
land-cover classes the error matrix of all possible transition and steadystate types is c
2xc2, so a full transition error matrix may be too un-
wieldy to be broadly useful in practice. To reduce the volume of resultsreported, user's and producer's accuracies could be limited to more
general transitions such as “any class to urban” (i.e., urban gain) and
“croplandtoanyclass”(i.e.,croplandloss)(e.g., Wickhametal.,2017).
Additionalaccuracymetricsmaybe desirabletoassessthe accuracy
of land-cover change trajectories over multiple dates. The approach in
thepreviousparagraphcouldbeusedforchangebetweenanytwodates
within the time series, but such analyses would be less effective for
evaluating a long time series trajectory. Cohen et al. (2010) developed
several accuracy measures applicable to a time series trajectory in
which segments partitioned the time series into periods of forest sta-
bility, disturbance, and recovery. These measures were based on the
number of segments in the trajectory, the labels of the segments, and
match scores representing the proportion of the trajectory in which the
reference and map condition agreed. Pontius Jr. et al. (2017) proposed
several ways to summarize change information that could be used in
accuracy assessment: 1) number of incidents, defined as the number of
times a pixel experiences a change across all time intervals; 2) number
of states, defined as the number of different categories that the pixel
represents over all time points; and 3) flow matrices which express
transitions of one category to a different category between two time
points. These change accuracy metrics could be obtained from the re-
ferencedataforeachpixelinthereferencesampleandcomparedtothe
corresponding map change information to provide the data for esti-
mating accuracy metrics. Simplicity and ease of interpretation are still
guiding principles for choosing how to characterize accuracy of change
products. Quantifying accuracy of multi-date land-cover products re-
mains a challenge and a subject of ongoing research (Wulder et al.,
2018,Section 4.2).
4.6. Use of accuracy data to compare maps
Land cover products obtained from remote sensing are often com-
pared to evaluate different mapping methods (Khatami et al., 2016).
For example, a series of classifiers may be applied to a single remotely
sensed dataset to determine which yields the most accurate classifica-
tion, or different approaches to classifier training may be used with a
single classifier to yield recommendations of protocols for the design of
the training stage of a particular classifier. In such studies, the relative
accuracy of the maps obtained is used to indicate the quality of the
mapping method. Such analyses are typically undertaken on the as-
sumption that the maps being compared are perfectly co-registered
spatially and that the same set of classes are used. Although seemingly
straightforward the comparison must adhere to protocols that ensure a
rigorous and credible comparison can be achieved. Further, map ac-
curacy comparisons may be motivated by different objectives, for ex-
ample choosing a classifier to use for a particular application or eval-
uating classification procedures under more general conditions of
application. These objectives may require different study designs andTable 5
Estimated errormatrix basedon sample data in Table4and assumingstratified
random sampling with equal allocation. Cell entries represent percent of area.
Standard errors are presented in parentheses for the user's and producer's ac-
curacy estimates. Estimated overall accuracy is 79.6% with a standard error of
5.1% which would yield a 95% confidence interval of 69.5% to 89.7%.
Map Reference Total% User% (SE) n
A B C D
A 50.4 7.2 2.4 0.0 60.0 84.0 (7.5) 25
B 4.0 18.0 0.0 3.0 25.0 72.0 (9.2) 25
C 0.0 0.8 8.0 1.2 10.0 80.0 (8.2) 25
D 1.0 0.4 0.4 3.2 5.0 64.0 (9.8) 25
Total% (SE) 55.4
(4.9)26.4
(4.6)10.8
(2.6)7.4
(1.9)
Prod% (SE) 91.0
(3.2)68.2
(10.8)74.1
(16.7)43.2
(11.1)
n 30 25 23 22S.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
14
different inference frameworks (Stehman, 2006). In particular, if the
results of the comparison are intended to be broadly applicable to a
range of conditions, more than a single test site needs to be evaluated.
Because map accuracy is typically estimated from a sample, it is
inappropriate to simply compare the estimated accuracy (e.g., overall
accuracy)directly.Instead,thecomparisonshouldtakeintoaccountthe
variancesoftheestimateswhichmaybeachievedbyatestbasedonuse
of the standard z score (Foody, 2009b). In essence this is essentially
ensuring that the confidence interval fitted to each estimate is con-
sidered.Thisisimportantbecausetheconfidenceintervalsfittedtotwo
dissimilar accuracy estimates may overlap indicating that, at the re-
levant level of confidence, the accuracy parameters (i.e., population
values) do not differ statistically from each other. The width of the
confidence intervals can therefore be important and the analyst has
someinfluenceoverthisasthewidthisinverselyrelatedtosamplesize.
The latter should be selected with care as it is quite possible to obtain
an exceptionally large sample but find that trivial non-meaningful dif-
ferences appear statistically significant (Fleiss et al., 2003; Foody,
2009b). Conversely, the use of a sample that is too small may result inconfidence intervals that are so wide that finding a statistically sig-
nificant difference is unlikely due to lack of statistical power. Fleiss
et al. (2003) andFoody (2009c) provide guidance on choosing the
sample size in relation to the aim of the study.
Afurtherconsiderationinthecomparisonofaccuracyvaluesrelates
to the data used in their derivation. Considering a basic pairwise
comparison, a key issue is whether the samples used to generate the
accuracy assessments are independent or related. Independent samples
are often assumed and standard equations suitable for such situations
are presented (Congalton and Green, 1999). However, it is common in
remote sensing studies to use a single reference dataset in the assess-
ment of each classification. For such related or “paired” samples the
equations need to be modified to account for covariance (Stehman,
1997a)oranalternativemethodsuchasaMcNemartestshouldbeused(Foody, 2004).
In relation to the aim of the comparison, the test to be undertaken
oftenhasadirectionalcomponent.Thus,ratherthansimplylookingfor
a difference in accuracy the a priori objective might be to determine if
the accuracy of one map is greater than another. In such circumstances
a test of non-inferiority or superiority should be undertaken (Fleiss
etal.,2003; Foody,2009b).Inadditionsometimesthedesireistoshow
that two maps are of equivalent accuracy. This should not be assessed
by showing no difference but by showing equivalence. These various
tests, whether for difference, non-inferiority, superiority or equiva-
lence, can all be undertaken on the basis of the confidence intervals
obtained for the classifications to be compared (Fleiss et al., 2003;
Foody, 2009b).
4.7. Spatial description of accuracy
The error matrix analyses described in Section 4.1 provide only a
global evaluation of thematic map accuracy but do not indicate how
accuracy may vary spatially across the mapped region (McGwire and
Fisher, 2001). A guide to the spatial variation in map quality can be
generated from some classifiers by mapping the uncertainty of class
allocation. With a standard maximum likelihood classification, for ex-
ample, it is possible to map the probabilities of class membership,
likelihood and typicality, on a per-case basis to indicate the spatial
pattern of class allocation uncertainty in addition to the most likely
classlabelforeachpixel(Foodyetal.,1992; Steeleetal.,1998).Similar
approaches to representing the uncertainty of allocation can be made
from fuzzy classifiers and machine leaning approaches. While re-
presentations of the uncertainty of class allocation do help convey
useful information on classification quality they do not necessarily re-
flect accuracy; a classifier may, for example, confidently mis-classify
cases.
Several approaches to reveal spatial patterns of accuracy have beendeveloped. Accuracy could, for example, be assessed for sub-regions of
themap(Foody,2005).Forthisapproachtobeviable,however,alarge
reference data set would typically be required. Spatial modelling
techniqueshavebeenappliedtodepictaccuracyspatially.Forexample,
Kyriakidis and Dungan (2001) used a geostatistical, stochastic simula-
tion approach and Park et al. (2016) employed indicator kriging.
Comber et al. (2012) andComber et al. (2013) applied geographically
weightedlogisticregressiontoestimatelocalaccuracy,and Tsutsumida
and Comber (2015) extended this approach to include the temporal
dimension of accuracy. Khatami et al. (2017) developed a method for
producing maps of accuracy based on the spectral feature space. For
large-area land-cover products, these methods that exploit spatial cor-
relation to predict accuracy at unsampled locations may be challenged
by the sparse density of the reference sample data that is common in
practice. Further, the computing time for some spatial prediction
methods may be excessive when working with a large area such as a
national or global map.
4.8. Kappa
The kappa coefficient expressed as a population parameter is
== + +
= + +O p p
p p 1ic
i i
ic
i i1
1
(24)
DespitePontius Jr. and Millones (2011) provocative “Death to Kappa”
proclamation, a cursory review of recent published accuracy assess-
ments reveals that reports of kappa's demise are premature. Kappa is a
goodillustrationoftheprincipleofprimacyinwhichanideaormethod
introduced in the early stages of a developing field will persist despite
evidence contraindicating its use. Although the notion of correcting for
chance agreement has some intuitive appeal, the criterion of map re-
levance raises the question of how chance agreement is related to the
reality of the map. For example, it is not possible to identify actual
pixels classified correctlyby random chance because chance agreement
is a model construct and different models yield different chance
agreement.Kappaalsofailsthemaprelevantcriterionbecauseasnoted
byYe et al. (2018), a random classification is usually not a realistic
alternative method to create a map.
Liu et al. (2007) showed that kappa was highly correlated with
overall accuracy, which is evident from Eq. (24), so reporting both
measures is redundant. In most cases, chance agreement is incon-
sistently applied as kappa is typically reported as an overall value
(analogous to overall accuracy) but chance-corrected agreement is
rarely reported at the individual class level via conditional kappa (i.e.,
chance-corrected analogs to user's and producer's accuracies). Pontius
Jr. and Millones (2011) stated that use of kappa rarely if ever has
changed the interpretation or conclusion of an accuracy assessment. In
practice, kappa seems to be reported more out of a sense of obligation
than to provide enlightenment of map accuracy. Thus kappa is to ac-
curacy assessment what the appendix is to the human body – it may
cause no serious harm if you have it and pay little attention to it, but it
does not fulfil a necessary function.
5. Statistical inference and accuracy assessment
Statistical inference is the process of generalizing from sample data
to produce estimates of population parameters. Key elements of in-
ference include how variability and bias of estimators are defined.
Design-based inference has traditionally been the inference framework
invoked in accuracy assessment (Stehman, 2000, 2001). In design-
based inference, uncertainty is attributed to the randomization present
in the sample selection, whereas the observations obtained on each
sample unit are regarded as fixed constants, not random variables.
Because probability sampling is a necessity when using design-based
inference,itisconsequentlyakeyelementcontributingtothestatisticalS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
15
rigorofaccuracyassessment(Stehman,1995; StehmanandCzaplewski,
1998). The emphasis on probability sampling as a good practice re-
commendation (Strahler et al., 2006; Olofsson et al., 2014) is an im-
portant development in the evolution of accuracy assessment methods.
As a matter of terminology, “design-based” refers to a method of
inference, not a method of sampling or estimation. That is, there is nosuch thing as a “design-based sample” or a “design-based estimator.
Although a probability sample is required to implement design-based
inference, data from a probability sample could be usedin any modeof
inference, so “design-based sampling”should notbe used as a synonym
for probability sampling. Similarly estimators such as the sample pro-
portionandthestratifiedestimatorsofEqs. (12)–(15) arenotinherently
design-based but the manner in which bias and variance of these esti-mators is defined determines whether it is being used in a design-based
or other inference setting.
An assumption of design-based inference is that measurement error
is negligible, which in the accuracy assessment setting translates to
these reference data being correct. Therefore, design-based inference
doesnotdirectlyaccommodatethelikelyrealitythatsomesampleunits
are assigned an incorrect reference label or that variability in the as-
signment of reference labels likely exists (e.g., inconsistency among
multiple interpreters). Measurement error models (Särndal et al., 1992,
Chapter 16) are needed to accommodate these features of reference
data uncertainty in the estimation methods and quantification of
variability.
Two other approaches to inference are model-based (Valliant et al.,
2000) and Bayesian (Green and Strawderman, 1994; Denham et al.,
2009;Magnussen, 2009). As indicated by its name, model-based in-
ference requires positing a model that treats the observation recorded
for a sampling unit as a random variable. For example, the observation
that a pixel is correctly classified would take the form of a model pre-
dicting whether the observation was y=1 if correctly classified and
y=0 otherwise. The model mustspecify a form for thevariance of this
randomvariableandmayalsospecifyafunctionalrelationshipbetween
the target variable of interest for estimation and other auxiliary vari-
ables observable on the sampling unit. One of the challenges of model-
based inference for accuracy assessment is that a model would need to
be specified for each accuracy estimate produced. Further, the model
assumptions would need to be verified as plausible. An advantage of
model-based inferenceisthat itdoesnotrequirea probabilitysampling
design,althoughitissuggestedthatprobabilitysamplingshouldstillbe
employed to convey the advantage of objectivity (Valliant et al., 2000,
p. 19). Model-based inference offers a viable option for using reference
data that were not obtained via a probability sampling design. Steele
et al. (2003) provide an example of model-based inference applied to
accuracy assessment and McRoberts (2006) applied model-based in-
ference to area estimation. Magnussen (2015) provides an informative
quantitative assessment comparing the variance of estimators derived
from model-based and design-based inference.
Bayesian inference is based on a probability distribution (called the
posterior distribution) for the parameter of interest where this dis-
tribution is conditional on the sample data observed (Denham et al.,
2009). The Bayesian perspective that a parameter is characterized by a
probability distribution is in contrast to the design-based approach in
which a parameter is viewed as a fixed, albeit unknown constant
characterizing a population. The Bayesian approach allows in-
corporating prior knowledge into the assessment, as for example a
completed accuracy assessment of a neighboring area (Denham et al.,
2009). This prior information may enhance the precision of the accu-
racy estimates. Denham et al. (2009) state, “This approach [Bayesian]
may appear to involve more work than a standard analysis, but any
analysis, Bayesian or frequentist, should involve a similar amount of
effort in testing assumptions and interpreting the results.” In reality,
one of the main advantages of design-based inference is the absence of
assumptions and so it requires virtually no effort to verify assumptions.
The primary assumption of design-based inference is that the referencedata are correct, but Bayesian inference, as well as model-based in-
ference,wouldsimilarlyneedtomakesomeaccommodationtoaccount
for error and variability in the reference class labels.
All three inference options, design-based, model-based, and
Bayesian offer a statistically rigorous basis for inference. Design-based
inference requires the fewest assumptions, but is dependent on prob-
ability sampling. Model-based inference can be applied when a prob-
ability sample has not been implemented or when the sample size is so
small that design-based inference lacks adequate precision (e.g., accu-
racy estimates for small sub-areas). Bayesian inference offers an alter-
native perspective of how parameters are viewed (i.e., the posterior
distribution) and the potential advantage to use prior information to
enhance precision.
6. Imperfect reference data
In the context of this article, accuracy assessment is the process of
determining the quality of the mapped representation of land cover
obtained via remote sensing. In essence, an accuracy assessment is
based on the assessment of error, the misclassifications where the class
labelinthemappedrepresentationdiffersfromthatobservedinreality.
Strictly, gold-standard, error-free, reference data depicting the ground
condition perfectly are required to undertake an accuracy assessment.
In reality, the reference data set is just another classification and may
contain error. In recognition of this situation many have called for the
referencedatasettobeofahigherqualitythanthemapitisbeingused
to assess (Olofsson et al., 2014). This may be a pragmatic approach to
adopt in an accuracy assessment but it does not exonerate us from
havingtoconsiderthatthereferenceclassificationmaybeimperfect.In
the site-specific approach to accuracy assessment that is used widely,
disagreements between the map and reference class labelling are taken
to indicate an error in the map obtained via remote sensing. However,
the mapped label could actually be correct and the error lies in the
reference label, just one way in which accuracy assessments in remote
sensing can be viewed as being harsh (Foody, 2008).
Determining the reference condition is a challenging aspect of ac-
curacy assessment. Imperfections in reference data may include: 1)
reference data error (i.e., an incorrect reference class label); 2) re-
ference class ambiguity in which a single class label does not ade-
quately characterize the reference condition of the pixel; and 3) in-
consistent reference class labels (i.e., different interpreters assign a
different reference class to the same pixel). These aspects of imperfect
reference data are discussed in the following three subsections.
CongaltonandGreen(1993,1999,Chapter4), Powelletal.(2004),and
Defourny et al. (2012) provide additional discussion of specific sources
of uncertainty associated with reference data.
6.1. Reference data error
Several studies have explored the effect of reference data error on
classification and estimation (e.g., Verbyla and Boles, 2000; Carlotto,
2009;Foody, 2010; McRoberts et al., 2018). In the conventional ap-
proach to accuracy assessment that is stressed in this article, a major
concern is that, even if small, reference data error may lead to sub-
stantial biases in accuracy and area estimators, for example, deflating
the apparent accuracy for a class and leading to over-estimation of its
areal extent (Foody, 2009a). Taking one simple scenario presented in
Foody (2013) for a binary classification in which both the map being
evaluated and the reference data are very accurate, with producer'saccuracy values of 90% and 95% respectively, the effect of the re-
ference error was to overestimate class abundance (area) by nearly a
factor of 6. The magnitude of the bias varies with the accuracies of the
data sets and the abundance of the classes. For example, Foody (2013)
provided an example in which the areal extent of a rare class was
overestimated by a factor of nearly 40 because of reference data error
even when accuracy was well within the range of results typicallyS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
16
reported in the literature. Reference data error also has the undesirable
effectofintroducingaprevalencedependencyintoaccuracyassessment
based on measures that are (when using a gold standard reference)
prevalent-independent (Foody, 2010).
Given that a binary error matrix is often used in studies of land
cover change and that the class of interest, change, is often rare, the
mis-estimation may be so large that the effect of reference data error
shouldnotbeignored(Foody,2013),especiallyastherearemethodsto
address the issue. For example, if the accuracy of the reference dataset
is known it is possible to compute the real accuracy of the map rather
than the apparent accuracy suggested by naïve interpretation of the
error matrix (Staquet et al., 1981; Foody, 2010). If the reference class
accuracy is not well known but there are a set of labels for each case,perhaps arising from a series of classifications of the same region or
multipleinterpreters,alatentclassanalysismaybeusedtoestimatethe
actual accuracy of the maps (Foody, 2012). Latent class analysis is an
example of a model-based approach to accuracy assessment that may
have considerable potential as an alternative to the standard design-
based approaches when reference data error is problematic.
6.2. Reference class ambiguity
Accommodating ambiguity in the reference class of a sample unit
has justifiably received a great deal of attention. As a minimum, the
percent of ambiguous sample pixels should be documented and it
should be stated whether such pixels have been included in the esti-
mation of the error matrix and accuracy measures. The recommended
approach is that ambiguous pixels should be included in the analysis
withaccommodationstoaddresstheirpotentialimpact.Insomestudies
ambiguity of reference class labels is addressed by having the inter-
preters rate their confidence in each interpretation. Accuracy results
can then be produced for sample subsets defined by these confidence
ratingstoassessifaccuracydiffersbyconfidencerating.Forexample,if
low confidence is associated with greater reference data ambiguity,
accuracylikely willdecrease asconfidencedeclines. Anotheroptionfor
accommodating reference class ambiguity is to assign a primary and an
alternate reference label, where the alternate label is included when a
single label does not suffice to characterize the sample unit. Agreement
can then be defined as a match between the map class and either the
primary or alternate reference label (e.g., Wickham et al., 2004;
Olofsson et al., 2014).
Theuseofprimaryandalternatelabelsiscloselyrelatedtothemore
elegantly formulated linguistic scale fuzzy reference labelling protocoldeveloped by Gopal and Woodcock (1994) in which the information
recorded for each land cover class of a sample unit would be: 1=ab-
solutely wrong, 2=understandable but wrong, 3=reasonable or ac-
ceptable answer, 4=good answer, and 5=absolutely right. Based on
thislinguisticscalereferencedata,twofuzzyaccuracymeasuresarethe
MAX operator in which the map is considered correct if the map label
matches the reference class with the greatest value on the linguistic
scale,andtheRIGHToperatorinwhichthemapisconsideredcorrectif
the map class matches a reference class that would be deemed an ac-
ceptableansweronthelinguisticscale(e.g.,3,4,or5).Accuracyresults
based on defining agreement using the primary and alternate reference
labels will closely approximate the results of the MAX and RIGHT op-
erators (Stehman et al., 2003). That is, agreement based on only the
primary reference label will mimic the MAX operator and agreement
with either the primary or alternate reference label will approximate
fuzzy accuracy based on the RIGHT operator.
Several other extensions have been developed to address the pro-
blem of reference class ambiguity. For example, Sarmento et al. (2013)
used fuzzy intervals to account for uncertainty in linguistic scale re-
ference data. Hagen (2003) andHagen-Zanker (2006) developed
methods that take into consideration spatial uncertainty of the re-
ference data in addition to reference class ambiguity. When the re-
ferencedataforeachsampledpixelisrecordedastheproportionofareaofeachclass,severaloptionsexistforquantifyingagreement,includingmethods that could be applied to a soft-classified map (Finn, 1993;
Foody, 1996; Lewis and Brown, 2001; Latifovic and Olthof, 2004;
Pontius Jr. and Cheuk, 2006; Pontius Jr. and Connors, 2009). A po-
tentially challenging aspect of some ofthese approaches is that sample-
based estimation formulas have only been provided for equal prob-
ability sampling, and even for such designs formulas for estimating
standard errors may not have been published.
6.3. Inconsistent reference class labelling
Whenthereferenceclassificationisobtainedbyhumaninterpreters,
it is highly likely that the interpreters will disagree on some proportion
of the sample units (Powell et al., 2004). Interpreter variability may
impact the analysis in several ways. If several interpreters view the
same sample unit, it is common practice to resolve disagreements
among interpreters by having the interpreters reach a consensus deci-
sion or to have an expert interpreter make a final decision on the re-
ference class. In addition to the well recognized issue of how to resolve
these interpreter inconsistencies (Section 3), there is also the impact of
variability among interpreters on the standard errors of the accuracy
andareaestimates. McRobertsetal.(2018) demonstratedthatstandard
errors of area estimates are inflated by interpreter variability, and in-corporatingthisaddeduncertaintyintostandarderrorestimatesmaybe
necessary for a valid assessment of reliability. Greater attention to and
quantification of interpreter consistency may motivate new analysis
methods to address this longstanding challenge of quality of reference
data.
7. Future needs and directions
This article has focused on basic methods for conducting a rigorous
and credible evaluation of map quality. There is considerable scope for
additionalanalyses,torevealaricherandmoreinformativeassessment
ofaccuracy. Thereare manypossibleways inwhich thebasic approach
to accuracy assessment can be usefully extended. We discuss several
such possibilities in the following subsections.
7.1. Technology and reference data
Recent technological developments may provide a means to help
acquire reference data to facilitate accuracy assessments. Three tech-
nological developments in particular have revolutionised the genera-
tion of reference data: citizen sensing, unmanned aerial vehicles, and
resources such as Google Earth that distribute free high resolution
imagery. Citizen science has a long history and is described by a range
of terms such as crowdsourcing and VGI (See et al., 2016). But the
proliferation of inexpensive location aware devices and web 2.0 tech-
nology that fosters sharing of information have made it simple for po-
tentially anyone to provide geolocated information on land cover. In
addition, this acquisition of data can be an active process in which ci-
tizens are steered to sites selected (e.g., following a probability sam-
pling design) or a passive one in which data acquired by citizens are
minedforreferencedata.Theformerapproachhasbeenusedtoacquire
reference data in a range of studies, especially using collaborative in-
ternet projects (Fritz et al., 2017; Laso Bayas et al., 2017; Foody et al.,
2018). If the active process does steer the volunteers to a locationsselected by a probability sampling design, these data can be used in
design-based inference. For example, the Degree Confluence Project
(Iwao et al., 2006) provides photographs over a systematic grid which
can be treated as a probability sample. The passive approach has also
been used for acquisition of reference data, often involving visual in-
terpretation of photography uploaded to internet sites (Iwao et al.,
2006;Foody and Boyd, 2013; Antoniou et al., 2016). Typically these
photographs are acquired opportunistically and not from a probabilitysampling design so are not ideally suited to a design-based inferenceS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
17
approach to accuracy assessment. Such data can, however, be usefully
integrated with reference data acquired by a probability sample to
enhance the accuracy assessment (Stehman et al., 2018).
A common concern raised with the use of crowdsourced data such
as that made available by citizen scientists is its quality, clearly an
important issue given the impacts of using imperfect reference data
notedelsewhere(Section6).However,theaccuracyofsuchdatacanbe
evaluated (Foody, 2012) and in some instances once characterized can
beusedtoaidaccurateestimation(Foodyetal.,2015).Theaccuracyof
citizen derived data can also be comparable to if not superior to au-
thoritative data (e.g., Dorn et al., 2015). A variety of strategies to en-
hance the accuracy of labelling by citizens have also been proposed(Foody et al., 2018; Prelec et al., 2017; Navajas et al., 2018).
The development of drones or unmanned aerial vehicles (UAVs),
and especially inexpensive systems capable of carrying a basic sensor
such as a camera, has revolutionised the acquisition of data that could
be used as a reference in accuracy assessment (Pla et al., 2017). Criti-
cally, a UAV based sensor can be deployed to acquire images that may
be interpreted to yield the reference classification. This has many ad-
vantages over ground based data collection. For example, UAVs can be
used to acquire data for sites that may be dangerous to visit on foot
(e.g., swamp), and the ability of UAV-based sensors to rapidly collect
data allows acquisition of large data volumes. Because the location of
deployment is in the control of the pilot the data acquisition can be
steered to sites selected by a probability sampling design if desired.
In many mapping studies the reference data do not arise from field
based work but from analyses of imagery with a much finer spatial
resolution than that used to produce the map being evaluated.
Commonly, for example, Landsat sensor data may be used to provide
referencedataforamapderivedfromcoarseresolutionMODISdata.As
with other data sources concerns with data quality arise, the classifi-
cation of the fine resolution image will contain error and this will im-
pactnegativelyontheaccuracyassessment(Foody,2008).However,inmany studies a very fine spatial resolution image can be an excellent
source of high quality reference data. Such activity was boosted by the
launch of a series of fine spatial resolution systems such as IKONOS in
1999thatprovidedmultispectralimageswitharesolutionof<5mand
further extended by the launch of systems such as Digital Globe's
WorldView systems that provide multispectral images with sub-metre
resolution (Toutin, 2009). Additionally, access to such data was sub-
stantially increased by their use in resources such as Google Earth. The
latter provides global coverage of imagery at a variety of resolutions.
Most critically in the context of this review article it provides fine re-
solution images, typically from systems such as WorldView, across the
world in a manner that is easy to use (Gorelick et al., 2017), and such
resources are widely used to generate reference data to support accu-
racy assessments (Cha and Park, 2007).
Although the temporal and spatial availability of very high resolu-
tion(VHR)imageryisuneven(Lesivetal.,2018),westilladvocatethat
a probability sampling design be implemented for the full region (i.e.,
the sample should not be limited to only those areas for which VHR
imagery is available) so that design-based inference can be invoked.
However, the transparency criterion of the response design will require
clear documentation of which sample pixels had VHR available. Fur-
ther, the analysis may include estimates for the subsets of the sample
for which VHR was and was not available to gain some insight into the
impact of using different reference data sources for different sample
pixels. In the ideal situation the reference classification would be ob-
tained bythesameprotocolforallsamplepixels, butundertheguiding
principle that the best reference data should be used, we would re-
commend using VHR where available even if it means forfeiting uni-
formity in how the reference class labels are obtained. Clearly em-
ploying different sources of reference data for different pixels in the
sample may create inconsistencies both spatially and temporally in the
reference classification, and methods to resolve these inconsistencies
are needed.7.2. Importance of classification errors
In the conventional approach to accuracy assessments all errors are
treated equally. Yet some errors may be more serious than others
(DeFries and Los,1999; Smitset al., 1999), and thus in thesestudies an
accuracyassessmentcouldbeenhancedbyaccountingfordifferencesinerror severity. Some classes may be more (dis)similar to others for a
range of reasons. For example, different classes of forest may be more
similar in terms of their role in the hydrological cycle than artificial
surfaces. In such a situation, misclassifications between forest classes
wouldbelessseriousthanthoseinvolvingaforestclassandanartificial
class.Mayaux et al. (2006, Table IV) formalize this concept using a
matrix of thematic distance that can then be incorporated to adjustaccuracy estimates based on dissimilarity of the classes. Also, some-
times classes are defined by dividing up a continuum (Ahlqvist, 2005)
and confusion between neighboring classes on the continuum is less
severe than confusion of classes located at the end points. If the feature
being mapped is continuous the analysis protocol must be modified
from the standard methods used to assess categorical, discrete features.
Fortunately, methods to assess accuracy for classifications that range
from those with variable degrees of error severity (Woodcock and
Gopal, 2000) through to continuous classes exist (Foody, 1996;
Riemann et al., 2010) but may benefit from additional research to fa-
cilitate issues such as rigorous comparison. This would be valuable in
some areas of topical interest such as the study of land cover changes
that are less severe than transformations of cover, for example mod-
ifications such as forest thinning that change the character of the land
cover but not its class type.
Itiscommonfortheresultsofanaccuracyassessmenttobereferred
to when a map is used. However, an accuracy assessment can be more
usefulthanjustadescriptivestatementofmapquality,itcanbeusedto
enhance studies that use the map. The impact of different classification
errors will impact the value of a map for use in a particular application(Stehman, 1999b), and identifying the cost or loss function of different
classification errors on estimation of a target value can be used to
compare the expected value of different maps for a given application
(de Bruin et al., 2001). Several recent examples illustrate how the re-
sults of an accuracy assessment of a map can be used to enhance the
analysis and interpretation of land cover studies using that map. Estes
et al. (2017) provided a comprehensive assessment of how errors in
cropland maps could impact downstream analyses of estimation of
carbon stocks, simulation of evapotranspiration, disaggregation of crop
yield and production, and simulation of household dynamics. In addi-
tion to assessing how quantitative estimates were influenced by land
cover map errors, Estes et al. (2017) also noted the importance of
evaluating how map errors impact the ability to locate specific featuresof interest, such as areas of high cropland cover. Tsendbazar et al.
(2016)compared the fitness for use of several global land cover maps
for applications such as general circulation models, agriculture assess-
ments, and biodiversity assessments. Their approach was to produce
weightedaccuracyestimatesbasedonincorporatingsimilaritymatrices
where similarity between classes was determined specific for each ap-
plicationoftheglobalmaps. Foody(2015) illustratesanotherimportant
use of accuracy assessment results by demonstrating how valuations of
ecosystem services derived from land cover class areal extent can differ
substantially when adjustments are made for classification error.
Olofsson et al. (2013) used confidence intervals for the area estimates
obtained from an accuracy assessment to conduct a sensitivity analysis
of a carbon flux model, illustrating another way in which an accuracy
assessment may inform applications of the map.
7.3. Other dimensions of map quality
As well asan increasing array ofdata sources toinform an accuracy
assessment there is scope to look at other map quality dimensions. The
core focus of this article has been on thematic accuracy but otherS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
18
indicators of map quality such as consistency, reliability, trust, and
precisionmaybeconsidered(Fonteetal.,2015).Also,itmaybehelpful
to explore issues of locational accuracy or completeness. The various
aspects of data quality may also gainfully be used together. For ex-
ample, a high locational accuracy is implicitly assumed with standard
site-specificaccuracyassessments,soasmallmis-locationerror,suchas
one pixel error, could substantially degrade a per-pixel accuracy as-
sessment of a region of heterogeneous land cover. Some means to ac-
commodate locational error within the accuracy assessment may pro-
vide a degree of tolerance that makes the approach less harsh (Foody,
2008). For example, accuracy results can be reported for the subset of
homogeneous locations,where homogeneityis defined as allpixels ina
3×3windowcentered onthesamplepixelhavingthesame mapclass.
The difference in accuracy estimates for the homogeneous subset
compared to the full sample provides an indication of the potential
impactofspatialmisregistrationbetweenthemapandreferencedataas
sample pixels within the homogeneous subset should not be greatly
affected by geolocation error. The non-homogeneous subset will gen-
erally have lower accuracy because of the possibility of geolocation
error but also because this subset includes all edge pixels which are
generally more difficult to classify correctly.
7.4. Object-based accuracy assessment
Object-based image analysis (OBIA) has become an increasingly
popular approach in land-cover mapping (e.g., Blaschke et al., 2014).
Ye et al. (2018) provide a comprehensive critique of OBIA map accu-
racy assessment based on their review of 209 articles published be-
tween 2003 and 2017. Assessing the accuracy of OBIA maps introduces
new dimensions to all three component protocols of accuracy assess-
ment because of the variable sizes of the objects mapped (Stehman and
Czaplewski, 1998). For example, depending on how the sample is se-
lected, the inclusion probabilities may be a function of the area of the
objects. That is, if objects are sampled based on whether they are in-
tersectedbypointsofasystematicgrid,theinclusionprobabilitywillbe
proportional to the area of the object, so objects of greater area will
have greater inclusion probabilities. Such a design still yields a prob-
ability sample but accounting for these unequal inclusion probabilities
adds complexity to the analysis. To avoid this unequal inclusion prob-
abilityfeature,alistofallobjectscouldbeconstructedandthesampled
objects selected with equal inclusion probabilities from this list. An
object-based assessment also introduces response design issues such as
choice of assessment unit (map polygon, reference polygon, or pixel),
heterogeneity of land cover within a polygon, and definition of agree-
ment.Ye et al. (2018) state that based on their review of the literature
there was no straightforward way to match mapped polygons with re-ference polygons. Further, when the objects differ in size accuracy
measures will need to take object area into account if an area-based
accuracy assessment is desired. Radoux et al. (2011), Radoux and
Bogaert (2017), Stehman and Wickham (2011) andWhiteside et al.
(2014)addressedvariousaspectsofaccuracyassessmentofOBIAmaps.
Ye et al. (2018) concluded that “… the literature shows no obvious
method to assess the accuracy of OBIA maps in a manner that ap-
preciatesthatOBIAmapsconsistofobjectsofvariousshapesandsizes.”
The good practice recommendations of Radoux and Bogaert (2017)
provideprogress inthisdirection asprotocolscontinue tobedeveloped
and tested in practice.
7.5. Eliminating bad practice
It is hoped that in the future not only will good practices (Olofsson
et al., 2014) be followed but aspects of bad practice eliminated. Fol-
lowing earlier discussion, three examples of bad practice that are
widespread are the often unjustified use of 85% target accuracy, nor-
malisation of the error matrix, and chance correction of agreement.
Whileatargetaccuracyisadesirablefeatureinanaccuracyassessment,the target should be selected for the application at-hand. The 85%target value had a clear and well-justified place in the literature, linked
toAnderson (1971) for large area mapping of broad land cover classes,
butitisnotandshouldnotbeconsidereduniversallyapplicable(Foody,2008). The key accuracy metrics and their targets should be identified
in advance of the analysis and these targets should be application
specific to indicate fitness for purpose. Thus the 85% threshold has no
universalstatusdespitesometimesbeingusedassuch.Normalisationof
the error matrix is touted as an aid to the interpretation of an error
matrix but its use is undesirable not least because it has the effect of
equalizing the producer's and user's accuracy which may be mean-
ingfully different (Stehman, 2004). Last but not least, the community
shouldceasetocorrectforchanceagreement.Thelattermayhavesome
value in assessing the performance of a classifier but it has no place in
the assessment of map accuracy. The source of misclassifications or
correct allocations is not a necessary component of an accuracy as-
sessment. Thus widely used measures such as the kappa coefficient of
agreement should not be used. The latter measures provide only an
indication of accuracy that is highly correlated with overall accuracy,
from which it is calculated, and conveys no useful new information on
map accuracy. Numerous calls for kappa to be removed from the
community's toolbox (Foody, 1992; Stehman, 1997a; Pontius Jr. and
Millones, 2011) need finally to be heeded.
8. Conclusions
Accuracy assessment has a long history and its origins and devel-
opment have paralleled the development of remote sensing methods to
map land cover. The error matrix and associated user's, producer's, and
overallaccuracieshaveremainedcoreelementsofaccuracyassessment.
Having served well in this capacity for nearly half a century, the error
matrixandestimatesofaccuracyandareaderivedfromtheerrormatrix
should continue to be relied upon as standard practice. Accuracy as-
sessment has expanded beyond the error matrix to account for im-
perfect reference data and to provide better spatial representation of
error. The critical role of sampling to obtain the reference data was
recognized early on (Hord and Brooner, 1976; Hay, 1979; Fitzpatrick-
Lins, 1981), and the recognition of design-based inference as the tra-ditional approach for inference in this sampling context formalized the
meaningof“statisticallyrigorous”whenappliedtoaccuracyassessment
(Stehman, 2000). Specifically, statistical rigor can be achieved by im-
plementing a probability sampling design and applying statistically
consistentestimators(i.e.,formulasspecifictothesamplingdesignused
to collect the reference data).
Advancesinthe availabilityandqualityofreference data(e.g., very
high resolution imagery, internet access to Google Earth) have trans-
formed response designs from dependence on hard copy aerial photo-
graphsandexpensivegroundvisitstointensivehumaninterpretationof
multiple data sources at a desktop computer. The development of tools
such as TimeSync (Cohen et al., 2010), Collect Earth (Bey et al., 2016)
and LACO-wiki (See et al., 2017) for convenient and efficient acquisi-
tion of imagery and other sources of reference data for use in the re-
sponse design offers a major advance over the early days of accuracy
assessment. Citizen science has opened up potentially new options for
obtaining reference data that will surely continue to be explored. Re-
sponse design methodology has advanced beyond simply recognizing
that reference data are not “ground truth” to specifying the impacts of
reference data error and variability and having options for accounting
for reference error and for quantifying the contribution of reference
data variability to the total variance.
Advances in remote sensing technology have enhanced the spatial
and temporal scale of issues that can be studied and reinforced theimportanceofrigorousandmaprelevantaccuracyassessments(Wulder
et al., 2018). National, continental, and global land-cover mapping ef-
forts have led to development and implementation of accuracy assess-
ments for these very expansive map coverages. The greater availabilityS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
19
and ease of access to imagery has created the opportunity for mon-
itoring land cover at a much denser temporal frequency. Accordingly,
accuracy assessment methodology must progress to develop methods
capableofassessingtheselongtimeseries,land-coverchangeproducts.
The increasing popularity of object-based mapping methods has
broughtaboutyetotherneedsformethodologicaladvancesinaccuracy
assessment.
Much has been accomplished in the nearly 50years of theory,
methods, and applications of accuracy assessment and area estimation
inthestudyoflandcover.Wecannodoubtexpectadvancestocontinue
as new technology as well as new opportunities and challenges arise in
the study of Earth's land cover. In the meantime, it is critical that
practitioners implement current methodology in a manner that ensures
a statistically rigorous and map relevant accuracy assessment.
Documentation of accuracy assessments must certainly improve to en-
hance transparency and facilitate reproducibility of methodology and
results.Greaterattentiontoqualityassuranceofreferencedatawillalso
contribute to the overall reliability of accuracy assessments.
Practitioners should continue to aspire to resolve the present day
challenges of accuracy assessment as we move into the next half cen-
tury of studying land cover.
Acknowledgments
Funding was provided by United States Geological Survey grant
G12AC20221 and NASA Carbon Monitoring System program grant
NNX13AP48G (S. Stehman). We thank the editor and referees for nu-
merous helpful suggestions to improve the manuscript.
References
Ahlqvist, O., 2005. Using uncertain conceptual spaces to translate between land cover
categories. Int. J. Geogr. Inf. Sci. 19, 831–857.
Anderson, J.R., 1971. Land use classification schemes. Photogramm. Eng. 37, 379–387.
Antoniou, V., Fonte, C.C., See, L., Estima, J., Arsanjani, J.J., Lupia, F., Minghini, M.,
Foody, G., Fritz, S., 2016. Investigating the feasibility of geo-tagged photographs assources of land cover input data. ISPRS International Journal of Geo-Information 5,
64.https://doi.org/10.3390/ijgi5050064.
Anuta, P.E., MacDonald, R.B., 1971. Crop surveys from multiband satellite photography
using digital techniques. Remote Sens. Environ. 2, 53–67.
Arevalo, P., Woodcock, C.E., Olofsson, P., 2019. Continuous monitoring of land change
activities and post-disturbance dynamics from Landsat time series: a test metho-dology for REDD+ reporting. Remote Sens. Environ. https://doi.org/10.1016/j.rse.
2019.01.013.
Aronoff, S., 1982a. Classification accuracy: a user approach. Photogramm. Eng. Remote
Sens. 48, 1299–1307.
Aronoff, S., 1982b. The map accuracy report: a user's view. Photogramm. Eng. Remote
Sens. 48, 1309–1312.
Badjana, H.M., Olofsson, P., Woodcock, C.E., Helmschrot, J., Wala, K., Akpagana, K.,
2017. Mapping and estimating land change between 2001 and 2013 in a hetero-
geneous landscape in West Africa: loss of forestlands and capacity building oppor-
tunities. International Journal of Applied Earth Observation Geoinformation 63,
15–23.
Benedetti,R., Piersimoni, F.,Postiglione, P.,2017.Spatially balancedsampling: Areview
and a reappraisal. Int. Stat. Rev. 85, 439–454.
Bey, A., Diaz, A.S.-P., Maniatis, D., Marchi, G., Mollicone, D., et al., 2016. Collect earth:
landuseandlandcoverassessmentthroughaugmentedvisualinterpretation.RemoteSens. 8 (10), 807.
Bishop, Y.M.M., Fienberg, S.E., Holland, P.W., 1975. Discrete Multivariate Analysis. MIT
Press, Cambridge, MA.
Blaschke, T., Hay, G.J., Kelly, M., Lang, S., Hofmann, P., et al., 2014. Geographic object-
basedimageanalysis–towardsanewparadigm.ISPRSJ.Photogramm.RemoteSens.87, 180–191.
Boschetti, L., Stehman, S.V., Roy, D.P., 2016. A stratified random sampling design in
space and time for regional to global scale burned area product validation. Remote
Sens. Environ. 186, 465–478.
Brus, D.J., Kempen, B., Heuvelink, G.B.M., 2011. Sampling for validation of digital soil
maps. Eur. J. Soil Sci. 62, 394–407.
Bryan, M.L., 1975. Interpretation of an urban scene using multi-channel radar imagery.
Remote Sens. Environ. 4, 49–66.
Card, D.H., 1982. Using known map category marginal frequencies to improve estimates
of thematic map accuracy. Photogramm. Eng. Remote Sens. 48, 431–439.
Carlotto, M.J., 2009. Effect of errors in ground truth on classification accuracy. Int. J.
Remote Sens. 30, 4831–4849.
Castilla, G., 2016. We must all pay more attention to rigor in accuracy assessment.
Remote Sens. 8, 288. https://doi.org/10.3390/rs8040288.Cha, S.Y., Park, C.H., 2007. The utilization of Google Earth images as reference data for
the multitemporal land cover classification with MODIS data of North Korea. Korean
Journal of Remote Sensing 23, 483–491.
Chapin, F.S., Zavaleta, E.S., Eviner, V.T., Naylor, R.L., Vitousek, P.M., et al., 2000.
Consequences of changing biodiversity. Nature 405, 234–242.
Cohen, W.B., Yang, Z., Kennedy, R., 2010. Detecting trends in forest disturbance and
recovery using yearly Landsat time series: 2. TimeSync - tools for calibration and
validation. Remote Sens. Environ. 114, 2911–2924.
Comber, A., Fisher, P., Wadsworth, R., 2005. What is land cover? Environment and
Planning B: Planning and Design 32, 199–209.
Comber, A., Fisher, P., Brunsdon, C., Khmag, A., 2012. Spatial analysis of remote sensing
image classification accuracy. Remote Sens. Environ. 127, 237–246.
Comber,A.,See,L.,Fritz,S.,VanderVelde,M.,Perger,C.,Foody,G.,2013.Usingcontrol
data to determine the reliability of volunteered geographic information about landcover. Int. J. Appl. Earth Obs. Geoinf. 23, 37–48.
Congalton, R.G., 1991. A review of assessing the accuracy of classifications of remotely
sensed data. Remote Sens. Environ. 37, 35–46.
Congalton, R.G., Green, K., 1993. A practical look at the sources of confusion in error
matrix generation. Photogramm. Eng. Remote Sens. 59, 641–644.
Congalton, R.G., Green, K., 1999. Assessing the Accuracy of Remotely Sensed Data:
Principles and Practices. CRC Press, Boca Raton, FL (137 pp).
Congalton, R.G., Oderwald, R.G., Mead, R.A., 1983. Assessing Landsat classification ac-
curacy using discrete multivariate analysis statistical techniques. Photogramm. Eng.
Remote Sens. 49, 1671–1678.
Costanza, R.,d'Arge, R., de Groot, R.,Farber, S., Grasso, M., et al.,1997. Thevalue of the
world's ecosystem services and natural capital. Nature 387, 253–260.
de Bruin, Bregt, A., van de Ven, M., 2001. Assessing fitness for use: the expected value of
spatial data sets. Int. J. Geogr. Inf. Sci. 15, 457–471.
De Gruijter, J.J., Ter Braak, C.J.F., 1990. Model-free estimation from spatial samples: a
reappraisal of classical sampling theory. Math. Geol. 22, 407–415.
Defourny, P., Mayaux, P., Herold, M., Bontemps, S., 2012. Global land-cover map vali-
dation experiences: toward the characterization of uncertainty. In: Giri, C.P. (Ed.),
Remote Sensing of Land Use and Land Cover: Principles and Applications. Taylor &
Francis, Boca Raton, pp. 207–224.
DeFries, R.S., Los, S.O., 1999. Implications of land-cover misclassification for parameter
estimatesingloballand-surfacemodels:anexamplefromthesimplebiospheremodel
(SiB2). Photogramm. Eng. Remote Sens. 65, 1083–1088.
Denham, R., Mengersen, K., Witte, C., 2009. Bayesian analysis of thematic map accuracy
data. Remote Sens. Environ. 113, 371–379.
Dorn, H., Törnros, T., Zipf, A., 2015. Quality evaluation of VGI using authoritative data -
A comparison with land use data in Southern Germany. ISPRS International Journal
of Geo-Information 4, 1657–1671.
Edwards Jr., T.C., Moisen, G.G., Cutler, D.R., 1998. Assessing map accuracy in an ecor-
egion-scale cover-map. Remote Sens. Environ. 63, 73–83.
Estes, L., Chen, P., Debats, S., Evans, T., Ferreira, S., Kuemmerle, T., Ragazzo, G.,
Sheffield, J., Wolf, A., Wood, E., Caylor, K., 2017. A large-area, spatially continuous
assessment of land cover map error and its impact on downstream analyses. Glob.
Chang. Biol. 24, 322–337.
Fattorini, L., Corona, P., Chirici, G., Pagliarella, M.C., 2015. Design-based strategies for
samplingspatialunitsfromregulargridswithapplicationstoforestsurveys,landuse,and land cover estimation. Environmetrics 26, 216–228.
Fichet, L.-V., Sannier, C., Makaga, E.M.K., Seyler, F., 2014. Assessing the accuracy of
forestcovermapfor1990,2000and2010atnationalscaleinGabon.IEEEJournalofSelected Topics in Applied Earth Observations and Remote Sensing 7, 1346–1356.
Finn, J.T., 1993. Use ofthe average mutual informationindex in evaluatingclassification
error and consistency. Int. J. Geogr. Inf. Sci. 7, 349–366.
Fitzpatrick-Lins, K., 1981. Comparison of sampling procedures and data analysis for a
land-use and land-cover map. Photogramm. Eng. Remote Sens. 47, 343–351.
Fleiss, J.L., Levin, B., Paik, M.C., 2003. Statistical Methods for Rates and Proportions, 3
rd
edition. Wiley, New Jersey.
Foley, J.A., DeFries, R., Asner, G.P., Barford, C., Bonan, G., Carpenter, S.R., Chapin, F.S.,
et al., 2005. Global consequences of land use. Science 309, 570–574.
Fonte,C.C.,Bastin,L.,See,L.,Foody,G.,Lupia,F.,2015.UsabilityofVGIforvalidationof
land cover maps. Int. J. Geogr. Inf. Sci. 29, 1269–1291.
Foody, G.M., 1992. On the compensation for chance agreement in image classification
accuracy assessment. Photogramm. Eng. Remote Sens. 58, 1459–1460.
Foody, G.M., 1996. Approaches for the production and evaluation of fuzzy land cover
classifications from remotely-sensed data. Int. J. Remote Sens. 17, 1317–1340.
Foody, G.M., 2002. Status of land cover classification accuracy assessment. Remote Sens.
Environ. 80, 185–201.
Foody, G.M., 2004. Thematic map comparison: evaluating the statistical significance of
differences in classification accuracy. Photogramm. Eng. Remote Sens. 70, 627–633.
Foody, G.M., 2005. Local characterization of thematic classification accuracy through
spatially constrained confusion matrices. Int. J. Remote Sens. 26, 1217–1228.
Foody, G.M., 2006. What is the difference between two maps? A remote senser's view. J.
Geogr. Syst. 8, 119–130.
Foody, G.M., 2008.Harshness inimage classification accuracy assessment. Int.J. Remote
Sens. 29, 3137–3158.
Foody, G.M., 2009a. The impact of imperfect ground reference data on the accuracy of
land cover change estimation. Int. J. Remote Sens. 30, 3275–3281.
Foody, G.M., 2009b. Classification accuracy comparison: hypothesis tests and the use of
confidence intervals in evaluations of difference, equivalence and non-inferiority.
Remote Sens. Environ. 113, 1658–1663.
Foody, G.M., 2009c. Sample size determination for image classification accuracy assess-
ment and comparison. Int. J. Remote Sens. 30, 5273–5291.
Foody, G.M., 2010. Assessing the accuracy of land cover change with imperfect groundS.V. Stehman and G.M. Foody Remote Sensing of Environment 231 (2019) 111199
20
reference data. Remote Sens. Environ. 114, 2271–2285.
Foody, G.M., 2012. Latent class modeling for site-and non-site-specific classification ac-
curacy assessment without ground data. IEEE Trans. Geosci. Remote Sens. 50,
2827–2838.
Foody,G.M.,2013.Groundreferencedataerrorandthemis-estimationoftheareaofland
cover change as a function of its abundance. Remote Sensing Letters 4, 783–792.
Foody, G.M., 2015. Valuing map validation: the need for rigorous land cover map ac-
curacy assessment in economic valuations of ecosystem services. Ecol. Econ. 111,
23–28.
Foody, G.M., Boyd, D.S., 2013. Using volunteered data in land cover map validation:
Mapping West African forests. IEEE Journal of Selected Topics in Applied Earth
Observations and Remote Sensing 6, 1305–1312.
Foody, G.M., Mathur, A., 2006. The use of small training sets containing mixed pixels for
accurate hard image classification: training on mixed spectral responses for classifi-cation by a SVM. Remote Sens. Environ. 103, 179–189.
Foody, G.M., See, L., Fritz, S., Van der Velde, M., Perger, C., Schill, C., Boyd, D.S.,
Comber, A., 2015. Accurate attribute mapping from volunteered geographic in-
formation: issues of volunteer quantity and quality. Cartogr. J. 52, 336–344.
Foody,G.M.,Pal,M.,Rocchini,D.,Garzon-Lopez,C.X.,Bastin,L.,2016.Thesensitivityof
mapping methods to reference data quality: training supervised image classificationswithimperfectreferencedata.ISPRSInternationalJournalofGeo-Information5(11),
199.
Foody, G., See, L., Fritz, S., Moorthy, I., Perger, C., Schill, C., Boyd, D., 2018. Increasing
the accuracy of crowdsourced information on land cover via a voting procedureweighted by information inferred from the contributed data. ISPRS International
Journal of Geo-Information 7 (3), 80.
Friedl, M.A., Woodcock, C., Gopal, S., Muchoney, D., Strahler, A.H., Barker-Schaaf, C.,
2000.Anoteonproceduresused foraccuracyassessmentinlandcover mapsderived
from AVHRR data. Int. J. Remote Sens. 21, 1073–1077.
Fritz, S., McCallum, I., Schill, C., Perger, C., Grillmayer, R., Achard, F., Kraxner, F.,
Obersteiner, M., 2009. Geo-Wiki.Org: the use of crowdsourcing to improve globalland cover. Remote Sens. 1, 345–544.
Fritz, S., See, L., Perger, C., McCallum, I., Schill, C., et al., 2017. A global dataset of
crowdsourced land cover and land use reference data. Scientific Data 4, 170075.
Fuller, R.M., Groom, G.B., Jones, A.R., 1994. The land cover map of Great Britain: an
automated classification of Landsat thematic mapper data. Photogramm. Eng.Remote Sens. 60, 553–562.
Gallego, F.J., 2004. Remote sensing and land cover area estimation. Int. J. Remote Sens.
25, 3019–3047.
Gallego, F.J., 2011. Validation of GIS layers in the EU: getting adapted to available re-
ference data. International Journal of Digital Earth 4 (Supplement 1), 42–57.
Gallego, F.J., 2012. The efficiency of sampling very high resolution images for area es-
timation in the European Union. Int. J. Remote Sens. 33, 1868–1880.
Gallego, J., Bamps, C., 2008. Using CORINE land cover and the point survey LUCAS for
area estimation. Int. J. Appl. Earth Obs. Geoinf. 10, 467–475.
Gallego, F.J., Stibig, H.J., 2013. Area estimation from a sample of satellite images: the
impact of stratification on the clustering efficiency. Int. J. Appl. Earth Obs. Geoinf.22, 139–146.
Gong, P., Wang, J., Yu, L., Zhao, Y., Zhao, Y., Liang, L., et al., 2013. Finer resolution
observation and monitoring of global land cover: first mapping results with LandsatTM and ETM+ data. Int. J. Remote Sens. 34, 2607–2654.
Gopal, S., Woodcock, C., 1994. Theory and methods for accuracy assessment of thematic
maps using fuzzy sets. Photogramm. Eng. Remote Sens. 60, 181–188.
Gordon, S.I., 1980. Utilizing LANDSAT imagery to monitor land-use change: a case study
in Ohio. Remote Sens. Environ. 9, 189–196.
Gorelick,N.,Hancher,M.,Dixon,M.,Ilyushchenko,S.,Thau,D.,Moore,R.,2017.Google
EarthEngine:planetary-scalegeospatialanalysisforeveryone.RemoteSens.Environ.202, 18–27.
Green, E.J., Strawderman, W.E., 1994. Determining accuracy of thematic maps. The
Statistician 43, 77–85.
Hagen, A., 2003. Fuzzy set approach to assessing similarity of categorical maps. Int. J.
Geogr. Inf. Sci. 17, 235–249.
Hagen-Zanker, A., 2006. Map comparison methods that simultaneously address overlap
and structure. J. Geogr. Syst. 8, 165–185.
Hammond, T.O., Verbyla, D.L., 1996. Optimistic bias in classification accuracy assess-
ment. Int. J. Remote Sens. 17, 1261–1266.
Haralick, R.M., Caspall, F., Simonett, D.S., 1969-1970. Using radar imagery for crop
discrimination:Astatisticalandconditionalprobabilitystudy.RemoteSens.Environ.
1, 131–142.
Hay, A.M., 1979. Sampling designs to test land-use map accuracy. Photogramm. Eng.
Remote Sens. 45, 529–533.
Healey, S.P., Cohen, W.B., Yang, Z., Brewer, C.K., Brooks, E.B., Gorelick, et al., 2018.
Mapping forest change using stacked generalization: an ensemble approach. RemoteSens. Environ. 204, 717–728.
Herold, M., Mayaux, P., Woodcock, C.E., Baccini, A., Schmullius, C., 2008. Some chal-
lenges in global land cover mapping: an assessment of agreement and accuracy in
existing 1 km datasets. Remote Sens. Environ. 112, 2538–2556.
Hollmann, R., Merchant, C.J., Saunders, R., Downy, C., Buchwitz, M., et al., 2013. The
ESA climate change initiative: Satellite data records for essential climate variables.
Bull. Am. Meteorol. Soc. 94, 1541–1552.
Hord, R.M., Brooner, W., 1976. Land-use map accuracy criteria. Photogramm. Eng.
Remote Sens. 42, 671–677.
Iwao, K., Nishida, K., Kinoshita, T., Yamagata, Y., 2006. Validating land cover maps with
degree confluence project information. Geophys. Res. Lett. 33, L23404.
Janssen, L.L.F., van der Wel, F.J.M., 1994. Accuracy assessment of satellite derived land-
cover data: a review. Photogramm. Eng. Remote Sens. 60, 419–426.Jensen, J.R., Christensen, E.J., Sharitz, R., 1984. Nontidal wetland mapping in South
Carolina using airborne multispectral scanner data. Remote Sens. Environ. 16, 1–12.
Kempeneers, P., McInerney, D., Sedano, F., Gallego, J., Strobl, P., et al., 2013. Accuracy
assessment of a remote sensing-based, pan-European forest cover map using multi-country national forest inventory data. IEEE Journal of Selected Topics in Applied
Earth Observations and Remote Sensing 6, 54–65.
Khatami, R., Mountrakis, G., Stehman, S.V., 2016. A meta-analysis of remote sensing
research on supervised pixel-based land-cover image classification processes: general
guidelines for practitioners and future research. Remote Sens. Environ. 177, 89–100.
Khatami, R., Mountrakis, G., Stehman, S.V., 2017. Mapping per-pixel predicted accuracy
of classified remote sensing images. Remote Sens. Environ. 191, 156–167.
Kyriakidis, P.C., Dungan, J.L., 2001. A geostatistical approach for mapping thematic
classification accuracy and evaluating the impact of inaccurate spatial data on eco-
logical model predictions. Environ. Ecol. Stat. 8, 311–330.
LasoBayas,J.-C.,See,L.,Fritz,S.,Sturn,T.,Perger,C.,etal.,2017.Crowdsourcingin-situ
data on land cover and land use using gamification and mobile technology. Remote
Sens. 8 (11), 905.
Latifovic, R., Olthof, I., 2004. Accuracy assessment using sub-pixel fractional error ma-
tricesofgloballandcoverproductsderivedfromsatellitedata.RemoteSens.Environ.
90, 153–165.
Lesiv, M., See, L., Laso Bayas, J.C., Sturn, T., Schepaschenko, D., Karner, M., Moorthy, I.,
McCallum, I., Fritz, S., 2018. Characterizing the spatial and temporal availability of
very high resolution satellite imagery in Google Earth and Microsoft Bing Maps as a
source of reference data. Land 7, 118.
Lewis,H.G.,Brown,M.,2001.Ageneralizedconfusionmatrixforassessingareaestimates
from remotely sensed data. Int. J. Remote Sens. 22, 3223–3235.
Linke, J., Fortin, M.-J., Courtenay, S., Cormier, R., 2017. High-resolution global maps of
21
st-centuryannualforestloss:independentaccuracyassessmentandapplicationina
temperate forest region of Atlantic Canada. Remote Sens. Environ. 188, 164–176.
Liu, C., Frazier,P., Kumar, L.,2007.Comparative assessmentof themeasures ofthematic
classification accuracy. Remote Sens. Environ. 107, 606–616.
Loveland, T.R., Reed, B.C., Brown, J.F., Ohlen, D.O., Zhu, Z., Yang, L., Merchant, J.W.,
2000. Development of a global land cover characteristics database and IGBP
DISCover from 1 km AVHRR data. Int. J. Remote Sens. 21, 1303–1330.
Lu, D., Weng, Q., 2007. A survey of image classification methods and techniques for
improving classification performance. Int. J. Remote Sens. 28, 823–870.
Lyons, M.B., Keith, D.A., Phinn, S.R., Mason, T.J., Elith, J., 2018. A comparison of re-
samplingmethodsforremotesensingclassificationandaccuracyassessment.RemoteSens. Environ. 208, 145–153.
Magnussen, S., 2009. A Bayesian approach to classification accuracy inference. Forestry
82, 211–226.
Magnussen,S.,2015.Argumentsforamodel-dependentinference?Forestry88,317–325.
Marsh, S.E., Walsh, J.L., Sobrevila, C., 1994. Evaluation of airborne video data for land-
cover classification accuracy assessment in an isolated Brazilian forest. Remote Sens.
Environ. 48, 61–69.
Mas, J.F., 1999. Monitoring land-cover changes: a comparison of change detection
techniques. Int. J. Remote Sens. 20, 139–152.
Mather, P., Tso, B., 2016. Classification Methods for Remotely Sensed Data. CRC press.
Mayaux, P., Eva, H., Gallego, J., Strahler, A.H., Herold, M., et al., 2006. Validation of the
Global Land Cover 2000 map. IEEE Trans. Geosci. Remote Sens. 44, 1728–1739.
McGwire, K.C., Fisher, P., 2001. Spatially variable thematic accuracy: beyond the con-
fusion matrix. In: Hunsaker, C.T., Goodchild, M.F., Friedl, M.A., Case, T.J. (Eds.),Spatial Uncertainty in Ecology: Implications for Remote Sensing and GISApplications. Springer, New York, pp. 308–329.
McRoberts, R.E., 2006. A model-based approach to estimating forest area. Remote Sens.
Environ. 103, 56–66.
McRoberts,R.E.,2011.Satelliteimage-basedmaps:scientificinferenceorprettypictures?
Remote Sens. Environ. 115, 715–724.
McRoberts, R.E., Walters, B.F., 2012. Statistical inference for remote sensing-based esti-
mates of net deforestation. Remote Sens. Environ. 124, 394–401.
McRoberts, R.E., Stehman, S.V., Liknes, G.C., Næsset, E., Sannier, C., Walters, B.F., 2018.
The effects of imperfect reference data on remote sensing-assisted estimators of landcover class proportions. ISPRS J. Photogramm. Remote Sens. 142, 292–300.
Munafò, M.R., Nosek, B.A., Bishop, D.V.M., Button, K.S., Chambers, C.D., et al., 2017. A
manifesto for reproducible science. Nat. Hum. Behav. 1, 0021.
Navajas, J., Niella, T., Garbulsky, G., Bahrami, B., Sigman, M., 2018. Aggregated
knowledge from a small number of debates outperforms the wisdom of large crowds.Nat. Hum. Behav. 2 (2), 126.
Nusser, S.M., Klaas, E.E., 2003. Survey methods for assessing land cover map accuracy.
Environ. Ecol. Stat. 10, 309–331.
Olofsson,P.,Stehman,S.V.,Woodcock,C.E.,Sulla-Menashe,D.,Sibley,A.M.,etal.,2012.
A global land-cover validation data set, part I: fundamental design principles. Int. J.
Remote Sens. 33, 5768–5788.
Olofsson, P., Foody, G.M., Stehman, S.V., Woodcock, C.E., 2013. Making better use of
accuracy data in land change studies: estimating accuracy and area and quantifying
uncertainty using stratified estimation. Remote Sens. Environ. 129, 122–131.
Olofsson, P., Foody, G.M., Herold, M., Stehman, S.V., Woodcock, C.E., Wulder, M.A.,
2014. Good practices for estimating area and assessing accuracy of land change.Remote Sens. Environ. 148, 42–57.
Overton, W.S., Stehman, S.V., 1995. The Horvitz-Thompson theorem as a unifying per-
spective for probability sampling: with examples from natural resource sampling.
Am. Stat. 49, 261–268.
Padilla,M., Olofsson,P.,Stehman,S.V.,Tansey, K.,Chuvieco,E., 2017.Stratificationand
sample allocation for reference burned area data. Remote Sens. Environ. 203,240–255.
Pal, M., Foody, G.M., 2012. Evaluation of SVM, RVM and SMLR for accurate imageS.V. Stehman and G.M. Foody
Remote Sensing of Environment 231 (2019) 111199
21
classification with limited ground data. IEEE Journal of Selected Topics in Applied
Earth Observations and Remote Sensing 5, 1344–1355.
Park, N.-W., Kyriakidis, P.C., Hong, S.-Y., 2016. Spatial estimation of classification ac-
curacy using indicator kriging with an image-derived ambiguity index. Remote Sens.
8, 320.
Pla, M., Duane, A., Brotons, L., 2017. Potential of UAV images as ground-truth data for
burn severity classification of Landsat imagery: approaches to a useful product for
post-fire management. Revista de Teledetección 49, 91–102.
Pontius Jr., R.G., 2019. Component intensities to relate difference by category with dif-
ference overall. Int. J. Appl. Earth Obs. Geoinf. 77, 94–99.
Pontius Jr., R.G., Cheuk, M.L., 2006. A generalized cross-tabulation matrix to compare
soft-classified maps at multiple spatial resolutions. Int. J. Geogr. Inf. Sci. 20, 1–30.
Pontius Jr., R.G., Connors, J., 2009. Range of categorical associations for comparison of
maps with mixed pixels. Photogramm. Eng. Remote Sens. 75, 696–963.
Pontius Jr., R.G., Millones, M., 2011. Death to kappa: birth of quantity disagreement and
allocationdisagreementforaccuracyassessment.Int.J.RemoteSens.32,4407–4429.
Pontius Jr., R.G., Santacruz, A., 2014. Quantity, exchange, and shift components of dif-
ference in a square contingency table. Int. J. Remote Sens. 35, 7543–7554.
Pontius Jr., R.G., Krithivasan, R., Sauls, L., Yan, Y., Zhang, Y., 2017. Methods to sum-
marize change among land categories across time intervals. J. Land Use Sci. 12 (4),
218–230.
Potapov, P.V., Dempewolf, J., Talero, Y., Hansen, M.C., Stehman, S.V., et al., 2014.
Nationalsatellite-basedhumidtropicalforestchangeassessmentinPeruinsupportof
REDD+ implementation. Environ. Res. Lett. 9 (2014), 124012 (13pp).
Powell, R.L., Matzke, N., de Souza Jr., C., Clark, M., Numata, I., Hess, L.L., Roberts, D.A.,
2004. Sources of error in accuracy assessment of thematic land-cover maps in the
Brazilian Amazon. Remote Sens. Environ. 90, 221–234.
Prelec, D., Seung, H.S., McCoy, J., 2017. A solution to the single-question crowd wisdom
problem. Nature 541 (7638), 532.
Radoux, J., Bogaert, P., 2017. Good practices for object-based accuracy assessment.
Remote Sens. 9, 646.
Radoux,J.,Bogaert,P.,Fasbender,D.,Defourny,P.,2011.Thematicaccuracyassessment
of geographic object-based image classification. Int. J. Geogr. Inf. Sci. 25, 895–911.
Ramankutty, N., Evan, A.T., Monfreda, C., Foley, J.A., 2008. Farming the planet: 1.
Geographic distribution of global agricultural lands in the year 2000. Glob.Biogeochem. Cycles 22, GB1003.
Riemann, R.,Wilson, B.T., Lister, A.,Parks,S., 2010. An effectiveassessment protocol for
continuous geospatial datasets of forest characteristics using USFS Forest inventory
and analysis (FIA) data. Remote Sens. Environ. 114, 2337–2352.
Sarmento, P., Fonte, C.C., Caetano, M., Stehman, S.V., 2013. Incorporating the un-
certainty of linguistic-scale reference data to assess accuracy of land-cover mapsusing fuzzy intervals. Int. J. Remote Sens. 34, 4008–4024.
Särndal, C.E., Swensson, B., Wretman, J., 1992. Model-Assisted Survey Sampling.
Springer-Verlag, New York.
Scepan, J., 1999. Thematic validation of high-resolution global land-cover data sets.
Photogramm. Eng. Remote. Sens. 65, 1051–1060.
See,L.,Mooney,P.,Foody,G.,Bastin,L.,Comber,A.,etal.,2016.Crowdsourcing,citizen
science or volunteered geographic information? The current state of crowdsourced
geographic information. ISPRS International Journal of Geo-information 5 (5), 55.
See, L., Laso Bayas, J.C., Schepaschenko, D., Perger, C., Dresel, C., Maus, V., Salk, C.,
Weichselbaum, J., Lesiv, M., McCallum, I., Moorthy, I., 2017. LACO-wiki: a new
online land cover validation tool demonstrated using GlobeLand30 for Kenya.
Remote Sens. 9 (7), 754.
Singh, A., 1989. Digital change detection techniques using remotely-sensed data. Int. J.
Remote Sens. 10, 989–1003.
Smits, P.C., Dellepiane, S.G., Schowengerdt, R.A., 1999. Quality assessment of image
classification algorithms for land-cover mapping: a review and a proposal for a cost-based approach. Int. J. Remote Sens. 20, 1461–1486.
Song, X.-P., Potapov, P.V., Krylov, A., King, L., Di Bella, C.M., et al., 2017. National-scale
soybean mapping and area estimation in the United States using medium resolution
satellite imagery and field survey. Remote Sens. Environ. 190, 383–395.
Staquet, M., Rozencweig, M., Lee, Y.J., Muggia, F.M., 1981. Methodology for the as-
sessment of new dichotomous diagnostic tests. J. Chronic Dis. 34, 599–610.
Steele, B.M., Winne, J.C., Redmond, R.L., 1998. Estimation and mapping of mis-
classification probabilities for thematic land cover maps. Remote Sens. Environ. 66,
192–202.
Steele, B.M., Patterson, D.A., Redmond, R.L., 2003. Toward estimation of map accuracy
without a probability test sample. Environ. Ecol. Stat. 10, 333–356.
Stehman, S.V., 1995. Thematic map accuracy assessment from the perspective of finite
population sampling. Int. J. Remote Sens. 16, 589–593.
Stehman, S.V., 1997a. Selecting and interpreting measures of thematic classification ac-
curacy. Remote Sens. Environ. 62, 77–89.
Stehman, S.V., 1997b. Estimating standard errors of accuracy assessment statistics under
cluster sampling. Remote Sens. Environ. 60, 258–269.
Stehman, S.V., 1999a. Basic probability sampling designs for thematic map accuracy
assessment. Int. J. Remote Sens. 20, 2423–2441.
Stehman, S.V., 1999b. Comparing thematic map accuracy based on map value. Int. J.
Remote Sens. 20, 2347–2366.
Stehman, S.V., 2000. Practical implications of design-based sampling inference for the-
matic map accuracy assessment. Remote Sens. Environ. 72, 35–45.
Stehman, S.V., 2001. Statistical rigor and practical utility in thematic map accuracy as-
sessment. Photogramm. Eng. Remote Sens. 67, 727–734.
Stehman,S.V.,2004.Acriticalevaluationofthenormalizederrormatrixinmapaccuracy
assessment. Photogramm. Eng. Remote Sens. 70, 743–751.
Stehman, S.V., 2005. Comparing estimators of gross change derived from complete
coverage mapping versus statistical sampling of remotely sensed data. Remote Sens.Environ. 96, 466–474.
Stehman, S.V., 2006. Design, analysis, and inference for studies comparing thematic ac-
curacyofclassifiedremotelysenseddata:aspecialcaseofmapcomparison.J.Geogr.
Syst. 8, 209–226.
Stehman, S.V., 2009a. Sampling designs for accuracy assessment of land cover. Int. J.
Remote Sens. 30, 5243–5272.
Stehman, S.V., 2009b. Model-assisted estimation as a unifying framework for estimating
the area of land cover and land-cover change from remote sensing. Remote Sens.
Environ. 113, 2455–2462.
Stehman, S.V., 2013. Estimating area from an accuracy assessment error matrix. Remote
Sens. Environ. 132, 202–211.
Stehman, S.V., 2014. Estimating area and map accuracy for stratified random sampling
when the strata are different from the map classes. Int. J. Remote Sens. 35,
4923–4939.
Stehman, S.V., Czaplewski, R.L., 1998. Design and analysis for thematic map accuracy
assessment: fundamental principles. Remote Sens. Environ. 64, 331–344.
Stehman, S.V., Selkowitz, D.J., 2010. A spatially stratified, multi-stage cluster sampling
design for assessing accuracy of the Alaska (USA) National Land-Cover Data (NLCD).
Int. J. Remote Sens. 31, 1877–1896.
Stehman, S.V., Wickham, J.D., 2011. Pixels, blocks of pixels, and polygons: choosing a
spatial unit for thematic accuracy assessment. Remote Sens. Environ. 115,3044–3055.
Stehman, S.V., Czaplewski, R.L., Nusser, S.M., Yang, L., Zhu, Z., 2000. Combining accu-
racy assessment of land-cover maps with environmental monitoring programs.Environ. Monit. Assess. 64, 115–126.
Stehman,S.V.,Wickham,J.D.,Smith,J.H.,Yang,L.,2003.Thematicaccuracyofthe1992
National Land-Cover Data (NLCD) for the Eastern United States: statistical metho-
dology and regional results. Remote Sens. Environ. 86, 500–516.
Stehman,S.V.,Olofsson,P.,Woodcock,C.E.,Herold,M.,Friedl,M.A.,2012.Agloballand
cover validation dataset, II: augmenting a stratified sampling design to estimate ac-
curacy by region and land-cover class. Int. J. Remote Sens. 33, 6975–6993.
Stehman, S.V., Fonte, C.C., Foody, G.M., See, L., 2018. Using volunteered geographic
information (VGI) in design-based statistical inference for area estimation and ac-
curacy assessment of land cover. Remote Sens. Environ. 212, 47–59.
Stevens, D.L., Olsen, A.R., 2004. Spatially balanced sampling of natural resources. J. Am.
Stat. Assoc. 99, 262–278.
Strahler, A.H., Boschetti, L., Foody, G.M., Friedl, M.A., Hansen, M.C., et al., 2006. Global
LandCoverValidation:RecommendationsforEvaluationandAccuracyAssessmentof
Global Land Cover Maps, EUR 22156 EN – DG. Office for Official Publications of the
European Communities, Luxembourg (48 pp).
Tchuenté, A.T.K., Roujean, J.L., De Jong, S.M., 2011. Comparison and relative quality
assessment of the GLC2000, GLOBCOVER, MODIS and ECOCLIMAP land cover datasets at the African continental scale. Int. J. Appl. Earth Obs. Geoinf. 13, 207–219.
Toll, D.L., 1985. Effect of Landsat thematic mapper sensor parameters on land cover
classification. Remote Sens. Environ. 17, 129–140.
Toutin, T., 2009. Fine spatial resolution optical sensors. In: Warner, T.A., Nellis, M.D.,
Foody, G.M. (Eds.), The SAGE Handbook of Remote Sensing. Sage, London, pp.
139–150.
Tsendbazar, N.-E., de Bruin, S., Mora, B., Schouten, L., Herold, M., 2016. Comparitive
assessment of thematic accuracy of GLC maps for specific applications using existingreference data. International Journal of Applied Earth Observation and
Geoinfomation 44, 124–135.
Tsendbazar, N.-E., Herold, M., de Bruin, S., Lesiv, M., Fritz, S., et al., 2018. Developing
and applying a muti-purpose land cover validation dataset for Africa. Remote Sens.Environ. 219, 298–309.
Tsutsumida,N.,Comber,A.J.,2015.Measuresofspatio-temporalaccuracyfortimeseries
land cover data. Int. J. Appl. Earth Obs. Geoinf. 41, 46–55.
Turner,B.L., Lambin, E.F., Reenberg, A.,2007. Theemergence of landchange science for
global environmental change and sustainability. Proc. Natl. Acad. Sci. 104,
20666–20671.
Valliant, R., Dorfman, A.H., Royall, R.M., 2000. Finite Population Sampling and
Inference: A Prediction Approach. John Wiley & Sons, Inc, New York.
Van Oort, P.A.J., 2007. Interpreting the change detection error matrix. Remote Sens.
Environ. 108, 1–8.
Verbyla, D.L., Boles, S.H., 2000. Bias in land cover change estimates due to mis-
registration. Int. J. Remote Sens. 21, 3553–3560.
Verbyla, D.L., Hammond, T.O., 1995. Conservative bias in classification accuracy as-
sessment due to pixel-by-pixel comparison of classified images with reference grids.
Int. J. Remote Sens. 16, 581–587.
Vitousek, P.M., Mooney, H.A., Lubchenco, J., Melillo, J.M., 1997. Human domination of
earth's ecosystems. Science 277, 494–499.
Wagner, J.E., Stehman, S.V., 2015. Optimizing sample size allocation to strata for esti-
mating area and map accuracy. Remote Sens. Environ. 168, 126–133.
Waldner, F., Defourny, P., 2017. Where can pixel counting area estimates meet user-
defined accuracy requirements? Int. J. Appl. Earth Obs. Geoinf. 60, 1–10.
Waldner, F., Schucknecht, A., Lesiv, M., Gallego, J., See, L., et al., 2019. Conflation of
expert and crowd reference data to validate global binary thematic maps. Remote
Sens. Environ. 221, 235–246.
Walsh, S.J., 1980. Coniferous tree species mapping using LANDSAT data. Remote Sens.
Environ. 9, 11–26.
Whiteside,T.G.,Maier,S.W.,Boggs,G.S.,2014.Area-basedandlocation-basedvalidation
of classified image objects. Int. J. Appl. Earth Obs. Geoinf. 28, 117–130.
Wickham,J.D.,Stehman,S.V.,Smith,J.H.,Yang,L.,2004.Thematicaccuracyofthe1992
National Land-cover Data for the western United States. Remote Sens. Environ. 91,452–468.
Wickham, J.D., Stehman, S.V., Fry, J.A., Smith, J.H., Homer, C.G., 2010. ThematicS.V. Stehman and G.M. Foody
Remote Sensing of Environment 231 (2019) 111199
22
accuracy of the NLCD 2001 land cover for the conterminous United States. Remote
Sens. Environ. 114, 1286–1296.
Wickham, J.D., Stehman, S.V., Gass, L., Dewitz, J., Fry, J.A., Wade, T.G., 2013. Accuracy
assessment of NLCD 2006 land cover and impervious surface. Remote Sens. Environ.
130, 294–304.
Wickham, J., Stehman,S.V., Gass, L., Dewitz, J.A., Sorenson, D.G., et al., 2017. Thematic
accuracy assessment of the 2011 National Land Cover Database (NLCD). Remote
Sens. Environ. 191, 328–341.
Woodcock, C.E., Gopal, S., 2000. Fuzzy set theory and thematic maps: accuracy assess-
ment and area estimation. Int. J. Geogr. Inf. Sci. 14, 153–172.
Wulder, M.A., Franklin, S.E., White, J.C., Linke, J., Magnussen, S., 2006a. An accuracy
assessment framework for large-area land cover classification products derived from
medium-resolution satellite data. Int. J. Remote Sens. 27, 663–683.
Wulder, M.A., White, J.C., Luther, J.E., Strickland, G., Remmel, T.K., Mitchell, S.W.,
2006b. Use of vector polygons for the accuracy assessment of pixel-based land cover
maps. Can. J. Remote. Sens. 32, 268–279.
Wulder, M.A., Coops, N.C., Roy, D.P., White, J.C., Hermosilla, T., 2018. Land cover 2.0.Int. J. Remote Sens. 39, 4254–4284.
Yang, L., Brus, D.J., Zhu, A.-X., Li, X., Shi, J., 2018. Accounting for access costs in vali-
dation of soil maps: a comparison of design-based sampling strategies. Geoderma
315, 160–169.
Ye, S., Pontius Jr., R.G., Rakshit, R., 2018. A review of accuracy assessment for object-
based image analysis: from per-pixel to per-polygon approaches. ISPRS J.
Photogramm. Remote Sens. 141, 137–147.
Zhao,Y.,Gong,P.,Yu,L.,Hu,L.,Li,X.,etal.,2014.Towardsacommonvalidationsample
set for global land-cover mapping. Int. J. Remote Sens. 35, 4795–4814.
Zhen, Z., Quackenbush, L.J., Stehman, S.V., Zhang, L., 2013. Impact of training and va-
lidation sample selection on classification accuracy and accuracy assessment whenusing reference polygons in object-based classification. Int. J. Remote Sens. 34,
6914–6930.
Zimmerman, P.L., Housman, I.W., Perry, C.H., Chastain, R.A., Webb, J.B., Finco, M.V.,
2013. An accuracy assessment of forest disturbance mapping in the western Great
Lakes. Remote Sens. Environ. 128, 176–185.S.V. Stehman and G.M. Foody
Remote Sensing of Environment 231 (2019) 111199
23
Hi
all,
I
have
a
presentation
on
the
paper
titled
“Land
cover
mapping
at
very
high
resolution
with
rotation
equiv ariant
CNNs:
Towar ds
small
yet
accur ate
models”.
I
plan
on
being
out
of
town
that
day
and
would
like
to
trade
presentations
with
someone.
Please
let
me
know
if
you’re
willing
to
trade
presentations.
The
proposed
paper
presentation
rubric
based
on
our
co-de velopment
session
during
today's
class
is
below .
Bhanu
will
be
implementing
this
as
a
Canv as
survey
to
be
completed
during
each
class
(in
person).
If
you
have
any
suggestions
for
additions/changes
or
questions,
you
can
shar e
them
in
this
thread
befor e
Wednesda y.
1.
what
are
some
challenges
in
SatML
that
might
be
different
or
similar
to
"standar d
ML"?
2.
what
are
some
outstanding
challenges/r esear ch
questions
to
addr ess
(methodologically
and
application-wise)?
3.
wher e/when/on
what
tasks
do
SatML
models
tend
to
under-per form
right
now?
4.
what
topics
are
you
inter ested
in
exploring
for
your
project?
CSE
598:
Machine
Learning
for
Remote
Sensing
Course
information
Meeting
times:
M/W
4:30-5:45pm,
Spring
2024
Location:
BYENG
M1-09
Website:
Canvas
(
https://canvas.asu.edu/courses/176585
)
Course
summary:
In
this
course
students
will
learn
about
the
state
of
the
art
in
machine
learning
research
for
remote
sensing
data
and
study
applications
in
various
domains
including
agriculture,
forestry,
and
coastal
environments.
The
course
will
study
how
remote
sensing
data
differ
from
other
data
modalities
in
machine
learning
(such
as
language
or
images),
key
research
topics
in
machine
learning
for
remote
sensing
(e.g.,
image
classification,
segmentation,
change
detection,
and
self-supervised
techniques),
and
open
challenges
such
as
domain
adaptation,
generalization
to
out-of-distribution
data,
and
deployment.
The
course
will
consist
primarily
of
student
presentations
of
key
research
papers,
collaborative
group
sessions,
and
a
course
project.
Difference
from
other
courses
at
ASU:
This
course
does
not
teach
foundational
concepts
of
machine
learning
and
data
mining,
which
students
are
expected
to
have
learned
from
other
courses
at
SCAI/ASU
prior
to
this
course
(see
Prerequisites).
This
course
focuses
on
how
specialized
machine
learning
techniques
are
developed
to
address
the
unique
characteristics
and
use
cases
of
remote
sensing
data.
This
includes
learning
about
advances
in
model
architectures,
model
evaluation,
training
algorithms,
sampling
techniques,
and
other
considerations.
The
course
will
teach
an
introduction
to
remote
sensing
data
and
data
sources
but
this
is
limited
to
an
understanding
of
the
data
as
a
modality
for
machine
learning
(similar
to
language
or
digital
photography
in
a
natural
language
processing
or
computer
vision
class),
and
does
not
study
remote
sensing
science
in
depth
as
in
remote
sensing
classes
at
ASU
(offered
in
ABS,
GLG,
GPH,
or
SES).
Prerequisites:
Students
must
have
taken
a
course
in
machine
learning
or
data
mining
in
a
prior
semester
(CSE
572,
CSE
475,
CSE
575,
or
equivalent;
minimum
grade
B-)
.
There
are
no
exceptions.
Students
are
not
expected
to
have
any
prerequisite
knowledge
of
remote
sensing.
Instructional
team
Name
Role
Email
Office
location
Office
hours
Dr.
Hannah 
Kerner
Professor 
/Instructor
hkerner@asu.edu
BYENG
570
M/W
3-4pm
Bhanu
Tokas
Grader/TA
btokas@asu.edu
BYENG 
424-AK
M/W
2-3pm
Course
topics
The
course
is
organized
into
five
units.
The
table
below
summarizes
the
key
topics
that
will
be
covered
in
each
unit.
Rows
highlighted
in
red
are
university
holidays;
rows
highlighted
in
blue
correspond
to
course
project
activities;
rows
highlighted
in
orange
are
guest
lectures.
Unless
otherwise
indicated,
all
class
periods
will
consist
primarily
of
student
presentations
and
discussions
of
selected
research
papers.
Class
#
Class
date
Unit
Unit
name
Topic
1
Monday,
January
8
1
Introduction
to
machine
learning
for
remote
sensing
Lecture:
Introduction
to
remote
sensing
(and
ML
for
remote
sensing)
2
Wednesday,
January
10
Overviews/Perspectives
Monday,
January
15
NO
CLASS
-
MLK
DAY
3
Wednesday,
January
17
Overviews/Perspectives
4
Monday,
January
22
Evaluation
5
Wednesday,
January
24
Datasets/Tools
6
Monday,
January
29
Datasets/Tools
7
Wednesday,
January
31
2
Core
methods
Change/anomaly
detection
8
Monday,
February
5
Domain
adaptation
9
Wednesday,
February
7
Transfer
learning
&
meta-learning
10
Monday,
February
12
Guest
lecture:
Jacob
Adler,
planetary
remote
sensing
and
machine
learning
11
Wednesday,
February
14
Guest
lecture:
Caleb
Robinson,
Microsoft
Geospatial
AI
for
Good
12
Monday,
February
19
Pre-proposal
storyboard
and
feedback
(in
class
activity)
13
Wednesday,
February
21
Transfer
learning
&
meta-learning
14
Monday,
February
26
Human
in
the
loop
&
reinforcement
learning
15
Wednesday,
February
28
3
Data-centric
ML
Data-centric
ML
Monday,
March
4
NO
CLASS
-
SPRING
BREAK
Wednesday,
March
6
NO
CLASS
-
SPRING
BREAK
16
Monday,
March
11
Data-centric
ML
17
Wednesday,
March
13
Literature
review
group
presentations
18
Monday,
March
18
4
Representation
learning
and
self-supervised
learning
Representation
learning
19
Wednesday,
March
20
Representation
learning
20
Monday,
March
25
Multi-modal
ML
21
Wednesday,
March
27
Multi-modal
ML
22
Monday,
April
1
Project
progress
updates/feedback
23
Wednesday,
April
3
Foundation
models
24
Monday,
April
8
Foundation
models
25
Wednesday,
April
10
Foundation
models
26
Monday,
April
15
5
Real
world
use
cases
and
considerations
Applications
(Ag,
Forestry,
etc.)
27
Wednesday,
April
17
Applications
(Ag,
Forestry,
etc.)
28
Monday,
April
22
Applications
(Ag,
Forestry,
etc.)
29
Wednesday,
April
24
Fairness/Ethics
Class
routine
The
majority
of
classes
will
consist
of
student
presentations
and
discussions
of
selected
research
papers
(except
those
indicated
with
special
activities
on
the
course
schedule
above).
Each
class
period
will
cover
2-3
research
papers.
 
Before
class:
students
must
read
one
of
the
2-3
research
papers
associated
with
the
lecture
and
submit
a
half-page
summary/critique
of
the
paper.
 
During
class:
Student
presentations
of
research
papers
(15
minutes
per
presentation).
Remainder
of
the
class
period
will
be
spent
on
questions
and
discussions
about
the
papers
presented.
Each
student
will
present
2
papers
over
the
course
of
the
semester.
 
End
of
class:
Each
student
in
class
will
give
a
score
to
each
presentation
during
class.
Scores
will
be
based
on
how
well
a
presentation
meets
a
list
of
criteria
for
a
good
paper
presentation.
(This
list
will
be
co-developed
by
the
class
in
the
first
session.)
Attendance:
In
person
attendance
is
required
.
Absences
will
reflect
on
grades
for
in-class
assignments.
It
is
understandable
that
students
may
need
to
miss
a
class
or
two
during
the
semester
due
to
illnesses
or
other
needs.
Students
are
required
to
contact
the
TA
before
an
assignment
is
due
if
extenuating
circumstances
arise
that
warrant
an
exception.
Assignment
types
and
point
distribution
There
are
1,000
base
points
allocated
according
to
the
following
table.
You
can
multiply
the 
percentages
by
1,000
to
get
the
number
of
points
for
a
given
assignment.
Category
Quantity
Assignment
types
Percent
of
total
grade
In-class
participation
(scorecards)
23
Scorecards
for
paper
presentations
(0.5%
per
class
period/scorecard);
1
free
pass
(lowest
score
dropped,
only
22
graded)
11%
Paper
summaries
23
Half
page
summary
and
critique
of
one
paper
(1%
per
paper/class
period);
1
free
pass
(lowest
score
dropped,
only
22
graded)
22%
Paper
presentations
2
15-minute
paper
presentations
(12%
each)
24%
Labs
2
Graded
labs
submitted
via
Canvas
(6%
each)
12%
Course
project
4
The
course
project
grade
will
be
based
on
the
project
deliv erables
including
project
proposal
(5%),
literature
review
(5%),
ﬁnal
presentation
(10%),
and
ﬁnal
repor t
(10%).
30%
Syllabus
quiz
1
Quiz
about
syllabus
in
Canvas
1%
Total
100%
Description
of
graded
assignment
types
Syllabus
quiz
The
syllabus
quiz
is
open-notes/open-internet
and
will
test
your
understanding
of
details
outlined
in
this
syllabus.
It
will
be
completed
outside
of
class,
individually,
and
via
Canvas.
There
will
be
only
one
attempt
permitted.
In-class
participation
(scorecards)
During
the
first
class
period,
students
and
the
instructor
will
collaboratively
design
a
scorecard
that
will
be
used
to
provide
feedback
about
paper
presentations.
The
scorecard
will
capture
a
list
of
criteria
that
students
agree
are
important
for
a
good
paper
presentation
that
facilitates
the
audience’s
understanding
of
the
paper.
Scorecards
will
be
handed
out
and
completed
during
class,
and
submitted
for
grading
at
the
end
of
class.
Grading
will
be
based
on
completion.
The
following
is
an
example
scorecard
template:
Student
name
Paper
presented
Criteria
1
Criteria
2
Criteria
3
Criteria
4
Jane
Doe
Kerner
et
al., 
2023.
“An 
example
paper 
title.”
ICLR.
✔
✔
-
✔
+
✔
John
Doe
Kerner
et
al., 
2021.
“An 
example
paper 
title.”
NeurIPS.
✔
+
✔
+
✔
✔
-
Paper
summaries
Prior
to
each
class,
each
student
must
read
one
of
the
2-3
papers
associated
with
the
class
lecture
and
submit
a
summary
and
critique
of
the
paper
by
answering
the
quiz
questions
due
before
that
class
period.
Paper
summaries
should
be
concise
and
reflect
your
own
understanding
and
thoughts
on
the
paper
(including
aspects
you
thought
could
be
improved,
i.e.,
your
critique).
You
can
also
record
what
questions
you
might
have
that
you
would
like
to
understand
better
about
the
paper,
which
you
can
consult
during
the
paper
discussion/question
time
in
class.
Remember
that
the
purpose
of
this
class
is
for
you
to
learn
from
these
papers!
Taking
shortcuts
such
as
finding
existing
summaries
or
using
ChatGPT
or
other
LLMs
to
generate
your
summaries
is
not
just
plagiarism,
but
it
is
also
a
waste
of
your
time.
(See
Academic
Integrity
section
for
policy
on
plagiarism
and
use
of
ChatGPT/LLMs.)
Paper
presentations
Paper
presentations
will
be
created
and
delivered
by
one
student
per
paper.
Students
will
be
randomly
assigned
to
papers.
Students
are
permitted
to
swap
papers
with
classmates;
to
request
a
swap,
a
student
should
1)
agree
to
a
swap
with
a
fellow
student,
2)
email
the
TA
with
the
swapping
student
in
cc
at
least
one
week
before
the
presentation
,
3)
the
TA
will
update
the
schedule
to
reflect
the
swap.
Paper
presentations
will
be
graded
using
the
scorecards
from
the
audience
watching
the
presentation.
Each
presentation
will
be
15
minutes
long
followed
by
at
least
5
minutes
of
discussion
time
(more
discussion
time
may
be
allowed
depending
on
the
number
of
papers
in
the
class
period).
Presentations
must
use
the
ASU
powerpoint
or
Google
Slides
template
and
be
uploaded
on
the
Paper
Presentation
assignment
on
Canvas
prior
to
the
start
of
class
the
day
of
the
student’s
paper
presentation.
Labs
There
will
be
two
labs
consisting
of
programming
exercises
related
to
Unit
1
and
Unit
2.
Labs
should
be
completed
individually
outside
of
class
and
submitted
via
Canvas.
Course
project
Course
projects
may
be
completed
as
an
individual
project
(1
student)
or
a
collaborative
project
(2
students).
A
collaborative
project
does
not
mean
that
group
members
split
up
components
of
a
single
project
that
would
otherwise
be
done
by
one
person.
If
you
choose
to
do
a
collaborative
project,
you
must
complete
a
larger
project
that
cannot
be
completed
by
a
single
person
in
the
time
of
the
course
(roughly
2x
what
an
individual
project
would
accomplish).
Your
project
proposal
must
explain
why
a
collaboration
is
needed.
All
projects,
individual
or
collaborative,
will
be
discussed
in
class
with
your
peers
and
the
instructor
before
submitting
the
project
proposal.
Your
goal
in
this
project
is
to
use
what
you’ve
learned
in
class
to
address
an
interesting
research
problem.
This
project
is
meant
to
be
a
substantial
research
effort
involving
a
topic
of
your
choice
that
will
give
you
some
experience
with
doing
original
research
in
machine
learning
for
remote
sensing
and
writing
up
your
results
in
a
conference
paper
format.
You
will
formulate
an
idea/hypothesis
that
you
will
describe
clearly,
relate
to
existing
work,
implement,
and
test
through
experiments
with
dataset(s).
This
will
involve
writing
code,
using
your
code
to
run
experiments
on
a
dataset,
making
figures
to
communicate
your
data/methods/results,
reading
background
papers,
and
writing
a
report
about
your
task,
the
algorithm(s)
you
used,
and
the
results
you
obtained.
Your
research
projects
may
have
a
variety
of
designs,
including
but
not
limited
to:
 
Design
a
new
algorithm
or
modify
an
existing
algorithm
to
improve
performance
on
some
task
or
dataset
(must
compare
to
at
least
one
existing
baseline
algorithm)
 
Empirical
evaluation
of
multiple
algorithms
using
a
new
dataset
or
evaluation
strategy
(e.g.,
benchmarking
or
fairness
evaluation)
 
Translate
an
existing
machine
learning
method
to
a
remote
sensing
task/dataset
that
has
not
been
tried
for
that
problem
before
 
Collect
a
new
dataset
to
address
a
novel
problem
or
new
aspect
of
an
existing
problem
involving
machine
learning
for
remote
sensing,
and
benchmark
existing
methods
 
Make
a
significant
contribution
to
an
open
source
library
(e.g.,
TorchGeo)
to
support
new
datasets,
methods,
or
other
experiments
related
to
machine
learning
and
remote
sensing
 
Conduct
a
survey
or
meta
analysis
of
methods
in
a
particular
research
area
within
machine
learning
for
remote
sensing
that
has
not
yet
(or
sufficiently)
been
surveyed
in
prior
work
Whatever
you
choose
to
work
on,
it
should
be
interesting
(to
you,
society,
research
community,
etc.).
Students
are
encouraged
to
choose
a
project
topic
from
a
list
of
topics
identified
by
stakeholders
in
the
research
community
at
ASU
and
elsewhere,
which
will
be
shared
with
students
after
the
start
of
the
class.
Students
may
also
choose
their
own
project
topic.
In
both
cases,
it
is
the
responsibility
of
the
student
to
understand
and
communicate
the
importance
of
the
chosen
research
topic/questions.
You
should
make
sure
that
your
analysis
is
not
trivial.
An
example
of
a
trivial
project
would
be
choosing
a
dataset
from
Kaggle,
applying
some
classifiers
from
Scikit-learn,
reporting
standard
classification
metrics,
and
summarizing
the
results.
That
said,
you
do
not
want
to
make
your
problem
so
complex
that
you
do
not
have
time
to
finish
it.
Project
deliverables
will
consist
of
a
project
proposal
,
literature
review
,
final
presentation
,
and
final
report
.
A
template
will
be
provided
for
proposals
and
literature
reviews.
Final
reports
will
be
formatted
in
the
style
of
a
machine
learning
conference
workshop
submission
(5
page
limit),
with
the
goal
that
some
student
projects
may
be
submitted
to
a
relevant
conference
workshop.
Final
presentations
will
be
in
the
form
of
poster
presentations
which
will
be
presented
during
an
open
poster
session
during
the
final
exam
period.
Grading
Your
grade
will
be
determined
based
on
the
following
grading
scheme:
Grade
Percentage
Grade
Percentage
A+
100-97%
B-
<84-80%
A
<97-94%
C+
<80-77%
A-
<94-90%
C
<77-70%
B+
<90-87%
D
<70-60%
B
<87-84%
E
<60%
Grade
appeals
Any
grade
appeal
must
happen
within
24
hours
of
the
grade’ s
posting.
Later
appeal
will 
not
be
considered.
Grade
appeals
must
be
sent
to
the
TA
by
Canvas
mail.
If
an 
automatically
graded
assignment
(other
than
labs
or
re-grades
announced
by
the 
instructional
team)
does
not
post
within
24
hours
of
completion,
the
student
should 
assume
the
assignment
was
scored
a
zero
and
should
contact
the
instructional
team 
with
any
discrepancy .
Grade
changes
will
only
be
made
if
there
is
an
error
in
assignment
grading.
No 
grade
changes
will
be
made
based
on
student
requests
for
rounding,
alternate 
interpretations
of
assignments,
or
any
other
reason.
Such
requests
will
be 
considered
invalid
and
will
be
ignored.
Lecture
notes
and
course
materials
All
contents
of
the
course
including
lectures,
notes,
and
assignments
distributed
to
the 
class
are
under
copyright
protection.
They
may
not
be
redistributed,
sold
or 
commercialized
without
the
express
permission
of
the
instructor .
In
other
words,
you 
may
not
post
the
course
materials
on
CourseHero,
share
them
with
other 
students,
or
any
other
re-distribution
of
the
material.
Copyright
policy
Course
content,
including
lectures,
are
copyrighted
materials
and
students
may
not 
share
outside
the
class,
upload
to
online
websites
not
approved
by
the
instructor ,
sell,
or 
distribute
course
content
or
notes
taken
during
the
conduct
of
the
course
(see
ACD 
304–06,
“Commercial
Note
Taking
Services
”
and
ABOR
Policy
5-308
F.14
for
more).
Students
may
not
upload
or
submit
any
material
that
is
not
the
student's
original
work, 
unless
the
students
comply
with
all
applicable
copyright
laws;
faculty
members
reserve 
the
right
to
delete
materials
on
the
grounds
of
suspected
copyright
infringement.
Absences
and
late
or
missed
assignments
Graded
events
for
which
a
student
is
not
present
will
be
scored
as
zero.
Any 
assignment
not
submitted
by
the
due
date/time
will
be
scored
zero.
In
extreme
cases, 
there
may
be
special
approvals
for
make-up
or
make-ahead
requests.
Required
documentation
for
Make-up/Make-ahead
requests
In
general,
requests
for
make-up
and
make-ahead
assignments
will
not
be
approved.
In 
extreme
cases,
students
must
follow
the
below
procedures
for
make-up
or
make-ahead 
requests.
If
the
student
knows
they
will
miss
a
graded
event
at
least
72
hours
prior
to
its 
occurrence,
they
may
be
eligible
for
a
make-ahead
event.
 
Make-ahead
events
must
be
coordinated
with
the
TA
at
least
72
hours 
prior
 
Required
documentation
must
be
submitted
to
the
TA
at
least
72
hours 
prior
 
The
TA
may
work
with
the
student
to
schedule
the
make-ahead
either 
before
or
after
the
graded
event
occurs
 
All
make-aheads
must
be
approved
by
the
instructor
upon
review
of 
documentation
 
Make-aheads
may
be
allowed
for
the
following
scenarios
with
required 
documentation:
 
Excused
absences
related
to
religious
observances/practices
that 
are
in
accord
with
ACD
304–04
,
“Accommodation
for
Religious 
Practices”
 
Excused
absences
related
to
university
sanctioned
events/activities 
that
are
in
accord
with
ACD
304–02
,
“Missed
Classes
Due
to 
University-Sanctioned
Activities”
 
Excused
absences
related
to
missed
class
due
to
military 
line-of-duty
activities
that
are
in
accord
with
ACD
304–1 1
 
Funeral
of
an
immediate
family
member:
If
you
need
to
attend
the 
funeral
of
an
immediate
family
member
(defined
as
grand-parent, 
parent,
spouse,
sibling
or
child),
you
need
the
instructor's
prior 
approval.
Proof
is
required.
In
the
case
of
an
emergency
situation
where
the
student
misses
a
class
due
to
an 
extreme
situation,
there
may
be
a
possibility
of
a
make-up.
 
Make-up
events
must
be
coordinated
with
the
TA
no
later
than
24
hours 
following
the
due-date/time
of
the
event
 
Required
documentation
must
be
submitted
to
the
TA
no
later
than
24 
hours
the
due-date/time
of
the
event
 
All
make-ups
must
be
approved
by
the
instructor
upon
review
of 
documentation
 
Make-ups
may
be
allowed
for
the
following
scenarios
with
documentation:
 
Medical
Problems:
You
need
to
submit
a
statement
with
the 
signature
of
the
doctor
and
the
seal
of
the
hospital
saying
that
you 
were
not
able
to
be
in
class
at
the
specified
time.
 
Travel
Accident:
You
need
to
submit
a
police
report
stating
that
you 
are
involved
in
an
accident
and
show
a
printout
on
“Google
maps” 
to
certify
that
you
could
not
arrive
on-time
for
the
graded
event.
 
Note:
medical
emergencies/conditions
may
qualify
for
special 
considerations
like
late
withdrawal
or
incomplete
grade,
but
do
not 
automatically
warrant
a
make-up.
Note
on
COVID-19
positive
tests.
Students
who
test
positive
for
a
COVID-19
variant 
who
are
not
hospitalized
and
capable
of
working
must
contact
the
TA
team
to
ensure 
they
can
complete
in-class
assignments
from
a
remote,
isolated
location.
Students
who 
test
positive
for
COVID-19
are
not
permitted
to
attend
in-person
sessions.
Note
on
technology
issues.
End-user
technology
issues
are
not
excuses
for
late
or 
missed
assignments.
It
is
the
student’ s
responsibility
to
ensure
all
course
work
can
be 
completed
including
a
back-up
plan
should
your
computer ,
internet
connection,
or
other
technology
fail.
Students
are
recommended
to
backup
all
work
using
Github,
a
USB 
drive,
an
external
drive,
or
a
cloud
service
such
as
Dropbox,
Google
Drive,
or
iCloud.
Electronic
communication
and
email
policy
All
communication
with
the
instructor
and
TA/grader
will
be
through
Canvas.
We
will
not 
respond
to
emails.
Please
use
the
following
order
of
operations
for
contacting
the
instructional
team:
 
Canvas
Discussion
Board:
If
you
have
a
question
that
is
not
of
a 
personal
nature,
please
post
the
question
on
the
Discussion
Board
on 
Canvas.
It’s
likely
that
another
student
has
the
same
question,
so
you
will 
be
helping
them
and
the
instructional
team
by
posting
your
question
here. 
Students
are
encouraged
to
respond
to
the
questions
of
their
classmates. 
Prior
to
posting
a
question
or
comment,
check
the
syllabus, 
announcements,
and
existing
posts
to
ensure
it's
not
redundant.
 
Attend
office
hours:
If
you
have
a
question
that
cannot
be
answered 
through
the
Discussion
Board,
attend
office
hours
hosted
by
the 
instructional
team.
 
Canvas
Mail:
If
your
communication
cannot
be
addressed
through
the 
discussion
board
or
office
hours,
then
you
can
send
a
message
via 
Canvas
to
the
TA.
The
TA
will
directly
answer
your
message,
unless
the 
TA
feels
that
my
direct
assistance
is
needed.
Additional
notes
about
communication:
 
Communication
will
be
reviewed
once
per
day,
Monday
through
Friday 
during
business
hours
(approximately
9am-5pm).
 
All
communication
must
be
directed
to
the
TA.
 
Messages
should
be
clear ,
self-contained,
and
to
the
point.
 
Messages
should
not
ask
questions
whose
answers
are
obviously
shown 
in
the
course
syllabus,
class
notes/class
materials,
or
other
materials
on 
Canvas.
 
Avoid
asking
questions
in
electronic
communications
that
should
be
raised 
either
in
class
or
in
individual
consultation
with
the
TA
during
office
hours. 
These
include
questions
of
an
excessively
conceptual
nature,
and 
questions
that
require
an
unreasonable
amount
of
time
from
the 
instructor/T A.
 
A
good
rule
of
thumb:
if
your
question
cannot
be
answered
in
a
short 
paragraph
(1-3
sentences),
then
it
is
not
appropriate
for
electronic 
communication.
Academic
integrity
Students
in
this
class
must
adhere
to
ASU’ s
academic
integrity
policy ,
which
can
be 
found
at
https://provost.asu.edu/academic-integrity/policy
).
Students
are
responsible 
for
reviewing
this
policy
and
understanding
each
of
the
areas
in
which
academic 
dishonesty
can
occur.
Plagiarism
includes
any
scenario
in
which
you
present
work 
that
is
not
your
own
as
if
it
is
your
own
(without
proper
citation);
this
includes
using 
outputs
generated
by
ChatGPT
as
your
own
work.
In
addition,
all
engineering
students 
are
expected
to
adhere
to
the
ASU
Academic
Integrity
Honor
Code
.
All
academic 
integrity
violations
will
be
reported
to
the
Fulton
Schools
of
Engineering 
Academic
Integrity
Office
(AIO).
The
AIO
maintains
a
record
of
all
violations
and
has 
access
to
academic
integrity
violations
committed
in
all
other
ASU
colleges/schools.
Specific
rules
for
this
class
are
as
follows.
All
assignments
and
projects
must
be
your 
own
individual
work
unless
specified
as
team
efforts.
You
are
encouraged
to
learn
from 
each
other
but
copying
solutions/code
is
not
allowed.
All
solutions
turned
in
for
credit 
are
to
be
your
individual
work
and
should
demonstrate
your
problem-solving
skills.
The 
instructor
reserves
the
right
to
question
a
student
orally
or
in
writing
and
to
use
their 
evaluation
of
the
student's
understanding
of
the
assignment
and
of
the
submitted 
solution
as
evidence
of
cheating.
Violators
of
this
policy
will
be
faced
with
severe 
penalties,
which
may
range
from
deducted
points
to
failure
of
the
course.
Harassment
and
Sexual
Discrimination
Arizona
State
University
is
committed
to
providing
an
environment
free
of
discrimination, 
harassment,
or
retaliation
for
the
entire
university
community ,
including
all
students, 
faculty
members,
staff
employees,
and
guests.
ASU
expressly
prohibits
discrimination, 
harassment,
and
retaliation
by
employees,
students,
contractors,
or
agents
of
the 
university
based
on
any
protected
status:
race,
color ,
religion,
sex,
national
origin,
age, 
disability ,
veteran
status,
sexual
orientation,
gender
identity ,
and
genetic
information. 
Title
IX
is
a
federal
law
that
provides
that
no
person
be
excluded
on
the
basis
of
sex 
from
participation
in,
be
denied
benefits
of,
or
be
subjected
to
discrimination
under
any 
education
program
or
activity .
Both
Title
IX
and
university
policy
make
clear
that
sexual
violence
and
harassment
based
on
sex
is
prohibited.
An
individual
who
believes
they 
have
been
subjected
to
sexual
violence
or
harassed
on
the
basis
of
sex
can
seek 
support,
including
counseling
and
academic
support,
from
the
university .
If
you
or 
someone
you
know
has
been
harassed
on
the
basis
of
sex
or
sexually
assaulted,
you 
can
find
information
and
resources
at
https://sexualviolenceprevention.asu.edu/faqs
. 
As
a
mandatory
reporter ,
I
am
obligated
to
report
any
information
I
become
aware
of 
regarding
alleged
acts
of
sexual
discrimination,
including
sexual
violence
and
dating 
violence.
ASU
Counseling
Services,
https://eoss.asu.edu/counseling
is
available
if
you 
wish
to
discuss
any
concerns
confidentially
and
privately .
ASU
online
students
may 
access
360
Life
Services,
https://goto.asuonline.asu.edu/success/online-resources.html
.
Disability
accommodations
Suitable
accommodations
are
made
for
students
having
disabilities.
Students
needing 
accommodations
must
register
with
the
ASU
Disabilities
Resource
Center
and
provide 
documentation
of
that
registration
to
the
instructor .
Students
should
communicate
the 
need
for
an
accommodation
in
enough
time
for
it
to
be
properly
arranged.
See
ACD 
304-08
Classroom
and
Testing
Accommodations
for
Students
with
Disabilities.
Class
behavior
Students,
faculty ,
staff,
and
other
individuals
do
not
have
an
unqualified
right
of
access 
to
university
grounds,
property ,
or
services
(see
SSM
104-02
).
Interfering
with
the 
peaceful
conduct
of
university-related
business
or
activities
or
remaining
on
campus 
grounds
after
a
request
to
leave
may
be
considered
a
crime.
All
incidents
and 
allegations
of
violent
or
threatening
conduct
by
an
ASU
student
(whether
on-
or 
off-campus)
must
be
reported
to
the
ASU
Police
Department
(ASU
PD)
and
the
Office
of 
the
Dean
of
Students.
Waiting
for
an
absent
instructor
In
the
event
the
instructor
fails
to
indicate
a
time
obligation,
the
time
obligation
will
be
15 
minutes
for
class
sessions
lasting
90
minutes
or
less,
and
30
minutes
for
class
sessions 
lasting
more
than
90
minutes.
Students
may
be
directed
to
wait
longer
by
someone
from 
the
academic
unit
if
they
know
the
instructor
will
arrive
shortly .
Syllabus
disclaimer
Any
information
in
this
syllabus
(other
than
grading
and
absence
policies)
may
be 
subject
to
change
with
reasonable
advance
notice.
Remember
to
check
your
ASU
email 
and
the
course
site
often.
Thanks
all
for
a
great
ﬁrst
class
today.
●
Link
to
lectur e
slides
from
today
are
now
posted
under
Modules
●
Presentation
rubric
proposed
from
class
discussion
is
available
for
comment
on
the
Discussions
tab
●
Syllabus
quiz
is
open
and
due
on
Januar y
10
-
don't
forget!
●
Wednesda y
we
will
have
our
ﬁrst
presentations
from
Gedeon
and
Nitin.
Mak e
sure
you
read
one
of
the
two
papers
and
complete
the
review
assignment
for
it
befor e
class.
See
you
Wednesda y!
We
have
two
paper
presentations
scheduled
for
next
class,
Wednesda y
Januar y
17.
The
presenter
assigned
to
the
paper
"Using
satellite
imager y
to
understand
and
promote
sustainable
development"
has
dropped
the
class,
so
I
am
asking
for
one
student
to
volunteer
to
switch
their
paper
presentation
to
this
paper/class
.
If
you
volunteer
to
switch,
your
previous
ﬁrst
paper
assignment
will
be
assigned
to
a
late-r egistering
student.
I
will
awar d
5
bonus
points
on
the
presentation
for
the
student
who
volunteers.
First
come
ﬁrst
served.
We
now
have
a
volunteer ,
so
no
further
action/consider ation
is
needed.
Thanks!
Reminder:
we
do
not
have
class
on
Monda y
due
to
the
national
Martin
Luther
King
Jr
Day
holida y. 
Project
ideas
database
initiated
+
extra 
credit
presentation
opportunity
Hannah
Kerner
(She/Her)
All
Sections
No
unread
replies.
No
replies.
Project
ideas
database:
I
have
added
a
database
of
project
ideas
to
the
course
spreadsheet
Links
to
an
external
site.
.
Please
be
sure
to
read
the
instructions
at
the
top
("how
to
use
this
spreadsheet").
You
may
see
more
project
ideas
added
to
this
spreadsheet
in
the
coming
weeks.
Extra
credit
presentation
oppor tunity:
Since
we
have
had
a
few
students
drop
the
class,
we
currently
have
eight
unassigned
paper
presentations
(mark ed
TBA
in
our
schedule)
Links
to
an
external
site.
.
To
allow
us
to
keep
our
paper
presentation
schedule
as
is,
we
are
offering
a
bonus
oppor tunity
for
paper
presentations.
Students
may
sign
up
for
a
third
paper
presentation
-
if
you
give
3
presentations,
i)
your
highest
2
presentation
grades
will
be
used
for
your
2
presentation
grades,
and
ii)
we
will
add
5
bonus
points
to
each
of
the
2
presentation
grades.
Contact
Bhanu
to
sign
up
for
a
presentation.
Assignments
will
be
ﬁrst
come
ﬁrst
served
and
you
must
sign
up
>=1
week
befor e
the
presentation.
Hannah
Kerner 
Lab
1
instructions
posted
Hannah
Kerner
(She/Her)
All
Sections
No
unread
replies.
No
replies.
I
have
posted
the
instructions
for
Lab
1.
It
will
be
due
Februar y
8
(two
weeks).
If
you
have
questions
about
the
assignment,
please
use
the
Discussion
Boar d
to
allow
other
students
to
beneﬁt
from
and
help
answer
your
questions.
Hannah
Kerner 
Instructor
office
hours
3:30-4pm
today
Hannah
Kerner
(She/Her)
All
Sections
No
unread
replies.
No
replies.
My
oﬃce
hours
will
start
late
at
3:30pm
until
4:00pm
today.
Hannah
Kerner
Instructor
office
hours
3:30-4pm
today
Hannah
Kerner
(She/Her)
All
Sections
No
unread
replies.
No
replies.
My
oﬃce
hours
will
start
late
at
3:30pm
until
4:00pm
today.
Hi
all,
I
would
like
to
shar e
with
you
a
new
paper
that
I
wrote
with
Esther
Rolf,
Konstantin
Klemmer ,
and
Caleb
Robinson
titled
"Mission
Critical
--
Satellite
Data
is
a
Distinct
Modality
in
Machine
Learning":
https:/ /arxiv .org/abs/2402.01444
Links
to
an
external
site.
It
is
not
one
of
our
assigned
papers,
but
I
highly
encour age
you
all
to
read
it
since
it
is
highly
relevant
to
our
class.
It
is
a
position
paper
that
explains
why
satellite
data
is
distinct
from
other
ML
modalities
and
argues
for
a
new
paradigm
of
ML
appr oaches
designed
for
its
unique
char acteristics
and
contexts.
PS
-
You
can
now
ﬁnd
a
Shar ed
Google
Drive
containing
the
presentations
given
in
class
so
far
linked
at
the
top
of
the
Modules
page.
See
you
this
afternoon!
Resources
from
Dr.
Jacob
Adler
guest 
lecture
Hannah
Kerner
(She/Her)
All
Sections
No
unread
replies.
No
replies.
I
have
posted
some
resour ces
from
today's
guest
lectur e
under
Modules
>
Additional
resour ces
(at
the
bottom
of
the
page),
including:
●
Lectur e
slides
●
Machine
Learning
for
Planetar y
Science
textbook
pdf
●
Feedback
survey
link
(anonymous
responses)
-
this
is
optional
but
helpful
for
guest
speak ers!
The
link
is
also
here
●
Links
to
an
external
site.
●
.
Dr.
Adler 's
email
is
jbadler2@asu.edu
in
case
you
want
to
follow
up
with
him.
Resources
from
Dr.
Caleb
Robinson
guest 
lecture
Hannah
Kerner
(She/Her)
All
Sections
No
unread
replies.
No
replies.
I
have
posted
some
resour ces
from
yester day's
guest
lectur e
under
Modules
>
Additional
resour ces
(at
the
bottom
of
the
page),
including:
●
Lectur e
slides
●
Feedback
survey
link
(anonymous
responses)
-
this
is
optional
but
helpful
for
guest
speak ers!
The
link
is
also
here
●
Links
to
an
external
site.
●
.
Dr.
Robinson 's
email
is
caleb.r obinson@micr osoft.com
in
case
you
want
to
follow
up
with
him.
PS:
Reminder
that
the
project
proposal
template,
due
Feb
28,
is
available
now.
I
encour age
everyone
to
start
ﬁlling
in
the
details
ASAP
so
you
can
see
what
gaps
there
are
in
your
plan
that
you
want
to
ﬁgur e
out
in
the
next
couple
of
weeks.
Hannah
Kerner 
Tour
of
ASU
space
exploration/planetary 
science
facilities
Hannah
Kerner
(She/Her)
All
Sections
No
unread
replies.
No
replies.
Ther e
was
a
request
after
Jacob
Adler 's
guest
lectur e
to
have
a
tour
of
ASU' s
space
explor ation
and
planetar y
science
facilities
to
see
some
of
the
labs
wher e
the
spacecr aft
and
missions
Dr.
Adler
described
are
being
developed,
built,
and
oper ated
at
ASU.
Dr.
Adler
is
offering
students
in
our
class
an
“ASU
planetar y
tour”
Tuesda y
2/27,
1:00-2:30pm.
Meet
at
1pm
on
the
east
side
of
Inter disciplinar y
Building
A,
1100
Cady
Mall,
Tempe,
AZ
85281
(building
due
west
of
the
MU)
for
a
tour
of
LROC
-
the
Lunar
Reconnaissance
Orbiter
Camer a
center
(1:00-1:25pm).
This
will
be
followed
by
a
tour
of
the
Mars
Space
Flight
Facility
(Moeur
building)
from
1:30-1:45.
Then
a
shor t
walk
to
ISTB4
for
a
tour
of
the
Earth
and
Space
Explor ation
galler y
(1:55-2:30).
If
you
would
like
to
attend,
please
add
your
name
to
this
spreadsheet
Links
to
an
external
site.
so
he
knows
how
many
people
to
expect.
Lab
2
and
Final
Project
Literature
Review 
instructions
posted
Hannah
Kerner
(She/Her)
All
Sections
No
unread
replies.
No
replies.
Instructions
for
Lab
2
and
the
ﬁnal
project
literature
review
assignments
are
now
posted.
If
you
have
any
questions,
please
post
them
on
the
Discussion
Boar d
so
other
students
can
beneﬁt,
other wise
send
a
Canv as
mail
to
Bhanu.
Hannah
Kerner 
Reminder:
tour
tomorrow
of
ASU
planetary 
exploration
facilities
Hannah
Kerner
(She/Her)
All
Sections
No
unread
replies.
No
replies.
As
a
reminder ,
Dr.
Adler
(who
gave
our
recent
guest
lectur e)
is
offering
students
in
our
class
an
“ASU
planetar y
tour”
tomorr ow
Tuesda y
2/27,
1:00-2:30pm.
Meet
at
1pm
on
the
east
side
of
Inter disciplinar y
Building
A,
1100
Cady
Mall,
Tempe,
AZ
85281
(building
due
west
of
the
MU)
for
a
tour
of
LROC
-
the
Lunar
Reconnaissance
Orbiter
Camer a
center
(1:00-1:25pm).
This
will
be
followed
by
a
tour
of
the
Mars
Space
Flight
Facility
(Moeur
building)
from
1:30-1:45.
Then
a
shor t
walk
to
ISTB4
for
a
tour
of
the
Earth
and
Space
Explor ation
galler y
(1:55-2:30).
If
you
would
like
to
attend,
please
add
your
name
to
this
spreadsheet
Links
to
an
external
site.
so
he
knows
how
many
people
to
expect.
Hannah
Kerner
Assistant
Professor
at
Arizona
State
University
 
Tempe,
AZ
About
I
am
currently
an
Assistant
Professor
in
the
School
of
Computing
and
Augmented
Intelligence
at
Arizona
State
Univ ersity .
My
resear ch
focuses
on
developing
machine
learning
systems
for
real-world
data
and
use
cases.
This
includes
remote
sensing
and
spatial
datasets,
fairness
(particularly
w.r.t.
geogr aphic
bias),
scientiﬁc
disco very
and
explor ation,
agricultur e
and
food
security ,
and
other
topics.
I
am
the
AI/Machine
Learning
Lead
for
NASA
Harvest
and
NASA
Acres
as
well
as
Center
Faculty
for
the
ASU
Center
for
Global
Disco very
and
Conser vation
Science
(
GDCS
).
I
was
recogniz ed
on
the
Forbes
30
Under
30
list
in
Science
in
2021.
I
also
write
and
speak
about
challenges
for
developing
AI/ML
applications
for
real
world
problems,
such
as
in
this
recent
article
in
MIT
Technology
Review
.

Prior
to
joining
the
faculty
at
ASU,
I
was
an
Assistant
Resear ch
Professor
at
the
Univ ersity
of
Maryland,
College
Park.
I
did
my
Ph.D .
at
Arizona
State
Univ ersity
on
machine
learning
methods
(especially
novelty
detection)
for
planetar y
explor ation
missions.
I
receiv ed
my
B.S.
in
computer
science
from
the
Univ ersity
of
North
Carolina
at
Chapel
Hill
wher e
I
conducted
resear ch
on
3D
motion
planning
for
autonomous
agents.
I
have
work ed
at
NASA’s
Jet
Propulsion
Labor atory,
Goddar d
Space
Flight
Center ,
and
Langle y
Resear ch
Center ,
as
well
as
the
commer cial
remote
sensing
company
Planet,
Inc.
I
am
passionate
about
advancing
oppor tunities
for
people
who
have
traditionally
been
underr epresented
in
or
excluded
from
computer
science
and
devote
much
of
my
time
outside
of
resear ch
to
these
effor ts.
You
can
download
my
CV
here
.
Recognition

●
Top
10
of
100
projects
solving
problems
related
to
the
UN
SDGs
with
AI,
International
Resear ch
Centr e
on
Artiﬁcial
Intelligence
(IRCAI),
for
NASA
Harvest
(2021)
●
Forbes
30
Under
30
in
Science
(2021)
●
Outstanding
Resear ch
Faculty
depar tment
awar d
(2021)
●
Radiant
Earth
Foundation ’s
15
Leading
Women
in
ML4EO
(2021)
●
Google
Women
Techmak ers
awar d
(2018)
News
●
Dec
2023:
Our
proposal
for
the
EarthVision
workshop
at
CVPR
2024
in
Seattle
was
accepted!
●
Dec
2023:
Our
proposal
for
the
2nd
Machine
Learning
for
Remote
Sensing
Workshop
at
ICLR
2024
in
Vienna
was
accepted!
●
Dec
2023:
I
was
appointed
to
the
NOAA
Science
Advisor y
Boar d
(SAB)
Data
Archiving
and
Access
Requir ements
Working
Group
(DAARWG),
which
is
a
3-year
Feder al
Advisor y
Committee
appointment!
●
Dec
2023:
A
preprint
of
our
paper ,
“Satellite
Data
Shows
Resilience
of
Tigrayan
Farmers
in
Crop
Cultiv ation
During
Civil
War,”
is
now
available
on
arxiv
!
●
Dec
2023:
I
gave
a
keynote
talk
at
the
NeurIPS
Sustainable
Computing
workshop
in
New
Orleans!
●
Dec
2023:
I
gave
three
resear ch
talks
and
hosted
several
sessions
on
machine
learning
at
the
AGU
Fall
Meeting
in
San
Francisco!
●
Nov
2023:
Our
paper
“Lightweight,
Pre-trained
Transformers
for
Remote
Sensing
Timeseries”
was
accepted
for
a
spotlight
presentation
at
the
NeurIPS
2023
Climate
Change
AI
Workshop!
●
Oct
2023:
Our
paper
“ConeQuest:
A
Benchmark
for
Cone
Detection
on
Mars”
was
accepted
to
WACV
2023!
●
Oct
2023:
Our
proposal
to
Google
Resear ch
(Collectiv e
&
Society
Center ed
AI)
titled
“A
Data-Centric
Appr oach
to
Impr ove
Geogr aphic
Equity
in
Geospatial
ML”
was
awar ded!
●
Oct
2023:
Our
paper
“Automated
Multi-class
Crater
Segmentation
in
Mars
Orbital
Images”
was
accepted
to
the
GeoAI
workshop
at
SIGSP ATIAL
2023!
●
Oct
2023:
Our
collabor ation
with
Maui
United
Way
to
suppor t
residents
impacted
by
the
Maui
wildﬁr es
was
highlighted
an
article
in
Maui
Times
!
●
Sep
2023:
Our
NSF
RAPID
proposal,
“RAPID:
Rapid
computational
modeling
of
wildﬁr es
and
management
with
emphasis
on
human
activity ,”
was
selected!
●
Sep
2023:
Our
proposal,
“Lowering
the
Barriers
to
Planetar y
Science
Studies
with
a
Large
Mars
Model, ”
was
selected
for
the
JPL
Strategic
Univ ersity
Resear ch
Program
-
congr ats
to
PhD
student
Mirali
Purohit
on
this
fellowship!
●
Sep
2023:
Our
paper ,
“GEO-Bench:
Towar d
Foundation
Models
for
Earth
Monit oring, ”
was
accepted
for
NeurIPS
2023
Datasets
and
Benchmarks
Track!
●
Aug
2023:
We
were
awar ded
a
grant
from
NASA,
“Machine
Learning
Datasets
for
Public
Good
with
a
Data-Centric
AI
Appr oach, ”
to
lead
the
development
of
new
ML
datasets
using
satellite
remote
sensing
data
in
partnership
with
NASA
and
ML
Commons!
●
Aug
2023:
Catherine
Nakalembe
and
I
gave
a
talk
on
Suppor ting
Global
Food
Security
with
Machine
Learning
and
Earth
Obser vations
for
the
2023
Computer
Vision
for
Ecology
summer
school
!
●
Aug
2023:
We
created
a
website
of
satellite
data
sour ces
in
suppor t
of
the
Maui
wildﬁr es
here
●
July
2023:
I
taught
a
guest
lectur e
on
AI
for
Agricultur e
for
Climate
Change
AI’s
summer
school,
which
you
can
watch
here
●
July
2023:
Our
latest
paper ,
“How
accur ate
are
existing
land
cover
maps
for
agricultur e
in
Sub-Sahar an
Africa?”
is
available
on
arxiv
!
●
May
2023:
Our
NASA
Multidomain
Reusable
Artiﬁcial
Intelligence
Tools
proposal
titled
“Anomaly
Visualization
for
Earth
and
Heliophysics
GNSS
Data
using
DORA ”
(PI:
Rebbapr agada/JPL)
was
selected!
●
May
2023:
We
hosted
a
successful
1st
Workshop
on
Machine
Learning
for
Remote
Sensing
at
ICLR
2023
and
Tutorial
on
the
same
at
CMU
Africa!
Read
accepted
papers
on
our
workshop
website
●
Apr
2023:
Our
latest
paper ,
“Lightweight,
Pre-trained
Transformers
for
Remote
Sensing
Timeseries, ”
is
available
on
arxiv
!
●
Apr
2023:
I
am
thrilled
to
be
aﬃliated
with
the
Center
for
Global
Disco very
and
Conser vation
Science
at
ASU!
The
Center ’s
mission
is
to
addr ess
human-envir onmental
challenges
on
land
and
sea.
●
Apr
2023:
I
had
fun
giving
the
keynote
talk
on
“Artiﬁcial
Intelligence:
From
Sci-Fi
to
Societal
Good”
at
the
Machiner y
Dealers
National
Association
meeting
in
Tucson!
●
Mar
2023:
Catherine
Nakalembe
and
I
published
a
Perspectiv e
article
titled
Consider ations
for
AI-EO
for
agricultur e
in
Sub-Sahar an
Africa
in
Envir onmental
Resear ch
Letters
[paper
link]
●
Mar
2023:
Kerner
Lab
group
members
Manthan
Satish,
Mirali
Purohit,
and
Adity a
Mohan
made
the
winning
solution
for
the
Wildﬁr e
Risk
and
Social
Disparity
track
at
SpaceH ACK
for
Sustainability
●
Mar
2023:
I
gave
an
invited
talk
for
the
NSF
AI
Planning
Institute
Seminar
at
Carnegie
Mellon
Univ ersity!
●
Mar
2023:
An
article
was
published
about
our
work
combatting
food
security
in
Maui
County
using
AI!
●
Feb
2023:
I
gave
the
colloquium
talk
for
the
School
of
Earth
and
Space
Explor ation
at
ASU!
See
the
recor ding
here
●
Feb
2023:
Our
group
traveled
to
Maui,
Lanai,
and
Molokai
to
kick
off
our
‘Aina
Data
Stewar ds
program!
●
Dec
2022:
Our
proposal
for
the
Machine
Learning
for
Remote
Sensing
workshop
at
ICLR
2023
was
accepted!
●
Nov
2022:
An
article
was
published
about
our
AI
and
Earth
Obser vations-enabled
food
security
project
in
Maui
County!
●
Nov
2022:
Our
paper
“OpenMapFlow:
A
Library
for
Rapid
Map
Creation
with
Machine
Learning
and
Remote
Sensing
Data ”
was
accepted
for
AAAI
2023!
●
Nov
2022:
Our
paper
“Multi-Region
Transf er
Learning
for
Segmentation
of
Crop
Field
Boundaries
in
Satellite
Images
with
Limited
Labels”
was
accepted
for
the
2nd
Annual
AAAI
Workshop
on
AI
to
Acceler ate
Science
and
Engineering
(AI2ASE)!
●
Nov
2022:
Inbal
Beck er-Reshef
and
I
gave
an
invited
talk
for
the
AI
Helps
Ukraine
fundr aiser
conf erence!
See
recor ding
here
●
Nov
2022:
Our
NASA
proposal
for
“NASA
ACRES:
A
Climate
Resilient
Ecosystem
Appr oach
to
Strengthening
US
Agricultur e”
(PI:
Whitcr aft/UMD)
was
awar ded!
(NASA
Earth
Science
Applications:
Agricultur e
program)
●
Oct
2022:
Our
NASA
proposal
for
“NASA
Harvest:
NASA
Food
Security
and
Agricultur e
Consor tium ”
(PI:
Beck er-Reshef/UMD)
was
awar ded!
●
Oct
2022:
I
was
a
selected
participant
in
the
ﬁrst
U.S.-Africa
Frontiers
of
Science,
Engineering,
and
Medicine
symposium,
held
in
partnership
with
the
African
Academy
of
Sciences
in
Nairobi,
Kenya!
●
Sep
2022:
We
traveled
to
Rwanda
for
the
AGRF
Summit
held
in
Kigali!
●
Sep
2022:
Our
NASA
proposal
for
“EO-Enabled
Regional
and
National
Agricultur al
Monit oring
in
West
Africa ”
(PI:
Nakalembe/UMD)
was
awar ded!
(NASA
SERVIR
program)
●
Sep
2022:
Our
NASA
proposal
for
an
“EO-Enabled
Food
Security
Dashboar d
to
Close
Critical
Data
Gaps
in
Highly
Food
Insecur e
Maui
County ”
(PI:
Kerner/ASU)
was
awar ded!
(NASA
ROSES
Equity
and
Envir onmental
Justice
program)
●
Aug
2022:
Our
paper
on
“Spectr al
Diversity
of
Rocks
and
Soils
in
Mastcam
Obser vations
Along
the
Curiosity
Rover’s
Traverse
in
Gale
Crater,
Mars”
was
accepted
to
JGR:
Planets
!
●
Aug
2022:
I
started
a
new
faculty
position
at
ASU
School
of
Computing
and
Augmented
Intelligence!
●
Aug
2022:
Catherine
Nakalembe
and
I
gave
an
invited
talk
for
the
Computer
Vision
for
Ecology
summer
school!
See
recor ding
here
●
July
2022:
We
presented
our
invited
paper
on
“Guiding
Field
Explor ation
on
Earth
and
Mars
with
Outlier
Detection ”
at
IGARSS!
●
July
2022:
Our
paper
on
“High-Resolution
Regional
Digital
Elevation
Models
and
Deriv ed
Products
from
MESSENGER
MDIS
Images”
was
accepted
to
Remote
Sensing
!
●
June
2022:
I
gave
an
invited
talk
for
the
Agricultur eVision
workshop
at
CVPR
2022!
●
June
2022:
We
gave
an
invited
tutorial
on
Machine
Learning
for
Remote
Sensing
at
CVPR
2022!
Materials
and
recor dings
here
●
June
2022:
Our
paper
“Phenological
normalization
can
impr ove
in-season
classiﬁcation
of
maiz e
and
soybean:
A
case
study
in
the
centr al
US
Corn
Belt”
was
accepted
to
Science
of
Remote
Sensing
!
●
May
2022:
We
traveled
to
Bonn,
Germany
and
gave
several
oral
and
poster
presentations
for
the
Living
Planet
Symposium
hosted
by
the
European
Space
Agency!
●
May
2022:
Our
paper
“Domain-Agnostic
Outlier
Ranking
Algorithms—A
Conﬁgur able
Pipeline
for
Facilitating
Outlier
Detection
in
Scientiﬁc
Datasets”
was
accepted
to
F r ontiers
in
Astr onomy
and
Space
Sciences
!
●
April
2022:
I
gave
the
keynote
speech
for
the
AI
for
Earth
Obser vations
(AI4EO)
Food
Security
Challenge
awar ds
ceremony!
See
recor ding
here
●
March
2022:
Our
book
on
“Machine
Learning
for
Planetar y
Science ”
was
published
by
Elsevier!
●
March
2022:
I
gave
a
Hyper wall
talk
at
the
NASA
booth
in
the
Commodity
Classic
conf erence
held
in
New
Orleans!
●
Feb
2022:
Kick ed
off
our
project
on
“Optimizing
Crop
Yield
Data
Collection
for
Supply
Chain
Enhancement”,
funded
by
Tetra
Tech
and
Bill
&
Melinda
Gates
Foundation!
(PI:
Nakalembe/UMD)
●
Feb
2022:
Our
paper
“Applications
and
Consider ations
for
AI-EO
for
Agricultur e
in
Sub-Sahar an
Africa ”
was
accepted
for
an
oral
presentation
at
AAAI
2022,
International
Workshop
on
Social
Impact
of
AI
for
Africa!
●
Jan
2022:
Our
CVPR
2022
workshop
proposal
was
accepted
for
the
3rd
International
Workshop
and
Prize
Challenge
on
Agricultur e-Vision:
Challenges
&
Oppor tunities
for
Computer
Vision
in
Agricultur e!
●
Dec
2021:
Our
team
at
NASA
Harvest
was
recogniz ed
in
the
Top
10
of
100
projects
solving
problems
related
to
the
UN
SDGs
with
AI,
International
Resear ch
Centr e
on
Artiﬁcial
Intelligence
(IRCAI)!
●
Dec
2021:
Our
paper
on
task-informed
meta-learning
for
crop
type
mapping
was
accepted
for
the
AI
for
Agricultur e
and
Food
Systems
(AIAFS)
workshop
at
AAAI!
●
Nov
2021:
Our
paper
on
Foundation
Models
for
Earth
Monit oring:
Proposal
for
a
Climate
Change
Benchmark
was
accepted
for
the
Climate
Change
AI
workshop
at
NeurIPS!
●
Oct
2021:
Our
NASA
E-Clips
educational
videos
explaining
how
NASA
data
is
used
for
agricultur e
and
food
security
are
live!
●
Oct
2021:
Our
work
on
rapid
response
cropland
mapping
with
ML/EO
in
Togo
was
featur ed
in
Radiant
Earth’s
article
on
Disco verable
and
Reusable
ML
Workﬂows
for
Earth
Obser vation
!
●
Sep
2021:
Our
paper
on
the
new
CropHar vest
dataset
was
accepted
for
NeurIPS
Datasets
and
Benchmarks
track!
●
Aug
2021:
Kick ed
off
our
project
on
planted
area
change
estimation
using
ML/EO
in
Ethiopia
and
Sudan,
part
of
FEWS
NET
East
Africa
virtual
crop
tour!
(PI:
Kerner/UMD)
●
Jul
2021:
Field
data
collection
campaign
in
Uganda
for
Street2Sat/Helmets
Labeling
Crops
project
completed!
(PI:
Nakalembe/UMD)
●
Apr
2021:
Proﬁle
article
featur ed
on
NASA
Applied
Sciences
website
!
●
Apr
2021:
Kick ed
off
our
project
on
Domain-agnostic
Outlier
Ranking
Algorithms
(DORA)
with
JPL,
funded
by
NASA
SMD
as
a
Cross-Divisional
AI/ML
Use
Case
Demonstr ation!
(PI:
Kerner/UMD)
●
Hannah
Kerner
●
Resear ch
●
Publications
●
Teaching
●
Talks
●
Resear ch
Group
●
Resour ces
Hannah
Kerner
Assistant
Professor
at
Arizona
State
University
 
Tempe,
AZ
 
Email

 
Twitter
 
LinkedIn
 
Github
 
Google
Scholar
 
ORCID
Current
Projects
ML
advances
for
remote
sensing/geospatial
data
Satellite
Earth
obser vation
(EO)
datasets
are
rapidly
gaining
inter est
in
the
AI
community
due
to
the
massiv e
datasets
involved
as
well
as
the
oppor tunities
for
addr essing
urgent
challenges
related
to
climate
change,
the
envir onment,
agricultur e
and
food
security ,
and
humanitarian
needs.
Howe ver,
there
are
currently
many
challenges
for
developing
AI
systems
that
use
EO
data,
namely ,
limited
public
labeled
datasets,
a
lack
of
harmonization
across
labels
and
sour ce
data,
substantial
effor t
requir ed
to
mak e
EO
data
“AI-ready ”,
and
models
that
do
not
learn
geogr aphic
context
or
exploit
spatial
metadata.
My
resear ch
focuses
on
algorithmic
advances
that
eﬃciently
learn
from
sparse,
heter ogeneous
data
and
incorpor ate
location/other
metadata
to
enable
models
to
learn
geogr aphic
context.

Agricultural
land
cover/land
use
classiﬁcation
and
monitoring
Information
about
wher e
and
which
types
of
crops
are
growing
in
addition
to
how
they
are
growing
and
expected
to
perform
(e.g.,
crop
conditions,
expected
yield,
farming
practices)
is
critical
for
informing
agricultur al
mark et
decisions
and
ensuring
global
food
security .
Machine
learning
and
remote
sensing
data
can
be
used
to
create
crop
type
maps
and
other
data
products
that
provide
information
such
as
crop-speciﬁc
land
cover
classiﬁcations
to
enable
timely
analysis
at
ﬁeld
scales
throughout
the
growing
season.
These
data
products
can
be
used
in
downstr eam
analyses
and
decision-making
such
as
agricultur al
production
statistics,
early
warning
for
impending
food
shor tages,
pest/disease
monit oring,
crop
damage
assessment,
and
more.
Novelty/anomaly
detection

Automatic
detection
of
out-of-distribution
(OOD)
samples
or
featur es
is
universally
needed
when
working
with
scientiﬁc
datasets.
This
capability
can
be
used
for
cleaning
datasets,
e.g.,
ﬂagging
ground-truth
labels
with
GPS
or
human
entry
error
or
identifying
wrongly
categoriz ed
objects
in
a
catalog.
It
could
also
be
used
for
disco very,
e.g.,
to
ﬂag
novel
samples
in
order
to
guide
instrument
acquisition
or
scientiﬁc
analysis.
My
resear ch
focuses
on
developing
methods
for
automatically
detecting
novel
or
out-of-distribution
samples
in
scientiﬁc
datasets
in
a
variety
of
domains
including
planetar y
explor ation,
pests/disease
in
crop
ﬁelds,
volcanic
thermal
anomalies,
natur al
hazar ds/disasters,
and
more.
Change
detection,
mapping,
and
area
estimation

Change
detection
in
remote
sensing
involves
automatic
identiﬁcation
of
the
occurr ence,
extent,
and/or
type
of
change
on
the
Earth's
or
another
planet' s
surface.
Such
methods
can
be
designed
to
detect
rapid
changes
(such
as
impact
craters)
or
changes
that
occur
more
gradually
over
time
(such
as
slow-mo ving
landslides).
My
resear ch
projects
also
involve
detecting
semantic
changes
in
land
cover
or
land
use
categories,
such
as
farm
abandonment/planting
status
or
conv ersion
of
forests
to
cropland,
and
strategies
for
estimating
the
areal
extent
of
that
change
to
inform
policy-
and
decision-making
(e.g.,
food
security
assessments).

Street2Sat:
Co-locating
ground
object
detections
with
satellite
images
In
many
cases,
ground-truth
annotation
is
requir ed
to
determine
the
label
that
should
be
associated
with
a
location
in
a
satellite
image
due
to
the
spatial
resolution
and
overhead
natur e
of
satellite
images.
Such
labels
are
scar ce
and
are
often
collected
oppor tunistically
and
on
a
project-t o-project
basis;
transformativ e
appr oaches
are
needed
to
more
scalably
and
affor dably
collect
ground-truth
labels,
especially
in
regions
currently
under-r epresented
in
machine
learning
datasets
(e.g.,
Sub-Sahar an
Africa).
We
are
developing
a
solution
called
Street2Sat,
which
transforms
a
set
of
geo-tagged
images
collected
from
a
vehicle
on
the
road
to
a
set
of
labeled
geo-r eferenced
points

with
locations
corresponding
to
the
object(s)
of
inter est
detected
in
the
images.
These
points
can
then
be
used
as
labels
for
satellite
images.
Geographic
Fairness
Fairness
and
compar ative
performance
of
AI
systems
can
be
evaluated
across
sub-gr oups
of
society
that
differ
by
factors
such
as
sociocultur al
identity ,
age,
gender ,
health
status,
geogr aphy ,
income,
and
education.

Geogr aphy
is
a
particularly
impor tant
dimension
because
it
is
correlated
with
many
other
factors
such
as
income,
access
to
infrastructur e,
health
status,
and
education.
Geogr aphy
also
inﬂuences
the
appear ance
and
context
of
objects
or
classes
in
datasets
used
in
AI.
Existing
ML
benchmark
datasets
and
models
have
been
shown
to
have
substantial
geogr aphic
bias
in
addition
to
race,
gender ,
and
language,
resulting
in
AI
systems
that
work
well
for
some
regions
and
not
others.
My
resear ch
involves
creating
new
datasets
and
algorithms
designed
to
mitigate
such
biases
and
create
geogr aphically
fair
AI
systems.
AI-Assisted
Exploration
Planetar y
explor ation
missions
seek
to
investigate
and
understand
never-
or

under-explor ed
landscapes
and
worlds.
AI
can
assist
scientists
and
mission
teams
in
this
goal
and
enhance
scientiﬁc
disco very
by
prioritizing
obser vations
of
scientiﬁc
inter est,
automatically
selecting
targets
for
obser vation,
revealing
patterns
and
relationships
in
high-dimensional
datasets,
and
more.
My
resear ch
projects
involve
developing
AI
systems
that
augment
the
capabilities
of
planetar y
missions
and
scientists
using
novelty
detection,
unsuper vised
clustering/mapping,
change
detection,
and
more.
Sitemap
 
FOLLOW:
 
GITHUB
 
FEED
©
2024
Hannah
Kerner.
Powered
by
Jekyll
&
AcademicPages
,
a
fork
of
Minimal
Mistakes
.
 
Hannah
Kerner
 
Resear ch
 
Publications
 
Teaching
 
Talks
 
Resear ch
Group
 
Resour ces
Hannah
Kerner
Assistant
Professor
at
Arizona
State
University
 
Tempe,
AZ
 
Email

 
Twitter
 
LinkedIn
 
Github
 
Google
Scholar
 
ORCID
Publications
1.
🆕
Kerner ,
H.
,
Nakalembe,
C.,
Yeh,
B.,
Zvonko v,
I.,
Skakun,
S.,
Beck er-Reshef,
I.,
and
McNally ,
A.
(2023).
Satellite
Data
Shows
Resilience
of
Tigrayan
Farmers
in
Crop
Cultiv ation
During
Civil
War.
arXiv
preprint,
link
.
2.
🆕
Prieur ,
N.
C.,
Amar o,
B.,
Gonzalez,
E.,
Kerner ,
H.
,
Medv edev,
S.,
Rubanenko,
L.,
Werner ,
S.,
Xiao,
Z.,
Zastr ozhno v,
D.,
and
Lapôtr e,
M.
G.
(2023).
Automatic
Char acterization
of
Boulders
on
Planetar y
Surfaces
From
High‐Resolution
Satellite
Images.
Journal
of
Geophysical
Resear ch:
Planets,
128(11)
,
e2023JE008013,
link
.
3.
🆕
Tseng,
G.,
Zvonko v,
I.,
Purohit,
M.,
Rolnick,
D.,
and
Kerner ,
H.
(2023).
Lightweight,
Pre-trained
Transformers
for
Remote
Sensing
Timeseries.
Neur al
Information
Pr ocessing
Systems
(NeurIPS),
Climate
Change
AI
W orkshop
,
link
.
4.
🆕
Purohit,
M.,
Adler ,
J.,
and
Kerner ,
H.
(2023).
ConeQuest:
A
Benchmark
for
Cone
Detection
on
Mars.
Pr oceedings
of
the
IEEE/CVF
Winter
Conf er ence
on
Applications
of
Computer
Vision
( W A CV )
,
pp.
6026-6035,
link
.
5.
🆕
Malvi,
S.,
Shah,
H.,
Chandar ana,
N.,
Purohit,
M.,
Adler ,
J.,
Kerner ,
H.
(2023).
Automated
Multi-class
Crater
Segmentation
in
Mars
Orbital
Images.
Pr oceedings
of
the
31st
A CM
SIGSP A TI AL
International
Conf er ence
on
Adv ances
in
Geogr aphic
Information
Systems
(A CM
SIGSP A TI AL
2023),
GeoAI
W orkshop
,
pp.
110-120,
link
.
6.
Kerner ,
H.
,
Nakalembe,
C.,
Yang,
A.,
Zvonko v,
I.,
McW eeny ,
R.,
Tseng,
G.,
and
Beck er-Reshef,
I.
(2023).
How
accur ate
are
existing
land
cover
maps
for
agricultur e
in
Sub-Sahar an
Africa?
arXiv
preprint,
link
.
7.
Tseng,
G.,
Zvonko v,
I.,
Purohit,
M.,
Rolnick,
D.,
and
Kerner ,
H.
(2023).
Lightweight,
Pre-trained
Transformers
for
Remote
Sensing
Timeseries.
arXiv
preprint,
link
.
8.
Lacoste,
A.,
Lehmann,
N.,
Rodriguez,
P.,
Sher win,
E.
D.,
Kerner ,
H.
,
Lütjens,
B.,
Irvin,
J.
A.,
Dao,
D.,
Alemohammad,
H.,
Drouin,
A.,
Gunturkun,
M.,
Huang,
G.,
Vazquez,
D.,
Newman,
D.,
Bengio,
Y.,
Ermon,
S.,
Zhu,
X.
(2023).
GEO-Bench:
Towar d
Foundation
Models
for
Earth
Monit oring.
In
Pr oceedings
of
the
Neur al
Information
Pr ocessing
Systems
(NeurIPS)
Datasets
and
Benchmarks
T r ack
,
link
.
9.
Nakalembe,
C.
and
Kerner ,
H.
(2023).
Consider ations
for
AI-EO
for
agricultur e
in
Sub-Sahar an
Africa.
Envir onmental
Resear ch
Letters,
18(4)
,
link
.
10.
Zvonko v,
I.,
Tseng,
G.,
Nakalembe,
C.,
and
Kerner ,
H.
(2023).
OpenMapFlow:
A
Library
for
Rapid
Map
Creation
with
Machine
Learning
and
Remote
Sensing
Data.
In
Pr oceedings
of
the
AAAI
Conf er ence
on
Ar tiﬁcial
Intelligence,
37(12)
,
14655-14663,
link
.
11.
Kerner ,
H.
,
Sundar ,
S.,
and
Satish,
M.
(2023).
Multi-Region
Transf er
Learning
for
Segmentation
of
Crop
Field
Boundaries
in
Satellite
Images
with
Limited
Labels.
In
Pr oceedings
of
the
AAAI
Conf er ence
on
Ar tiﬁcial
Intelligence
W orkshops
,
link
.
12.
Manimurugan,
S.,
Singar am,
R.,
Nakalembe,
C.,
and
Kerner ,
H.
(2022).
Geo-r eferencing
crop
labels
from
street-le vel
images
using
Structur e
from
Motion.
In
Pr oceedings
of
the
73r d
International
Astr onautical
Congr ess
(I A C )
,
link
.
13.
Rice,
M.S.,
Seeger ,
C.,
Bell,
J.,
Calef,
F.,
St
Clair ,
M.,
Eng,
A.,
Fraeman,
A.A.,
Hughes,
C.,
Horgan,
B.,
Jacob,
S.,
Johnson,
J.,
Kerner ,
H.
,
Kinch,
K.,
Lemmon,
M.,
Million,
C.,
Starr ,
M.,
and
Wellingt on,
D.
(2022).
Spectr al
diversity
of
rocks
and
soils
in
Mastcam
obser vations
along
the
Curiosity
rover’s
traverse
in
Gale
crater,
Mars.
Journal
of
Geophysical
Resear ch:
Planets
,
e2021JE007134,
link
.
14.
Kerner ,
H.
R.
and
Adler ,
J.
B.
(2022).
Guiding
Field
Explor ation
on
Earth
and
Mars
with
Outlier
Detection.
Pr oceedings
of
the
International
Geoscience
and
Remote
Sensing
Symposium
(IGARSS
2022)
,
pp.
5333-5336,
link
.
15.
Manheim,
M.
R.,
Henriksen,
M.
R.,
Robinson,
M.
S.,
Kerner ,
H.
R.
,
Karas,
B.
A.,
Beck er,
K.
J.,
Chojnacki,
M.,
Sutton,
S.
S.,
Blewett,
D.
T.
(2022).
High-Resolution
Regional
Digital
Elevation
Models
and
Deriv ed
Products
from
MESSENGER
MDIS
Images.
Remote
Sensing,
14
,
3564,
link
.
16.
Kerner ,
H.
R.
,
Sahajpal,
R.,
Pai,
D.
B.,
Skakun,
S.,
Puricelli,
E.,
Hosseini,
M.,
Meyer,
S.,
and
Beck er-Reshef,
I.
(2022).
Phenological
normalization
can
impr ove
in-season
classiﬁcation
of
maiz e
and
soybean:
A
case
study
in
the
centr al
US
Corn
Belt.
Science
of
Remote
Sensing,
6
,
100059,
link
.
17.
Kerner ,
H.
R.
,
Rebbapr agada,
U.,
Wagstaff,
K.
L.,
Lu,
S.,
Duba yah,
B.,
Huff,
E.,
Raman,
V.,
and
Kulshr estha,
S.
(2022).
Domain-Agnostic
Outlier
Ranking
Algorithms—A
Conﬁgur able
Pipeline
for
Facilitating
Outlier
Detection
in
Scientiﬁc
Datasets.
F r ontiers
in
Astr onomy
and
Space
Sciences,
9
,
867947,
link
.
18.
Nakalembe,
C.
L.
and
Kerner ,
H.
R.
(2022).
Applications
and
Consider ations
for
AI-EO
for
Agricultur e
in
Sub-Sahar an
Africa.
Association
for
the
Adv ancement
of
Ar tiﬁcial
Intelligence
(AAAI)
W orkshops
,
International
Workshop
on
Social
Impact
of
AI
for
Africa.
19.
Tseng,
G.,
Kerner ,
H.
,
Rolnick,
D.
(2022).
TIML:
Task-Informed
Meta-Learning
for
crop
type
mapping.
Association
for
the
Adv ancement
of
Ar tiﬁcial
Intelligence
(AAAI)
W orkshops
,
AI
for
Agricultur e
and
Food
Systems
(AIAFS).
20.
Handwer ger,
A.
L.,
Jones,
S.
Y.,
Amaty a,
P.,
Kerner ,
H.
R.
,
Kirschbaum,
D.
B.,
and
Huang,
M.
H.
(2021).
Strategies
for
landslide
detection
using
open-access
synthetic
aper ture
radar
backscatter
change
in
Google
Earth
Engine.
Natur al
Hazar ds
and
Ear th
System
Sciences
Discussions,
22
,
pp.
753-773,
link
.
21.
Tseng,
G.,
Zvonko v,
I.,
Nakalembe,
C.,
Kerner ,
H.
(2021).
CropHar vest:
a
global
satellite
dataset
for
crop
type
classiﬁcation.
Neur al
Information
Pr ocessing
Systems
(NeurIPS)
Datasets
and
Benchmarks
,
link
.
22.
Lacoste,
A.,
Sher win,
E.,
Kerner ,
H.
,
Alemohammad,
H.,
Lutjens,
B.,
Irvin,
J.,
Dao,
D.,
Chang,
A.,
Gunturkun,
M.,
Drouin,
A.,
Rodriguez,
P.,
Vazquez,
D.
(2021).
Towar d
Foundation
Models
for
Earth
Monit oring:
Proposal
for
a
Climate
Change
Benchmark.
Pr oceedings
of
the
Neur al
Information
Pr ocessing
Systems
(NeurIPS)
W orkshops
,
Tackling
Climate
Change
with
AI,
link
.
23.
Shirzaei,
M.,
Khoshmanesh,
M.,
Ojha,
C.,
Werth,
S.,
Kerner ,
H.
,
Carlson,
G.,
Sherpa,
S.
F.,
Zhai,
G.,
and
Lee,
J.
Persistent
impact
of
spring
ﬂoods
on
crop
loss
in
U.S.
Midwest.
W eather
and
Climate
Extr emes,
34
,
100392,
link
.
24.
Hupper tz,
R.,
Nakalembe,
C.,
Kerner ,
H.
(2021).
Using
transf er
learning
to
study
burned
area
dynamics:
A
case
study
of
Refugee
settlements
in
West
Nile,
Northern
Uganda.
Pr oceedings
of
the
A CM/SIGKIDD
Conf er ence
on
K nowledge
Disco v er
and
Data
Mining
(KDD)
W orkshops
,
Humanitarian
Mapping,
link
.
25.
Paliyam,
M.,
Nakalembe,
C.,
Kerner ,
H.
(2021).
Street2Sat:
A
Machine
Learning
Pipeline
for
Gener ating
Ground-truth
Geo-r eferenced
Labeled
Datasets
from
Street-Le vel
Images.
Pr oceedings
of
the
International
Conf er ence
on
Machine
Learning
(ICML)
W orkshops
,
Tackling
Climate
Change
with
AI,
link
.
26.
Gray,
P.
C.,
Chamorr o,
D.
F.,
Ridge,
J.
T.,
Kerner ,
H.
R.
,
Ury,
E.
A.,
and
Johnst on,
D.
W.
Tempor ally
Gener alizable
Land
Cover
Classiﬁcation:
A
Recurr ent
Conv olutional
Neur al
Network
Unveils
Major
Coastal
Change
through
Time.
Remote
Sensing,
13(19)
,
3953,
link
.
27.
Tseng,
G.,
Kerner ,
H.
,
Nakalembe,
C.,
and
Beck er-Reshef,
I.
(2021).
Learning
to
predict
crop
type
from
heter ogeneous
sparse
labels
using
meta-learning.
P
r oceedings
of
the
IEEE
Conf er ence
on
Computer
Vision
and
P attern
Recognition
(CVPR)
W orkshops
,
EarthVision
2021,
link
.
28.
Horton,
P.,
Kerner ,
H.
,
Jacobs,
S.,
Cisner os,
E.,
Wagstaff,
K.
L.,
and
Bell
III,
J.
F.
(2021).
Integr ating
Novelty
Detection
Capabilities
with
MSL
Mastcam
Oper ations
to
Enhance
Data
Analysis.
IEEE
Aerospace
Conf erence,
Big
Sky,
Montana,
March
6-13,
link
.
29.
Lawal,
A.,
Kerner ,
H.
,
Beck er-Reshef,
I.,
Meyer,
S.
(2021).
Mapping
the
Location
and
Extent
of
2019
Prevent
Planting
Acres
in
South
Dakota
Using
Remote
Sensing
Techniques.
Remote
Sensing,
13(13)
,
2430,
link
.
30.
Tseng,
G.,
Kerner ,
H.
,
Nakalembe,
C.,
and
Beck er-Reshef,
I.
(2020).
Annual
and
in-season
mapping
of
cropland
at
ﬁeld
scale
with
sparse
labels.
Pr oceedings
of
the
Neur al
Information
Pr ocessing
Systems
(NeurIPS)
W orkshops
,
Tackling
Climate
Change
with
AI,
link
.
31.
Azari,
A.
R.,
Bierstek er,
J.
B.,
Dewe y,
R.
M.,
Doran,
G.,
Forsber g,
E.,
Harris,
C.
D.
K.,
Kerner ,
H.
R.
,
Skinner ,
K.
A.,
Smith,
A.
W.
(2020).
Integr ating
Machine
Learning
for
Planetar y
Science:
Perspectiv es
for
the
Next
Decade.
White
P aper
t o
the
NRC
Planetar y
Science
and
Astr obiology
Decadal
Sur v e y
2023-2032
,
link
.
32.
Wagstaff,
K.
L.,
Francis,
R.,
Kerner ,
H.
,
Lu,
S.,
Nerrise,
F.
(2020).
Novelty-Driv en
Onboar d
Targeting
for
Mars
Rovers.
International
Symposium
on
Ar tiﬁcial
Intelligence,
Robotics
and
A ut omation
in
Space
(i-SAIRAS)
,
link
.
33.
Hosseini,
M.,
Kerner ,
H.
,
Sahajpal,
R.,
Puricelli,
E.,
Lu,
Y-H.,
Lawal,
A.,
Humber ,
M.
L.,
Mitkish,
M.,
Meyer,
S.,
Beck er-Reshef,
I.
Evaluating
the
Impact
of
the
2020
Iowa
Derecho
on
Corn
and
Soybean
Fields
Using
Synthetic
Aper ture
Radar .
Remote
Sensing,
12(23)
,
3878,
link
.
34.
Kerner ,
H.
R.
,
Sahajpal,
R.,
Skakun,
S.,
Beck er-Reshef,
I.,
Bark er,
B.,
Hosseini,
M.
(2020).
Resilient
In-Season
Crop
Type
Classiﬁcation
in
Multispectr al
Satellite
Obser vations
using
Growth
Stage
Normalization.
Pr oceedings
of
the
A CM
SIGKDD
Conf er ence
on
K nowledge
Disco v er y
and
Data
Mining
W orkshops
,
link
.
35.
Kerner ,
H.
R.
,
Tseng,
G.,
Beck er-Reshef,
I.,
Bark er,
B.,
Munshell,
B.,
Paliyam,
M.,
Hosseini,
M.
(2020).
Rapid
Response
Crop
Maps
in
Data
Sparse
Regions.
Pr oceedings
of
the
A CM
SIGKDD
Conf er ence
on
K nowledge
Disco v er y
and
Data
Mining
W orkshops
,
link
.
36.
Kerner ,
H.
R.
,
Wagstaff,
K.
L.,
Bue,
B.
D.,
Wellingt on,
D.
F.,
Jacob,
S.,
Horton,
P.,
Bell,
J.
F.,
Kwan,
C.
Ben
Amor ,
H.
(2020).
Comparison
of
Novelty
Detection
Methods
for
Multispectr al
Images
in
Rover-Based
Planetar y
Explor ation
Missions.
Data
Mining
and
K nowledge
Disco v er y ,
34,
pp.
1642–1675,
link
.
37.
Kerner ,
H.
R.
,
Nakalembe,
C.,
Beck er-Reshef,
I.
(2020).
Field-Le vel
Crop
Type
Classiﬁcation
with
k-Near est
Neighbors:
A
Baseline
for
a
New
Kenya
Smallholder
Dataset.
Pr oceedings
of
the
International
Conf er ence
on
Learning
Repr esentations
(ICLR)
W orkshops
,
link
.
38.
Kerner ,
H.
R.
,
Hardgrove,
C.,
Czarnecki,
S.,
Gabriel,
T.
S.
J.,
Mitrofano v,
I.,
Litvak,
M.,
Sanin,
A.,
Liso v,
D.
(2020).
Analysis
of
Activ e
Neutr on
Measur ements
from
the
Mars
Science
Labor atory
Dynamic
Albedo
of
Neutr ons
Instrument:
Intrinsic
Variability ,
Outliers,
and
Implications
for
Future
Investigations.
Journal
of
Geophysical
Resear ch:
Planets,
125(5)
,
e2019JE006264,
link
.
39.
Kerner ,
H.
R.
,
Wagstaff,
K.
L.,
Bue,
B.
D.,
Gray,
P.,
Bell
III,
J.
F.,
Ben
Amor ,
H
(2019).
Deep
Learning
Methods
Towar d
Gener alized
Change
Detection
on
Planetar y
Surfaces.
Journal
of
Selected
T opics
in
Applied
Ear th
Obser v ations
and
Remote
Sensing,
12(10)
,
pp.
3900-3918,
link
.
40.
Kerner ,
H.
R.
,
Wellingt on,
D.
F.,
Wagstaff,
K.
L.,
Bell
III,
J.
F.,
Kwan,
C.,
Ben
Amor ,
H.
(2019).
Novelty
Detection
for
Multispectr al
Images
with
Application
to
Planetar y
Explor ation.
Pr oceedings
of
the
AAAI
Conf er ence
on
Ar tiﬁcial
Intelligence
,
pp.
9484-9491,
link
.
41.
Kerner ,
H.
R.
,
Ben
Amor ,
H.,
Bell
III,
J.
F.
(2018).
Context-Dependent
Image
Quality
Assessment
of
JPEG-Compr essed
Mars
Science
Labor atory
Mastcam
Images
using
Conv olutional
Neur al
Networks.
Computers
and
Geosciences,
118
,
pp.
109-121,
link
.
42.
Kwan,
C.,
Chou,
B.,
Kwan,
L.,
Larkin,
J.,
Ayhan,
B.,
Bell
III,
J.
F.,
Kerner ,
H.
R.
(2017).
Demosaicing
Enhancement
using
Pixel-Le vel
Fusion.
Signal,
Image
and
Video
Pr ocessing,
12(4)
,
pp.
749-756,
link
.
Sitemap
 
FOLLOW:
 
GITHUB
 
FEED
©
2024
Hannah
Kerner.
Powered
by
Jekyll
&
AcademicPages
,
a
fork
of
Minimal
Mistakes
.
 
Hannah
Kerner
 
Resear ch
 
Publications
 
Teaching
 
Talks
 
Resear ch
Group
 
Resour ces

Hannah
Kerner
Assistant
Professor
at
Arizona
State
University
 
Tempe,
AZ
 
Email
 
Twitter
 
LinkedIn
 
Github
 
Google
Scholar
 
ORCID
Teaching
Teaching
philosophy
For
students
to
thriv e
and
be
successful,
it
is
impor tant
for
them
to
understand
why
they
are
learning
something
and
to
feel
invested
in
learning
the
concepts
being
taught.
The
motiv ation
for
a
topic
should
come
ﬁrst,
not
as
an
after thought.
My
teaching
style
is
inter activ e
and
problem-based
as
I
encour age
students
to
engage
with
the
material
and
think
about
how
it
relates
to
things
they
are
learning
and
experiencing
outside
the
classr oom.
Students
can
ﬁnd
limitless
resour ces
to
learn
computer
science
and
AI
concepts
online.
I
feel
that
my
role
as
their
instruct or
is
to
not
just
teach
them
the
concepts
in
the
course
syllabus
but,
as
with
some
machine
learning
models,
equip
them
with
better
meta-learning
tools
that
enable
them
to
learn
new
concepts
and
tools
more
eﬃciently
in
the
futur e.
Courses
at
ASU
●
Spring
2024:
Machine
Learning
for
Remote
Sensing
(CSE
598)
●
Fall
2023:
Foundations
of
Machine
Learning
(CSE
475)
●
Spring
2023:
Data
Mining
(CSE
572)
●
Fall
2022:
Data
Mining
(CSE
572)
Courses
elsewhere
●
Spring
2022:
Open
Sour ce
GIS
(GEOG
670,
Univ ersity
of
Maryland)
●
Spring
2015:
CS
for
People
Who
Don’t
Know
CS
(Yet!)
(UNC
Chapel
Hill)
Guest
lectures
●
November
2023:
GEOG272
Introduction
to
Earth
Obser vation
Science
(Univ ersity
of
Maryland,
Instruct or:
Catherine
Nakalembe)
●
July
2023:
Introduction
to
AI
for
Agricultur e
(Climate
Change
AI
Summer
School),
[link]
●
Feb
2023:
Machine
Learning
Challenges
in
the
Real
World
(Oregon
State
Univ .,
Instruct or:
Kiri
Wagstaff )
●
Aug
2022,
2023:
Computer
Vision
for
Ecology
(CV4E)
Summer
School
(Caltech)
[
2022
link
]
●
Fall
2019,
2021,
2022:
Coding
for
Explor ation
(Ariz ona
State
Univ .,
Instruct or:
JD
Das)
●
July
2021:
PRAIRIE
AI
summer
school
(PAISS)
●
Spring
2021:
Remote
Sensing
for
Sustainable
Development
(Univ .
of
Strasbour g,
Instruct or:
Inbal
Beck er-Reshef )
●
Spring
2020:
Remote
Sensing
(UMD ,
Instruct or:
Mong-Han
Huang)
Tutorials
and
capacity
building
●
May
2023:
Machine
Learning
for
Remote
Sensing
Summit,
hosted
in
Kigali,
Rwanda
at
CMU
Africa
and
led
by
ASU,
NASA
Harvest,
and
CMU
(https:/ /nasahar vest.github.io/ml-for-r emote-sensing/iclr2023/tut orial)
●
Feb
2023:
‘Āina
Data
Stewar ds
program
kickoff,
hosted
at
Univ ersity
of
Hawaii
Maui
College
and
led
by
ASU,
UMD ,
and
NASA
Harvest/A CRES
consor tium
team
members
(https:/ /nasahar vest.github.io/mauinui#data)
●
Sep
2022:
Argentina
Multilater al
training
program
led
by
NASA
Harvest,
Univ ersity
of
Maryland
(UMD),
the
Buenos
Aires
Grain
Exchange
(Bolsa
de
Cereales),
the
Argentinian
Ministr y
of
Education,
and
the
International
Development
Bank
[article]
●
Aug
2022:
Scalable
Cropland
Mapping
Workshop,
hosted
at
Univ ersity
of
Maryland
for
Regional
Centr e
on
Mapping
Resour ces
and
Development
(RCMRD)
[agenda]
●
Jun
2022:
Machine
Learning
for
Remote
Sensing
and
Applications
in
Agricultur e
and
Food
Security
tutorial
(invited)
at
Computer
Vision
and
Pattern
Recognition
(CVPR)
[agenda]
●
Mar
2022:
AI
and
Earth
Obser vations
for
Agricultur e
tutorials
led
by
UMD
and
NASA
Harvest
team
members
at
Regional
Centr e
on
Mapping
Resour ces
and
Development
(RCMRD)
and
Rwanda
Space
Agency
(RSA)
●
Sep
2021:
Crop
Yield
Modeling
Workshop,
Regional
Center
for
Mapping
of
Resour ces
for
Development
(RCMRD)
(NASA
Harvest
and
NASA
SERVIR,
PI:
Nakalembe)
[article]
●
Aug-Dec
2021,
2022:
Course
on
remote
sensing
and
ML
technologies
for
agricultur e,
part
of
Educational
program
to
strengthen
agricultur al
and
agroindustrial
educational
establishments
of
the
Province
of
Buenos
Aires,
on
agricultur al
estimates
and
new
applied
technologies
(NASA
Harvest
and
Bolsa
de
Cereales)
[article]
Outreach
●
2020-:
From
Prison
Cells
to
Ph.D .
(ment or)
●
2019-2022:
Brooke
Owens
Fellowship
(reviewer/ment or)
●
2018-2019:
ASU
Prison
Education
Program,
Adobe
Mountain
School
(instruct or)
●
2016-2019:
Girls
Who
Code,
Maie
Bartlett
Hear d
K-8
School
(instruct or)
Educational
videos

●
Real
World:
Food
Security
-
Monit oring
Crops
from
Space
●
AskSME:
Dr.
Hannah
Kerner ,
Artiﬁcial
Intelligence
Lead,
Close-up
with
a
NASA
Subject
Matter
Exper t
Sitemap
 
FOLLOW:
 
GITHUB
 
FEED
©
2024
Hannah
Kerner.
Powered
by
Jekyll
&
AcademicPages
,
a
fork
of
Minimal
Mistakes
.
Talks
Selected
Invited
Talks
Upcoming
●
March
2024:
Machine
Learning
and
Data
Management
for
Earth
Obser vation
Workshop,
Forum
Digitale
Technologien,
Berlin
Past
●
December
2023:
Computational
Sustainability
workshop
at
NeurIPS
2023,
New
Orleans
●
October
2023:
Keynote,
Digital
Agricultur e
and
Advanced
Analytics
Symposium
(DA3),
Kansas
State
Univ ersity ,
Manhattan
●
October
2023:
Computer
Science
and
Engineering
Colloquium,
Washingt on
Univ ersity
in
St.
Louis
●
October
2023:
“Demystifying
Artiﬁcial
Intelligence:
how
it
works
and
how
it
can
beneﬁt
society .”
Sagewood
Retir ement
Community ,
Phoenix
●
April
2023:
“Artiﬁcial
Intelligence:
From
Sci-Fi
to
Societal
Good. ”
Machiner y
Dealers
National
Association
(MDN A)
Conv ention
&
Annual
Meeting,
Tucson.
●
March
2023:
“AI
for
the
Digital
Planet. ”
NSF
AI
Planning
Institute
Seminar ,
Carnegie
Mellon
Univ ersity ,
Pittsbur gh.
●
Februar y
2023:
“AI
for
the
Digital
Planet(s). ”
School
of
Earth
and
Space
Explor ation
Colloquium,
Arizona
State
Univ ersity ,
Tempe.
[recor ding]
●
December
2022:
“Suppor ting
Food
Security
in
Africa
using
Machine
Learning
and
Earth
Obser vations. ”
Machine
Learning
and
the
Physical
Sciences
Workshop,
Neur al
Information
Processing
Systems
(NeurIPS)
2022,
New
Orleans/Vir tual.
●
November
2022:
“AI
and
Earth
Obser vations
for
Global
Agricultur al
Monit oring
and
Food
Security .”
AI
Helps
Ukraine
fundr aiser
conf erence,
Virtual.
[recor ding]
●
August
2022:
“Suppor ting
Food
Security
in
Africa
using
Machine
Learning
and
Earth
Obser vations. ”
Computer
Vision
for
Ecology
Summer
School,
Caltech/Vir tual.
[recor ding]
●
June
2022:
“Street2Sat:
turning
roadside
images
into
ground-truth
labeled
datasets
for
machine
learning. ”
Agricultur eVision
workshop,
IEEE/CVF
Conf erence
on
Computer
Vision
and
Pattern
Recognition
(CVPR),
New
Orleans.
●
April
2022:
“AI
for
Earth
Obser vations
and
Food
Security—going
beyond
test
metrics. ”
AI4F oodSecurity
Awards
Ceremony
(keynote),
Virtual.
[recor ding]
●
Januar y
2022:
“Advancing
Global
Food
Security
and
Sustainable
Development
with
ML
and
Earth
Obser vations. ”
Remote
Sensing
Inter disciplinar y
Graduate
Education
Program
(IGEP)
Seminar ,
Virginia
Tech,
Blacksbur g,
VA.
●
Januar y
2022:
“Advancing
Global
Food
Security
and
Sustainable
Development
with
ML
and
Earth
Obser vations. ”
AI
Seminar ,
Oregon
State
Univ ersity ,
Virtual.
[recor ding]
●
December
2021:
“Advancing
Global
Food
Security
and
SDGs
with
Machine
Learning
and
Earth
Obser vations. ”
Computer
Science
Depar tment
Seminar ,
Univ ersity
of
Maryland,
College
Park,
MD.
●
December
2021:
“Advancing
Global
Food
Security
and
SDGs
with
Machine
Learning
and
Earth
Obser vations. ”
Forward
Summit,
Puer to
Rico
Science
Trust,
Virtual.
●
November
2021:
“Advancing
Global
Food
Security
and
SDGs
with
Machine
Learning
and
Earth
Obser vations. ”
iCube
Institute
Seminar ,
Strasbour g,
France.
●
October
2021:
“Enhancing
Global
Food
Security
with
Machine
Learning
and
Planet
Data. ”
Planet
Explor e
2021,
Virtual.
●
August
2021:
“Advancing
Global
Food
Security
and
SDGs
with
Machine
Learning
and
Earth
Obser vations. ”
NASA
Marshall
Space
Flight
Center ,
IMPACT
Tech
Talk,
Virtual.
●
July
2021:
“AI
+
EO
for
Agricultur e.”
NASA
Headquar ters
Lunch
and
Learn,
Virtual.
●
July
2021:
“Enhancing
Global
Food
Security
with
Machine
Learning
and
Earth
Obser vations. ”
PAISS
Summer
School,
Virtual.
●
May
2021:
Novelty-guided
onboar d
targeting
and
tactical
planning
for
Mars
rovers.”
Applications
of
Statistical
Methods
and
Machine
Learning
in
the
Space
Sciences,
Space
Science
Institute,
Virtual.
●
April
2021:
“Enhancing
Global
Food
Security
with
Machine
Learning
and
Planet
Data. ”
Planet
Colloquium,
Virtual.
●
March
2021:
“Mars,
Machine
Learning,
and
the
Sear ch
for
Life
beyond
Earth:
How
the
Mars
Perse verance
and
Curiosity
rovers
can
use
machine
learning
to
detect
the
unknown. ”
Ubiquity
Ventur es
Public
Event,
Virtual.
●
Februar y
2021:
“Enhancing
Global
Food
Security
with
Machine
Learning
and
Earth
Obser vations. ”
World
Resour ces
Institute
AI
for
Impact
Series,
Virtual.
●
Januar y
2021:
“The
Power
of
ML
and
EO
to
Enhance
Global
Food
Security .”
ESIP
Winter
Meeting,
Virtual.
●
December
2020:
“Eyes
in
the
sky
without
boots
on
the
ground:
Using
satellites
and
machine
learning
to
monit or
agricultur e
and
food
security
during
COVID-19. ”
NeurIPS
Workshop
on
AI
for
Earth
Science,
Virtual.
●
March
2020:
“Monit oring
Agricultur e
at
the
Field
Scale
using
Satellite
Data
and
Machine
Learning. ”
Measuring
Development
2020:
Data
Integr ation
and
Data
Fusion,
Virtual.
[recor ding]
●
Januar y
2020:
“Enhancing
Planetar y
Explor ation
Mission
Planning
and
Data
Analysis
using
Machine
Learning. ”
Solar
System
Explor ation
Division
Seminar ,
NASA
Goddar d
Space
Flight
Center ,
Greenbelt,
MD.
●
Januar y
2020:
“Machine
Learning
for
Agricultur al
Monit oring. ”
Advancing
Application
of
Machine
Learning
Tools
for
NASA’s
Earth
Obser vation
Data,
Washingt on,
DC.
●
November
2019:
“Actionable
Insights
from
Remote
Sensing
Enabled
by
Machine
Learning,
from
Earth
to
Mars. ”
International
Space
Univ ersity ,
Strasbour g,
France.
●
October
2019:
“Actionable
Insights
from
Remote
Sensing
Enabled
by
Machine
Learning,
from
Earth
to
Mars. ”
Women
in
Data
Science
at
Stanfor d
Earth,
Palo
Alto,
CA.
●
October
2019:
“Machine
Learning
for
Remote
Sensing. ”
Committee
on
Seismology
and
Geodynamics
(COSG)
Fall
Meeting,
National
Academies
of
Science,
Engineering,
and
Medicine,
Washingt on,
DC.
●
Hannah
Kerner
●
Resear ch
●
Publications
●
Teaching
●
Talks
●
Resear ch
Group
●
Resour ces
Hannah
Kerner
Assistant
Professor
at
Arizona
State
University
 
Tempe,
AZ
 
Email

 
Twitter
 
LinkedIn
 
Github
 
Google
Scholar
 
ORCID
Students
Doctoral
Students

Name
Year*
Current
Aﬃliation
Chenwei
Cui
2028
ASU
Computer
Science
(PhD)
Gedeon
Muhawenayo
2028
ASU
Computer
Science
(PhD)
Mirali
Purohit
2027
ASU
Computer
Science
(PhD)
Rahul
Nair
2027
ASU
Computer
Science
(PhD)
Gabriel
Tseng
2025
McGill/Mila
(PhD)
*
Expected
graduation
year
Masters
Students
Name
Year
*
Topic
Current
Aﬃliation
Aditya
Mohan
2024
Anomaly
detection
in
remote
sensing
ASU
CS
(MS)
Kartik
Jawanjal
2024
Object
detection
and
depth
estimation
in
street-level
images
ASU
CS
(MS)
Bhanu
Tokas
2024
Object
detection
and
depth
estimation
in
street-level
images
ASU
CS
(MS)
Keun
Park
2024
Automatic
map
feature
extraction
(DARPA
challenge)
ASU
CS
(MS)
Snehal
Chaudhari
2024
Computer
vision
for
ﬁeld
boundary
segmentation
ASU
CS
(MS)
Aninda
Ghosh
2024
Computer
vision
for
ﬁeld
boundary
segmentation
ASU
Robotics
(MS)
Manthan
Satish
2023
Computer
vision
for
ﬁeld
boundary
segmentation
ASU
Robotics
(MS)
Ivan
Zvonkov
2023
Scalable
and
deployed
ML
for
remote
sensing
UMD
CS
(MS)
*
Expected
graduation
year
Undergraduate
Students
Name
Ye
ar
*
Topic
Last
Known
Aﬃliation
Sloan
Cooney
20
24
Computer
vision
for
medical
imaging
ASU
CS
(Barrett
Honors
Thesis)
Rini
Jain
20
24
Deep
Learning
Generation
of
Digital
Elevation
Maps
ASU
CS
(Barrett
Honors
Thesis)
Anant
Rastogi
20
24
Deep
Learning
Generation
of
Digital
Elevation
Maps
ASU
CS
(Barrett
Honors
Thesis)
Nicholas
Johnson
20
24
Crop
yield
prediction
ASU
CS
(Barrett
Honors
Thesis)
Adam
Yang
20
23
Label
annotation
for
satellite
images,
land
cover
benchmarking
UMD
CS
Ekenedilichuk
wu
Ndu
20
22
Label
annotation
for
satellite
images
UMD
CS
Maryann
Vazhapilly
20
22
Addressing
dataset
shift
for
operational
ML
systems
for
crop
mapping
UMD
CS
Madhava
Paliyam
20
22
Segmentation
for
crop
type
mapping,
Street2Sat
object
detection
and
depth
estimation
Palantir
Eva
Utzschneider
20
22
Cropland
mapping
for
planted
area
change
assessment
US
Dept
of
Justice
Sophia
Owens
20
22
Label
annotation
for
satellite
images
Kamstrup
Chin-Yun
Kuei
20
22
Label
annotation
for
satellite
images
UMD
GEOG
Logan
Daytner
20
22
Label
annotation
for
satellite
images
UMD
GEOG
Arushi
Patel
20
22
Out-of-distribution
detection
for
science
applications
Bank
of
America
Kevin
Liu
20
21
Street2Sat
object
detection
and
data
visualization
UMD
CS
Bryce
Dubayah
20
21
DORA
pipeline
for
outlier
detection,
volcanic
thermal
anomalies
DeepCell
Sakshum
Kulshrestha
20
21
Novelty-guided
targeting
for
Mars
rovers
Waymo
Yao
Poudima
20
21
Cropland
mapping
in
Mali
UMD
iSchool
Bissaka
Kenah
20
21
Segmentation
for
crop
type
mapping
in
US
Midwest
Morgan
Stanley
Favour
Nerrise
20
20
Novelty-guided
targeting
for
Mars
rovers
Stanford
CS
*
Latest
year
working
with
me
High
School
Students
Name
Yea
r*
Topic
Last
Known
Aﬃliation
Vedant
Janapaty
202
3
Crop
conditions
metrics
from
satellite
observations
in
Hawaii
Silver
Creek
High
School
Saketh
Sundar
202
3
Field
boundary
delineation
in
satellite
images,
species
detection
River
Hill
High
School
Dhruv
Pai
202
2
In-season
crop
type
mapping,
WASDE
report
explorer
tool
Stanford
CS
Vinay
Raman
202
2
DORA
pipeline
for
outlier
detection
Montgomery
Blair
High
School
*
Latest
year
working
with
me
Interns
and
Postdocs
Name
Ye
ar
*
Topic
Current
Aﬃliation
Benjamin
Yeh
(Intern)
20
23
Rapid
response
with
ML
and
Earth
observations
NASA
Harvest
Abena
Boatemaa
Asare-Ansah
(Intern)
20
23
Cropland
mapping
with
ML
and
Earth
observations
NASA
Harvest
Adebowale
Daniel
Adebayo
(Intern)
20
23
Cropland
area
estimation
with
ML
and
Earth
observations
NASA
Harvest
Nils
Prieur
(Postdoc)
20
23
Computer
vision
for
automated
boulder
detection
in
satellite
images
Stanford
EPSP
NDspace
team
20
22
Depth
estimation
for
Street2Sat
using
structure
from
motion
(Deloitte
Gravity
Challenge)
n/a
Robert
Huppertz
20
21
Burned
area
mapping
near
refugee
settlements
Orbio
*
Latest
year
working
with
me
Other
thesis
committee
membership
Name
Yea
r*
Topic
Current
Aﬃliation
Saisumana
Konatam
202
4
Predicting
energy
poverty
using
ML
and
remote
sensing
UMD
CS
(undergraduate)
Madeline
Shwarz
202
6
Feature
matching
for
spatio-temporal
change
detection
ASU
SESE
(PhD)
Paul
Horton
202
3
Data
science
systems
for
planetary
science
and
astronomy
ASU
SESE
(PhD)
Matthew
Watson
202
3
Fourier
Analysis
for
Aﬃne
Transformation
Invariance
in
Neural
Networks
ASU
CS
(MS)
Yiming
Zhang
202
3
Urban
area
change
detection
in
high-resolution
satellite
images
UMD
GEOG
(PhD)
Ujjwala
Anantheswaran
202
2
Event
Detection
as
Multi-Task
Text
Generation
ASU
CS
(MS)
*
Expected
graduation
year
Masters
Capstone
Students
Name
Year*
Topic
Current
Aﬃliation
David
Adamkiewic
z
Spring
2021
Shedding
Light
on
Opaque
Statistics
of
the
World’s
Largest
Agricultural
Consumer
(China)
NGA
Ryan
McWeeney
Spring
2021
Inter-comparison
of
cropland
detection
accuracy
in
public
land
cover
products
Maxar
Amanda
Lopez
Spring
2021
Assessing
impact
of
conﬂict
on
planted
area
change
in
Tigray
Region,
Ethiopia
USDA
Adam
Martin
Fall
2021
Leveraging
Machine
Learning
to
Map
Salient
Classes
in
Jezero
Crater,
Mars
Leidos
Afolarin
Lawal
Spring
2020
Mapping
prevent
plant
acres
due
to
US
Midwest
2019
ﬂooding
Cloud
to
Street
*
Capst one
semester
and
year
Sitemap
 
FOLLOW:
 
GITHUB
 
FEED
©
2024
Hannah
Kerner.
Powered
by
Jekyll
&
AcademicPages
,
a
fork
of
Minimal
Mistakes
.
 
Hannah
Kerner
 
Resear ch
 
Publications
 
Teaching
 
Talks
 
Resear ch
Group
 
Resour ces

Hannah
Kerner
Assistant
Professor
at
Arizona
State
University
 
Tempe,
AZ
 
Email
 
Twitter
 
LinkedIn
 
Github
 
Google
Scholar
 
ORCID
Resources
Here
are
some
resour ces
I’ve
compiled
for
students
or
others
looking
to
learn
more
about
remote
sensing,
machine
learning,
etc.
Introduction
to
Earth
Obser vation
Introduction
to
Machine
Learning/Deep
Learning
–
(Mostly
Non-T echnical)
Book
Recommendations
I
read
a
lot
of
books
from
a
wide
variety
of
genr es
and
ﬁnd
huge
value
in
reading
books
that
are
not
related
to
my
resear ch/exper tise.
Here
are
my
favorite
books
from
each
year,
in
hopes
that
they
bring
you
value
and
joy
as
well:
2023
●
Demon
Copperhead
by
Barbar a
Kingsolv er
●
An
Immense
W orld:
How
Animal
Senses
Re v eal
the
Hidden
Realms
Ar ound
Us
by
Ed
Yong
●
A t omic
Habits
by
James
Clear
●
The
Comfor t
Crisis
by
Michael
Easter
●
House
of
Sticks
by
Ly
Tran
●
Maame
by
Jessica
Geor ge
●
Cloud
Cuckoo
Land
by
Anthony
Doerr
2022
●
In
Def ense
of
F ood:
An
Eater ’ s
Manif est o
by
Michael
Pollan
●
Cr ying
in
H
Mar t
by
Michelle
Zauner
●
Sapiens:
A
Brief
Hist or y
of
Humankind
by
Yuval
Noah
Harari
●
The
Undocumented
Americans
by
Karla
Cornejo
Villa vicencio
●
Br aiding
Sweetgr ass
by
Robin
Wall
Kimmer er
●
Pir anesi
by
Susanna
Clark e
●
Caste:
The
Origins
of
Our
Discontents
by
Isabel
Wilk erson
2021
●
Homegoing
by
Yaa
Gyasi
●
Ab yssinian
Chr onicles
by
Moses
Isegawa
●
Hood
F eminism
by
Mikki
Kendall
●
Deep
W ork
by
Cal
Newpor t
●
E ducated
by
Tara
Westover
●
A
Pr omised
Land
by
Barack
Obama
●
A
W oman
is
No
Man
by
Nadia
Murad
2020
●
The
Last
Girl
by
Nadia
Murad
●
The
Nick el
Bo ys
by
Colson
Whitehead
●
When
The y
Call
Y ou
a
T err orist
by
Patrisse
Khan-Cullors
and
Asha
Bandele
●
Exhalation
by
Ted
Chiang
●
The
A ut obiogr aphy
of
My
Mother
by
Jamaica
Kincaid
●
K now
My
Name
by
Chanel
Miller
●
The
W ater
Dancer
by
Ta-Nehisi
Coates
2019
●
The
Deepest
W ell
by
Nadine
Burk e-Harris
●
The
Fir e
Next
Time
by
James
Baldwin
●
Call
Me
By
Y our
Name
by
Andr e
Acimon
●
Invisible
W omen:
Data
Bias
in
a
W orld
Designed
for
Men
by
Caroline
Criado
Perez
●
The
Kindness
of
the
Hangman
by
Henr y
Oster
●
So
Y ou
W ant
t o
T alk
About
Race
by
Ijeoma
Oluo
●
Ut opia
for
Realists
by
Rutger
Bregman
Sitemap
 
FOLLOW:
 
GITHUB
 
FEED
©
2024
Hannah
Kerner.
Powered
by
Jekyll
&
AcademicPages
,
a
fork
of
Minimal
Mistakes
.
Lightweight, Pre-trained Transformers for
Remote Sensing Timeseries
Gabriel Tseng1,2Ruben Cartuyvels1,3Ivan Zvonkov4Mirali Purohit5
David Rolnick1,2Hannah Kerner5
1Mila – Quebec AI Institute
2McGill University
3KU Leuven
4University of Maryland, College Park
5Arizona State University
Abstract
Machine learning methods for satellite data have a range of societally relevant
applications, but labels used to train models can be difficult or impossible to
acquire. Self-supervision is a natural solution in settings with limited labeled data,
but current self-supervised models for satellite data fail to take advantage of the
characteristics of that data, including the temporal dimension (which is critical for
many applications, such as monitoring crop growth) and availability of data from
many complementary sensors (which can significantly improve a model’s predictive
performance). We present Presto (the Pretrained Remote Sensing Transf ormer), a
model pre-trained on remote sensing pixel-timeseries data. By designing Presto
specifically for remote sensing data, we can create a significantly smaller but
performant model. Presto excels at a wide variety of globally distributed remote
sensing tasks and performs competitively with much larger models while requiring
far less compute. Presto can be used for transfer learning or as a feature extractor
for simple models, enabling efficient deployment at scale.
1 Introduction
Machine learning is increasingly being applied to the remote sensing domain, in particular to under-
stand the evolution of the Earth’s surface over time (Brown et al., 2022; V oosen, 2020; Abys et al.,
2024; Wang et al., 2020b). These applications can have important societally beneficial outcomes,
ranging from tracking progress on sustainable development goals (Ferreira et al., 2020) to improved
weather forecasting (English et al., 2013; V oosen, 2020) to disaster management (Kansakar and
Hossain, 2016). However, labeled datasets often contain labels that are few, sparse, and unreliable
(Bressan et al., 2022), especially for under-resourced geographies, leading to poor global gener-
alization (Yifang et al., 2015; Kerner et al., 2020; Nakalembe et al., 2021). This has spurred the
investigation of self-supervised learning algorithms for remote sensing data.
Current self-supervised approaches for remote sensing data have drawn from methods in computer
vision, yielding models that treat remote sensing data as single-timestep images (Jean et al., 2019;
Manas et al., 2021; Ayush et al., 2021). Such models (i) cannot benefit from patterns that emerge
when an area is monitored over time, which is especially important for agriculture and other seasonal
landcover, (ii) typically only consider a single satellite product (such as Sentinel-2 multispectral data),
despite there being hundreds of publicly available satellite data products (GEE), (iii) are typically
large and computationally expensive (Reed et al., 2022; Cong et al., 2022; Fuller et al., 2023), making
the deployment of these models at scale challenging, and (iv) cannot natively handle the labels for
Preprint.arXiv:2304.14065v4  [cs.CV]  5 Feb 2024
Figure 1: Presto learns from structurally-masked remote sensing pixel-timeseries . We construct a
multi-sensor remote sensing pixel-timeseries, and randomly select one of the four masking strategies
described in Section 3.3. The encoder-decoder model is trained to reconstruct the original timeseries.
At fine-tuning time, we discard the decoder and only use the encoder’s output. The downstream task
may have incomplete inputs (missing timesteps or sensors) since the encoder is specifically trained on
such inputs. Presto receives both static-in-time and dynamic-in-time inputs and the location metadata
of each pixel timeseries.
many remote sensing datasets, which are points or irregularly shaped polygons (Rao et al., 2020;
Batjes et al., 2017), requiring additional methods to handle these labels (Wang et al., 2020a).
We introduce the Pretrained Remote Sensing Transf ormer (Presto), a lightweight model designed to
ingest pixel-timeseries inputs from a variety of Earth observation sensors and data products. Presto
operates on individual pixels, using the temporal and multimodal structure of the data instead of the
image structure. To learn powerful representations of remote sensing data that can be adapted to a
wide range of tasks, Presto leverages a self-supervised masked autoencoding approach, reconstructing
unobserved timepoints and sensory modalities. This allows Presto to be robust to missing data and to
flexibly accommodate diverse input formats. We find Presto excels even in image-based tasks where
the temporal dimension is completely absent.
Presto addresses the following requirements, which are critical to the useful deployment of pre-trained
models in the remote sensing context:
•Computational efficiency : When deployed, models built for remote sensing data are typically used
to make contiguous geospatial predictions over millions (or billions) of samples to form a predicted
map. The computational performance of models is therefore one of the primary considerations at
deployment time. Van Tricht (2021), Hengl et al. (2017) and Robinson et al. (2019) are all global-
or large- scale map making efforts that prioritized efficiency over accuracy when deploying remote
sensing models at scale. Presto is competitive with ViT or ResNet based models, despite having up
to1000×fewer trainable parameters and requiring orders of magnitude fewer FLOPs at inference
time.
•Ability to process inputs of varying shapes : Different downstream tasks may require very
different remote sensing inputs. For example, for crop mapping and yield estimation, Sainte
Fare Garnot et al. (2020) and You et al. (2017) discarded all spatial information in the inputs in
favor of emphasizing temporal patterns. We test Presto on a wide range of downstream inputs (for
example, with spatial information present or absent, and with single or multiple timesteps of data),
and find it is competitive with models designed specifically for those inputs.
•Ability to process a range of remote sensing datasets : For fuel moisture estimation, Rao et al.
(2020) found that the inclusion of derived products in addition to raw inputs significantly improved
performance. Presto can ingest a range of static-in-time and dynamic-in-time raw input data as well
as derived product inputs widely used in Earth observation (such as NDVI (Rouse et al., 1974)).
•Ability to handle missing data : The coverage of remote sensing products is often spatially and
temporally incomplete. For example, certain regions experience very high ( >90%) cloud coverage,
reducing the utility of optical measurements such as Sentinel-2 imagery (Sudmanns et al., 2019).
Because Presto ingests a variety of remote sensing inputs, it can leverage alternative data sources if
2
one is missing (for instance, relying on Sentinel-1, which sees through clouds, if Sentinel-2 images
are cloudy).
Our results support the surprising conclusion that a pixel-based approach can in some cases match or
outperform sophisticated computer vision-based approaches. We hypothesize that this is possible
because (i) Presto learns from many semantically dense data sources, allowing it to extract informative
patterns from pixel-timeseries, and (ii) many remote sensing tasks require significantly smaller
receptive fields than those provided by computer vision-based models. Brown et al. (2022) leveraged
such properties to train a model 100×smaller than standard models while achieving state-of-the-art
land-cover segmentation results.
2 Related Work
Architectures for Remote Sensing When processing remote sensing timeseries, transformers have
been extensively investigated either as unmodified architectures (Rußwurm and Körner, 2020) or
as architectures designed for specific tasks (Sainte Fare Garnot et al., 2020; Tarasiou et al., 2023).
Recurrent networks have also been investigated (Kerner et al., 2020; Rußwurm and Körner, 2020).
When treating remote sensing data as single or few (up to 3) timestep images, architectures from
computer vision are commonly used, ranging from ResNets (Manas et al., 2021; Ayush et al., 2021;
Rußwurm et al., 2020) to Vision Transformers (Cong et al., 2022; Reed et al., 2022; Fuller et al.,
2023).
Self-supervised learning for Remote Sensing While contrastive learning has been investigated for
remote sensing (Manas et al., 2021), recent self-supervised learning research has focused on masked
autoencoders (Yuan et al., 2022; Cong et al., 2022; Reed et al., 2022; Fuller et al., 2023). However,
these approaches (i) focus on learning from raw satellite data products (ignoring derived products such
as elevation) and typically only ingest data from a single sensor (the exception being the CROMA
model of Fuller et al. (2023), which ingests both Sentinel-1 and Sentinel-2 data), (ii) ingest very
few or no timesteps (Reed et al. (2022) and Fuller et al. (2023) ingest only one timestep while Cong
et al. (2022) ingest up to three timesteps), (iii) expect data in a certain size (for instance, ViT based
models require spatial dimensions to be present), so that missing data is not handled natively, and (iv)
generally yield larger models ranging from 2.5 million parameters (Yuan and Lin, 2020) to over 300
million parameters for ViT-based methods, making their deployment in compute-constrained settings
challenging.
3 Method
We aim to learn a model, f, which can learn useful representations in a self-supervised manner given
unlabelled remote sensing pixel-timeseries data while meeting the usability requirements outlined
in Section 1. This model can then be applied to a wide variety of downstream remote sensing tasks.
These downstream tasks may contain input data from a range of sensors with differing numbers of
timesteps.
Our approach is based on the masked autoencoding framework (He et al., 2022), in which the network
architecture includes both an encoder ( f) and a decoder ( g). During pre-training, part of the input is
masked out and the encoder embeds the remaining (non-masked) part of the input. The decoder aims
to reconstruct the masked-out part of the input, given the encoder’s output. At fine-tuning time, we
discard gand only use f(either as a feature extractor or a fine-tuneable model) for downstream tasks.
In the sections below, we discuss how Presto customizes this general framework for multi-sensor
remote sensing timeseries data. An overview of the Presto pre-training methodology is shown in
Figure 1, and full pre-training details are in Section A.1.
3.1 Pre-training Data
Self-supervised models for remote sensing must generalize to a wide range of geographies and
tasks (Lacoste et al., 2023). We therefore aimed to collect a globally representative pre-training
dataset. We followed the sampling strategy of Brown et al. (2022) to construct a dataset of 21.5M
pixel samples, each with a resolution of 10m per pixel. Appendix A.1.1 describes the pre-training
dataset construction process in detail. Presto was trained on pixel-timeseries of 12-month contiguous
3
Figure 2: Presto learns to reconstruct channels that are completely masked in a spatially
cohesive manner . In this experiment, we masked only the Sentinel-2 RGB channels; Presto was able
to reconstruct these channels even when they were absent from the input. The reconstructions are
spatially consistent even though Presto only receives single pixel inputs.
intervals, sampled from a 2-year period from the beginning of 2020 until the end of 2021, with
each month represented by one timestep (similar to the approach adopted by Tseng et al. (2021)).
Derived data products that result from the analysis of lower level data (e.g., Parkinson et al. (2006))
can significantly improve model performance (Rao et al., 2020; Hengl et al., 2017). We therefore
pre-trained Presto on a diverse set of directly-sensed and derived Earth observation products which
we pre-processed and exported using Google Earth Engine (Gorelick et al., 2017).
A pre-training batch contained several pixel-timeseries samples, each of which is a concatenation of
dynamic-in-time datapoints with each timestep representing a month (yielding T= 12 timesteps in
total). The following dynamic-in-time data products were used, yielding 15channels: (i) Sentinel-2
(S2) multispectral data, (ii) Sentinel-1 (S1) radar data, (iii) ERA5 climate reanalysis data, (iv) NDVI
(Rouse et al., 1974) derived from Sentinel-2 data and (v) land cover classes Vfrom Dynamic World.
To every pixel-timeseries we appended two static-in-time products: (i) topography data from the
SRTM digital elevation model (90m Digital Elevation Data, 2003) and (ii) location coordinates of
each pixel. Hence, one pre-training sample x, comprising a pixel-timeseries t∈[RT×15;VT×1]and
static variables s∈R1×5, is summarized as follows:
x=h
tS1
i;tS2
i;tERA5
i;tNDVI
i;tDW
i|i= 1, ...,12	
;sTG;sLoci
(1)
From now on, we use “pixel-timeseries” to refer to both the dynamic and the static variables.
3.2 Encoding and tokenization
We transformed the pixel-timeseries xinto a number of tokens (each represented by an embedding e)
to be processed by the Presto transformer. Per timestep 0≤i < T , we split the input variables into
channel groups Caccording to their type of sensor or source: e.g., the S1 bands form one channel
group. We describe these groups in more detail in Appendix A.1.3. Each real-valued channel group
represents a different sensor, native spatial resolution or (in the case of Sentinel-2 channel-groups)
region of the electromagnetic spectrum. We projected each channel group to a common latent space
of dimension deby separate learned linear projections hC: e.g., eS1
i=hS1(tS1
i). The Dynamic World
classes are categorical, so we embedded them by indexing them into an embedding matrix.
Unlike natural images in which the data and its label are self-contained, remote sensing labels are
inherently associated to a place and time on Earth (i.e., a latitude/longitude and timestamp). In
addition, while natural images contain RGB channels from the same camera sensor, Presto’s pixel-
timeseries input contains channels from multiple remote sensing instruments and data products. We
therefore wanted to communicate to the model: (i) the location of the datapoint (already present in
4
Table 1: We evaluated Presto on a wide variety of downstream tasks , including segmentation
(seg.), multi-label (ml) scene classification (class.) and regression (reg.) tasks. There is diversity in
terms of data composition, geographic area and training set size. Input shape describes the shape of a
single sample, in terms of [Height, Width, Timesteps, Channels]. We bold the temporal dimension,
to highlight time-series versus single-timestep inputs.
Dataset Task RegionInput shape
[H, W, T, C]Train
samples
CropHarvest Seg.Kenya
[1, 1, 12, 18]1,345
Brazil 203
Togo 1,319
S2-Agri 100 Class. France [5, 5, 24, 10] 1,500
TreeSatML
Class.Germany[6, 6, 1, 2]45,337[6, 6, 1, 11]
EuroSat Class. Europe[64, 64, 1, 3]21,600[64, 64, 1, 11]
Fuel Moisture Reg. USA [1, 1, 3, 19] 1,578
Algae Blooms Reg. USA [1, 1, 12, 19] 777
the input as static variable through coordinates sLoc) and a variable’s (ii) timestamp and (iii) channel
group. We did this by adding encodings to the previously described embeddings e. The complete
encoding has dimension deand contains a concatenation of positional, month, and learned channel
encodings described below.
•Positional: We used the sinusoidal positional encoding originally used by Vaswani et al. (2017).
•Month: We added an encoding representing the month being captured by each token, because we
expect timesteps from similar months to have similar features even if they are from different years.
We assign an integer to each month ranging from 0to11, yielding:
pmonth,2i= sin ((2 π×month )/12) (2)
pmonth,2i+1= cos ((2 π×month )/12) (3)
For static-in-time variables, the positional and month encodings were set to zero.
•Channel Group: Each token is associated with a set of input channels. In multispectral SatMAE
(Cong et al., 2022), a fixed encoding was used to communicate input-band information with
different channels representing different wavelengths, which is possible because only input data
from one sensor (Sentinel-2) is used. However, since Presto’s input data includes multiple remote
sensing products, we applied a learnable encoding for each channel group from the set of possible
channel groups C={S1,S2 RGB , ...,ERA5 ,TG,Loc}.
The transformer input E∈R(T·|Cdynamic|+|Cstatic|)×de(for encoder dimension de) is a concatenation of:
•Dynamic variables, for timesteps i < T and channel groups c∈ C :ec
i=hc(tc
i) +
[pc
channel ;psin(i);pmonth(i) ]
• Topographical data: eTG=hTG(sTG) + [pTG
channel ; 0; 0]
• Coordinates: eLoc=hLoc(sLoc)
3.3 Pre-training via Structured Masking
A key requirement for Presto was to perform well even with incomplete inputs (i.e., when there are
missing timesteps, channels, or both). When masking out part of the input x, we therefore tailored
the masking strategies to encourage the model to learn representations that perform well when given
a subset of bands or timesteps for downstream tasks. For a T×Dinput of Ttimesteps and Dtotal
input channels, we used the following masking techniques (illustrated in Figure 1), where Presto
considers a token to be a 1×dinput (a single timestep of dgrouped channels). The coordinates were
never masked but the static topological tokens can be.
1.Random :(t×d)masked values, with t < T andd < D
5
Table 2: Mean F1 score across all CropHarvest tasks. Presto outpeforms TIML (Tseng et al., 2022)
and MOSAIKS-1D while requiring the adaptation of far fewer parameters. The TIML and
MOSAIKS-1D model did not receive Dynamic World as input, so we measured Presto’s performance
both with and without it.
#. parameters
Model Total Adapted Mean F1
Random Forest 0.441
MOSAIKS-1D R 418K 8193 0.738
TIML 91K 91K 0.802
Presto R402K 1290.835
no DW 0.836
Figure 3: Presto is robust to incomplete inputs . We measured the AUC ROC score of Presto with
Linear probing (Presto R) on the CropHarvest dataset when no Dynamic World input is passed, and
with a subset of input months (the x-axis). We plot the performance of MOSAIKS-1D and TIML
when they receive the full 12 months of input (dashed horizontal lines) - Presto Rrecovered the
performance of these models given only a subset of input months.
2.Channel-groups :(T×d)masked values, with d < D
3.Contiguous timesteps :(t×D)masked values, t < T
4.Timesteps :(t×D)masked values, with t < T
For each training instance, we randomly sampled from the above strategies to construct a mask.
To handle both the categorical and continuous inputs we used the following loss function, which
balances the continuous and categorical losses for every batch so that each reconstructed value
receives the same weighting in the final loss: Ltotal=LMSE+λNcat
NcontLCE.LMSEis the mean squared
error reconstruction loss used for the continuous values, LCEis the cross entropy loss used for the
categorical values, Ncontis the number of masked continuous values and Ncatis the number of masked
categorical values in the batch. λis a hyperparameter, which we set to 2.
4 Experiments
In all experiments described below, we use a Presto model with identical encoder and decoder
configurations (2 attention layers with 8 heads, an embedding size of 128 and an MLP ratio of 4). We
investigated the effect of different encoder configurations in Table 8.
For downstream evaluation, we took the encoder-decoder model learned during pre-training and
discarded the decoder. As in He et al. (2022), we passed a global pool of all the encoder’s output
tokens to a downstream classifier. We evaluated the performance of three different models: Presto R,
Presto RF, and Presto FT, defined below.
6
Figure 4: We obtained per-image predictions using Presto by computing a mean and standard deviation
of Presto’s per-pixel outputs, and passing this concatenated vector to a downstream classifier. We
illustrate this for the EuroSat task.
•Feature extraction. Rolf et al. (2021) demonstrated the utility of neural networks as feature-
extractors on top of which computationally efficient classifiers could be trained. Presto Rand
Presto RFconsist respectively of linear or logistic regressions and random forests trained on
Presto’s embeddings. Since only the regression/random forest is trained, this a computationally
efficient method for adapting Presto to a wide range of tasks.
•Fine-tuning . Presto FTconsists of the Presto encoder, followed by a linear transformation of the
pooled tokens to the desired outputs. This entire model (the encoder and the linear transformation)
is fine-tuned on the training data from each evaluation task. We used a subset of the (downstream)
training data for validation.
During pre-training, we used a validation task consisting of classifying all points in the CropHarvest
dataset (Tseng et al., 2021) according to their FAO indicative crop classifications. For this validation
task, we excluded points used for evaluation (Section 5.1).
For evaluation, we compared Presto to state-of-the-art task-specific baselines (Section 5). Because
there are no other global self-supervised models for pixel-timeseries, we adapted MOSAIKS (Rolf
et al., 2021) for timeseries data by performing convolutions over the temporal rather than spatial
dimension (MOSAIKS-1D). We used the output features with random forests (MOSAIKS-1D RF)
and regressions (MOSAIKS-1D R).
5 Evaluation Tasks & Results
We evaluated Presto using six evaluation tasks spanning diverse task types, geographic locations (4
continents and 38 countries), input data modalities, and fine-tuning dataset sizes (Table 1). Whenever
possible, we benchmarked Presto against the state-of-the-art model for that task.
Applying Presto to downstream tasks is computationally efficient . While other methods require a
cluster of GPUs for fine-tuning (Cong et al., 2022), we fine-tuned Presto on a single GPU or CPU.
For the fuel moisture task described in Section 5.1, fine-tuning Presto took under 6 minutes on a 2017
MacBook Pro’s CPU. When Presto is used as a feature extractor, simple models can be trained which
require few parameters to be learned, as we show in Table 2. Even when fully fine-tuned, Presto’s
small size meant that relatively few parameters needed to be trained (Tables 5 and 6). This makes
Presto accessible to practitioners, especially those lacking significant computational resources.
Below, we describe the tasks used to evaluate Presto and discuss Presto’s performance on these tasks.
5.1 Timeseries Tasks
•Crop type Segmentation : The CropHarvest (Tseng et al., 2021) evaluation datasets consist of
binary pixel classification of (i) maize in Kenya, (ii) coffee in Brazil and (iii) cropland in Togo. We
compared Presto to the baselines provided by CropHarvest and to Task-Informed Meta-Learning
(TIML, Tseng et al., 2022), which achieved state-of-the-art results on these datasets.
7
Table 3: RMSE results on the regression tasks. The literature baselines are not directly comparable,
since they use different input datasets or private test data (or both). Rao et al. (2020) reported an
RMSE of 25 on the fuel moisture dataset with a physics-assisted neural network and the algae bloom
competition winner reported an RMSE of 0.761, indicating our results are within the scope of utility.
Best results are highlighted blue , with second best results in bold . Models have a high variance in
performance across tasks, so we calculated the mean difference in RMSE from the linear regression
baseline across both tasks. Presto performed most consistently, both when used as a feature-extractor
and when fine-tuned.
Fuel
MoistureAlgae
BloomsMean
difference
Linear Regression 28.20 0.850 0%
Random Forest 23.84 1.249 15.7%
MOSAIKS-1D RF 28.75 0.972 8.15%
Presto FT(random init.) 26.07 0.955 2.40%
Presto FT 25.28 0.815−7.24%
Presto RF 25.98 0.884−1.94%
Table 4: Results on the TreeSatAI dataset. We compared Presto to the dataset’s benchmark models.
The MLPs contain 3 layers (with 563K-723K parameters respectively) and are tuned for this task. We
froze the Presto encoder’s 402k parameters and trained a random forest on its outputs with default
scikit-learn hyperparameters.
Weighted Micro
Model Data F1 mAP F1 mAP
MLP
S110.09 29.42 12.82 33.09
LightGBM 11.86 32.79 14.07 35.11
Presto RF 38.34 35.45 40.79 38.64
MLP
S251.97 64.19 54.59 65.83
LightGBM 48.17 61.99 52.52 61.66
Presto RF 55.29 61.53 58.29 63.31
•Fuel Moisture : The live fuel moisture dataset (Rao et al., 2020) measures live fuel moisture content
in the Western U.S. Rao et al. (2020)’s baseline used 5-fold cross validation to evaluate model
performance; for future comparability, we used a single geographically separated test set (a test set
covering a different geographic area than the training set).
•Algae Blooms : The algae blooms dataset (alg, 2023) measures the severity of cyanobacterial algal
blooms in different parts of the U.S. We used the subset of the dataset in the Midwestern U.S. The
dataset was originally released as part of a competition, so the test data is not available. In addition,
competitors could download many Earth observation datasets to train their models, making direct
comparisons to competition results difficult. Since the competition’s winning solution used a tree-
based method, we benchmarked against a regression and a random forest using a geographically
separated test set.
5.1.1 Timeseries Results
Presto excels at timeseries tasks, significantly outperforming the state-of-the-art for CropHarvest
(Table 2) and outperforming all baselines for the regression tasks (Table 3).
We found that Presto is performant when passed only a subset of timesteps compared to the 12
timesteps used for pre-training. Presto remained performant when receiving only 3 input timesteps
for the fuel moisture task (Table 3). We also evaluated Presto when a subset of input months are
passed for the CropHarvest dataset (Figure 3). Using a subset of the 12 months, Presto surpassed the
performance of TIML and MOSAIKS-1D which used all input months.
8
Figure 5: EuroSat accuracy of a kNN@5 classifier given pre-trained model embeddings at a variety
of input resolutions (following Reed et al. (2022)) as a function of FLOPs required to encode an
image (note the log scale on the x-axes). All image-based models resized images to 224×224,
so the FLOPs required to encode an image do not change with image resolution. Presto achieved
competitive results with image-based models while requiring up to four orders of magnitude less
FLOPs to encode an image.
Presto is also robust to the removal of input channels. On the CropHarvest dataset (Table 2), Presto
remained performant without the Dynamic World input, showing a negligible difference in mean F1
score compared to the full input.
5.2 Image-based Tasks
Presto is designed to ingest single pixel-timeseries. When one prediction is required for a set of pixels
(as for image-based tasks and the Image-Timeseries tasks in Section 5.3), we used the following
approach to obtain per-image predictions from Presto’s pixel outputs (Figure 4): (i) we encoded
the pixels in an image individually, yielding N output tokens, (ii) we calculated the mean and
standard deviation of these N output tokens per dimension and concatenated the result, yielding a
2de-dimensional vector (where deis Presto’s output token size, or 128), and (iii) we passed this mean
and standard deviation vector to a downstream classifier.
•TreeSatAI : The TreeSatAI dataset consists of detecting the presence of one or more tree species
(out of 20 possible species) in forestry images in Germany (Ahlswede et al., 2023). We used the
train and test splits provided by Ahlswede et al. (2023) and compared Presto to the deep learning
and tree-based baselines provided with the dataset. As done for the baselines, we evaluated models
using only Sentinel-2 (S2) or Sentinel-1 (S1) data.
•EuroSAT : The EuroSAT dataset classifies Sentinel-2 multispectral images in Europe with one of
10 landcover classes (Helber et al., 2019). We used the train and test splits provided by Neumann
et al. (2019). We compared Presto to SatMAE, ConvMAE and ScaleMAE using a k Nearest
Neighbors (kNN) classifier at a variety of input resolutions, as was done by Reed et al. (2022).
We also compared fine-tuned Presto against Seasonal Contrast (SeCo) (Manas et al., 2021) and
Geography-Aware Self-Supervised Learning (GASSL) (Ayush et al., 2021). EuroSAT provides
all multispectral Sentinel-2 bands, but most other models ingest only RGB images. We evaluated
Presto both when it received all multispectral bands as input (MS) and when it only received the
RGB bands.
5.2.1 Image-based Results
Despite being pre-trained on pixel-timeseries data, Presto is competitive on single-timestep image
datasets against much larger models . We followed the setup of Reed et al. (2022) in measuring the
performance of a kNN-classifier on Presto’s output embeddings for the EuroSat dataset at varying
resolutions. Presto achieved comparable average accuracy (over all image resolutions) to larger
ViT-based models with RGB data and significantly outperformed these models with multispectral
(MS) data (Figure 5), while requiring orders of magnitude less compute to encode the images in both
cases and for any resolution.
Presto is performant even when only a small subset of input channels are available compared to
the pre-training channels. For the EuroSAT task (Table 5), Presto received either the full Sentinel-2
9
Table 5: EuroSAT finetuning accuracy. Presto is the only backbone that can handle both MS and
RGB inputs (separate SatMAE models are trained for RGB and MS inputs). We reported Presto
results for full resolution; results at reduced resolutions are in Table 11.
Backbone InputsParams
(M)Accuracy
GASSL ResNet-18 RGB 11.69 0.895
SeCo ResNet-18 RGB 11.69 0.931
SatMAE ViT-Large RGB 303.10 0.955
SatMAE ViT-Large MS 305.96 0.990
Random
init.PrestoRGB0.400.745
MS 0.924
Presto PrestoRGB0.400.849
MS 0.953
Table 6: Results on the S2-Agri 100dataset. We followd (Yuan et al., 2022) in reporting overall
accuracy (OA), Kappa Cohen score ( κ) and macro-F 1score. All results are an average of 3 runs -
standard errors are reported in Table 16.
Params
(M)Pre
Trained?OA κ F1
SITS
Former2.565.13 0.55 42.12
✓ 67.03 0.56 42.83
Presto 0.445.98 0.35 27.45
✓ 68.89 0.58 40.41
input or only RGB bands (which represent only a single token, since only one timestep is available).
Similarly, we evaluated Presto when it receives either Sentinel-2 or Sentinel-1 data for the TreeSatAI
task (Table 4). In both cases, Presto was competitive with methods designed to ingest single-timestep,
single-sensor data.
5.3 Image-Timeseries Tasks
•S2-Agri 100: The S2-Agri dataset (Sainte Fare Garnot et al., 2020) classifies crop types in agricul-
tural parcels. We used a variant of S2-Agri (S2-Agri 100) developed by Yuan et al. (2022) for the
SITS-Former model in which 100 parcels for each crop type are used for training and validation
respectively (all other parcels are used for testing), and a 5×5pixel patch from each parcel is
used for input. We benchmarked Presto against both the pre-trained and randomly initialized
SITS-Former model.
5.3.1 Image-Timeseries Results
The S2-Agri 100dataset consists of 24 timesteps at 10 to 30 day intervals (compared to Presto’s
pre-training data, which consists of 12-month timeseries). Presto remained performant on this dataset,
achieving comparable results with SITS-Former despite having 6×fewer parameters (shown in Table
6). This shows that Presto can ingest timeseries at different temporal resolutions and at varying
intervals .
In addition, the S2-Agri dataset is missing pixel location metadata, which is always passed to Presto
during pre-training. S2-Agri was sampled from a single S2-tile, so we used the location of the central
pixel of this tile for all pixels in the dataset. Even with this much less accurate location metadata,
Presto remained performant.
10
Table 7: Structured masking strategies yield the best downstream performance . We measured
Presto R’s F1 score on the CropHarvest validation task. Combining structured strategies outperformed
the “Random” masking employed by (He et al., 2022).
Channel
GroupsRandom TimestepsContiguous
TimestepsF1
Score
✓ 0.646
✓ 0.653
✓ 0.664
✓ 0.649
✓ ✓ ✓ ✓ 0.665
5.4 Ablations
We conducted three ablations to better understand Presto’s performance:
•Structured masking strategies perform best : Table 7 shows results from ablating the masking
strategies. Unlike other masked autoencoder methods (He et al., 2022), we found that combining
structured masking with random masking outperforms random masking alone.
•Pre-training Presto is critical to achieve strong performance : In Tables 3, 5 and Table 6, we
compared the performance of a randomly -initialized Presto architecture with the pre-trained model.
Pre-training yielded a significant increase in performance (a 50% increase in accuracy on the
S2-Agri 100dataset). Even when the downstream training dataset size was large (EuroSat has
21,600 training samples), pre-training yielded a 14% increase in accuracy given RGB inputs and
up to 22% increase in accuracy at lower resolutions (Table 11). For TreeSatAI with S1 data (Table
15), a randomly initialized model slightly outperformed the pre-trained model. We hypothesize
that this is due to the difference in input relative to the pre-training data, since the TreetSatAI input
consists of a single image from only one timestep and one channel group.
•Presto’s performance scales with model size : To measure how different model sizes affect Presto’s
performance, we pre-trained two larger Presto variants: a deeper variant with 4 encoder layers
instead of 2, and a wider variant with a doubled encoder size (Table 8). Performance improved
as model size increased, suggesting that practitioners who can afford greater computational costs
could obtain better results by training a larger Presto model.
6 Discussion & Conclusion
Limitations Presto is designed to ingest 10m/px resolution imagery and is pre-trained on products
at this scale. This decision is motivated by the free, global availability over time of products at
this scale (such as Sentinel-1 and Sentinel-2). Presto does not natively process very-high resolution
imagery such as <1m/px imagery from commercial satellites or drones, which can be costly and
often lack complete coverage globally and temporally. In addition, Presto is a pixel-timeseries model.
While we demonstrated Presto’s flexibility on single-timestep image datasets, image-based models
may be preferred if a user’s goal is to process entire images to make a prediction. We observed that
Presto’s performance on the EuroSAT dataset plateaued as the input resolution increased (Table 5),
due to images from classes where the relevant pixels for the class are a minority of the pixels in the
image (e.g., highways). In such scene classification challenges, image-based models which can learn
the shape of the relevant pixels may be better suited. We discuss this further in Section A.6.
Conclusion We present Presto: a lightweight, pre-trained timeseries transformer for remote sensing.
By leveraging structure unique to remote sensing data—specifically, (i) an important temporal
dimension, (ii) associated metadata and (iii) a diversity of sensors, we are able to train an extremely
lightweight model which achieves state-of-the-art results in a wide variety of globally distributed
evaluation tasks. Computational efficiency is of paramount importance in remote sensing settings
and often determines which models ultimately get selected for deployment. We demonstrated that
strong performance can be achieved while meeting this constraint, and that self-supervised learning
can provide significant benefits even for small models.
11
Table 8: Effect of model size on validation performance . To understand the effect of model size
on performance, we pre-train two larger variants of Presto. As in Table 7, we measure Presto R’s
performance on the CropHarvest validation task. The number of parameters includes both the encoder
and decoder parameters. The FLOPS are computed for a “full” input (12 timesteps, with no missing
channels), when passed through the encoder and decoder.
Depth Width# params
(M)FLOPs
(M)F1
score
2 128 0.81 88.94 0.665
2 256 2.02 220.81 0.687
4 128 1.21 132.42 0.669
Impact statement
Machine learning applications to remote sensing have a wide range of societally beneficial outcomes,
ranging from tracking progress on sustainable development goals (Ferreira et al., 2020) to improved
weather forecasting (English et al., 2013; V oosen, 2020) to disaster management (Kansakar and
Hossain, 2016).
Presto is designed to be accessible to a wide range of practitioners; we achieve this by only training
Presto on publicly available data and by keeping the model size small enough so it can be leveraged
in compute-constrained environments. In addition to increasing Presto’s accessibility, its small size
also lowers its carbon footprint (Strubell et al., 2019).
As described by Tuia et al. (2023), a natural concern when applying machine learning algorithms to
remote sensing data is its use to collect information about individuals who are unaware that data is
being collected, and therefore cannot consent to this practice. We therefore encourage deployment
of Presto in collaboration with local communities and stakeholders (Krafft; Kshirsagar et al., 2021;
Nakalembe and Kerner, 2023).
Acknowledgements
This work was supported by NASA under the NASA Harvest Consortium on Food Security and
Agriculture (Award #80NSSC18M0039). This research was enabled in part by compute resources
provided by Mila (mila.quebec); in addition, we acknowledge material support from NVIDIA
Corporation in the form of computational resources. We thank Esther Rolf and Caleb Robinson for
reviewing drafts of this manuscript.
References
Earth engine data catalogue. https://developers.google.com/earth-engine/datasets/
catalog . Accessed: 2023-01-31.
Tick tick bloom: Harmful algal bloom detection challenge.
https://www.drivendata.org/competitions/143/tick-tick-bloom/page/649/, 2023. Accessed:
2023-03-10.
S. 90m Digital Elevation Data. The CGIAR consortium for spatial information, 2003.
C. Abys, S. Skakun, and I. Becker-Reshef. Two decades of winter wheat expansion and intensification
in russia. Remote Sensing Applications: Society and Environment , 2024.
S. Ahlswede, C. Schulz, C. Gava, P. Helber, B. Bischke, M. Förster, F. Arias, J. Hees, B. Demir, and
B. Kleinschmit. Treesatai benchmark archive: A multi-sensor, multi-label dataset for tree species
classification in remote sensing. Earth System Science Data , 2023.
K. Ayush, B. Uzkent, C. Meng, K. Tanmay, M. Burke, D. Lobell, and S. Ermon. Geography-aware
self-supervised learning. In CVPR , 2021.
N. H. Batjes, E. Ribeiro, A. Van Oostrum, J. Leenaars, T. Hengl, and J. Mendes de Jesus. Wosis:
providing standardised soil profile data for the world. Earth System Science Data , 2017.
12
V . Böhm, W. J. Leong, R. B. Mahesh, I. Prapas, E. Nemni, F. Kalaitzis, S. Ganju, and R. Ramos-
Pollan. Sar-based landslide classification pretraining leads to better segmentation. In Artificial
Intelligence for Humanitarian Assistance and Disaster Response Workshop at NeurIPS , 2022.
P. O. Bressan, J. M. Junior, J. A. C. Martins, M. J. de Melo, D. N. Gonçalves, D. M. Freitas, A. P. M.
Ramos, M. T. G. Furuya, L. P. Osco, J. de Andrade Silva, et al. Semantic segmentation with
labeling uncertainty and class imbalance applied to vegetation mapping. International Journal of
Applied Earth Observation and Geoinformation , 2022.
C. F. Brown, S. P. Brumby, B. Guzder-Williams, T. Birch, S. B. Hyde, J. Mazzariello, W. Czerwinski,
V . J. Pasquarella, R. Haertel, S. Ilyushchenko, K. Schwehr, M. Weisse, F. Stolle, C. Hanson,
O. Guinan, R. Moore, and A. M. Tait. Dynamic world, near real-time global 10 m land use land
cover mapping. Scientific Data , Jun 2022.
Y . Cong, S. Khanna, C. Meng, P. Liu, E. Rozi, Y . He, M. Burke, D. B. Lobell, and S. Ermon.
SatMAE: Pre-training transformers for temporal and multi-spectral satellite imagery. In A. H.
Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, NeurIPS , 2022. URL https://openreview.
net/forum?id=WBhqzpF6KYH .
S. Di Tommaso, S. Wang, V . Vajipey, N. Gorelick, R. Strey, and D. B. Lobell. Annual field-
scale maps of tall and short crops at the global scale using gedi and sentinel-2. arXiv preprint
arXiv:2212.09681 , 2022.
S. English, T. McNally, N. Bormann, K. Salonen, M. Matricardi, A. Moranyi, M. Rennie,
M. Janisková, S. Di Michele, A. Geer, et al. Impact of satellite data, 2013.
B. Ferreira, M. Iten, and R. G. Silva. Monitoring sustainable development by means of earth
observation data and machine learning: A review. Environmental Sciences Europe , 2020.
A. Fuller, K. Millard, and J. R. Green. CROMA: Remote sensing representations with contrastive
radar-optical masked autoencoders. In Thirty-seventh Conference on Neural Information Process-
ing Systems , 2023. URL https://openreview.net/forum?id=ezqI5WgGvY .
P. Gao, T. Ma, H. Li, Z. Lin, J. Dai, and Y . Qiao. Convmae: Masked convolution meets masked
autoencoders. arXiv preprint arXiv:2205.03892 , 2022.
N. Gorelick, M. Hancher, M. Dixon, S. Ilyushchenko, D. Thau, and R. Moore. Google earth engine:
Planetary-scale geospatial analysis for everyone. Remote sensing of Environment , 2017.
M. C. Hansen, P. V . Potapov, R. Moore, M. Hancher, S. A. Turubanova, A. Tyukavina, D. Thau, S. V .
Stehman, S. J. Goetz, T. R. Loveland, et al. High-resolution global maps of 21st-century forest
cover change. Science , 2013.
K. He, X. Chen, S. Xie, Y . Li, P. Dollár, and R. Girshick. Masked autoencoders are scalable vision
learners. In CVPR , 2022.
P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: A novel dataset and deep learning
benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied
Earth Observations and Remote Sensing , 2019.
T. Hengl, J. Mendes de Jesus, G. B. Heuvelink, M. Ruiperez Gonzalez, M. Kilibarda, A. Blagoti ´c,
W. Shangguan, M. N. Wright, X. Geng, B. Bauer-Marschallinger, et al. Soilgrids250m: Global
gridded soil information based on machine learning. PLoS one , 2017.
N. Jean, S. Wang, A. Samar, G. Azzari, D. Lobell, and S. Ermon. Tile2vec: Unsupervised representa-
tion learning for spatially distributed data. In AAAI , 2019.
P. Kansakar and F. Hossain. A review of applications of satellite earth observation data for global
societal benefit and stewardship of planet earth. Space Policy , 2016.
H. Kerner, G. Tseng, I. Becker-Reshef, C. Nakalembe, B. Barker, B. Munshell, M. Paliyam, and
M. Hosseini. Rapid response crop maps in data sparse regions. In ACM SIGKDD Conference on
Data Mining and Knowledge Discovery Workshops , 2020.
A. Krafft. ASU researcher combats food insecurity with AI. https://news.asu.edu/20230303-solutions-
asu-researcher-combats-food-insecurity-ai. Accessed: 2023-09-21.
M. Kshirsagar, C. Robinson, S. Yang, S. Gholami, I. Klyuzhin, S. Mukherjee, M. Nasir, A. Ortiz,
F. Oviedo, D. Tanner, et al. Becoming good at ai for good. In AAAI/ACM Conference on AI, Ethics,
and Society , 2021.
13
A. Lacoste, N. Lehmann, P. Rodriguez, E. D. Sherwin, H. Kerner, B. Lütjens, J. A. Irvin, D. Dao,
H. Alemohammad, A. Drouin, et al. Geo-bench: Toward foundation models for earth monitoring.
arXiv preprint arXiv:2306.03831 , 2023.
O. Manas, A. Lacoste, X. Giró-i Nieto, D. Vazquez, and P. Rodriguez. Seasonal contrast: Unsuper-
vised pre-training from uncurated remote sensing data. In CVPR , 2021.
C. Nakalembe and H. Kerner. Considerations for ai-eo for agriculture in sub-saharan africa. Environ-
mental Research Letters , 2023.
C. Nakalembe, C. Justice, H. Kerner, C. Justice, and I. Becker-Reshef. Sowing seeds of food security
in africa. Eos (Washington. DC) , 102, 2021.
M. Neumann, A. S. Pinto, X. Zhai, and N. Houlsby. In-domain representation learning for remote
sensing. arXiv preprint arXiv:1911.06721 , 2019.
C. Parkinson, A. Ward, and M. King. Earth science reference handbook. National Aeronautics and
Space Administration: Washington, DC, USA , 2006.
C. Pelletier, G. I. Webb, and F. Petitjean. Temporal convolutional neural network for the classification
of satellite image time series. Remote Sensing , 2019.
K. Rao, A. P. Williams, J. F. Flefil, and A. G. Konings. Sar-enhanced mapping of live fuel moisture
content. Remote Sensing of Environment , 2020.
C. J. Reed, R. Gupta, S. Li, S. Brockman, C. Funk, B. Clipp, S. Candido, M. Uyttendaele, and
T. Darrell. Scale-mae: A scale-aware masked autoencoder for multiscale geospatial representation
learning. arXiv preprint arXiv:2212.14532 , 2022.
C. Robinson, L. Hou, K. Malkin, R. Soobitsky, J. Czawlytko, B. Dilkina, and N. Jojic. Large scale
high-resolution land cover mapping with multi-resolution data. In CVPR , 2019.
E. Rolf, J. Proctor, T. Carleton, I. Bolliger, V . Shankar, M. Ishihara, B. Recht, and S. Hsiang. A
generalizable and accessible approach to machine learning with global satellite imagery. Nature
communications , 2021.
J. W. Rouse, R. H. Haas, J. A. Schell, D. W. Deering, et al. Monitoring vegetation systems in the
great plains with erts. NASA Spec. Publ , 351(1):309, 1974.
M. Rußwurm and M. Körner. Self-attention for raw optical satellite time series classification. ISPRS
journal of photogrammetry and remote sensing , 2020.
M. Rußwurm, S. Wang, M. Korner, and D. Lobell. Meta-learning for few-shot land cover classification.
InCVPR Workshops , pages 200–201, 2020.
V . Sainte Fare Garnot, L. Landrieu, S. Giordano, and N. Chehata. Satellite image time series
classification with pixel-set encoders and temporal self-attention. CVPR , 2020.
E. Strubell, A. Ganesh, and A. McCallum. Energy and policy considerations for deep learning in
nlp. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .
Association for Computational Linguistics, 2019.
M. Sudmanns, D. Tiede, H. Augustin, and S. Lang. Assessing global sentinel-2 coverage dynamics
and data availability for operational earth observation (eo) applications using the eo-compass.
International journal of digital earth , 2019.
M. Tarasiou, E. Chavez, and S. Zafeiriou. ViTs for SITS: Vision Transformers for Satellite Image
Time Series. In CVPR , 2023.
G. Tseng, I. Zvonkov, C. L. Nakalembe, and H. Kerner. Cropharvest: A global dataset for crop-type
classification. In NeurIPS, Datasets and Benchmarks Track , 2021. URL https://openreview.
net/forum?id=JtjzUXPEaCu .
G. Tseng, H. Kerner, and D. Rolnick. TIML: Task-informed meta-learning for crop type mapping. In
AI for Agriculture and Food Systems at AAAI , 2022.
D. Tuia, K. Schindler, B. Demir, G. Camps-Valls, X. X. Zhu, M. Kochupillai, S. Džeroski, J. N.
van Rijn, H. H. Hoos, F. Del Frate, et al. Artificial intelligence to advance earth observation: a
perspective. arXiv preprint arXiv:2305.08413 , 2023.
K. Van Tricht. Mapping crops at global scale! what works and what doesn’t? https://blog.vito.
be/remotesensing/worldcereal-benchmarking , 2021. Accessed: 2023-07-31.
14
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. NeurIPS , 2017.
P. V oosen. Europe builds ‘digital twin’of earth to hone climate forecasts, 2020.
S. Wang, W. Chen, S. M. Xie, G. Azzari, and D. B. Lobell. Weakly supervised deep learning for
segmentation of remote sensing imagery. Remote Sensing , 2020a.
S. Wang, S. Di Tommaso, J. M. Deines, and D. B. Lobell. Mapping twenty years of corn and soybean
across the us midwest using the landsat archive. Scientific Data , 2020b.
B. Yifang, P. Gong, and C. Gini. Global land cover mapping using earth observation satellite data:
Recent progresses and challenges. ISPRS journal of photogrammetry and remote sensing , 2015.
J. You, X. Li, M. Low, D. Lobell, and S. Ermon. Deep gaussian process for crop yield prediction
based on remote sensing data. Proceedings of the AAAI Conference on Artificial Intelligence , 2017.
Y . Yuan and L. Lin. Self-supervised pretraining of transformers for satellite image time series
classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing ,
14:474–487, 2020.
Y . Yuan, L. Lin, Q. Liu, R. Hang, and Z.-G. Zhou. Sits-former: A pre-trained spatio-spectral-temporal
representation model for sentinel-2 time series classification. International Journal of Applied
Earth Observation and Geoinformation , 106:102651, 2022.
15
A Appendix
Reproducibility
All code and data used to train and evaluate Presto will be made available upon publication, and
the code is currently available at https://github.com/nasaharvest/presto . In addition, we
discuss specific implementation details in Appendices A.1 and A.4. We have strived to make the
Presto codebase accessible to other practitioners; to this end, we include a demo Jupyter notebook
demonstrating how Presto can be applied to a new downstream task, which is available at https:
//github.com/nasaharvest/presto/blob/main/downstream_task_demo.ipynb .
A.1 Pre-training details
We outline training hyperparameters below:
•Training length : We train the model for 20 epochs, with a batch size of 4096 (resulting in 5950
batches per epoch). On a single NVIDIA V100 GPU, this takes 431
4hours.
•Optimizer and learning rate : We train the model with an AdamW optimizer. We use a cosine
annealing schedule for our learning rate, with a maximum learning rate of 0.001 at the 2ndepoch.
We apply a weight decay of 0.05, and βs of (0.9, 0.95).
•Masking : We use a masking ratio of 0.75, randomly selecting (for each instance) a masking
strategy from the ones described in Section 3.3. If the masking strategy cannot mask the right
number of tokens, we randomly mask additional tokens to achieve the correct masking ratio.
A.1.1 Pre-training data
Figure 6: The distribution of the pre-training dataset described in Section 3.1.
Remote sensing models can be deployed in a wide range of geographies, with few labelled datapoints
available at fine-tuning time (Kerner et al., 2020; Böhm et al., 2022). We therefore aim to collect
a globally representative pre-training dataset. We achieve this by following the sampling strategy
used by Dynamic World (Brown et al., 2022). We divide the Earth into three regions: the Western
Hemisphere and two regions in the Eastern Hemisphere. These regions are further divided into
ecoregions, and stratified samples are gathered from each region using land cover classes as sampling
strata. Figure 6 shows the resulting geographical distribution. Each sample represents a 510×510
pixel tile with a spatial resolution of 10 meter per pixel. To obtain pixel-timeseries we grid-sample
2,500 pixels from each sample, yielding a total of 21,535,000 pixel samples (each with 24 one-month
timesteps).
A.1.2 Input data
We leverage the following data products when pre-training Presto:
16
Table 9: Model sizes and FLOPs required to encode a single EuroSat image (or pixel, for Presto), as
measured by the thop library. When plotting results in Table 5, we multiply the FLOPs for Presto by
the number of pixels encoded for an image. At its highest resolution, EuroSAT images are 64×64,
so Presto FLOPs for a full resolution image can be obtained by multiplying the per-pixel FLOPs by
4,096. We include this value in brackets for completeness.
Model Backbone Params (M) MegaFlops
SatMAE (RGB) (Cong et al., 2022) ViT-Large 303.10 59,685.69
SatMAE (MS) (Cong et al., 2022) ViT-Large 305.96 535,515.25
ScaleMAE (Reed et al., 2022) ViT-Large 303.10 59,685.69
ConvMAE (Gao et al., 2022) ConvMAE-Large 88.78 23,315.58
SeCo (Manas et al., 2021) ResNet-18 11.69 149.37
GASSL (Ayush et al., 2021) ResNet-18 11.69 149.37
Presto RGB pixel (image) Presto 0.40 0.79 (3,235.84)
Presto MS pixel (image) Presto 0.40 2.37 (9,707.52)
•Sentinel-1 Synthetic Aperture Radar observations (S1): The VV (emit and receive at vertical
polarization) and VH (emit at vertical and receive at horizontal polarization) bands: 2 real-valued
dynamic values per monthly timestep.
•Sentinel-2 Multispectral images (S2): We removed the 60m resolution bands, yielding bands
with 10m and 20m resolution with channels in the visible, near-infrared and short-wave infrared
range: 10 real-valued dynamic values per timestep.
•ERA5 Climate Reanalysis Meteorological data (ERA5): Monthly total precipitation and temper-
ature at 2 metres above the ground: 2 real-valued dynamic values per timestep.
•NDVI (Rouse et al., 1974): Computed from the red (B4) and near-infrared (B8) Sentinel-2 bands:
1 real-valued dynamic value per timestep.
•Dynamic World Land Cover classes (DW, Brown et al., 2022): Land cover classes produced for
every non-cloudy Sentinel-2 image: 1 dynamic categorical value from the set of possible classes V
per timestep. We took the mode of classes for all timesteps within a month.
•Topography data (TG), from the Shuttle Radar Topography Mission’s Digital Elevation Model:
The elevation and slope of each pixel, real-valued and static in time.
•Coordinates (Loc): 3D static in time Cartesian coordinates computed from the latitude and longi-
tude of the pixel’s geographical location: sLoc= [cos( lat)×cos(lon),cos(lat)×sin(lon),sin(lat)].
A.1.3 Channel Groups
As described in Section 3.2, we transform the pixel timeseries xinto a number of tokens, where each
token is a linear transformation of a subset of the input channels. We group together channels which
(i) come from the same sensor or product, (ii) have equivalent native spatial resolutions and (iii)
represent similar parts of the electromagnetic spectrum (for Sentinel-2 channel groups). We group
the input data into the following channel groups:
•Sentinel-1 : The VV and VH bands from the Sentinel-1 sensor
•Sentinel-2 RGB : The B2, B3 and B4 bands from the Sentinel-2 sensor
•Sentinel-2 Red Edge : The B5, B6 and B7 bands from the Sentinel-2 sensor
•Sentinel-2 Near Infra Red (10m) : The B8 band from the Sentinel-2 sensor
•Sentinel-2 Near Infra Red (20m) : The B8A band from the Sentinel-2 sensor
•Sentinel-2 Short Wave Infra Red : The B11 and B12 bands from the Sentinel-2 sensor
•NDVI : The normalized difference vegetation index, calculated from the Sentinel-2 B4 and B8
bands.
•ERA5 Climatology : Precipitation and temperature at 2m from the ERA5 Climate Reanalysis
product
•Topography : The elevation and slope of a pixel, calculated by the SRTM’s DEM
•Location : The cartesian coordinates of a pixel, computed from the pixel’s latitude and longitude
17
Table 10: Full results for regression tasks from Table 3, including standard error computed from three
runs.
Fuel Moisture Algae Blooms Mean difference
Linear Regression 28.20 0.850 0%
Random Forest 23.84±0.42 1.249±0.02 15.7%
MOSAIKS-1D RF 28.75±0.15 0 .972±0.01 8.15%
Presto FT(random init.) 26.07±0.52 0 .955±0.05 2.40%
Presto FT 25.28±0.30 0 .815±0.03 −7.24%
Presto RF 25.98±0.66 0 .884±0.01 −1.94%
A.2 FLOP calculations
We use the thop library ( https://github.com/Lyken17/pytorch-OpCounter ) to calculate the
FLOPs required to encode a EuroSAT image (as plotted in Table 5(b)). For the SatMAE, ScaleMAE
and ConvMAE models, all images were resized to 224×224, so the FLOPs required to encode
an image is independent of resolution. For Presto, we computed the FLOPs required to encode a
single pixel and multiplied this by the number of pixels in an image at each resolution (e.g. the
“64” resolution has 64×64pixels, so we multiply the FLOPs required to encode a single pixel by
64×64 = 4096 ). The FLOPs calculated by the thop library are recorded in Table 9.
A.3 Baselines
In addition to task-specific baselines, we benchmark Presto against:
•Random Forests : Random forests are powerful baselines in remote sensing as they they remain
competitive with state-of-the-art methods (Pelletier et al., 2019; Kerner et al., 2020). Tree-based
methods, especially random forests, are commonly deployed in large-scale machine learning for
remote sensing applications (Hansen et al., 2013; Van Tricht, 2021; Di Tommaso et al., 2022).
•MOSAIKS-1D : We adapt MOSAIKS (Rolf et al., 2021) for timeseries data. MOSAIKS-1D uses
patches from the pre-training dataset and convolves over the temporal dimension instead of the
spatial dimension. We benchmark MOSAIKS-1D on all timeseries evaluation tasks. Because this
does not work for categorical inputs, we exclude Dynamic World. As with Presto, we use the
output features with random forests (MOSAIKS-1D RF) and with regressions (MOSAIKS-1D R).
A.4 Downstream Results
We include complete results for the evaluation tasks. These include error bars, as well as additional
results reported for the CropHarvest (Table 12 and Figure 3), regression tasks (Table 10), EuroSAT
(Tables 11, 13 and 14), TreeSatAI (Table 15) and Sen2-Agri 100(Table 16) datasets.
We run all downstream classifiers with 3 seeds ( 0,42,84), with the exception of the kNN classifiers
and the linear regression (which are deterministic). In the tables in the main paper (Tables 2, 4, 6 and
3) we report the average of these runs; the standard error is reported in Tables 12,15, 16 and 10.
•Presto as a feature extractor : When used as a feature extractor, a random forest, regression of
K-nearest-neighbours classifier is trained on Presto’s output embeddings. In this case, we use
scikit-learn models with the default hyperparameters. For the CropHarvest tasks, the class labels
are extremely imbalanced; we therefore set class_weight equal to balanced for those tasks, for
both Presto and MOSAIKS-1D.
•Fine-tuning Presto : When fine-tuning Presto, we use the same hyperparameters across all tasks:
an AdamW optimizer with a learning rate of 3e-4 and weight decay of 0.05.
As discussed in Section 5.2, we obtain per-image predictions using Presto by computing a mean and
standard deviation of Presto’s output pixels, and passing a concatenation of these two vectors to a
downstream classifier. This is illustrated in Figure 4.
18
Table 11: Accuracy results for pre-trained and from-scratch Presto when fine-tuned on EuroSat, at
varying resolutions. We hypothesize that the drop in performance for the full resolution (64) RGB
input is due to the model construction; the model outputs for all pixels in the image (4,096 pixels
for the full resolution) are aggregated and passed to a linear layer for classification, yielding a noisy
gradient signal.
Resolution 2 4 8 16 32 64
random init.RGB0.703±0.005 0 .684±0.032 0 .694±0.013 0 .739±0.004 0 .750±0.018 0 .745±0.009
pre-trained 0.792±0.010 0 .837±0.006 0 .847±0.016 0 .865±0.006 0 .872±0.002 0 .849±0.004
random init.MS0.837±0.014 0 .884±0.010 0 .895±0.006 0 .907±0.13 0 .924±0.005 0 .924±0.003
pre-trained 0.898±0.005 0 .925±0.004 0 .939±0.000 0 .950±0.002 0 .958±0.001 0 .953±0.004
Table 12: Additional results for the CropHarvest task. In addition to the F1 scores reported in the
main paper, we report AUC ROC scores, with standard error bars computed with three runs.
Model Kenya Brazil Togo Mean
F1Random Forest 0.559±0.003 0 .000±0.000 0 .756±0.002 0.441
MOSAIKS-1D R 0.790±0.027 0 .746±0.084 0 .679±0.024 0.738
TIML 0.838±0.000 0 .835±0.012 0 .732±0.002 0 .802
Presto R 0.816±0.000 0.891±0.000 0 .798±0.000 0.835
no DW 0.861±0.000 0.888±0.000 0 .760±0.000 0.836
AUC ROCRandom Forest 0.578±0.006 0 .941±0.004 0 .892±0.001 0.803
MOSAIKS-1D R 0.693±0.036 0 .890±0.038 0 .836±0.005 0.806
TIML 0.794±0.003 0 .988±0.001 0 .890±0.000 0 .890
Presto R 0.834±0.000 0.997±0.000 0 .921±0.000 0.917
no DW 0.863±0.000 0.989±0.000 0 .912±0.000 0.921
A.5 Disentangling the effect of pre-training
To understand the effect of pre-training Presto, we fine-tune Presto and train it from scratch on
EuroSat (Table 5), the regression tasks (Table 3 in the main paper) and TreeSatAI (Table 15). We
omit the CropHarvest dataset because it was expressly designed as a few-shot-learning dataset. Its
small size makes the construction of validation sets with which to control the finetuning (e.g. with
early stopping) challenging.
Overall, we find a consistent and significant improvement from the use of pre-trained Presto compared
to a randomly initialized version of the model. For the EuroSat task, pre-training consistently delivers
an incresse in accuracy score >0.1(representing increases in accuracy of up to 25%). This effect is
consistent with what we observe on the TreeSatAI dataset for S2 data and on the regression tasks
(where pre-training reduces RMSE by to 15% on the algae blooms task). For the TreeSatAI dataset
with S1 data, pre-training penalizes the model compared to random initialization - we hypothesize
that this is due to the difference in input (a single timestep and single channel group image) relative
to the pre-training data. The benefit of pre-training effect is especially pronounced on the S2-Agri 100
dataset; we hypothesize this is due to the small training set size.
A.6 Presto’s failure modes
Presto processes pixel-timeseries independently, without spatial context from other pixels or locations.
This means that when we make image-based predictions (such as for scene classification), Presto’s
independent pixel representations must be aggregated into a single prediction. We opt for a simple
concatenation of the element-wise mean and standard deviation of the representations, from which
a classifier makes a prediction. Information gets lost in such a simple aggregation, which impacts
Presto’s performance on such tasks.
19
Table 13: Additional results for the EuroSat task - results for the ScaleMAE, SatMAE and ConvMAE
models are from (Reed et al., 2022). We report kNN classifier results for different values of k, and at
varying input resolutions.
Resolution 16 32 64
k 5 20 100 5 20 100 5 20 100
SatMAE 0.729 0.727 0.695 0.871 0.876 0.854 0.934 0.931 0.913
ScaleMAE 0.751 0.744 0.699 0.912 0.901 0.869 0.960 0.956 0.935
ConvMAE 0.835 0.826 0.788 0.909 0.898 0.863 0.947 0.940 0.914
Presto (RGB) 0.869 0.828 0.713 0.869 0.829 0.712 0.869 0.829 0.713
Presto (MS) 0.916 0.892 0.844 0.920 0.892 0.846 0.921 0.893 0.846
Table 14: Additional results for the EuroSat task for Presto when run with reduced resolutions
(compared to those used by (Reed et al., 2022) and reported in Table 13). We report kNN classifier
results for different values of k, and at varying input resolutions.
Resolution 2 4 8
k 5 20 100 5 20 100 5 20 100
Presto (RGB) 0.843 0.811 0.699 0.860 0.820 0.706 0.869 0.826 0.710
Presto (MS) 0.873 0.852 0.799 0.895 0.874 0.824 0.911 0.886 0.838
Table 15: Additional results for the TreeSatAI (as in (Ahlswede et al., 2023), we report precision
and recall in addition to F1score and mAP). In addition, we report the results of finetuning Presto
(Presto FT) from the pre-trained weights and from a random initialization.
Model Data Aggregation F1 mAP Precision Recall
MLP
S1Weighted10.09 29 .42 33 .29 7 .13
LightGBM 11.86 32 .79 37 .96 8 .06
Presto FT(random init.) 40.36±0.77 39 .77±0.79 30 .69±0.82 64 .69±1.09
Presto FT 38.69±0.78 37 .41±0.58 30 .09±0.74 61 .20±0.85
Presto RF 38.34±0.07 35 .45±0.03 29 .67±0.07 57 .23±0.06
MLP
Micro12.82 33 .09 63 .01 7 .13
LightGBM 14.07 35.11 55.49 8 .06
Presto FT(random init.) 42.04±0.73 43 .00±0.80 31 .20±1.00 64 .69±1.09
Presto FT 41.65±0.46 40 .75±0.69 31 .58±0.47 61 .20±0.85
Presto RF 40.79±0.04 38 .64±0.02 31 .69±0.03 57 .23±0.06
MLP
S2Weighted51.97 64 .19 74 .59 42 .23
LightGBM 48.17 61 .99 74 .27 40 .04
Presto FT(random init.) 52.74±0.50 57 .24±0.64 45 .87±1.17 64 .29±1.51
Presto FT 53.63±0.42 59 .16±1.24 47 .15±1.40 65 .11±3.21
Presto RF 55.29±0.08 61 .53±0.09 56 .93±0.07 58 .56±0.09
MLP
Micro54.49 65 .83 77 .18 42 .23
LightGBM 52.52 61 .66 76 .27 40 .04
Presto FT(random init.) 52.56±0.41 58 .08±0.66 44 .56±1.03 64 .29±1.51
Presto FT 53.31±0.18 59 .77±1.13 45 .51±1.46 65 .11±3.21
Presto RF 58.29±0.06 63 .31±0.06 58 .04±0.05 58 .56±0.09
20
Table 16: Full results on the S2-Agri 100dataset, including standard errors obtained from 3 runs.
To obtain standard errors for the SITS-Former, we run the official code ( https://github.com/
linlei1214/SITS-Former ) with 3 seeds. Best results are highlighted .
Params (M) Pre-trained? OA κ F1
SITS
Former2.565.13±3.01 0.55±0.03 42.12±0.52
✓ 67.03±2.24 0.56±0.02 42.83±0.30
Presto 0.445.98±2.74 0.35±0.02 27.45±0.64
✓ 68.89±1.05 0.58±0.01 40.41±0.25
Figure 7: Accuracy of kNN@5 classifier with Presto RGB representations on the EuroSat dataset vs.
the input resolution, for different categories. Some categories have been left out for clarity.
(a) Forest
 (b) Annual Crop
 (c) Highway
 (d) River
Figure 8: the RGB bands of example images from EuroSat classes.
For example, Presto’s performance on the EuroSat dataset reaches a plateau when increasing the
input resolution. As Figure 7 shows, this is mainly caused by a failure to accurately predict specific
classes (for example, the Highway andRiver classes). Figure 8 shows example images for these
classes, as well as for the Forest andAnnualCrop classes, on which Presto achieves higher accuracies.
While in the Forest andAnnualCrop images, most pixels of the image actually represent the labelled
class, in the Highway andRiver images only a relatively small part of the image actually contains the
label (a highway or river). We hypothesize that since many pixels in the Highway andRiver images
do not actually represent that class, the crude token-aggregation method we use to represent images
is insufficiently discriminative to accurately classify these images.
Other pre-trained remote sensing models use much more powerful mechanisms for aggregating
spatial information. For example, ViT models convolve over patches and then apply an attention
mechanism between spatial patches. If image-based predictions are needed and these predictions are
highly dependent on the occurrence of objects in subregions of the image, models which natively
process this important spatial information may be better suited.
We plan on exploring techniques to mitigate this difficulty with Presto in future work.
21
Geography-Aware Self-Supervised Learning
Kumar Ayush*
Stanford UniversityBurak Uzkent*
Stanford UniversityChenlin Meng*
Stanford UniversityKumar Tanmay
IIT Kharagpur
Marshall Burke
Stanford UniversityDavid Lobell
Stanford UniversityStefano Ermon
Stanford University
Abstract
Contrastive learning methods have signiﬁcantly nar-
rowed the gap between supervised and unsupervised learn-
ing on computer vision tasks. In this paper, we explore their
application to geo-located datasets, e.g. remote sensing,
where unlabeled data is often abundant but labeled data
is scarce. We ﬁrst show that due to their different char-
acteristics, a non-trivial gap persists between contrastive
and supervised learning on standard benchmarks. To close
the gap, we propose novel training methods that exploit the
spatio-temporal structure of remote sensing data. We lever-
age spatially aligned images over time to construct tempo-
ral positive pairs in contrastive learning and geo-location
to design pre-text tasks. Our experiments show that our
proposed method closes the gap between contrastive and
supervised learning on image classiﬁcation, object detec-
tion and semantic segmentation for remote sensing. More-
over, we demonstrate that the proposed method can also be
applied to geo-tagged ImageNet images, improving down-
stream performance on various tasks. Project Webpage can
be found at this link geography-aware-ssl.github.io.
1. Introduction
Inspired by the success of self-supervised learning meth-
ods [3, 13], we explore their application to large-scale re-
mote sensing datasets (satellite images) and geo-tagged nat-
ural image datasets. It has been recently shown that self-
supervised learning methods perform comparably well or
even better than their supervised learning counterpart on im-
age classiﬁcation, object detection, and semantic segmenta-
tion on traditional computer vision datasets [21, 10, 13, 3,
2]. However, their application to remote sensing images is
largely unexplored, despite the fact that collecting and la-
*Equal Contribution. Contact: {kayush, buzkent, chen-
lin}@cs.stanford.edubeling remote sensing images is particularly costly as anno-
tations often require domain expertise [37, 38, 36, 16, 5].
In this direction, we ﬁrst experimentally evaluate the per-
formance of an existing self-supervised contrastive learning
method, MoCo-v2 [13], on remote sensing datasets, ﬁnding
a performance gap with supervised learning using labels.
For instance, on the Functional Map of the World (fMoW)
image classiﬁcation benchmark [5], we observe an 8% gap
in top 1 accuracy between supervised and self-supervised
methods.
To bridge this gap, we propose geography-aware con-
trastive learning to leverage the spatio-temporal structure
of remote sensing data. In contrast to typical computer vi-
sion images, remote sensing data are often geo-located and
might provide multiple images of the same location over
time. Contrastive methods encourage closeness of represen-
tations of images that are likely to be semantically similar
(positive pairs). Unlike contrastive learning for traditional
computer vision images where different views (augmenta-
tions) of the same image serve as a positive pair, we pro-
pose to use temporal positive pairs from spatially aligned
images over time. Utilizing such information allows the
representations to be invariant to subtle variations over time
(e.g., due to seasonality). This can in turn result in more
discriminative features for tasks focusing on spatial vari-
ation, such as object detection or semantic segmentation
(but not necessarily for tasks involving temporal variation
such as change detection). In addition, we design a novel
unsupervised learning method that leverages geo-location
information, i.e., knowledge about where the images were
taken. More speciﬁcally, we consider the pretext task of
predicting where in the world an image comes from, similar
to [11, 12]. This can complement the information-theoretic
objectives typically used by self-supervised learning meth-
ods by encouraging representations that reﬂect geograph-
ical information, which is often useful in remote sensing
tasks [31]. Finally, we integrate the two proposed methods
1arXiv:2011.09980v7  [cs.CV]  8 Mar 2022
Figure 1: Left shows the original MoCo-v2 [3] framework. Right shows the schematic overview of our approach.
into a single geography-aware contrastive learning objec-
tive.
Our experiments on the functional Map of the World [5]
dataset consisting of high spatial resolution satellite im-
ages show that we improve MoCo-v2 baseline signiﬁcantly.
In particular, we can improve the accuracy on target ap-
plications utilizing image recognition [5], object detec-
tion [39, 1], and semantic segmentation [46]. In particular,
we improve it by∼8%classiﬁcation accuracy when testing
the learned representations on image classiﬁcation, ∼2%
AP on object detection, ∼1%mIoU on semantic segmen-
tation, and∼3%top-1 accuracy on land cover classiﬁca-
tion ˙Interestingly, our geography-aware learning can even
outperform the supervised learning counterpart on temporal
data classiﬁcation by ∼2%. To further demonstrate the ef-
fectiveness of our geography-aware learning approach, we
extract the geo-location information of ImageNet images
using FLICKR API similar to [7], which provides us with
a subset of 543,435 geo-tagged ImageNet images. We ex-
tend the proposed approaches to geo-located ImageNet, and
show that geography-aware learning can improve the per-
formance of MoCo-v2 by ∼2%on image classiﬁcation,
showing the potential application of our approach to any
geo-tagged dataset. Figure 1 shows our contributions in de-
tail.
2. Related Work
Self-supervised methods use unlabeled data to learn rep-
resentations that are transferable to downstream tasks ( e.g.
image classiﬁcation). Two commonly seen self-supervised
methods are pre-text task andcontrastive learning .
Pre-text tasks Pre-text task based learning [22, 41, 29, 49,
43, 28] can be used to learn feature representations when
data labels are not available. [9] rotates an image and then
trains a model to predict the rotation angle. [48] trains a
network to perform colorization of a grayscale image. [26]
represents an image as a grid, permuting the grid and then
predicting the permutation index. In this study, we use geo-location classiﬁcation as a pre-text task, in which a deep
network is trained to predict a coarse geo-location of where
in the world the image might come from.
Contrastive Learning Recent self-supervised contrastive
learning approaches such as MoCo [13], MoCo-v2 [3], Sim-
CLR [2], PIRL [22], and FixMatch [32] have demonstrated
superior performance and have emerged as the fore-runner
on various downstream tasks. The intuition behind these
methods are to learn representations by pulling positive
image pairs from the same instance closer in latent space
while pushing negative pairs from difference instances fur-
ther away. These methods, on the other hand, differ in the
type of contrastive loss, generation of positive and negative
pairs, and sampling method.
Although growing rapidly in self-supervised learning
area, contrastive learning methods have not been explored
on large-scale remote sensing dataset. In this work, we pro-
vide a principled and effective approach for improving rep-
resentation learning using MoCo-v2 [13] for remote sensing
data as well geo-located conventional datasets.
Unsupervised Learning in Remote Sensing Images Un-
like in traditional computer vision areas, unsupervised
learning on remote sensing domain has not been studied
comprehensively. Most of the studies utilize small-scale
datasets speciﬁc to a small geographical region [4, 17, 30,
15, 19], a few classes [35, 25] or a highly-speciﬁc modal-
ity, i.e. hyperspectral images [23, 47]. Most of these
studies focus on the UCM-21 dataset [45] consisting of
less than 1,000 images from 21 classes. A more recent
study [36] proposes large-scale weakly supervised learn-
ing using a multi-modal dataset consisting of satellite im-
ages and paired geo-located wikipedia articles. While be-
ing effective, this method requires each satellite image to
be paired to its corresponding article, limiting the number
of images that can be used.
Geography-aware Computer Vision Geo-location data
has been studied extensively in prior works. Most of these
2
"gsd":                            "img_width":                        "img_height":                       "country_code":                   "cloud_cover":                           “timestamp": 2.10264849663 2421 2165 IND 6 2015-11-02 T05:44:14Z2.06074237823 2410 2156 IND 0 2016-03-09 T05:25:30Z1.9968634 2498 2235 IND 1 2017-02-02 T05:47:02Z2.2158575 2253 2015 IND 0 2017-02-27 T05:24:30Z1.24525177479 4016 3592 IND 0 2015-04-09 T05:36:04Z1.4581833 3400 3041 IND 2 2016-12-28 T05:57:06Z1.2518295 4003 3581 IND 0 2017-04-12 T05:51:49ZFigure 2: Images over time concept in the fMoW dataset. The metadata associated with each image is shown underneath.
We can see changes in contrast, brightness, cloud cover etc. in the images. These changes render spatially aligned images
over time useful for constructing additional positives.
-45.829337,
170.730800,
Te Matai,
New Zealand,
Otago10.595525,
76.041355,
Guruvayoor ,
India,
Kerala,
Thrissur29.008740,
-81.963415,
Lake W eir,
Florida,Marion
United States28.318038,
-81.540688,
Celebration,
United States,
Florida,Osceola21.881018,
-102.275360,
Aguascalientes,
México,
Aguascalientes,
Aguascalientes59.61 1778,
-151.449966,
Homer ,
United States,
Alaska,
Kenai 
Peninsula Borough37.345989,
-77.645680,
Beach,
United States,
Virginia,
Chesterﬁeld37.303518,
-121.897773,
San Jose,
United States,
California,
Santa Clara22.494622,
114.028447,
⼤浪,
⾹港,
新界,
元朗區27.876676,
-82.798231,
Walsingham,
United States,
Florida,
Pinellas
-18.646245,
24.614868,
Botswana,
Chobe
Figure 3: Some examples from GeoImageNet dataset. Below each image, we list their latitudes, longitudes, city, country
name. In our study, we use the latitude and longitude information for unsupervised learning. We recommend readers to
zoom-in to visualize the details of the pictures.
Figure 4: Left The histogram of number of views. Right the
histogram of standard deviation in years per area in fMoW
dataset.
studies utilizes geo-location of an image as a prior to im-
prove image recognition accuracy [33, 14, 24, 20, 6]. Other
studies [44, 11, 12, 42] use geo-tagged training datasets to
learn how to predict the geo-location of previously unseen
images at test time. In our study, we leverage geo-tag infor-
mation to improve unsupervised and self-supervised learn-
ing methods.
3. Problem Deﬁnition
We consider a geo-tagged visual dataset
{((x1
i,···,xTi
i),lati,loni)}N
i=1, where the ith data-point consists of a sequence of images Xi= (x1
i,···,xTi
i)
at a shared location, with latitude and longitude equal
to lati,lonirespectively, over time ti= 1,...,Ti. When
Ti>1, we refer to the dataset to have temporal information
or structure. Although temporal information is often not
available in natural image datasets ( e.g. ImageNet), it is
common in remote sensing. While the temporal structure is
similar to that of conventional videos, there are some key
differences that we exploit in this work. First, we consider
relatively short temporal sequences, where the time differ-
ence between two consecutive “frames” could range from
months to years. Additionally unlike conventional videos
we consider datasets where there is no viewpoint change
across the image sequence.
Given our setup, we want to obtain visual representa-
tionszti
iof imagesxti
isuch that the learned representation
can be transferred to various downstream tasks. We do not
assume access to any labels or human supervision beyond
the lati,lonigeo-tags. The quality of the representations
is measured by their performance on various downstream
tasks. Our primary goal is to improve the performance
of self-supervised learning by utilizing the geo-coordinates
and the unique temporal structure of remote sensing data.
3
3.1. Functional Map of the World
Functional Map of the World (fMoW) is a large-scale
publicly available remote sensing dataset [5] consisting of
approximately 363,571 training images and 53,041 test im-
ages across 62 highly granular class categories. It provides
images (temporal views) from the same location over time
(x1
i,···,xTi
i)as well as geo-location metadata (lati,loni)
for each image. Fig. 4 shows the histogram of the number
of temporal views in fMoW dataset. We can see that most of
the areas have multiple temporal views where Tican range
from 1 to 21, and on average there is about 2.5-3 years of
difference between the images from an area. Also, we show
examples of spatially aligned images in Fig. 2. As seen in
Fig. 5, fMoW is a global dataset consisting of images from
seven continents which can be ideal for learning global re-
mote sensing representations.
3.2. GeoImageNet
Following [7], we extract geo-coordinates for a sub-
set of images in ImageNet [8] using the FLICKR API.
More speciﬁcally, we searched for geo-tagged images
in ImageNet using the FLICKR API, and were able to
ﬁnd 543,435 images with their associated coordinates
(lati,loni)across 5150 class categories. This dataset is
more challenging than ImageNet-1k as it is highly imbal-
anced and contains about 5 ×more classes. In the rest of
the paper, we refer to this geo-tagged subset of ImageNet as
GeoImageNet .
We show some examples from GeoImageNet in Fig. 3.
As shown in the ﬁgure, for some images we have geo-
coordinates that can be predicted from visual cues. For ex-
ample, we see that a picture of a person with a Sombrero
hat was captured in Mexico. Similarly, an Indian Elephant
picture was captured in India, where there is a large popula-
tion of Indian Elephants. Next to it, we show the picture of
an African Elephant (which is larger in size). If a model is
trained to predict where in the world the image was taken,
it should be able to identify visual cues that are transferable
to other tasks (e.g., visual cues to differentiate Indian Ele-
phants from the African counterparts). Figure 5 shows the
distribution of images in the GeoImageNet dataset.
4. Method
In this section, we brieﬂy review contrastive loss func-
tions for unsupervised learning and detail our proposed ap-
proach to improve Moco-v2 [3], a recent contrastive learn-
ing framework, on geo-located data.
4.1. Contrastive Learning Framework
Contrastive [13, 2, 3, 34, 27] methods attempt to learn
a mappingfq:xt
i↦→zt
i∈Rdfrom raw pixels xt
ito
semantically meaningful representations zt
iin an unsuper-
Figure 5: Topshows the distribution of the fMoW and Bot-
tom shows the distribution of GeoImageNet.
vised way. The training objective encourages representa-
tions corresponding to pairs of images that are known a pri-
ori to be semantically similar (positive pairs) to be closer
to each other than typical unrelated pairs (negative pairs).
With similarity measured by dot product, recent approaches
in contrastive learning differ in the type of contrastive loss
and generation of positive and negative pairs. In this work,
we focus on the state-of-the-art contrastive learning frame-
work MoCo-v2 [3], an improved version of MoCo [13], and
study improved methods for the construction of positive and
negative pairs tailored to remote sensing applications.
The contrastive loss function used in the MoCo-v2
framework is InfoNCE [27], which is deﬁned as follows for
a given data sample:
Lz=−logexp(z·ˆz/λ)
exp(z·ˆz/λ) +∑N
j=1exp(z·kj/λ),(1)
wherezandˆzare query and key representations obtained
by passing the two augmented views of xt
i(denotedvand
v′in Fig. 1) through query and key encoders, fqandfkpa-
rameterized by θqandθkrespectively. Here zandˆzform
a positive pair. The Nnegative samples, {kj}N
j=1, come
from a dictionary of representations built as a queue. We
refer readers to [13] for details on this. λ∈R+is the tem-
perature hyperparameter.
The key idea here is to encourage representations of pos-
itive (semantically similar) pairs to be closer, and negative
4
(semantically unrelated) pairs to be far apart as measured
by dot product. The construction of positive and negative
pairs plays a crucial role in this contrastive learning frame-
work. MoCo and MoCo-v2 both use perturbations (also
called “data augmentation”) from the same image to create
a positive example and perturbations from different images
to create a negative example. Commonly used perturbations
include random color jittering, random horizontal ﬂip, and
random grayscale conversion.
2016-04-17T15:49:27Z2012-11-21T15:17:29Z2016-11-10T16:00:51Z2016-11-10T16:00:51Z2011-06-06T15:56:51Z
Figure 6: Demonstration of temporal positives in eq. 2. An
image from an area is paired to the other images includ-
ing itself from the same area captured at different time. We
show the time stamps for each image underneath the im-
ages. We can see the color changes in the stadium seatings
and surrounding areas.
Temporal Positive Pairs Different from many commonly
seen natural image datasets, remote sensing datasets of-
ten have extra temporal information, meaning that for a
given location (lati,loni), there exists a sequence of spa-
tially aligned images Xi= (x1
i,···,xTi
i)over time. Unlike
in traditional videos where nearby frames could experience
large changes in content ( e.g. from a cat to a tree), in re-
mote sensing the content is often more stable across time
due to the ﬁxed viewpoint. For instance, a place on ocean
is likely to remain as ocean for months or years, in which
case satellite images taken across time at the same location
should share high semantic similarities. Even for locations
where non-trivial changes do occur over time, certain se-
mantic similarities could still remain. For instance, key fea-
tures of a construction site are likely to remain the same
even as the appearance changes due to seasonality.
Given these observations, it is natural to leverage tempo-
ral information for remote sensing while constructing pos-
itive or negative pairs since it can provide us with extra
semantically meaningful information of a place over time.
More speciﬁcally, given an image xt1
icollected at time t1,
we can randomly select another image xt2
ithat is spatially
aligned with xt1
i(i.e.xt2
i∈Xi). We then apply perturba-
tions ( e.g. random color jittering) as used in MoCo-v2 to the
spatially aligned image pair xt1
iandxt2
i, providing us with
atemporal positive pair (denotedvandv′in Figure 1) that
can be used for training the contrastive learning frameworkby passing them through query and key encoders, fqandfk
respectively (see Fig. 1). Note that when t1=t2, the tem-
poral positive pair is the same as the positive pair used in
MoCo-v2.
Given a data sample xt1
i, our TemporalInfoNCE objec-
tive function can be formulated as follows:
Lzt1
i=−logexp(zt1
i·zt2
i/λ)
exp(zt1
i·zt2
i/λ) +N∑
j=1exp(zt1
i·kj/λ),(2)
wherezt1
iandzt2
iare the encoded representations of the
randomly perturbed temporal positive pair xt1
i,xt2
i. Similar
as equation 1, Nis number of negative samples, {kj}N
j=1
are the encoded negative pairs and λ∈R+is the temper-
ature hyperparameter. Again, we refer readers to [13] for
details on construction of these negative pairs.
Note that compared to equation 1, we use two real im-
ages from the same area over time to create positive pairs.
We believe that relying on real images for positive pairs en-
courages the network to learn better representations for real
data than the one relying on synthetic images. On the other
hand, our objective in equation 2 enforces the representa-
tions to be invariant to changes over time. Depending on
the target task, such inductive bias can be desirable or unde-
sirable. For example, for a change detection task, learning
representations that are highly sensitive to temporal changes
can be more preferable. However, for image classiﬁcation
or object detection task, learning temporally invariant fea-
tures should not degrade the downstream performance.
4.2. Geo-location Classiﬁcation as a Pre-text Task
In this section, we explore another aspect of remote sens-
ing images, geolocation metadata , to further improve the
quality of the learned representations. In this direction, we
design a pre-text task for unsupervised learning. In our
pre-text task, we cluster the images in the dataset using
their coordinates (lati,loni). We use a clustering method
to construct Kclusters and assign an area with coordinates
(lati,loni)a categorical geo-label ci∈C ={1,···,K}.
Using the cross entropy loss function, we then optimize a
geo-location predictor network fcas
Lg=K∑
k=1−p(ci=k) log(ˆp(ci=k|fc(xt
i)), (3)
whereprepresent the probability of the cluster k represent-
ing the true cluster and ˆprepresents the predicted probabili-
ties forKclusters. In our experiments, we represent fcwith
a CNN parameterized by θc. With this objective, our goal is
to learn location-aware representations that can potentially
transfer well to different downstream tasks.
5
4.3. Combining Geo-location and Contrastive
Learning Losses
In the previous section, we designed a pre-text task lever-
aging the geo-location meta-data of the images to learn
location-aware representations in a standalone setting. In
this section, we combine geo-location prediction and con-
trastive learning tasks in a single objective to improve the
contrastive learning-only and geo-location learning-only
tasks. In this direction, we ﬁrst integrate the geo-location
learning task into the contrastive learning framework using
the cross-entropy loss function where the geo-location pre-
diction network uses features zt
ifrom the query encoder as
Lg=−K∑
i=1p(ci=k) log(ˆp(ci=k|fc(zt
i)). (4)
In the combined framework (see Fig. 1), fcis represented
by a linear layer parameterized by θc. Finally, we propose
the objective for joint learning as the linear combination of
TemporalInfoNCE and geo-classiﬁcation loss with coefﬁ-
cientsαandβrepresenting the importance of contrastive
learning and geo-location learning losses as
arg min
θq,θk,θcLf=αLzt1+βLg. (5)
By combining two tasks, we learn representations to
jointly maximize agreement between spatio-temporal pos-
itive pairs, minimize agreement between negative pairs and
predict the geo-label of the images from the positive pairs.
5. Experiments
In this study, we perform unsupervised representation
learning on fMoW and GeoImageNet datasets. We then
evaluate the learned representations/pre-trained models on
a variety of downstream tasks including image recognition,
object detection and semantic segmentation benchmarks on
remote sensing and conventional images.
Figure 7: Left shows the number of clusters per label and
Right shows the number of unique labels per cluster in
fMoW and GeoImageNet. Labels represent the original
classes in fMoW and GeoImageNet.Implementation Details for Unsupervised Learning For
contrastive learning , similar to MoCo-v2 [3], we use
ResNet-50 to paramaterize the query and key encoders,
fqandfk, in all experiments. We use following hyper-
parameters in the MoCo-v2 pre-training step: learning rate
of 1e-3, batch size of 256, dictionary queue of size 65536,
temperature scaling of 0.2 and SGD optimizer. We use sim-
ilar setups for both fMoW and GeoImageNet datasets. Fi-
nally, for each downstream experiment, we report results for
the representations learned after 200 epochs.
For geo-location classiﬁcation task , we run k-Means
clustering algorithm to cluster fMoW and GeoImageNet
intoK= 100 geo-clusters given their latitude and longi-
tude pairs. We show the clusters in Fig. 8. As seen in the ﬁg-
ure, while both datasets have similar clusters there are some
differences, particularly in North America and Europe. In
Fig. 7 we analyze the clusters in GeoImageNet and fMoW.
The ﬁgure shows that the number of clusters per class on
GeoImageNet tend to be skewed towards smaller numbers
than fMoW whereas the number of unique classes per clus-
ter on GeoImageNet has more of a uniform distribution. For
fMoW, we can conclude that each cluster contain samples
from most of the classes. Finally, when adding the geo-
location classiﬁcation task into the contrastive learning we
tuneαandβto be 1.
Methods We compare our unsupervised learning approach
tosupervised learning for image recognition task. For ob-
ject detection, and semantic segmentation we compare them
Figure 8: Top andBottom show the distributions of the
fMoW and GeoImageNet clusters.
6
to pre-trained weights obtained using (a) supervised learn-
ing, and (b) random initilization while ﬁne-tuning on the
target task dataset. Finally, for ablation analysis we provide
results using different combinations of our methods. When
appending only geo-location classiﬁcation task or temporal
positives into MoCo-v2 we use MoCo-v2+Geo andMoCo-
v2+TP . When adding both of our approaches into MoCo-
v2we use MoCo-v2+Geo+TP .
5.1. Experiments on fMoW
We ﬁrst perform experiments on fMoW image recogni-
tion task. Similar to the common protocol of evaluating un-
supervised pre-training methods [3, 13], we freeze the fea-
tures and train a supervised linear classiﬁer. However, in
practice, it is more common to ﬁnetune the features end-to-
end in a downstream task. For completeness and a better
comparison, we report end-to-end ﬁnetuning results for the
62-class fMoW classiﬁcation as well. We report both top-1
accuracy and F1-scores for this task.
BackboneF1-Score↑
(Frozen/Finetune)Accuracy↑
(Frozen/Finetune)
Sup. Learning (IN wts. init.) *ResNet50 -/64.72 -/69.07
Sup. Learning (Scratch) * ResNet50 -/64.71 -/69.05
Geoloc. Learning * ResNet50 48.96/52.23 52.40/56.59
MoCo-V2 (pre. on IN) ResNet50 31.55/57.36 37.05/62.90
MoCo-V2 ResNet50 55.47/60.61 60.69/64.34
MoCo-V2+Geo ResNet50 61.60/66.60 64.07/69.04
MoCo-V2+TP ResNet50 64.53/67.34 68.32/71.55
MoCo-V2+Geo+TP ResNet50 63.13/66.56 66.33/70.60
Table 1: Experiments on fMoW on classifying single im-
ages. * indicates a model trained up to epoch with the high-
est accuracy on the validation set. We use the same set up
for Sup. Learning and Geoloc. Learning in the remaining
experiments. Frozen corresponds to linear classiﬁcation on
frozen features. Finetune corresponds to end-to-end ﬁne-
tuning results for the fmow classiﬁcation.
Classifying Single Images In Table 1, we report the results
on single image classiﬁcation on fMoW. We would like to
highlight that in this case we classify each image individ-
ually. In other words, we do not use the prior information
that multiple images over the same area (x1
i,x2
i,...,xTi
i)
have the same labels (yi,yi,...,yi). For evaluation, we
use 53,041 images. Our results on this task (linear classi-
ﬁcation on frozen features) show that MoCo-v2 performs
reasonably well on a large-scale dataset with 60.69% accu-
racy, 8%less than the supervised learning methods. Sup.
Learning (IN wts. init.) andSup. Learning (Scratch) cor-
respond to supervised learning method starting from ima-
genet pre-trained weights and random weights respectively.
This result aligns with MoCo-v2’s performance on the Ima-
geNet dataset [3]. Next, by incorporating geo-location clas-
siﬁcation task into MoCo-v2, we improve by 3.38% in top-
1 classiﬁcation accuracy. We further improve the resultsto68.32% using temporal positives, bridging the gap be-
tween the MoCo-v2 baseline and supervised learning to less
than1%. However, when we perform end-to-end ﬁnetun-
ing for the classiﬁcation task, we observe that our method
surpasses the supervised learning methods by more than
2%. For completeness, we also include results for MoCo-
v2 pre-trained on Imagenet dataset (4th row in Table 1) and
ﬁnd that the distribution shift between Imagenet and down-
stream dataset leads to suboptimal performance.
Classifying Temporal Data In the next step, we change
how we perform testing across multiple images over an
area at different times. In this case, we predict labels
from images over an area i.e. make a prediction for each
t∈{1,...,Ti}, and average the predictions from that area.
We then use the most conﬁdent class prediction to get area-
speciﬁc class predictions. In this case, we evaluate the per-
formance on 11,231 unique areas that are represented by
multiple images at different times. Our results in Table 2
show that doing area-speciﬁc inference improves the classi-
ﬁcation accuracies by 4-8%over image-speciﬁc inference.
Even incorporating temporal positives, we can improve the
accuracy by 6.1%by switching from image classiﬁcation to
temporal data classiﬁcation. Overall, our methods outper-
form the baseline Moco-v2 by 4-6%and supervised learn-
ing by 1-2%. Here we only report temporal classiﬁcation
on top of frozen features.
Backbone F1-Score↑ Accuracy↑
Sup. Learning (IN wts. init.) *ResNet50 68.72 (+4.01) 73.22 (+4.15)
Sup. Learning (Scratch) * ResNet50 68.73 (+4.02) 73.24 (+4.19)
Geoloc. Learning * ResNet50 52.01 (+3.05) 56.12 (+3.72)
MoCo-V2 (pre. on IN) ResNet50 35.93 (+4.38) 42.56 (+5.51)
MoCo-V2 ResNet50 63.96 (+8.49) 68.64 (+7.95)
MoCo-V2+Geo ResNet50 66.93 (+5.33) 70.48 (+6.41)
MoCo-V2+TP ResNet50 70.11 (+5.58) 74.42 (+6.10)
MoCo-V2+Geo+TP ResNet50 69.56 (+6.43) 72.76 (+6.43)
Table 2: Experiments on fMoW on classifying temporal
data. In the table, we compare the results to the ones on
single image classiﬁcation. Here we present results corre-
sponding to linear classiﬁcation on frozen features only.
5.2. Transfer Learning Experiments
Previously, we performed pre-training experiments on
fMoW dataset and quantiﬁed the quality of the representa-
tions by supervised training a linear layer for image recogni-
tion on fMoW. In this section, we perform transfer learning
experiments on different low level tasks.
5.2.1 Object Detection
For object detection, we use the xView dataset [16] consist-
ing of high resolution satellite images captured with similar
sensors to the ones in the fMoW dataset. The xView dataset
7
pre-train AP50↑
Random Init. 10.75
Sup. Learning (IN wts. init.) 14.44
Sup. Learning (Scratch) 14.42
MoCo-V2 15.45 (+4.70)
MoCo-V2-Geo 15.63 (+4.88)
MoCo-V2-TP 17.65 (+6.90)
MoCo-V2-Geo+TP 17.74 (+6.99)
Table 3: Object detection results on the xView dataset.
consists of 846 very large ( ∼2000×2000 pixels) satellite
images with bounding box annotations for 60 different class
categories including airplane, passenger vehicle, maritime
vessel, helicopter etc.
Implementation Details We ﬁrst divide the set of large
images into 700 training and 146 test images. Then, we
process the large images to create 416 ×416 pixels images
by randomly sampling the bounding box coordinates of the
small image and we repeat this process 100 times for each
large image. In this process, we ensure that there is less than
25% overlap between any two bounding boxes from the
same image. We then use RetinaNet [18] with pre-trained
ResNet-50 backbone and ﬁne-tune the full network on the
xView training set. To train RetinaNet, we use learning rate
of 1e-5 and a batch size of 4 and Adam optimizer.
Qualitative Analysis Table 3 shows the object detection
performance on the xView test set. We achieve the best re-
sults with the addition of temporal positive pair, and geo-
location classiﬁcation pre-text task into MoCo-v2. With
our ﬁnal model, we can outperform the randomly initial-
ized weights by 7%AP and the supervised learning on the
fMoW by 3.3%AP.
5.2.2 Image Segmentation
In this section, we perform downstream experiments on the
task of Semantic Segmentation on SpaceNet dataset [40].
The SpaceNet datasets consists of 5000 high resolution
satellite images with segmentation masks for buildings.
Implementation Details We divide our SpaceNet dataset
into training and test sets of 4000 and 1000 images respec-
tively. We use PSAnet [50] network with ResNet-50 back-
bone to perform semantic segmentation. We train PSAnet
network with a batch size of 16 and a learning rate of 0.01
for 100 epochs and use SGD optimizer.
Qualitative Analysis Table 4 shows the segmentation per-
formance of differently initialized backbone weights on the
SpaceNet test set. Similar to object detection, we achieve
the best IoU scores with the addition of temporal positives
and geo-location classiﬁcation task. Our ﬁnal model out-
performs the randomly initialized weights and supervised
learning by 3.58% and2.94% IoU scores. We observe that
the gap between the best and worst models shrinks goingfrom the image recognition to object detection, and seman-
tic segmentation task. This aligns with the performance of
the MoCo-v2 pre-trained on ImageNet and ﬁne-tuned on
the Pascal-VOC object detection and semantic segmenta-
tion experiments [13, 3].
pre-train mIOU↑
Random Init. 74.93
Imagenet Init. 75.23
Sup. Learning (IN wts. init.) 75.61
Sup. Learning (Scratch) 75.57
MoCo-V2 78.05 (+3.12)
MoCo-V2-Geo 78.42 (+3.49)
MoCo-V2-TP 78.48 (+3.55)
MoCo-V2-Geo+TP 78.51 (+3.58)
Table 4: Semantic segmentation results on Space-Net.
pre-train Top-1 Accuracy ↑
Random Init. 51.89
Imagenet Init. 53.46
Sup. Learning (IN wts. init.) 54.67
Sup. Learning (Scratch) 54.46
MoCo-V2 55.18 (+3.29)
MoCo-V2-Geo 58.23 (+6.34)
MoCo-V2-TP 57.10 (+5.21)
MoCo-V2-Geo+TP 57.63 (+5.74)
Table 5: Land Cover Classiﬁcation on NAIP dataset.
5.2.3 Land Cover Classiﬁcation
Finally, we perform transfer learning experiments on land
cover classiﬁcation across 66 land cover classes using high
resolution remote sensing images obtained by the USDA’s
National Agricultural Imagery Program (NAIP). We use the
images from the California’s Central Valley for the year of
2016. Our ﬁnal dataset consists of 100,000 training and
50,000 test images. Table 5 shows that our method outper-
forms the randomly initialized weights by 6.34% and super-
vised learning by 3.77%.
5.3. Experiments on GeoImageNet
After fMoW, we adopt our methods for unsupervised
learning on fMoW for improving representation learning on
the GeoImageNet. Unfortunately, since ImageNet does not
contain images from the same area over time we are not able
to integrate the temporal positive pairs into the MoCo-v2
objective. However, in our GeoImageNet experiments we
show that we can improve MoCo-v2 by introducing geo-
location classiﬁcation pre-text task.
Qualitative Analysis Table 6 shows the top-1 and top-5
classiﬁcation accuracy scores on the test set of GeoIma-
geNet. Surprisingly, with only geo-location classiﬁcation
task we can achieve 22.26% top-1 accuracy. With MoCo-v2
baseline, we get 38.51accuracy, about 3.47% more than su-
pervised learning method. With the addition of geo-location
classiﬁcation, we can further improve the top-1 accuracy by
1.45%. These results are interesting in a way that MoCo-v2
8
(200 epochs) on ImageNet-1k performs 8%worse than su-
pervised learning whereas it outperforms supervised learn-
ing on our highly imbalanced GeoImageNet with 5150 class
categories which is about 5×more than ImageNet-1k.
BackboneTop-1
(Accuracy)↑Top-5
(Accuracy)↑
Sup. Learning (Scratch) ResNet50 35.04 54.11
Geoloc. Learning ResNet50 22.26 39.33
MoCo-V2 ResNet50 38.51 57.67
MoCo-V2+Geo ResNet50 39.96 58.71
Table 6: Experiments on GeoImageNet. We divide the
dataset into 443,435 training and 100,000 test images across
5150 classes. We train MoCo-V2 and MoCo-V2+Geo for
200 epochs whereas Sup. and Geoloc. Learning are
trained until they converge .
6. Conclusion
In this work, we provide a self-supervised learning
framework for remote sensing data, where unlabeled data is
often plentiful but labeled data is scarce. By leveraging spa-
tially aligned images over time to construct temporal posi-
tive pairs in contrastive learning and geo-location in the de-
sign of pre-text tasks, we are able to close the gap between
self-supervised and supervised learning on image classiﬁca-
tion, object detection and semantic segmentation on remote
sensing and other geo-tagged image datasets.
Acknowledgement
This research is based upon work supported in part by the
Ofﬁce of the Director of National Intelligence (ODNI), In-
telligence Advanced Research Projects Activity (IARPA),
via 2021-2011000004. The views and conclusions con-
tained herein are those of the authors and should not be
interpreted as necessarily representing the ofﬁcial policies,
either expressed or implied, of ODNI, IARPA, or the U.S.
Government. The U.S. Government is authorized to repro-
duce and distribute reprints for governmental purposes not-
withstanding any copyright annotation therein.
This research was also supported by Stanford Data
for Development Initiative, HAI, IARPA SMART, ONR
(N00014-19-1-2145), and NSF grants #1651565 and
#1733686.
References
[1] Kumar Ayush, Burak Uzkent, Marshall Burke, David Lobell,
and Stefano Ermon. Generating interpretable poverty maps
using object detection in satellite images. arXiv preprint
arXiv:2002.01612 , 2020. 2
[2] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learningof visual representations. arXiv preprint arXiv:2002.05709 ,
2020. 1, 2, 4
[3] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
Improved baselines with momentum contrastive learning.
arXiv preprint arXiv:2003.04297 , 2020. 1, 2, 4, 6, 7, 8
[4] Anil M Cheriyadat. Unsupervised feature learning for aerial
scene classiﬁcation. IEEE Transactions on Geoscience and
Remote Sensing , 52(1):439–451, 2013. 2
[5] Gordon Christie, Neil Fendley, James Wilson, and Ryan
Mukherjee. Functional map of the world. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 6172–6180, 2018. 1, 2, 4
[6] Grace Chu, Brian Potetz, Weijun Wang, Andrew Howard,
Yang Song, Fernando Brucher, Thomas Leung, and Hartwig
Adam. Geo-aware networks for ﬁne-grained recognition. In
Proceedings of the IEEE International Conference on Com-
puter Vision Workshops , pages 0–0, 2019. 3
[7] Terrance de Vries, Ishan Misra, Changhan Wang, and Lau-
rens van der Maaten. Does object recognition work for ev-
eryone? In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition Workshops , pages 52–
59, 2019. 2, 4
[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248–255. Ieee, 2009. 4
[9] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-
supervised representation learning by predicting image rota-
tions. arXiv preprint arXiv:1803.07728 , 2018. 2
[10] Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,
Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap your own latent-a new approach
to self-supervised learning. Advances in Neural Information
Processing Systems , 33, 2020. 1
[11] James Hays and Alexei A Efros. Im2gps: estimating geo-
graphic information from a single image. In 2008 ieee con-
ference on computer vision and pattern recognition , pages
1–8. IEEE, 2008. 1, 3
[12] James Hays and Alexei A Efros. Large-scale image geolo-
calization. In Multimodal location estimation of videos and
images , pages 41–62. Springer, 2015. 1, 3
[13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
9729–9738, 2020. 1, 2, 4, 5, 7, 8
[14] Hayate Iso, Shoko Wakamiya, and Eiji Aramaki. Density
estimation for geolocation via convolutional mixture density
network. arXiv preprint arXiv:1705.02750 , 2017. 3
[15] Neal Jean, Sherrie Wang, Anshul Samar, George Azzari,
David Lobell, and Stefano Ermon. Tile2vec: Unsupervised
representation learning for spatially distributed data. In Pro-
ceedings of the AAAI Conference on Artiﬁcial Intelligence ,
volume 33, pages 3967–3974, 2019. 2
[16] Darius Lam, Richard Kuzma, Kevin McGee, Samuel Doo-
ley, Michael Laielli, Matthew Klaric, Yaroslav Bulatov, and
9
Brendan McCord. xview: Objects in context in overhead
imagery. arXiv preprint arXiv:1802.07856 , 2018. 1, 7
[17] Yansheng Li, Chao Tao, Yihua Tan, Ke Shang, and Jinwen
Tian. Unsupervised multilayer feature learning for satellite
image scene classiﬁcation. IEEE Geoscience and Remote
Sensing Letters , 13(2):157–161, 2016. 2
[18] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ´ar. Focal loss for dense object detection. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 2980–2988, 2017. 8
[19] Xiaoqiang Lu, Xiangtao Zheng, and Yuan Yuan. Remote
sensing scene classiﬁcation by unsupervised representation
learning. IEEE Transactions on Geoscience and Remote
Sensing , 55(9):5148–5157, 2017. 2
[20] Oisin Mac Aodha, Elijah Cole, and Pietro Perona. Presence-
only geographical priors for ﬁne-grained image classiﬁca-
tion. In Proceedings of the IEEE International Conference
on Computer Vision , pages 9596–9606, 2019. 3
[21] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,
Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,
and Laurens van der Maaten. Exploring the limits of weakly
supervised pretraining. In Proceedings of the European Con-
ference on Computer Vision (ECCV) , pages 181–196, 2018.
1
[22] Ishan Misra and Laurens van der Maaten. Self-supervised
learning of pretext-invariant representations. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 6707–6717, 2020. 2
[23] Lichao Mou, Pedram Ghamisi, and Xiao Xiang Zhu. Un-
supervised spectral–spatial feature learning via deep resid-
ual conv–deconv network for hyperspectral image classiﬁca-
tion. IEEE Transactions on Geoscience and Remote Sensing ,
56(1):391–406, 2017. 2
[24] Eric Muller-Budack, Kader Pustu-Iren, and Ralph Ewerth.
Geolocation estimation of photos using a hierarchical model
and scene classiﬁcation. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 563–579,
2018. 3
[25] T Nathan Mundhenk, Goran Konjevod, Wesam A Sakla,
and Koﬁ Boakye. A large contextual dataset for classiﬁca-
tion, detection and counting of cars with deep learning. In
European Conference on Computer Vision , pages 785–800.
Springer, 2016. 2
[26] Mehdi Noroozi and Paolo Favaro. Unsupervised learning
of visual representations by solving jigsaw puzzles. In
European Conference on Computer Vision , pages 69–84.
Springer, 2016. 2
[27] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018. 4
[28] Deepak Pathak, Ross Girshick, Piotr Doll ´ar, Trevor Dar-
rell, and Bharath Hariharan. Learning features by watch-
ing objects move. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2701–
2710, 2017. 2
[29] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Featurelearning by inpainting. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
2536–2544, 2016. 2
[30] Adriana Romero, Carlo Gatta, and Gustau Camps-Valls. Un-
supervised deep feature extraction for remote sensing image
classiﬁcation. IEEE Transactions on Geoscience and Remote
Sensing , 54(3):1349–1362, 2015. 2
[31] Evan Sheehan, Chenlin Meng, Matthew Tan, Burak Uzkent,
Neal Jean, Marshall Burke, David Lobell, and Stefano Er-
mon. Predicting economic development using geolocated
wikipedia articles. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data
Mining , pages 2698–2706, 2019. 1
[32] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao
Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han
Zhang, and Colin Raffel. Fixmatch: Simplifying semi-
supervised learning with consistency and conﬁdence. arXiv
preprint arXiv:2001.07685 , 2020. 2
[33] Kevin Tang, Manohar Paluri, Li Fei-Fei, Rob Fergus, and
Lubomir Bourdev. Improving image classiﬁcation with lo-
cation context. In Proceedings of the IEEE international
conference on computer vision , pages 1008–1016, 2015. 3
[34] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
trastive multiview coding. arXiv preprint arXiv:1906.05849 ,
2019. 4
[35] Burak Uzkent, Aneesh Rangnekar, and Matthew J Hoffman.
Tracking in aerial hyperspectral videos using deep kernelized
correlation ﬁlters. IEEE Transactions on Geoscience and
Remote Sensing , 57(1):449–461, 2018. 2
[36] Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang,
Marshall Burke, David Lobell, and Stefano Ermon. Learning
to interpret satellite images in global scale using wikipedia.
arXiv preprint arXiv:1905.02506 , 2019. 1, 2
[37] Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang,
Marshall Burke, David Lobell, and Stefano Ermon. Learning
to interpret satellite images using wikipedia. In Proceedings
of the Twenty-Eighth International Joint Conference on Arti-
ﬁcial Intelligence , 2019. 1
[38] Burak Uzkent, Evan Sheehan, Chenlin Meng, Zhongyi Tang,
Marshall Burke, David B Lobell, and Stefano Ermon. Learn-
ing to interpret satellite images using wikipedia. In IJCAI ,
pages 3620–3626, 2019. 1
[39] Burak Uzkent, Christopher Yeh, and Stefano Ermon. Efﬁ-
cient object detection in large images using deep reinforce-
ment learning. In Proceedings of the IEEE/CVF Winter Con-
ference on Applications of Computer Vision , pages 1824–
1833, 2020. 2
[40] Adam Van Etten, Dave Lindenbaum, and Todd M Bacastow.
Spacenet: A remote sensing dataset and challenge series.
arXiv preprint arXiv:1807.01232 , 2018. 8
[41] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. Extracting and composing robust
features with denoising autoencoders. In Proceedings of the
25th international conference on Machine learning , pages
1096–1103, 2008. 2
[42] Nam V o, Nathan Jacobs, and James Hays. Revisiting im2gps
in the deep learning era. In Proceedings of the IEEE Inter-
10
national Conference on Computer Vision , pages 2621–2630,
2017. 3
[43] Xiaolong Wang and Abhinav Gupta. Unsupervised learn-
ing of visual representations using videos. In Proceedings of
the IEEE international conference on computer vision , pages
2794–2802, 2015. 2
[44] Tobias Weyand, Ilya Kostrikov, and James Philbin. Planet-
photo geolocation with convolutional neural networks. In
European Conference on Computer Vision , pages 37–55.
Springer, 2016. 3
[45] Yi Yang and Shawn Newsam. Bag-of-visual-words and spa-
tial extensions for land-use classiﬁcation. In Proceedings of
the 18th SIGSPATIAL international conference on advances
in geographic information systems , pages 270–279, 2010. 2
[46] Xiwen Yao, Junwei Han, Gong Cheng, Xueming Qian, and
Lei Guo. Semantic annotation of high-resolution satellite
images via weakly supervised learning. IEEE Transactions
on Geoscience and Remote Sensing , 54(6):3660–3671, 2016.
2
[47] Lefei Zhang, Liangpei Zhang, Bo Du, Jane You, and
Dacheng Tao. Hyperspectral image unsupervised classiﬁ-
cation by robust manifold matrix factorization. Information
Sciences , 485:154–169, 2019. 2
[48] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
image colorization. In European conference on computer
vision , pages 649–666. Springer, 2016. 2
[49] Richard Zhang, Phillip Isola, and Alexei A Efros. Split-brain
autoencoders: Unsupervised learning by cross-channel pre-
diction. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 1058–1067, 2017. 2
[50] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen
Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise
spatial attention network for scene parsing. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
pages 267–283, 2018. 8
11
SSL4EO-L:
Datasets and Foundation Models for Landsat Imagery
Adam J. Stewart1, Nils Lehmann2, Isaac A. Corley3, Yi Wang2, 4, Yi-Chia Chang1,
Nassim Ait Ali Braham2, 4, Shradha Sehgal1, Caleb Robinson5, Arindam Banerjee1
1University of Illinois Urbana-Champaign,2Technical University of Munich,
3University of Texas at San Antonio,4German Aerospace Center,
5Microsoft AI for Good Research Lab
Abstract
The Landsat program is the longest-running Earth observation program in history,
with 50+ years of data acquisition by 8 satellites. The multispectral imagery cap-
tured by sensors onboard these satellites is critical for a wide range of scientific
fields. Despite the increasing popularity of deep learning and remote sensing,
the majority of researchers still use decision trees and random forests for Land-
sat image analysis due to the prevalence of small labeled datasets and lack of
foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset
designed for Self-Supervised Learning for EarthObservation for the Landsat fam-
ily of satellites (including 3 sensors and 2 product levels) and the largest Landsat
dataset in history (5M image patches). Additionally, we modernize and re-release
the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML
benchmark datasets for Landsats 4–5 TM and Landsat 7 ETM+ SR. Finally, we
pre-train the first foundation models for Landsat imagery using SSL4EO-L and
evaluate their performance on multiple semantic segmentation tasks. All datasets
and model weights are available via the TorchGeo1library, making reproducibility
and experimentation easy, and enabling scientific advancements in the burgeoning
field of remote sensing for a multitude of downstream applications.
1 Introduction
On July 23rd, 1972, the National Aeronautics and Space Administration (NASA) launched Land-
sat 1. Designed by Virginia T. Norwood, the Multispectral Scanner (MSS) onboard Landsats 1–5
provided invaluable measurements of the Earth’s surface in both the visible and infrared spectra.
Although she passed away earlier this year, her legacy as “The Mother of Landsat” lives on, as the
Landsat program has become the longest-running Earth observation program in history [1].
The Landsat satellite program stretches over 50 years and includes 9 generations of satellites, each
with its own set of sensors . Landsats 1–3 carried the Return Beam Vidicon (RBV), an RGB analog
camera [2]. However, its lower number of spectral bands and electrical issues meant it was rarely
used for research purposes. Instead, the Multispectral Scanner (MSS) onboard Landsats 1–5 was the
primary scientific instrument, with a line-scanning and rotating mirror-based camera [3]. Landsats
4–5 also included the Thematic Mapper (TM), with a greater number of spectral bands (from 4 to 7)
and finer spatial resolution (from 80 m to 30 m) [4]. Although it failed to reach orbit and eventually
crashed down to Earth, Landsat 6 would have carried the Enhanced Thematic Mapper (ETM), which
added a 15 m resolution panchromatic band. Landsat 7 carried the Enhanced Thematic Mapper Plus
(ETM+), which upgraded the thermal band from 120 m to 60 m resolution [5]. Onboard Landsats
1https://github.com/microsoft/torchgeo
Preprint. Under review.arXiv:2306.09424v2  [cs.LG]  22 Oct 2023
0
Wavelength ( µm)0
0.5 1.0 1.5 2.0Landsat 8–9
(OLI/TIRS)Landsat 7
(ETM+)Landsat 4–5
(TM)
89
7 65 4
32187 5 43217 5 4321
11 12153010015306030120
Resolution (m)
11 1066Figure 1: Spectral wavelengths and spatial resolutions of each band captured by the Landsat sensors
used in our study. Numbers are band indices and colors are for visualization purposes only. As
each sensor has a different number of spectral bands, spatial resolutions, and wavelengths, it is not
possible to train a single one-size-fits-all model.
8–9, the Operational Land Imager (OLI) adds new coastal aerosol and cirrus bands for improved
cloud masking, while the Thermal Infrared Sensor (TIRS) adds an additional thermal band [6]. See
Figure 1 for a rundown of the spectral wavelengths and spatial resolutions of the primary sensors of
interest in our study and Figure 2 for a timeline of when these satellites were active.
In addition to the differences between sensors onboard each satellite, the United States Geological
Survey (USGS) distributes several different Landsat products with varying processing levels. Level-
1 data, also known as Top of Atmosphere (TOA), are images that have undergone registration against
ground control points (GCPs) and orthorectification against digital elevation models (DEMs). These
products are particularly useful for cloud masking and other atmospheric applications. Level-2 data
includes Surface Reflectance (SR) and other products that have undergone atmospheric correction.
These products are useful for a wide range of land surface applications [7].
In recent years, there has been significant activity at the intersection of self-supervised learning
(SSL) and remote sensing (RS) due to the wide availability of petabytes of free, unlabeled satellite
imagery. An early example of this is Tile2Vec [8], which uses geographic distance between sampled
patches and a triplet margin loss for contrastive learning. Geography-Aware SSL [9] instead uses
multiple images occurring at the same geospatial location at different points in time to form positive
pairs, in combination with an additional subnetwork that tries to guess the latitude/longitude from
the learned representation. More recently, masked autoencoders have seen a surge in popularity, in-
cluding SatMAE [10] and Scale-MAE [11]. Other papers instead focus on dataset curation, allowing
generic SSL techniques from computer vision to be applied. Seasonal Contrast (SeCo) [12] creates
a new dataset using random Gaussian sampling around cities to diversify the pre-training dataset.
SSL4EO-S12 [13] further extends this idea by avoiding overlap between image samples.
Recent review papers [14, 15, 16] targeting the intersection between SSL and RS detail prior work on
this topic and offer benchmark results on several models and SSL techniques popular in computer
vision, including MoCo [17], SwA V [18], SimSiam [19], Barlow Twins [20], SimCLR [21], and
BYOL [22]. Among these, the authors find that although SimCLR and SwA V work well on the
ImageNet [23] dataset, MoCo and BYOL tend to learn better representations on RS imagery [15, 16].
Due to their higher spatial resolution and faster repeat period, a lot of recent work, especially in
the SSL space, focuses on Sentinel-2 [24, 25, 13], Maxar [9, 26, 27], and Planet [28, 29] satellites.
However, many applications are not suited to these satellites. In particular, applications involving
long-term trends—including agriculture [30, 31, 32, 33, 34], climate change [35, 36, 37, 38, 39],
2
1970 1980 1990 2000 2010 2020987654321Landsat Mission
Sep 2021–PresentFeb 2013–PresentApr 1999–PresentOct 1993Mar 1984–Jun 2013Jul 1982–Jun 2001Mar 1978–Sep 1983Jan 1975–Mar 1983Jul 1972–Jan 1978Figure 2: Timeline of all Landsat missions. Crosshatched regions represent partial or complete
sensor failure, including electrical issues with RBV on Landsat 1 and SLC-off on Landsat 7. Each
bar ranges from launch date to decommissioning date. Note that satellites may be placed in standby
mode before the decommissioning date, as is the case with Landsat 4.
deforestation [40, 41, 42, 43, 44], and ecology [45, 46, 47, 48, 49]—require a much longer temporal
history. While Sentinel-2 has an 8 year history, Landsat’s 50+ year history makes it essential for
monitoring long-term land surface changes. There are an order of magnitude more papers that use
Landsat as compared to Sentinel, and Landsat continues to dominate the scientific literature even
after the launch of Sentinel-2 and MODIS [50]. The United States Geological Survey (USGS)
estimates that Landsat imagery provides users with an annual benefit of $4.2 billion [51].
In this work, we further extend the ideas proposed in SeCo [12] and SSL4EO-S12 [13] to the Landsat
imagery domain. Specifically, we improve on the sampling method of the two previously mentioned
papers and sample Landsat imagery across the world and across three different sensors and two
product levels. We pre-train ResNet [52] and ViT [53] models using SimCLR v1 and MoCo v2 on
each combination of sensor and product to produce a suite of pre-trained Landsat foundation models
that can be used in downstream tasks. To test these models, we modernize two older datasets based
on Landsat 7 and 8 imagery, and create several additional crop classification and land cover mapping
tasks. In summary, the contributions of this paper include:
• the first ever SSL dataset for the Landsat family of satellites,
• the largest Landsat dataset in history (1M images per sensor/product, 5M in total),
• modernized and re-released versions of the L7 Irish and L8 Biome cloud detection datasets,
• two new benchmark datasets that can be used across all Landsat sensors and product levels,
• the first ever benchmark datasets for TM and ETM+ SR imagery, and
• the first ever foundation models pre-trained on Landsat imagery.
Importantly, all of these SSL techniques, datasets, and pre-trained models are distributed via the
TorchGeo library [54], allowing for ease of use, experimentation, and reproduciblity.
2 Datasets
In this section, we detail our methodology behind the collection of the SSL dataset we create, in-
cluding differences from prior work. We also introduce the existing and newly created benchmark
datasets we use to evaluate the representations learned by our pre-trained models.
3
2.1 SSL4EO-L pre-training dataset
For our SSL pre-training dataset, we extend the methodology introduced by Manas et al. [12] and
refined by Wang et al. [13]. Specifically, we iterate over the following steps:
1) sample one of the 10K most populous cities in the world [55] uniformly at random;
2) sample a 264×264px (7.92×7.92km) patch from a Gaussian distribution with a 50 km
standard deviation centered around the centroid of the sampled city;
3) ensure the patch does not overlap with any existing sampled patches;
4) ensure that there exist 4 patches of imagery from 4 different seasons—each selected from
a 60-day window centered about the vernal and autumnal equinoxes and the summer and
winter solstices (within a 2-year window)—with less than 20% cloud coverage;
5) ensure that none of these patches contain nodata pixels;
6) if the previous three criteria are met, download the imagery corresponding to the patch.
If any step in this algorithm fails (there is overlap, or a location does not have a set of 4 cloud-free,
nodata-free images), the sample is skipped and we start over at step 1. This algorithm is designed to
maximize the diversity of images in the dataset, relying on the assumption that most of the diversity
in land cover is centered around large cities, with a gradual transition between urban, suburban,
farmland, and forest. Uniform sampling would instead result in images that are 70% ocean, 10%
desert, and 9% forest, resulting in very little dataset diversity [12]. Note that this sampling strategy
does result in decreased sampling from regions with persistent cloud cover (tropical rainforests) or
lower populations (desert, taiga, tundra, and polar biomes). By sampling different points in time, we
allow seasonal differences to act as natural forms of data augmentation during contrastive learning.
Differences between our sampling strategy and the one used by SSL4EO-S12 are as follows.
SSL4EO-S12 used Euclidean distance between patch centroids and a grid heuristic to detect overlap
between patches. This method has an O 
N2/M
average run-time complexity, where Nis the
total number of samples and Mis the number of grid cells. We replace this with an O(NlogN)
R-tree [56], removing the 1–3% overlap reported by Wang et al. [14] due to use of this grid heuristic.
Among the cloud-free images in the aforementioned time windows, we sort by cloud cover instead
of date to provide the best possible image patches. We also skip patches containing nodata pixels
due to sampling near the border of a scene, which we found to be prevalent (on the order of 25%) in
prior datasets. We found it necessary to increase the cloud coverage threshold from 10% to 20% due
to the larger patch size (Sentinel-2 has a 10 m resolution, but Landsat has a 30 m resolution, result-
ing in patches that cover 9 ×the area) and avoidance of nodata pixels. Finally, since the resolution
of most bands are the same, we resample all thermal and panchromatic bands to a 30 m resolution,
allowing all bands to be concatenated into a single file.
We download all data from Google Earth Engine (GEE) [57], with a total of 250K locations, each
sampled at 4 different seasons, for a total of 1M unlabeled image patches per sensor/product and
5M in total. Each image is 264×264px, corresponding to 7.92×7.92km at 30 m/px resolution.
There are separate datasets for TM TOA, ETM+ TOA, ETM+ SR, OLI/TIRS TOA, and OLI SR.
We decided not to include RBV and MSS sensors due to the limited data availability on GEE and
the fact that it is not possible to create a benchmark dataset for these sensors due to their age. Since
TM and ETM+ use the same sensor for SR bands, we did not create a separate dataset for TM SR.
For similar reasons, there is a single dataset for OLI/TIRS and OLI-2/TIRS-2. TM data is collected
from 4 different seasons in 2009–2010, as the TM sensor failed in November 2011. ETM+ data is
collected from 2001–2002, as the scan line corrector (SLC) failed in May, 2003, resulting in images
with significant nodata pixels. OLI/TIRS data is collected from 2021–2022. See Figure 3 for a map
of the geographical distribution for each sensor. Note that it is not possible to sample high latitudes
due to lack of winter imagery.
All TOA and SR datasets represent a parallel corpus (the TOA and SR images are taken at the same
locations and dates). Due to differences in collection years and cloud coverage/nodata pixels, it was
not possible to create a parallel corpus between sensors. However, approximately 50% of TM and
ETM+, 40% of TM and OLI/TIRS, and 40% of ETM+ and OLI/TIRS images are sampled from
the same location, allowing for multimodal data fusion studies. The official scale factors suggested
4
(a) OLI/TIRS
(b) TM
(c) ETM+
Figure 3: Geographical distribution of the SSL4EO-L dataset, including the (a) Landsat 8–9
OLI/TIRS, (b) Landsat 4–5 TM, and (c) Landsat 7 ETM+ splits. Surface reflectance (SR) and
top of atmosphere (TOA) products are sampled from the same locations per sensor.
by the USGS to map between Level-1 and Level-2 Landsat imagery2and the visualization range
recommended by GEE for each sensor are used to map from float32 to uint8. The resulting datasets
are 274–385 GB when compressed and can be downloaded from Hugging Face3using TorchGeo.
2.2 Dataset archaeology
In order to benchmark the ability of our learned representations to transfer to downstream appli-
cations, we require curated benchmark datasets for evaluation. Although there exist ∼10 semantic
segmentation datasets for OLI/TIRS TOA, an extensive literature review found almost no benchmark
datasets for other sensors, products, or tasks. This is due to both their age (deep learning was not
commonplace in the field of remote sensing until recently) and the fact that semantic segmentation
is the primary task for which lower resolution satellite imagery is used.
A single classification dataset, Statlog [58], was found for the MSS sensor. However, this dataset
is composed of 3×3px images, making it unsuitable for evaluation of CNN and ViT backbones.
For the task of semantic segmentation for cloud cover, three ETM+ TOA datasets were found: L7
SPARCS [59], L7 Irish [60, 61], and L7 Scaramuzza [62]. Each of these datasets also has a cor-
responding dataset for OLI/TIRS TOA (L8 SPARCS [63, 64], L8 Biome [65, 66], and L8 Scara-
muzza [67]), making it possible to compare learned representations across sensors. No benchmark
datasets for TM or ETM+ SR were ever found. The L7 SPARCS dataset, while thought to be lost to
time, was eventually recovered from a hard drive found in the closet of one of the dataset’s authors.
The majority of the aforementioned cloud segmentation datasets are official datasets used by the
USGS to validate their cloud detection algorithms. Among these datasets, we chose to use L7 Irish
and L8 Biome due to their larger size and greater number of citations.
2.2.1 L7 Irish dataset
The L7 Irish dataset, originally selected by Irish et al. [68] and later digitized by Scaramuzza et al.
[61], is a validation dataset for cloud cover assessment algorithms composed of 206 Landsat 7
ETM+ Level-1G scenes and manually generated cloud masks divided between 9 unique biomes.
Each scene is a 9-band, roughly 8000×8000 px multispectral image with 30 m/px resolution.
Cloud masks consist of 5 classes: 1) fill, 2) cloud shadow, 3) clear, 4) thin cloud, and 5) cloud.
There are 2015 [69] and 2019 [60] versions of this dataset available for download. Unfortunately,
both versions have numerous issues that make them difficult to use for evaluation. The 2015 version
contains 1 scene with a corrupted thermal band file, 2 scenes that are missing masks, 1 scene with an
2https://www.usgs.gov/faqs/how-do-i-use-scale-factor-landsat-level-2-science-products
3https://huggingface.co/torchgeo
5
inconsistent filename format, and the documented class labels do not match the actual class labels
used. Additionally, there is no way to programmatically download the entire dataset. All 206 files
must be manually downloaded, one at a time, with a limit of 6 parallel downloads, requiring 3–4 hrs
of constant supervision and clicking each link every 5 min. The 2019 version has even more issues,
including 5 scenes with corrupted thermal band files, 1 scene missing geolocation, 6 scenes with
inconsistent filename formats, and inconsistent thermal band resolutions. Although 17% of masks
matched the documented labels, the other 83% of masks use a completely different mapping, with
both clear and fill classes mapped to the same value.
In order to use this dataset for evaluation, we start with the 2015 version and use scenes from the
2019 version to replace corrupted images and missing masks. We correct the class mapping of
copied masks and copy the fill pixels from the images to the masks. We convert all images to Cloud
Optimized GeoTIFFs (COGs), resample to 30 m resolution, and stack them into single multi-band
files with consistent filenames. The compression algorithm used by COGs resulted in a dataset that
is 33% of the original size and therefore faster to download and load from disk. The final ML-ready
dataset is available on Hugging Face and can be automatically downloaded using TorchGeo.
2.2.2 L8 Biome dataset
The L8 Biome dataset, created by Foga et al. [65], is a validation dataset for cloud cover assessment
algorithms consisting of 96 Landsat 8 OLI/TIRS Level-1T scenes and manually generated cloud
masks evenly divided between 8 unique biomes. Each scene is an 11-band, roughly 9000×9000 px
multispectral image with 30 m/px resolution. Cloud masks consist of the same 5 classes as L7 Irish.
Comparatively, L8 Biome has fewer issues than L7 Irish. The masks lack geolocation, but we can
copy this from the image files. While the dataset can be programmatically downloaded, it requires
scraping a webpage for 96 different URLs for each scene. We convert the raw uint16 images to uint8
to match L7 Irish, and create compressed COGs of all files, resulting in a dataset 9% of the original
size. We resample all images to 30 m/px resolution and stack them in single multi-band files. The
dataset is available on Hugging Face and can be automatically downloaded using TorchGeo.
2.3 SSL4EO-L benchmark dataset
As there are no existing benchmark datasets for TM or ETM+ SR, we need to design our own.
Crucially, we want a single benchmark dataset that can be used for a consistent comparison across all
5 sensors/products for which we are pre-training models. We create our own land cover classification
datasets based on NLCD [70] and CDL [71] masks, described in more detail below. They are the
only large, Landsat-based semantic segmentation masks with a long enough history to benchmark
foundation models for historical satellites.
Our sampling strategy is similar to the one used for our pre-training dataset, with a few differ-
ences. As CDL only exists for the continental U.S. (CONUS), we restrict our sampling strategy to
CONUS. To achieve maximum coverage, especially in lower population regions where agriculture
is most prevalent, we replace the city-centered Gaussian distribution with a uniform sampling distri-
bution. We choose a single 60-day window centered around August 1stwhen crop types are easiest
to distinguish. As CDL data is not available before the ETM+ SLC failure, we do not exclude no-
data pixels for this sensor. Additionally, nodata masks are copied from SLC-off imagery to masks
so as to avoid penalizing models for making incorrect predictions where there is no data. The 2019
NLCD and CDL datasets are used for ETM+ and OLI/TIRS evaluation since 2019 is the most recent
year for which both datasets exist. The 2011 datasets are used for TM since 2011 is the most recent
year for which both Landsat 5 and NLCD/CDL overlap. These years are different than the years
collected for our pre-training dataset, allowing us to accurately measure performance on images that
the pre-trained model has never seen before.
The resulting dataset consists of 25K Landsat, NLCD, and CDL triplets, converted from float32
to uint8 using the same scaling as above. All images have the same resolution and dimensions as
the pre-training dataset. The datasets form a parallel corpus between TOA and SR products, and
have approximately 85% spatial overlap across sensors, although not necessarily during the same
year, allowing for multimodal data fusion studies. All datasets are available for download from
Hugging Face using the TorchGeo library, making it easy for other researchers to compare against
our preliminary benchmark results.
6
NLCD The National Land Cover Database (NLCD) [70] is a land cover product produced every
2–3 years by the USGS, in collaboration with the Multi-Resolution Land Characteristics (MRLC)
consortium. The dataset spans the entire U.S. from 2001–2019. The final products are generated
at a 30 m resolution by random forest models trained on spectral, spatial, temporal, and ancillary
data [72, 73, 74]. We use the 21 class version, with an estimated overall accuracy of 77.5±1.0% [75].
CDL The Cropland Data Layer (CDL) [32] is an annual land cover product produced by the U.S.
Department of Agriculture (USDA) National Agricultural Statistics Service (NASS) focusing on
crop classification. Although the dataset is available starting in 1997, full CONUS coverage is not
available until 2008. The dataset consists of 134 classes, primarily for agricultural crops grown in
the U.S. Labels are generated at a 30 m resolution using a decision tree classifier. The most common
crop classes are estimated to have an accuracy of 85–95% [32]. All non-agricultural classes are
taken from NLCD, and should be considered to have a similar accuracy.
3 Experimental setup
For pre-training we conduct experiments similar to those performed in SSL4EO-S12 [13] for each
sensor/product in the dataset described in Section 2.1. We pre-train various ResNet [52] and ViT [53]
backbones initialized with ImageNet weights using the SimCLR v1 [21] and MoCo v2 [17] SSL
methods. RGB ImageNet weights are repeated (RGBRGB. . . ) and scaled ( 3/CforCchannels)
in the first convolutional layer in order to handle multispectral images. During pre-training we
use the same default augmentations and hyperparameters as SimCLR and MoCo with a couple of
exceptions. As saturation and hue are undefined for multispectral imagery, we skip these parts of
color jitter. Instead, we use the random season contrast technique proposed by Manas et al. [12]
by utilizing 2 randomly sampled multitemporal images from the same location as the augmented
views. Additionally, although grayscale is undefined for multispectral imagery, we take the average
of all bands to compute random grayscale images. We pre-train each model for 200 epochs using
a batch size of 1024. All pre-training experiments are performed on a GPU cluster, with 80 GB of
memory per GPU. Each experiment takes anywhere from 15–40 hrs depending on the number of
spectral bands and model size, each trained in parallel on 4 ×GPUs, for a total of ∼4K GPU hours
including hyperparameter tuning.
For benchmarking, we freeze the encoder and fine-tune a U-Net [76] decoder for all cloud detection
and land cover classification datasets mentioned above. For the L7 Irish and L8 Biome datasets,
we use a random 60-20-20 train-val-test split. For the NLCD and CDL datasets, we use a random
70-15-15 train-val-test split. NLCD and CDL classes are limited to those with > 1% area, with
remaining classes mapped to the background class. Splits are defined using a fixed random seed for
reproducibility. Random horizontal and vertical flip and random resized crop data augmentations
are used during training. Models are trained for a minimum of 20 epochs and a maximum of 100
epochs using early stopping and a learning rate schedule patience of 6 epochs. Only learning rate
undergoes hyperparameter tuning, with the most common optimal learning rate being 3e-3. All
benchmarking experiments are conducted on NVIDIA RTX A6000 (2.5 hr/experiment) and A100
(1 hr/experiment) GPUs for a total of ∼200 GPU hours. Configuration files and training scripts for
reproducing all experiments are made available in the TorchGeo library [54].
4 Benchmark results
In order to evaluate the effectiveness of our pre-trained models, we report overall accuracy and
mean intersection over union (mIoU) on four semantic segmentation datasets. Table 1 demonstrates
substantial gains over ImageNet, with up to an 18.43% accuracy and 24.25 mIoU improvement for
MoCo and up to a 14.43% accuracy and 18.69 mIoU improvement for SimCLR. Although MoCo
outperforms ImageNet in 5 out of 6 experiments, SimCLR shows mixed results, outperforming
ImageNet in only 2 out of 6 experiments. Our SimCLR models suffered from convergence issues
with the smaller batch size we used, and may improve with better hyperparameter tuning.
Note that both our sampling method and pretext task are explicitly designed to ignore clouds. During
sampling, we only select patches from scenes with < 20% cloud cover, decreasing the frequency of
clouds in our pre-training dataset. Our pretext task involves mapping patches taken from 2 different
seasons to the same representation. If one patch contains partial cloud cover, the model must learn to
7
Table 1: Cloud detection benchmark results. Overall accuracy and mean intersection over union
(mIoU) are reported for the test splits of the L7 Irish (Landsat 7 ETM+ TOA) and L8 Biome (Landsat
8 OLI/TIRS TOA) datasets for a range of backbones and pre-training techniques. All predictions are
made by U-Nets with frozen backbones. Three random seeds are used to compute mean ±standard
deviation of the performance.
L7 Irish L8 Biome
Backbone Pre-training Accuracy mIoU Accuracy mIoU
ResNet-18ImageNet 64.08 ±3.40 47.21 ±3.71 41.86 ±0.46 24.67 ±0.37
MoCo 74.79±2.20 59.77 ±2.79 42.70 ±5.02 27.33 ±4.14
SimCLR 34.80 ±11.36 21.46 ±8.53 39.17 ±5.12 24.44 ±3.89
ResNet-50ImageNet 61.77 ±3.27 44.75 ±3.41 45.73 ±6.08 29.78 ±5.23
MoCo 69.62±1.94 53.42 ±2.29 45.95±5.17 29.23 ±4.44
SimCLR 49.37 ±12.86 33.41 ±11.20 48.77±6.42 32.41 ±5.56
ViT-S16ImageNet 68.22 ±1.39 51.78 ±1.59 47.29±1.88 30.98 ±1.61
MoCo 86.65±0.43 76.03 ±0.67 46.66±3.59 30.33 ±3.14
SimCLR 82.65 ±0.27 70.47 ±0.35 42.33 ±1.80 26.99 ±1.67
ignore it. The fact that our SSL techniques work at all, let alone outperform ImageNet, demonstrates
the generalizability of our pre-trained model weights to different downstream applications.
Performance metrics for our land cover/land use tasks are reported in Table 2. Again, MoCo con-
sistently outperforms ImageNet in 25 out of 30 experiments across all sensors and product levels,
while SimCLR is unable to beat ImageNet in 24 out of 30 experiments. Performance gains by MoCo
are more modest in this task with a larger number of classes, but still reach as high as 6.60% overall
accuracy and 7.13 mIoU. There are exceptions to this, particularly for ETM+ TOA, but with ad-
ditional hyperparameter tuning of the pre-trained model it may be possible to exceed performance
of ImageNet. We attempted to use weights based on class frequency in our cross-entropy loss, but
these resulted in reduced accuracy and mIoU.
Figure 4 shows an example prediction made by a ResNet-18 backbone pre-trained on SSL4EO-L
using MoCo and a U-Net fine-tuned on CDL. Although the model is unable to predict detailed fea-
tures like roads and field corners, it removes much of the noise introduced by the pixel-wise decision
tree classifier used to produce CDL. Black pixels in the mask represent uncommon crop types that
are mapped to the background class. The model tends to pick the most common agricultural classes
like corn and soybean given no examples of these crop types in the training dataset. Although winter
wheat and fallow (idle farmland) are sometimes misclassified by the model, this is not unexpected.
Pasture Winter Wheat Fallow Corn Soybean Other
(a) Image
 (b) Mask
 (c) Prediction
Figure 4: Landsat 8 OLI SR image, ground truth mask, and prediction made by a U-Net with a
ResNet-18 backbone pre-trained using MoCo and SSL4EO-L and fine-tuned on CDL 2019.
8
Table 2: SSL4EO-L benchmark results. Overall accuracy and mean intersection over union (mIoU)
are reported for the test splits of the NLCD and CDL datasets for a range of sensors, product levels,
backbones, and pre-training techniques. All predictions are made by U-Nets with frozen backbones.
Satellite
(Sensor)Level
(Product)NLCD CDL
Backbone Pre-training Accuracy mIoU Accuracy mIoU
Landsats 4–5
(TM)Level-1
(TOA)ResNet-18ImageNet 65.63 48.84 66.11 49.38
MoCo 67.65 51.11 68.70 52.32
SimCLR 60.86 43.74 61.94 44.86
ResNet-50ImageNet 66.63 49.96 67.42 50.85
MoCo 68.75 53.28 69.45 53.20
SimCLR 62.05 44.98 62.80 45.77
ViT-S16ImageNet 68.93 52.59 68.27 51.83
MoCo 67.17 50.57 67.60 51.07
SimCLR 66.82 50.17 66.92 50.28
Landsat 7
(ETM+)Level-1
(TOA)ResNet-18ImageNet 66.11 49.38 65.84 49.08
MoCo 65.22 48.39 62.84 45.81
SimCLR 58.76 41.60 56.47 39.34
ResNet-50ImageNet 64.01 47.06 66.23 49.51
MoCo 66.60 49.92 64.12 47.19
SimCLR 57.17 40.02 54.95 37.88
ViT-S16ImageNet 62.06 45.01 57.67 40.52
MoCo 63.75 46.79 60.88 43.70
SimCLR 63.33 46.34 59.06 41.91
Level-2
(SR)ResNet-18ImageNet 63.34 46.34 60.70 43.58
MoCo 64.18 47.25 67.30 50.71
SimCLR 57.26 40.11 54.42 37.48
ResNet-50ImageNet 64.29 47.38 61.66 44.57
MoCo 64.37 47.46 62.35 45.30
SimCLR 57.79 40.64 55.69 38.59
ViT-S16ImageNet 63.54 46.56 51.38 34.57
MoCo 64.09 47.21 52.37 35.48
SimCLR 63.99 47.05 53.17 36.21
Landsats 8–9
(OLI/TIRS)Level-1
(TOA)ResNet-18ImageNet 66.40 49.70 65.21 48.38
MoCo 67.82 51.30 65.74 48.96
SimCLR 62.14 45.08 60.01 42.86
ResNet-50ImageNet 67.73 51.20 66.45 49.76
MoCo 69.17 52.87 67.29 50.70
SimCLR 64.66 47.78 62.08 45.01
ViT-S16ImageNet 65.52 48.72 62.38 45.33
MoCo 67.11 50.49 64.62 47.73
SimCLR 66.12 49.39 63.88 46.94
Level-2
(SR)ResNet-18ImageNet 65.46 48.65 62.88 45.85
MoCo 67.01 50.39 68.05 51.57
SimCLR 59.93 42.79 57.44 40.30
ResNet-50ImageNet 66.29 49.58 64.17 47.24
MoCo 67.44 50.88 65.96 49.21
SimCLR 63.65 46.68 60.01 43.17
ViT-S16ImageNet 65.71 48.93 62.78 45.75
MoCo 66.81 50.16 64.17 47.24
SimCLR 65.04 48.20 62.61 45.46
9
Winter wheat is planted in the fall and may be harvested before our summer imagery is taken. Sim-
ilarly, fallow can look much like pasture as weeds begin to grow in empty fields.
Fill Cloud Shadow Clear Thin Cloud Cloud
(a) Image
 (b) Mask
 (c) Prediction
Figure 5: Landsat 7 ETM+ TOA image, ground truth mask, and prediction made by a U-Net with a
ResNet-18 backbone pre-trained using MoCo and SSL4EO-L and fine-tuned on L7 Irish.
Figure 5 shows an example prediction made by a U-Net pre-trained on SSL4EO-L and fine-tuned on
L7 Irish. The model is able to correctly detect the majority of clouds in the image, but fails to detect
cloud shadow due to its infrequent appearance in the training dataset. However, the model actually
does a better job than the human annotator in the lower left corner, where the “ground truth” mask
misses substantial cloud and thin cloud.
5 Limitations
There are a few limitations of the sampling method we chose to create our pre-training dataset. Due
to low light levels near the poles, Landsat satellites do not capture images above 81.8° latitudes [77],
and do not produce SR products above 76° latitudes.4The additional 23.5° tilt of the Earth’s axis
during the winter [78] means that it is not possible to collect imagery for all 4 seasons above 52.5°
latitude. It may be possible to relax this constraint and allow for sampling from locations where 3
out of 4 seasons have imagery. Due to cloud cover and lower populations, there is very little imagery
of tropical rainforests or polar regions, both of which are common applications of Landsat data.
The benchmark datasets we create are limited to the United States and may not adequately re-
flect performance in other regions where agricultural practices and crops differ greatly. Ideally, we
would create additional global datasets. There exist large global Landsat-based datasets including
the Global Forest Cover Change dataset [40]. However, these datasets do not exist during all times
when these satellites are active. We would also like to have classification datasets in addition to
semantic segmentation datasets. It may be possible to classify images by biome, although this task
may be too easy. In future work, we would like to add pre-trained models for MSS data, although
this will require a different sampling technique due to limited coverage over most of the world.
6 Conclusion
In this paper we introduce the SSL4EO-L pre-training dataset, the first ever SSL dataset for Landsat
imagery and the largest Landsat dataset in history. We pre-train the first foundation models for
the Landsat family of satellites, enabling progress in a multitude of scientific fields that can benefit
from remote sensing and deep learning. Additionally, we revitalize the L7 Irish and L8 Biome
datasets. We create the first benchmark datasets for the TM and ETM+ SR sensors, allowing direct
comparison across all modern Landsat sensors and products. All datasets, model weights, training
code, and scripts used to produce our results are distributed via the TorchGeo library, allowing for
ease of experimentation and reproduction of our results.
4https://www.usgs.gov/landsat-missions/landsat-collection-2-surface-reflectance
10
Acknowledgments and Disclosure of Funding
The authors gratefully acknowledge the computational and data resources provided through the
joint high-performance data analytics (HPDA) project “terrabyte” of the German Aerospace Center
(DLR) and the Leibniz Supercomputing Center (LRZ). This work was supported by the Helmholtz
Association’s Initiative and Networking Fund on the HAICORE@FZJ partition. This work made
use of the Illinois Campus Cluster, a computing resource that is operated by the Illinois Campus
Cluster Program (ICCP) in conjunction with the National Center for Supercomputing Applications
(NCSA) and which is supported by funds from the University of Illinois at Urbana-Champaign. The
work was supported in part by the National Science Foundation (NSF) through awards IIS 21-31335,
OAC 21-30835, DBI 20-21898, as well as a C3.ai research award and the Taiwan-UIUC Fellowship.
References
[1] Laura E. P. Rocchio. Virginia T. Norwood: The mother of Landsat. Landsat Science , August
2020.
[2] Bill P. Clark. Landsat 3 Return Beam Vidicon response artifacts: A report on RBV pho-
tographic product characteristics and quality coding system. Technical report, EROS Data
Center, U.S. Geological Survey, August 1981.
[3] Christopher Engebretson. Landsat Multispectral Scanner (MSS) Collection 2 (C2) Level 1
(L1) Data Format Control Book (DFCB). Technical report, Department of the Interior, U.S.
Geological Survey, September 2020. LSDS-1416.
[4] Christopher Engebretson. Landsat Thematic Mapper (TM) Level 1 (L1) Data Format Control
Book (DFCB). Technical report, Department of the Interior, U.S. Geological Survey, February
2018. LSDS-284.
[5] Jim Lacasse. Landsat 7 (L7) Enhanced Thematic Mapper Plus (ETM+) Level 1 (L1) Data
Format Control Book (DFCB). Technical report, Department of the Interior, U.S. Geological
Survey, August 2016. LSDS-272.
[6] Christopher Engebretson. Landsat 8–9 Operational Land Imager (OLI) - Thermal Infrarer
Sensor (TIRS) Collection 2 Level 1 (L1) Data Format Control Book (DFCB). Technical report,
Department of the Interior, U.S. Geological Survey, September 2020. LSDS-1822.
[7] Nicholas E. Young, Ryan S. Anderson, Stephen M. Chignell, Anthony G. V orster, Rick
Lawrence, and Paul H. Evangelista. A survival guide to Landsat preprocessing. Ecology ,
98(4):920–932, 2017.
[8] Neal Jean, Sherrie Wang, Anshul Samar, George Azzari, David Lobell, and Stefano Ermon.
Tile2Vec: Unsupervised representation learning for spatially distributed data. In Proceedings
of the AAAI Conference on Artificial Intelligence , volume 33, pages 3967–3974, 2019.
[9] Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, Marshall Burke, David Lo-
bell, and Stefano Ermon. Geography-aware self-supervised learning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pages 10181–10190, 2021.
[10] Yezhen Cong, Samar Khanna, Chenlin Meng, Patrick Liu, Erik Rozi, Yutong He, Marshall
Burke, David Lobell, and Stefano Ermon. SatMAE: Pre-training transformers for temporal
and multi-spectral satellite imagery. Advances in Neural Information Processing Systems , 35:
197–211, 2022.
[11] Colorado J. Reed, Ritwik Gupta, Shufan Li, Sarah Brockman, Christopher Funk, Brian
Clipp, Salvatore Candido, Matt Uyttendaele, and Trevor Darrell. Scale-MAE: A scale-
aware masked autoencoder for multiscale geospatial representation learning. arXiv preprint
arXiv:2212.14532 , 2022.
[12] Oscar Manas, Alexandre Lacoste, Xavier Giró-i-Nieto, David Vazquez, and Pau Rodriguez.
Seasonal Contrast: Unsupervised pre-training from uncurated remote sensing data. In Pro-
ceedings of the IEEE/CVF International Conference on Computer Vision , pages 9414–9423,
2021.
11
[13] Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying Liu, Conrad M. Albrecht, and
Xiao Xiang Zhu. SSL4EO-S12: A large-scale multi-modal, multi-temporal dataset for self-
supervised learning in Earth observation. arXiv preprint arXiv:2211.07044 , 2022.
[14] Di Wang, Jing Zhang, Bo Du, Gui-Song Xia, and Dacheng Tao. An empirical study of remote
sensing pretraining. IEEE Transactions on Geoscience and Remote Sensing , 2022.
[15] Yi Wang, Conrad M. Albrecht, Nassim Ait Ali Braham, Lichao Mou, and Xiao Xiang Zhu.
Self-supervised learning in remote sensing: A review. arXiv preprint arXiv:2206.13188 , 2022.
[16] Paul Berg, Minh-Tan Pham, and Nicolas Courty. Self-supervised learning for scene classifica-
tion in remote sensing: Current state of the art and perspectives. Remote Sensing , 14(16):3995,
2022.
[17] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with Momentum
Contrastive learning. arXiv preprint arXiv:2003.04297 , 2020.
[18] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand
Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances
in Neural Information Processing Systems , 33:9912–9924, 2020.
[19] Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
15750–15758, 2021.
[20] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow Twins: Self-
supervised learning via redundancy reduction. In International Conference on Machine Learn-
ing, pages 12310–12320. PMLR, 2021.
[21] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In International Conference on Machine
Learning , pages 1597–1607. PMLR, 2020.
[22] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, et al. Bootstrap Your Own Latent-a new approach to self-supervised learning.
Advances in Neural Information Processing Systems , 33:21271–21284, 2020.
[23] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-
scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern
Recognition , pages 248–255. Ieee, 2009.
[24] Yuxing Chen and Lorenzo Bruzzone. Self-supervised SAR-optical data fusion of Sentinel-1/-2
images. IEEE Transactions on Geoscience and Remote Sensing , 60:1–11, 2021.
[25] Marrit Leenstra, Diego Marcos, Francesca Bovolo, and Devis Tuia. Self-supervised pre-
training enhances change detection in Sentinel-2 imagery. In Pattern Recognition. ICPR Inter-
national Workshops and Challenges: Virtual Event, January 10-15, 2021, Proceedings, Part
VII, pages 578–590. Springer, 2021.
[26] Jamie Tolan, Hung-I Yang, Ben Nosarzewski, Guillaume Couairon, Huy V o, John Brandt, Jus-
tine Spore, Sayantan Majumdar, Daniel Haziza, Janaki Vamaraju, et al. Sub-meter resolution
canopy height maps using self-supervised learning and a vision transformer trained on Aerial
and GEDI Lidar. arXiv preprint arXiv:2304.07213 , 2023.
[27] Jules Bourcier, Thomas Floquet, Gohar Dashyan, Tugdual Ceillier, Karteek Alahari, and Joce-
lyn Chanussot. Self-supervised pretraining on satellite imagery: A case study on label-efficient
vehicle detection. arXiv preprint arXiv:2210.11815 , 2022.
[28] Bo Peng, Qunying Huang, Jamp V ongkusolkit, Song Gao, Daniel B. Wright, Zheng N.
Fang, and Yi Qiang. Urban flood mapping with bitemporal multispectral imagery via a self-
supervised learning framework. IEEE Journal of Selected Topics in Applied Earth Observa-
tions and Remote Sensing , 14:2001–2016, 2020.
12
[29] Fabien H. Wagner, Ricardo Dalagnol, Alber H. Sánchez, Mayumi Hirye, Samuel Favrichon,
Jake H. Lee, Steffen Mauceri, Yan Yang, and Sassan Saatchi. K-textures, a self supervised
hard clustering deep learning algorithm for satellite images segmentation. arXiv preprint
arXiv:2205.08671 , 2022.
[30] Richard J. Kauth and G. S. Thomas. The tasselled cap–a graphic description of the spectral-
temporal development of agricultural crops as seen by Landsat. In LARS Symposia , page 159,
1976.
[31] James E. V ogelmann, Stephen M. Howard, Limin Yang, Charles R. Larson, Bruce K. Wylie,
and Nick Van Driel. Completion of the 1990s National Land Cover Data Set for the conter-
minous United States from Landsat Thematic Mapper data and ancillary data sources. Pho-
togrammetric Engineering and Remote Sensing , 67(6), 2001.
[32] Claire Boryan, Zhengwei Yang, Rick Mueller, and Mike Craig. Monitoring US agriculture: the
US Department of Agriculture, National Agricultural Statistics Service, Cropland Data Layer
program. Geocarto International , 26(5):341–358, 2011.
[33] David M. Johnson, Richard Mueller, et al. The 2009 Cropland Data Layer. Photogrammetric
Engineering and Remote Sensing , 76(11):1201–1205, 2010.
[34] David M. Johnson. Using the Landsat archive to map crop cover history across the United
States. Remote Sensing of Environment , 232:111286, 2019.
[35] Josefino C. Comiso and Konrad Steffen. Studies of Antarctic sea ice concentrations from
satellite data and their applications. Journal of Geophysical Research: Oceans , 106(C12):
31361–31385, 2001.
[36] Jeff Dozier and Danny Marks. Snow mapping and classification from Landsat Thematic Map-
per data. Annals of Glaciology , 9:97–103, 1987.
[37] Alex S. Gardner, Geir Moholdt, Ted Scambos, Mark Fahnstock, Stefan Ligtenberg, Michiel
Van Den Broeke, and Johan Nilsson. Increased West Antarctic and unchanged East Antarctic
ice discharge over the last 7 years. The Cryosphere , 12(2):521–547, 2018.
[38] Andrew K. Melkonian, Michael J. Willis, Matthew E. Pritchard, and Adam J. Stewart. Recent
changes in glacier velocities and thinning at Novaya Zemlya. Remote Sensing of Environment ,
174:244–257, 2016.
[39] Thomas J. Ballinger, Robert V . Rohli, Michael J. Allen, David A. Robinson, and Thomas W.
Estilow. Half-century perspectives on North American spring snowline and snow cover as-
sociations with the Pacific-North American teleconnection pattern. Climate Research , 74(3):
201–216, 2018.
[40] Matthew C. Hansen, Peter V . Potapov, Rebecca Moore, Matt Hancher, Svetlana A. Turubanova,
Alexandra Tyukavina, David Thau, Stephen V . Stehman, Scott J. Goetz, Thomas R. Loveland,
et al. High-resolution global maps of 21st-century forest cover change. Science , 342(6160):
850–853, 2013.
[41] Holly K. Gibbs, Sandra Brown, John O. Niles, and Jonathan A. Foley. Monitoring and estimat-
ing tropical forest carbon stocks: Making REDD a reality. Environmental Research Letters , 2
(4):045023, 2007.
[42] Holly K. Gibbs, Aaron S. Ruesch, Frédéric Achard, Murray K. Clayton, Peter Holmgren,
Navin Ramankutty, and Jonathan A. Foley. Tropical forests were the primary sources of new
agricultural land in the 1980s and 1990s. Proceedings of the National Academy of Sciences ,
107(38):16732–16737, 2010.
[43] Robert E. Kennedy, Zhiqiang Yang, and Warren B. Cohen. Detecting trends in forest distur-
bance and recovery using yearly Landsat time series: 1. LandTrendr—Temporal segmentation
algorithms. Remote Sensing of Environment , 114(12):2897–2910, 2010.
[44] David Skole and Compton Tucker. Tropical deforestation and habitat fragmentation in the
Amazon: Satellite data from 1978 to 1988. Science , 260(5116):1905–1910, 1993.
13
[45] Robert E. Kennedy, Serge Andréfouët, Warren B. Cohen, Cristina Gómez, Patrick Griffiths,
Martin Hais, Sean P. Healey, Eileen H. Helmer, Patrick Hostert, Mitchell B. Lyons, et al.
Bringing an ecological view of change to Landsat-based remote sensing. Frontiers in Ecology
and the Environment , 12(6):339–346, 2014.
[46] Pol Coppin, Inge Jonckheere, Kristiaan Nackaerts, Bart Muys, and Eric Lambin. Review
ArticleDigital change detection methods in ecosystem monitoring: A review. International
Journal of Remote Sensing , 25(9):1565–1596, 2004.
[47] Zhe Zhu. Change detection using Landsat time series: A review of frequencies, preprocessing,
algorithms, and applications. ISPRS Journal of Photogrammetry and Remote Sensing , 130:
370–384, 2017.
[48] Zhe Zhu and Curtis E. Woodcock. Continuous change detection and classification of land
cover using all available Landsat data. Remote Sensing of Environment , 144:152–171, 2014.
[49] Curtis E. Woodcock, Thomas R. Loveland, Martin Herold, and Marvin E. Bauer. Transitioning
from change detection to monitoring with remote sensing: A paradigm shift. Remote Sensing
of Environment , 238:111558, 2020.
[50] Michael A. Wulder, David P. Roy, V olker C. Radeloff, Thomas R. Loveland, Martha C. Ander-
son, David M. Johnson, Sean Healey, Zhe Zhu, Theodore A. Scambos, Nima Pahlevan, et al.
Fifty years of Landsat science and impacts. Remote Sensing of Environment , 280:113195,
2022.
[51] Crista L. Straub, Stephen R. Koontz, John B. Loomis, et al. Economic valuation of Landsat
imagery. Open-File Report - US Geological Survey , 2019.
[52] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition , pages 770–778, 2016.
[53] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 , 2020.
[54] Adam J. Stewart, Caleb Robinson, Isaac A. Corley, Anthony Ortiz, Juan M. Lavista Ferres,
and Arindam Banerjee. TorchGeo: Deep learning with geospatial data. In Proceedings of the
30th International Conference on Advances in Geographic Information Systems , pages 1–12,
2022.
[55] Pareto Software, LLC. World cities database, March 2023. URL https://simplemaps.com/data/
world-cities.
[56] Antonin Guttman. R-trees: A dynamic index structure for spatial searching. In Proceedings
of the 1984 ACM SIGMOD International Conference on Management of Data , pages 47–57,
1984.
[57] Noel Gorelick, Matt Hancher, Mike Dixon, Simon Ilyushchenko, David Thau, and Rebecca
Moore. Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote Sens-
ing of Environment , 2017. URL https://doi.org/10.1016/j.rse.2017.06.031.
[58] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL https://archive.
ics.uci.edu/ml.
[59] M. Joseph Hughes and Daniel J. Hayes. Automated detection of cloud and cloud shadow
in single-date Landsat imagery using neural networks and spatial post-processing. Remote
Sensing , 6(6):4907–4926, 2014.
[60] U.S. Geological Survey. L7 Irish cloud validation masks. U.S. Geological Survey data release,
2016. URL https://doi.org/10.5066/F7XD0ZWC.
14
[61] Pasquale L. Scaramuzza, Michelle A. Bouchard, and John L. Dwyer. Development of the
Landsat data continuity mission cloud-cover assessment algorithms. IEEE Transactions on
Geoscience and Remote Sensing , 50(4):1140–1154, 2011.
[62] Pasquale L. Scaramuzza. Landsat 7 Collection 2 cloud truth mask validation set. U.S. Geolog-
ical Survey data release, 2022. URL https://doi.org/10.5066/P9ASLQQE.
[63] M. Joseph Hughes and Robert Kennedy. High-quality cloud masking of Landsat 8 imagery
using convolutional neural networks. Remote Sensing , 11(21):2591, 2019.
[64] U.S. Geological Survey. L8 SPARCS cloud validation masks. U.S. Geological Survey data
release, 2016. URL https://doi.org/10.5066/F7FB5146.
[65] Steve Foga, Pat L. Scaramuzza, Song Guo, Zhe Zhu, Ronald D. Dilley Jr., Tim Beckmann,
Gail L. Schmidt, John L. Dwyer, M. Joseph Hughes, and Brady Laue. Cloud detection al-
gorithm comparison and validation for operational Landsat data products. Remote Sensing of
Environment , 194:379–390, 2017.
[66] U.S. Geological Survey. L8 Biome cloud validation masks. U.S. Geological Survey data
release, 2016. URL https://doi.org/10.5066/F7251GDH.
[67] Pasquale L. Scaramuzza. Landsat 8 Collection 2 cloud truth mask validation set. U.S. Geolog-
ical Survey data release, 2021. URL https://doi.org/10.5066/P9FI4A0Y.
[68] Richard R. Irish, John L. Barker, Samuel N. Goward, and Terry Arvidson. Characterization of
the Landsat-7 ETM+ automated cloud-cover assessment (ACCA) algorithm. Photogrammetric
Engineering and Remote Sensing , 72(10):1179–1188, 2006.
[69] U.S. Geological Survey. L7 Irish cloud validation masks. USGS ScienceBase Catalog, 2015.
URL https://www.sciencebase.gov/catalog/item/573ccf18e4b0dae0d5e4b109.
[70] Jon Dewitz and U.S. Geological Survey. National Land Cover Database (NLCD) 2019 products
(ver. 2.0). U.S. Geological Survey data release, June 2021. URL https://doi.org/10.5066/
P9KZCM54.
[71] USDA National Agricultural Statistics Service (USDA-NASS). Cropland Data Layer (CDL).
Published crop-specific data layer, 2019. URL https://nassgeodata.gmu.edu/CropScape/. Ac-
cessed 2023.
[72] Collin Homer, Jon Dewitz, Suming Jin, George Xian, Catherine Costello, Patrick Danielson,
Leila Gass, Michelle Funk, James Wickham, Stephen Stehman, et al. Conterminous United
States land cover change patterns 2001–2016 from the 2016 national land cover database.
ISPRS Journal of Photogrammetry and Remote Sensing , 162:184–199, 2020.
[73] Suming Jin, Collin Homer, Limin Yang, Patrick Danielson, Jon Dewitz, Congcong Li, Zhe
Zhu, George Xian, and Danny Howard. Overall methodology design for the United States
national land cover database 2016 products. Remote Sensing , 11(24):2971, 2019.
[74] Limin Yang, Suming Jin, Patrick Danielson, Collin Homer, Leila Gass, Stacie M. Bender,
Adam Case, Catherine Costello, Jon Dewitz, Joyce Fry, et al. A new generation of the United
States National Land Cover Database: Requirements, research priorities, design, and imple-
mentation strategies. ISPRS Journal of Photogrammetry and Remote Sensing , 146:108–123,
2018.
[75] James Wickham, Stephen V . Stehman, Daniel G. Sorenson, Leila Gass, and Jon A. Dewitz.
Thematic accuracy assessment of the NLCD 2016 land cover for the conterminous United
States. Remote Sensing of Environment , 257:112357, 2021.
[76] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks
for biomedical image segmentation. In Medical Image Computing and Computer-Assisted
Intervention–MICCAI 2015: 18th International Conference, Proceedings, Part III 18 , pages
234–241, Munich, Germany, October 2015. Springer.
15
[77] Robert Bindschadler. Landsat coverage of the earth at high latitudes. Photogrammetric Engi-
neering & Remote Sensing , 69(12):1333–1339, 2003.
[78] John D. Boon. The tilt of the earth’s axis and the consequences thereof. Field and Laboratory ,
13(1):2, 1945.
16
A Appendix
A.1 Ethics statement
Although satellite imagery in general can pose ethical concerns for surveillance and military appli-
cations, the imagery used to pre-train our models is low resolution (30 m/px) and cannot be used
for such purposes. The primary applications Landsat imagery is useful for are Earth observation, in-
cluding downstream tasks like climate change, agriculture, and ecology. While model training does
contribute to greenhouse gas emissions, we believe that the benefits of such foundation models,
especially their ability to reduce training demands for end users, outweigh these contributions.
A.2 Licensing
All data used to create our datasets is released by the USGS under public domain, and may be
used, shared, transferred, or redistributed without restriction. All datasets and models we create are
released under a CC0 1.0 Universal license. All code, including training scripts and core Torch-
Geo contributions, is released under an MIT license. The authors bear all responsibility in case of
violation of rights.
A.3 Downloading
All data, metadata, and pre-trained models used or created in this paper can be downloaded from
https://huggingface.co/torchgeo, either manually or using TorchGeo (see Listing 1). Dataset images
are stored in the widely used GeoTIFF format. These datasets and models will be maintained in
perpetuity and may be improved over time. All datasets include dataset cards describing the dataset
size, source, and license. All models include model cards describing the library used to load them,
source, and license.
from torchgeo.datasets import SSL4EOL
ds = SSL4EOL(root="data", split="oli_sr", download=True)
Listing 1: Example download script for the OLI SR split of the SSL4EO-L pre-training dataset.
A.4 Reproducibility
Instructions to recreate the pre-training and benchmark datasets, results, or plots, can be found
at https://github.com/microsoft/torchgeo/blob/releases/v0.5/experiments/ssl4eo/landsat/README.
md. Listing 2 shows example code for pre-training on SSL4EO-L and fine-tuning/evaluating on
our benchmark datasets, and can be modified to control other aspects of the training process or to
train on a different sensor/product. The TorchGeo v0.5 release is the first release containing the
datasets and models used and created in this paper. If you encounter any problems, please open an
issue on GitHub and we will clarify the documentation.
17
from lightning.pytorch import Trainer
from torchgeo.datamodules import (
SSL4EOLDataModule, SSL4EOLBenchmarkDataModule
)
from torchgeo.trainers import MoCoTask, SemanticSegmentationTask
# Pre-train on SSL4EO-L using MoCo
datamodule = SSL4EOLDataModule(split="oli_sr", seasons=2, download=True)
task = MoCoTask(model="resnet18", weights=True, in_channels=7)
trainer = Trainer(max_epochs=200)
trainer.fit(model=task, datamodule=datamodule)
# Fine-tune and evaluate performance
datamodule = SSL4EOLBenchmarkDataModule(sensor="oli_sr", product="cdl")
task = SemanticSegmentationTask(model="unet", backbone="resnet18")
trainer = Trainer(max_epochs=100)
trainer.fit(model=task, datamodule=datamodule)
trainer.test(model=task, datamodule=datamodule)
Listing 2: Example training script to pre-train and benchmark a model on SSL4EO-L.
A.5 Class distribution
The benchmark datasets we use suffer from extreme class imbalance. Below are tables documenting
the value, description, and percentage of each class in all datasets. Fill/background classes are
ignored during training and are not considered when computing these statistics.
A.5.1 Cloud detection datasets
Clear pixels cover more area than all other classes combined.
Table 3: Class distribution for cloud detection datasets.
Value Description L7 Irish L8 Biome
0 Fill - -
64 Cloud Shadow 0.7 1.5
128 Clear 66.1 50.5
192 Thin Cloud 10.2 14.7
255 Cloud 23.0 33.2
A.5.2 SSL4EO-L benchmark datasets
The top 3 classes cover more area than all other classes combined. Only classes with > 1% area are
considered during evaluation, the rest are mapped to the background class. TM data is downloaded
from 2011, while ETM+ and OLI data is downloaded from 2019. The TOA and SR versions have
the same geographic locations, and therefore the same class distribution.
18
Table 4: Class distribution for SSL4EO-L NLCD.
Value Description TM ETM+ OLI
0 Background - - -
11 Open Water 2.4 2.2 2.3
21 Developed, Open Space 2.7 2.7 2.6
22 Developed, Low Intensity 1.7 1.7 1.7
31 Barren Land (Rock/Sand/Clay) 1.0 1.0 1.0
41 Deciduous Forest 9.2 9.2 8.8
42 Evergreen Forest 12.2 11.9 12.1
43 Mixed Forest 3.4 3.4 3.2
52 Shrub/Scrub 22.4 22.8 23.6
71 Grassland/Herbaceous 14.9 14.6 14.6
81 Pasture/Hay 6.2 5.9 5.8
82 Cultivated Crops 16.6 17.3 17.1
90 Woody Wetlands 4.5 4.4 4.3
95 Emergent Herbaceous Wetlands 1.6 1.5 1.6
- Other 1.2 1.4 1.3
Table 5: Class distribution for SSL4EO-L CDL.
Value Description TM ETM+ OLI
0 Background - - -
1 Corn 4.6 4.9 4.7
5 Soybeans 3.6 4.1 3.9
24 Winter Wheat 1.9 1.6 1.6
36 Alfalfa 0.9 1.1 1.2
37 Other Hay/Non Alfalfa 1.2 1.6 1.6
61 Fallow/Idle Cropland 1.4 1.9 1.8
111 Open Water 1.7 1.7 1.7
121 Developed/Open Space 3.3 2.9 2.8
122 Developed/Low intensity 1.4 1.5 1.5
131 Barren 1.1 1.1 1.1
141 Deciduous Forest 11.9 10.6 10.2
142 Evergreen Forest 13.3 12.7 12.9
143 Mixed Forest 1.5 3.2 2.9
152 Shrubland 22.4 24.2 25.0
176 Grass/Pasture 20.3 16.6 16.5
190 Woody Wetlands 3.9 4.2 4.1
195 Herbaceous Wetlands 1.3 1.4 1.5
- Other 4.2 4.7 4.8
19
A.6 Spectral bands
0
Wavelength ( µm)0
0.5 1.0 1.5 2.0Landsat 8–9
(OLI/TIRS)Landsat 7
(ETM+)Landsat 6
(ETM)Landsat 4–5
(TM)Landsat 1–5
(MSS)Landsat 3
(RBV)Landsat 1–2
(RBV)
89
7 65 4
32187 5 432187 5 43217 5 432143211321
11 121530100153060153012030120804080
Resolution (m)
11 10666
Figure 6: Spectral wavelengths and spatial resolutions of each band captured by all Landsat sensors.
On Landsats 1–3, MSS bands were actually numbered 4–7. Landsat 9 introduced new and improved
OLI-2/TIRS-2 sensors, but the bands are identical, so the sensors were combined in this figure.
A.7 Data visualization
(a) Spring (2021-03-22)
 (b) Summer (2022-06-13)
 (c) Fall (2022-10-19)
 (d) Winter (2022-12-22)
Figure 7: Example location showing the time-series nature of SSL4EO-L. Each location has imagery
from 4 different seasons. Images are selected from a 60-day window centered about the vernal and
autumnal equinoxes and the summer and winter solstices in order to maximize seasonal changes.
Images are limited to a 2-year window to minimize man-made changes. Image location is Ci County,
Handan, Heibei, China.
20
A.8 Model complexity
Table 6: Complexity of backbone models used in this paper. Includes the number of parameters,
memory requirements, floating point operations per second (FLOPS), and multiply-accumulate op-
erations (MACs) of each model. All experiments were performed on an NVIDIA A100 GPU.
Model # Params (M) Memory (MB) FLOPS (G/s) MACs (G)
ResNet-18 11.21 44.87 622.49 136.21
ResNet-50 23.56 94.46 366.32 281.72
ViT-S16 22.46 89.83 423.61 281.28
A.9 Sampling algorithm
Procedure DownloadSSL4EO( N= 250 ,000, S= 4, σ= 50 km)
Data: M={µ}centroids of 10K most populous cities in the world
Result: Downloads non-overlapping, cloud-free, nodata-free images from Nlocations during
Sseasons
X← {}
while len( X)< N :
µ∼ U(M)
x∼ N(µ, σ)
# Ensure x does not overlap with existing sampled patches
ifOverlaps (x,X):# 264 px buffer
continue
# Look for S cloud-free, nodata-free images at location x
T← {}
t←0
fortinrange( S):# 60-day and 2-year window around equinoxes/solstices
ifCloudCover( x, t):# 20% threshold
continue
ifNoData( x, t):
continue
T←T∪ {t}
iflen( T)< S:
continue
# Download from Google Earth Engine
Download( x,T)
X←X∪ {x}
21
Detecting trend and seasonal changes in satellite image time series
Jan Verbesselta,⁎, Rob Hyndmanb, Glenn Newnhama, Darius Culvenora
aRemote Sensing Team, CSIRO Sustainable Ecosystems, Private Bag 10, Melbourne VIC 3169, Australia
bDepartment of Econometrics and Business Statistics, Monash University, Melbourne VIC 3800, Australia
abstract article info
Article history:
Received 24 June 2009Received in revised form 13 August 2009Accepted 18 August 2009
Keywords:Change detectionNDVI
Time series
Trend analysisMODISPiecewise linear regressionVegetation dynamicsPhenologyA wealth of remotely sensed image time series covering large areas is now available to the earth science
community. Change detection methods are often not capable of detecting land cover changes within time
series that are heavily in ﬂuenced by seasonal climatic variations. Detecting change within the trend and
seasonal components of time series enables the classi ﬁcation of different types of changes. Changes occurring
in the trend component often indicate disturbances (e.g. ﬁres, insect attacks), while changes occurring in the
seasonal component indicate phenological changes (e.g. change in land cover type). A generic changedetection approach is proposed for time series by detecting and characterizing Breaks For Additive Seasonal
and Trend (BFAST). BFAST integrates the decomposition of time series into trend, seasonal, and remainder
components with methods for detecting change within time series. BFAST iteratively estimates the time andnumber of changes, and characterizes change by its magnitude and direction. We tested BFAST by simulating16-day Normalized Difference Vegetation Index (NDVI) time series with varying amounts of seasonality and
noise, and by adding abrupt changes at different times and magnitudes. This revealed that BFAST can
robustly detect change with different magnitudes (>0.1 NDVI) within time series with different noise levels(0.01 –0.07σ) and seasonal amplitudes (0.1 –0.5 NDVI). Additionally, BFAST was applied to 16-day NDVI
Moderate Resolution Imaging Spectroradiometer (MODIS) composites for a forested study area in south
eastern Australia. This showed that BFAST is able to detect and characterize spatial and temporal changes in aforested landscape. BFAST is not speci ﬁc to a particular data type and can be applied to time series without
the need to normalize for land cover types, select a reference period, or change trajectory. The method can be
integrated within monitoring frameworks and used as an alarm system to ﬂag when and where changes
occur.
Crown Copyright © 2009 Published by Elsevier Inc. All rights reserved.
1. Introduction
Natural resource managers, policy makers and researchers de-
mand knowledge of land cover changes over increasingly large spatial
and temporal extents for addressing many pressing issues such as
global climate change, carbon budgets, and biodiversity ( DeFries et al.,
1999; Dixon et al., 1994 ). Detecting and characterizing change over
time is the natural ﬁrst step toward identifying the driver of the
change and understanding the change mechanism. Satellite remote
sensing has long been used as a means of detecting and classifying
changes in the condition of the land surface over time ( Coppin et al.,
2004; Lu et al., 2004 ). Satellite sensors are well-suited to this task
because they provide consistent and repeatable measurements at a
spatial scale which is appropriate for capturing the effects of many
processes that cause change, including natural (e.g. ﬁres, insect
attacks) and anthropogenic (e.g. deforestation, urbanization, farming)
disturbances ( Jin and Sader, 2005 ).The ability of any system to detect change depends on its capacity
to account for variability at one scale (e.g. seasonal variations), while
identifying change at another (e.g. multi-year trends). As such, change
in ecosystems can be divided into three classes: (1) seasonal change ,
driven by annual temperature and rainfall interactions impacting plant
phenology or proportional cover of land cover types with different
plant phenology; (2) gradual change such as interannual climate
variability (e.g. trends in mean annual rainfall) or gradual change in
land management or land degradation; and (3) abrupt change , caused
by disturbances such as deforestation, urbanization, ﬂoods, and ﬁres.
Although the value of remotely sensed long term data sets for
change detection has been ﬁrmly established ( de Beurs and Henebry,
2005 ), only a limited number of time series change detection methods
have been developed. Two major challenges stand out. First, methods
must allow for the detection of changes within complete long term
data sets while accounting for seasonal variation. Estimating change
from remotely sensed data is not straightforward, since time series
contain a combination of seasonal, gradual and abrupt changes, in
addition to noise that originates from remnant geometric errors,
atmospheric scatter and cloud effects ( Roy et al., 2002 ). ThoroughRemote Sensing of Environment 114 (2010) 106 –115
⁎Corresponding author. Tel.: +61 395452265; fax: +61 395452448.
E-mail address: Jan.Verbesselt@csiro.au (J. Verbesselt).
0034-4257/$ –see front matter. Crown Copyright © 2009 Published by Elsevier Inc. All rights reserved.
doi:10.1016/j.rse.2009.08.014
Contents lists available at ScienceDirect
Remote Sensing of Environment
journal homepage: www.elsevier.com/locate/rse
reviews of existing change detection methods by Coppin et al. (2004)
andLu et al. (2004) have shown, however, that most methods focus on
short image time series (only 2 –5 images). The risk of confounding
variability with change is high with infrequent images, since
disturbances can occur in between image acquisitions ( de Beurs and
Henebry, 2005 ). Several approaches have been proposed for analyzing
image time series, such as Principal Component Analysis (PCA) ( Crist
and Cicone, 1984 ), wavelet decomposition ( Anyamba and Eastman,
1996 ), Fourier analysis ( Azzali and Menenti, 2000 ) and Change Vector
Analysis (CVA) ( Lambin and Strahler, 1994 ). These time series analysis
approaches discriminate noise from the signal by its temporal
characteristics but involve some type of transformation designed to
isolate dominant components of the variation across years of imagery
through the multi-temporal spectral space. The challenge of these
methods is the labeling of the change components, because each
analysis depends entirely on the speci ﬁc image series analyzed.
Compared to PCA, Fourier analysis, and wavelet decomposition, CVA
allows the interpretation of change processes, but can still only be
performed between two periods of time (e.g. between years or
growing seasons) ( Lambin and Strahler, 1994 ), which makes the
analysis dependent on the selection of these periods. Furthermore,
change in time series is often masked by seasonality driven by yearly
temperature and rainfall variation. Existing change detection techni-
ques minimize seasonal variation by focussing on speci ﬁc periods
within a year (e.g. growing season) ( Coppin et al., 2004 ), temporally
summarizing time series data ( Bontemps et al., 2008; Fensholt et al.,
2009 ) or normalizing re ﬂectance values per land cover type ( Healey
et al., 2005 ) instead of explicitly accounting for seasonality.
Second, change detection techniques need to be independent of
speci ﬁc thresholds or change trajectories. Change detection methods
that require determination of thresholds often produce misleading
results due to different spectral and phenological characteristics of
land cover types ( Lu et al., 2004 ). The determination of thresholds
adds signi ﬁcant cost to efforts to expand change detection to large
areas. Trajectory based change detection has been proposed to move
towards a threshold independent change detection by characterizing
change by its temporal signature ( Hayes and Cohen, 2007; Kennedy
et al., 2007 ). This approach requires the de ﬁnition of the change
trajectory speci ﬁc for the type of change to be detected and spectral
data to be analyzed (e.g. short-wave infrared or near-infrared basedindices). Furthermore, the method will only function if the observed
spectral trajectory matches one of the hypothesized trajectories.
Trajectory based change detection can be interpreted as a supervised
change detection method while there is a need for an unsupervised,
more generic, change detection approach independent of the data
type and change trajectory.
The purpose of this research is to develop a generic change detection
approach for time series, involving the detection and characterization of
Breaks For Additive Seasonal and Trend (BFAST). BFAST integrates the
iterative decomposition of time series into trend, seasonal and noise
components with methods for detecting changes, without the need to
select a reference period, set a threshold, or de ﬁne a change trajectory.
The main objectives are:
(1) The detection of multiple abrupt changes in the seasonal and
trend components of the time series; and
(2) The characterization of gradual and abrupt ecosystem change
by deriving the time, magnitude, and direction of change
within the trend component of the time series.
We assessed BFAST for a large range of ecosystems by simulating
Normalized Difference Vegetation Index (NDVI) time series with
varying amounts of seasonal variation and noise, and by adding
abrupt changes with different magnitudes. We applied the approach
on MODIS 16-day image composites (hereafter called 16-day time
series) to detect major changes in a forested area in south eastern
Australia. The approach is not speci ﬁc to a particular data type andcould be applied to detect and characterize changes within other
remotely sensed image time series (e.g. Landsat) or be integrated
within monitoring frameworks and used as an alarm system to
provide information on when and where changes occur.
2. Iterative change detection
We propose a method that integrates the iterative decomposition
of time series into trend, seasonal and noise components with
methods for detecting and characterizing changes (i.e. breakpoints)
within time series. Standard time series decomposition methods
assume that trend and seasonal components are smooth and slowly
changing, and so these are not directly applicable to the problem of
identifying change. For example, the Seasonal-Trend decomposition
procedure (STL) is capable of ﬂexibly decomposing a series into trend,
seasonal and remainder components based on a LOcally wEighted
regreSsion Smoother (LOESS) ( Cleveland et al., 1990 ). This smoothing
prevents the detection of changes within time series.
2.1. Decomposition model
We propose an additive decomposition model that iteratively ﬁts a
piecewise linear trend and seasonal model. The general model is of the
form Yt=Tt+St+et,t=1,…,n, where Ytis the observed data at time
t,Ttis the trend component, Stis the seasonal component, and etis the
remainder component. The remainder component is the remaining
variation in the data beyond that in the seasonal and trend
components ( Cleveland et al., 1990 ). It is assumed that Ttis piecewise
linear, with break points t1⁎,…,tm⁎and de ﬁnet0⁎=0, so that:
Tt=αj+βjt ð1Þ
fortj−1⁎<t≤tj⁎and where j=1,…,m. The intercept and slope of
consecutive linear models, αjandβj, can be used to derive the
magnitude and direction of the abrupt change (hereafter referred to
as magnitude) and slope of the gradual change between detected
break points. The magnitude of an abrupt change at a breakpoint is
derived by the difference between Ttattj−1⁎andtj⁎, so that:
Magnitude = αj−1−αj/C16/C17
+βj−1−βj/C16/C17
t ð2Þ
and the slopes of the gradual change before and after a break point are
βj−1andβj. This technique represents a simple yet robust way to
characterize changes in time series. Piecewise linear models, as a
special case of non-linear regression ( Venables and Ripley, 2002 ), are
often used as approximations to complex phenomena to extract basic
features of the data ( Zeileis et al., 2003 ).
Similarly, the seasonal component is ﬁxed between break points, but
can vary across break points. Furthermore, the seasonal break points
may occur at different times from the break points detected in the trend
component. Let the seasonal break points be given by t1#,…,tp#,a n d
deﬁnet0#=0. Then for tj−1#<t≤tj#, we assume that:
St=γi;j if time tis in season i;i=1 ;…;s−1;
−Ps−1
i=1γi;jif time tis in season 0 ;(
ð3Þ
where sis the period of seasonality (e.g. number of observations per
year) and γi,jdenotes the effect of season i. Thus, the sum of the seasonal
component, Stacross ssuccessive times is exactly zero for tj−1#<t≤tj#.
This prevents apparent changes in trend being induced by seasonal
breaks happening in the middle of a seasonal cycle. The seasonal term
can be re-expressed as:
St=Xs−1
i=1γi;jdt;i−dt;0/C16/C17
ð4Þ107 J. Verbesselt et al. / Remote Sensing of Environment 114 (2010) 106 –115
where dt,i=1 when tis in season iand 0 otherwise. Therefore, if tis in
season 0, then dt,i−dt,0=−1. For all other seasons, dt,i−dt,0=1when t
is in season i≠0.dt,iis often referred to as a seasonal dummy variable
(Makridakis et al., 1998 , pp. 269 –274); it has two allowable values (0
and 1) to account for the seasons in a regression model. The regression
model expressed by Eq. (4)can also be interpreted as a model without
intercept that contains s−1 seasonal dummy variables.
2.2. Iterative algorithm to detect break points
Our method is similar to that proposed by Haywood and Randal
(2008) for use with monthly tourism data. Following Haywood and
Randal (2008) , we estimate the trend and seasonal components
iteratively. However, we differ from their method by: (1) using STL to
estimate the initial seasonal component Sˆt; (2) using a robust procedure
when estimating the coef ﬁcients αj,βjandγi,j; (3) using a preliminary
structural change test; and 4) forcing the seasonal coef ﬁcients to always
sum to 0 (rather than adjusting them afterward). An alternative
approach proposed by Shao and Campbell (2002) combines the
seasonal and trend term in a piecewise linear regression model without
iterative decomposition. This approach does not allow for an individual
estimation of breakpoints in the seasonal and trend component.
Sequential test methods for detecting break points (i.e. abrupt
changes) in a time series have been developed, particularly within
econometrics ( Bai and Perron, 2003; Zeileis et al., 2003 ). These
methods also allow linear models to be ﬁtted to sections of a time
series, with break points at the times where the changes occur. The
optimal position of these breaks can be determined by minimizing the
residual sum of squares, and the optimal number of breaks can be
determined by minimizing an information criterion. Bai and Perron
(2003) argue that the Akaike Information Criterion usually over-
estimates the number of breaks, but that the Bayesian Information
Criterion (BIC) is a suitable selection procedure in many situations
(Zeileis et al., 2002; Zeileis et al., 2003; Zeileis and Kleiber, 2005 ).
Before ﬁtting the piecewise linear models and estimating the break-
points it is recommended to test whether breakpoints are occurring in
the time series ( Bai and Perron, 2003 ). The ordinary least squares
(OLS) residuals-based MOving SUM (MOSUM) test, is selected to test
for whether one or more breakpoints are occurring ( Zeileis, 2005 ). If
the test indicates signi ﬁcant change ( P<0.05), the break points are
estimated using the method of Bai and Perron (2003) , as implemented
byZeileis et al. (2002) , where the number of breaks is determined by
the BIC, and the date and con ﬁdence interval of the date for each break
are estimated.
The iterative procedure begins with an estimate of S ̂tby using the
STL method, where S ̂tis estimated by taking the mean of all seasonal
sub-series (e.g. for a monthly time series the ﬁrst sub-series contains
the January values). Then it follows these steps.
Step 1 If the OLS-MOSUM test indicates that breakpoints are
occurring in the trend component, the number and position
of the trend break points ( t1⁎,…,tm⁎) are estimated from the
seasonally adjusted data, Yt−S ̂t.
Step 2 The trend coef ﬁcients, αjandβjforj=1,…,m, are then
computed using robust regression of Eq. (1)based on M-
estimation ( Venables and Ripley, 2002 ). The trend estimate is
then set to T ̂t=α ̂j+β ̂jtfort=t j−1⁎+1,…,tj⁎.
Step 3 If the OLS-MOSUM test indicates that breakpoints are
occurring in the seasonal component, the number and
position of the seasonal break points ( t1#,…,tp#) are estimated
from the detrended data, Yt−T ̂t.
Step 4 The seasonal coef ﬁcients, γi,jforj=1,…,mandi=1,…,s−1,
are then computed using a robust regression of Eq. (4)based
on M-estimation. The seasonal estimate is then set to ˆSt=Ps−1
i=1 ˆγi;jdt;i−dt;0/C0/C1
fort=t j−1#+1,…,tj#.These steps are iterated until the number and position of the
breakpoints are unchanged. We have followed the recommendations
ofBai and Perron (2003) and Zeileis et al. (2003) concerning the
fraction of data needed between the breaks. For 16-day time series,
we used a minimum of one year of data (i.e. 23 observations) between
successive change detections, corresponding to 12% of a 9 year data
span (2000 –2008). This means that if two changes occur within a
year, only the most signi ﬁcant change will be detected.
3. Validation
The proposed approach can be applied to a variety of time series,
and is not restricted to remotely sensed vegetation indices. However,
validation has been conducted using Normalized Difference Vegeta-
tion Index (NDVI) time series, the most widely used vegetation index
in medium to coarse scale studies. The NDVI is a relative and indirect
measure of the amount of photosynthetic biomass, and is correlated
with biophysical parameters such as green leaf biomass and the
fraction of green vegetation cover, whose behavior follows annual
cycles of vegetation growth ( Myneni et al., 1995; Tucker, 1979 ).
We validated BFAST by (1) simulating 16-day NDVI time series, and
(2) applying the method to 16-day MODIS satellite NDVI time series(2000 –2008). Validation of multi-temporal change detection methods
is often not straightforward, since independent reference sources for a
broad range of potential changes must be available during the change
interval. Field validated single-date maps are unable to represent the
type and number of changes detected ( Kennedy et al., 2007 ). We
simulated 16-day NDVI time series with different noise, seasonality,
and change magnitudes in order to robustly test BFAST in a controlled
environment. However, it is challenging to create simulated time
series that approximate remotely sensed time series which contain
combined information on vegetation phenology, interannual climate
variability, disturbance events, and signal contamination (e.g. clouds)
(Zhang et al., 2009 ). Therefore, applying the method to remotely
sensed data and performing comparisons with in-situ data remains
necessary. In the next two sections, we apply BFAST to simulated and
MODIS NDVI time series.
3.1. Simulation of NDVI time series
NDVI time series are simulated by extracting key characteristics from
MODIS 16-day NDVI time series. We selected two MODIS NDVI time
series (as described in Section 3.2 ) representing a grassland and a pine
plantation ( Fig. 1 ), expressing the most different phenology in the study
area, to extract seasonal amplitude, noise level, and average value.
Simulated NDVI time series are generated by summing individually
simulated seasonal, noise, and trend components. First, the seasonal
component is created using an asymmetric Gaussian function for each
season. This Gaussian-type function has been shown to perform well
when used to extract seasonality by ﬁtting the function to time series
(Jönsson and Eklundh, 2002 ). The amplitude of the MODIS NDVI time
series was estimated using the range of the seasonal component derived
with the STL function, as shown in Fig. 2 . The estimated seasonal
amplitudes of the real forest and grassland MODIS NDVI time series
were 0.1 and 0.5 ( Fig. 1 ). Second, the noise component was generated
using a random number generator that follows a normal distribution N
(µ=0,σ=x), where the estimated xvalues were 0.04 and 0.02, to
approximate the noise within the real grass and forest MODIS NDVI time
series ( Lhermitte et al., submitted for publication ). Vegetation index
speciﬁc noise was generated by randomly replacing the white noise by
noise with a value of −0.1, representing cloud contamination that often
remains after atmospheric correction and cloud masking procedures.
Third, the real grass and forest MODIS NDVI time series were
approximated by selecting constant values 0.6 and 0.8 and summingthem with the simulated noise and seasonal component. A comparison
between real and simulated NDVI time series is shown in Fig. 1 .108 J. Verbesselt et al. / Remote Sensing of Environment 114 (2010) 106 –115
Based on the parameters required to simulate NDVI time series
similar to the real grass and forest MODIS NDVI time series ( Fig. 1 ), we
selected a range of amplitude and noise values for the simulation
study ( Table 1 ). These values are used to simulate NDVI time series of
different quality (i.e. varying signal to noise ratios) representing a
large range of land cover types.
The accuracy of the method for estimating the number, timing and
magnitude of abrupt changes was assessed by adding disturbances with
as p e c i ﬁc magnitude to the simulated time series. A simple disturbance
was simulated by combining a step function with a speci ﬁc magnitude
(Table 1 ) and linear recovery phase ( Kennedy et al., 2007 ). As such, the
disturbance can be used to simulate, for example, a ﬁre in a grassland or
an insect attack on a forest. Three disturbances were added to the sum of
simulated seasonal, trend, and noise components using simulation
parameters in Table 1 . An example of a simulated NDVI time series with
three disturbances is shown in Fig. 3 . A Root Mean Square Error (RMSE)
was derived for 500 iterations of all the combinations of amplitude,
noise and magnitude of change levels to quantify the accuracy of
estimating: (1) the number of detected changes, (2) the time of change,
and (3) the magnitude of change.3.2. Spatial application on MODIS image time series
We apply BFAST to real remotely sensed time series, and compare
the detected changes with a spatial validation data set. BFAST provides
information on the number, time, magnitude and direction of changes
in the trend and seasonal components of a time series. We focussed on
the timing and magnitude of major changes occurring within the trend
component.
We selected the 16-day MODIS NDVI composites with a 250 m
spatial resolution (MOD13Q1 collection 5), since this product provides
frequent information at the spatial scale at which the majority of
human-driven land cover changes occur ( Townshend and Justice,
1988 ).The MOD13Q1 16-day composites were generated using a
constrained view angle maximum NDVI value compositing technique
(Huete et al., 2002 ). The MOD13Q1 images were acquired from February
24th of 2000 to the end of 2008 (23 images/year except for the year
2000) for a multi-purpose forested study area ( Pinus radiata plantation)
in South Eastern Australia (Lat. 35.5° S, Lon. 148.0° E). The images
contain data from the red (620 –670 nm) and near-infrared (NIR, 841 –
876 nm) spectral wavelengths. We used the binary MODIS Quality
Fig. 1. Real and simulated 16-day NDVI time series of a grassland (top) and pine plantation (bottom).
Fig. 2. The STL decomposition of a 16-day NDVI time series of a pine plantation into seasonal, trend, and remainder components. The seasonal component is esti mated by taking the
mean of all seasonal sub-series (e.g. for a monthly time series the ﬁrst sub-series contains the January values). The sum of the seasonal, trend, and remainder components equals the
data series. The solid bars on the right hand side of the plot show the same data range, to aid comparisons. The range of the seasonal amplitude is approxi mately 0.1 NDVI.109 J. Verbesselt et al. / Remote Sensing of Environment 114 (2010) 106 –115
Assurance ﬂags to select only cloud-free data of optimal quality. The
quality ﬂags, however, do not guarantee cloud-free data for the MODIS
250 m pixels since the algorithms used to screen clouds use bands at
coarse resolution. Missing values are replaced by linear interpolation
between neighboring values within the NDVI series ( Verbesselt et al.,
2006 ).
The 16-day MODIS NDVI image series were analyzed, and the
changes revealed were compared with spatial forest inventory
information on the ‘year of planting ’ofP. radiata . Time of change at a
16-day resolution was summarized to a yearly temporal resolution to
facilitate comparison with the validation data. The validation protocol
was applied under the assumption that no other major disturbances
(e.g. tree mortality) would occur that would cause a change in the
NDVI time series bigger than the change caused by harvesting and
planting activities.
4. Results
4.1. Simulated NDVI time series
Fig. 3 illustrates how BFAST decomposes and ﬁts different time series
components. It can be seen that the ﬁtted and simulated components
are similar, and that the magnitude and timing of changes in the trend
component are correctly estimated. The accuracies (RMSE) of the
number of estimated changes are summarized in Fig. 4 . Only results for
seasonal amplitude 0.1 and 0.5 are shown but similar results were
obtained for 0.3 NDVI amplitude. Three properties of the method are
illustrated. First, the noise level only has an in ﬂuence on the estimation
of the number of changes when the magnitude of the change is −0.1,
and is smaller than the overall noise level. The noise level is expressed as
4σ, i.e. 99% of the noise range, to enable a comparison with the
magnitude ( Fig. 4 ). Second, the noise level does not in ﬂuence the RMSE
when no changes are simulated (magnitude=0), indicating a low
commission error independent of the noise level. Third, the seasonalamplitude does not have an in ﬂuence on the accuracy of change
detection. In Fig. 5 only simulation results for an amplitude 0.1 are
shown, since similar results were obtained for other amplitudes (0.3 and
0.5). Overall, Fig. 5 illustrates that the RMSE of estimating the time and
magnitude of change estimation is small and increases slowly for
increasing noise levels. Only when the magnitude of change is small
(−0.1) compared to the noise level (>0.15), the RMSE increases rapidly
for increasing noise levels.
4.2. Spatial application on MODIS image time series
The application of BFAST to MODIS NDVI time series of a P. radiata
plantation produced estimates of the time and magnitude of major
changes. These results are shown spatially in Figs. 6 and 7 .T h et i m eo f
change estimated by BFAST is summarized each year to facilitate
comparison. Only areas for which we had validation data available were
visualized in Figs. 6 and 7 . The overall similarity between the time of
planting and time of detected change illustrates how BFAST can be used
to detect change in a forest plantation ( Fig. 6 ). Differences in the
estimated time of change can be interpreted using differences in the
magnitude of change estimated by BFAST. Fig. 7 shows that detected
changes can have either a positive or a negative magnitude of change.
This can be explained by the fact that planting in pine plantations in the
study area corresponds with a harvesting operation in the preceding
year (personal communication with C. Stone). Harvesting operations
cause a signi ﬁcant decrease in the NDVI times series, whereas planting
causes a more gradual increase in NDVI. Firstly, if planting occurred
before 2002, the NDVI time series did not contain any signi ﬁcant
decrease in NDVI caused by the harvesting operations, since the MODIS
NDVI time series only started in early 2000. BFAST therefore detected
change with a positive magnitude, indicating regrowth ( Fig. 7 ),
corresponding to a time of change during or later than the plant date
(Fig. 6 ).Fig. 8 (top) illustrates detected changes within a NDVI time
series extracted from a single MODIS pixel within a pine plantation with
a planting activity during 2001. Secondly, if planting occurred after
2003, the time series contained a signi ﬁcant decrease in NDVI caused by
the harvesting operations. Major change detected as a consequence are
changes corresponding to harvesting preceding the planting operation,
and are therefore detected before the planting date ( Fig. 6 ) and have a
negative magnitude ( Fig. 7 ).Fig. 8 (middle) illustrates detected changes
within a NDVI time series with harvesting operation activity during
2004. These points illustrate BFAST's capacity to detect and characterizeTable 1
Parameter values for simulation of 16-day NDVI time series.
Parameters Values
Amplitude 0.1,0.3,0.5
σNoise 0.01,0.02, …, 0.07
Magnitude −0.3,−0.2,−0.10
Fig. 3. Simulated 16-day MODIS NDVI time series with a seasonal amplitude=0.3, σ=0.02 and change magnitude= −0.3. The simulated data series is the sum of the simulated
seasonal, trend and noise series ( ––– ), and is used as an input in BFAST. The estimated seasonal, trend and remainder series are shown in red. Three break points are detected within
the estimated trend component ( ⋯). The solid bars on the right hand side of the plot show the same data range, to aid comparisons. (For interpretation of the references to color in
thisﬁgure legend, the reader is referred to the web version of this article.)110 J. Verbesselt et al. / Remote Sensing of Environment 114 (2010) 106 –115
change, but also con ﬁrm the importance of simulating time series in a
controlled environment, since it is dif ﬁcult to ﬁnd validation data to
account for all types of change occurring in ecosystems.
Fig. 8 (bottom) shows an example of changes detected by BFAST in
an area where harvesting and thinning activities were absent. Fig. 9
illustrates how BFAST decomposed the NDVI time series and ﬁtted
seasonal, trend and remainder components. In 2002 and 2006 the study
area experienced a severe drought, which caused the pine plantations to
be stressed and the NDVI to decrease signi ﬁcantly. Severe tree mortality
occurred in 2006, since trees were drought-stressed and not able todefend themselves against insect attacks ( Verbesselt et al., 2009 ). This
explains why the change detected in 2006 is bigger (magnitude of the
abrupt change) and the recovery (slope of the gradual change) is slower
than the change detected in 2003, as shown in Fig. 9 . This example
illustrates how the method could be used to detect and characterize
changes related to forest health.
5. Discussion and further work
The main characteristics of BFAST are revealed by testing the
approach using simulated time series and by comparing detectedchanges in 16-day MODIS NDVI time series with spatial forest inventory
data. Simulation of NDVI time series illustrated that the iterative
decomposition of time series into a seasonal and trend component was
not in ﬂuenced by the seasonal amplitude and by noise levels smaller
than the simulated change magnitude. This enabled the robust detection
of abrupt and gradual changes in the trend component. As such, full time
series can be analyzed without having to select only data during a
speciﬁc period (e.g. growing season), or can avoid the normalization of
reﬂectance values for each land cover type to minimize seasonal varia-
bility ( Healey et al., 2005 ). Seasonal adjustment by decomposing time
series, as implemented in the BFAST approach, facilitates the detection
of change in the trend component independent of seasonal amplitude or
land cover type information. Considerations for further research fall into
four main categories:
(1) Further research is necessary to study BFAST's sensitivity to
detecting phenological change in the seasonal component. This
research has focussed on the detection and characterization of
changes within the trend component of 16-day NDVI time
series. Changes in the seasonal component were not simulated,
and BFAST's sensitivity to detecting seasonal changes using
simulated data was not assessed. However, changes occurring
Fig. 4. RMSEs for the estimation of number of abrupt changes within a time series, as shown in Fig. 3 (a=amplitude of the seasonal component, m=magnitude of change). The units
of the xandy-axes are 4 σ(noise) and the number of changes (RMSE). See Table 1 for the values of parameters used for the simulation of the NDVI time series. Similar results were
obtained for a=0.3.
Fig. 5. RMSEs for the estimation of the time and magnitude of abrupt changes within a time series ( a=amplitude of the seasonal component, m=magnitude of change). The units of
thex-axis are 4 σNDVI, and y-axis are relative time steps between images (e.g. 1 equals a 16-day period) (left) and NDVI (right). See Table 1 for the values of parameters used for the
simulation of NDVI time series. Similar results were obtained for a=0.3 and 0.5.111 J. Verbesselt et al. / Remote Sensing of Environment 114 (2010) 106 –115
in the seasonal component can be detected using BFAST. The
application of BFAST to 16-day MODIS NDVI time series on a
forested area (40,000 ha) revealed that seasonal breaks were
detected in only 5% of the area. The small number of seasonal
breaks occurring in the study area could be explained by the fact
that a seasonal change is only detected when a change between
land cover types with a signi ﬁcantly different phenology occurs.
Time series with a higher temporal resolution (e.g. daily or 8-
day) could increase the accuracy of detecting seasonal changes
but might also impact the ability to detect subtle changes due to
higher noise levels. Zhang et al. (2009) illustrated that
vegetation phenology can be estimated with high accuracy
(absolute error of less than 3 days) in time series with a
temporal resolution of 6 –16 days, but that accuracy depends on
the occurrence of missing values. It is therefore necessary to
study BFAST's capacity to detect phenological change caused by
climate variations or land use change in relation to the temporal
resolution of remotely sensed time series.
(2) Future algorithm improvements may include the capacity to
add functionality to identify the type of change with informa-
tion on the parameters of the ﬁtted piecewise linear models(e.g. intercept and slope). In this study we have focussed on the
magnitude of change, but the spatial application on MODIS
NDVI time series illustrated that change needs to be interpreted
by combining the time and magnitude of change. Alternatively,
different change types can be identi ﬁed based on whether
seasonal and trend breaks occur at the same time or not and
whether a discontinuity occurs (i.e. magnitude >0) ( Shao and
Campbell, 2002 ). Parameters of the ﬁtted piecewise linear
models can also be used to compare long term vegetation
trends provided by different satellite sensors. Fensholt et al.
(2009) , for example, used linear models to analyze trends in
annually integrated NDVI time series derived from Advanced
Very High Resolution Radiometer (AVHRR), SPOT VEGETA-
TION, and MODIS data. BFAST enables the analysis of long NDVI
time series and avoids the need to summarize data annually
(i.e. loss of information) by accounting for the seasonal and
trend variation within time series. This illustrates that further
work is needed to extend the method from detecting change to
classifying the type of change detected.
(3) Evaluating BFAST's behavior for different change types (e.g. ﬁres
versus deserti ﬁcation) in a wide variety of ecosystems remains
Fig. 6. Comparison between the year of Pinus radiata planting derived from spatial forest inventory data and the BFAST estimate of the year of major change occurring in MODIS NDVI
image time series (2000 –2008) for a forested area in south eastern Australia.112 J. Verbesselt et al. / Remote Sensing of Environment 114 (2010) 106 –115
Fig. 7. BFAST estimated magnitudes of major changes occurring in MODIS NDVI image time series (2000 –2008) for a forested area in south eastern Australia. Negative values
generally indicate harvesting, while positive values indicate forest growth.
Fig. 8. Detected changes in the trend component (red) of 16-day NDVI time series (black) extracted from a single MODIS pixel within a pine plantation, that was planted in 2001
(top), harvested in 2004 (middle), and with tree mortality occurring in 2007(bottom). The time of change ( ––– ), together with its con ﬁdence intervals (red) are also shown. (For
interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.)113 J. Verbesselt et al. / Remote Sensing of Environment 114 (2010) 106 –115
important. BFAST is tested by combining different magnitudes of
an abrupt change with a large range of simulated noise and
seasonal variations representing a wide range of land cover types.
BFAST is able to detect different change types, however, it
remains important to understand how these change types (e.g.
woody encroachment) will be detected in ecosystems with
drastic seasonal changes (e.g. strong and variable tropical dry
seasons) and severe noise in the spectral signal (e.g. sun angle
and cloud cover in mountainous regions).
(4) The primary challenge of MODIS data, despite its high temporal
resolution, is to extract useful information on land cover
changes when the processes of interest operate at a scale below
the spatial resolution of the sensor ( Hayes and Cohen, 2007 ).
Landsat data have been successfully applied to detect changes
at a 30 m spatial resolution. However, the temporal resolution
of Landsat, i.e. 16-day, which is often extended by cloud cover,
can be a major obstacle. The fusion of MODIS with Landsatimages to combine high spatial and temporal resolutions has
helped to improve the mapping of disturbances ( Hilker et al.,
2009 ). It is our intention to use BFAST in this integrated manner
to analyze time series of multi-sensor satellite images, and to
be integrated with data fusion techniques.
This research ﬁts within an Australian forest health monitoring
framework, where MODIS data is used as a ‘ﬁrst pass ’ﬁlter to identify
the regions and timing of major change activity ( Stone et al., 2008 ).
These regions would be targeted for more detailed investigation using
ground and aerial surveys, and ﬁner spatial and spectral resolution
imagery.
6. Conclusion
We have presented a generic approach for detection and character-
ization of change in time series. ‘Breaks For Additive Seasonal and Trend ’
(BFAST) enables the detection of different types of changes occurring in
time series. BFAST integrates the decomposition of time series into
trend, seasonal, and remainder components with methods for detecting
multiple changes in time series. BFAST iteratively estimates the dates
and number of changes occurring within seasonal and trend compo-
nents, and characterizes changes by extracting the magnitude and
direction of change. Changes occurring in the trend component indicate
gradual and abrupt change, while changes occurring in the seasonal
component indicate phenological changes. The approach can be appliedto other time series data without the need to select speci ﬁcl a n dc o v e r
types, select a reference period, set a threshold, or de ﬁne a change
trajectory.
Simulating time series with varying amounts of seasonality and
noise, and by adding abrupt changes at different times and magnitudes,
revealed that BFAST is robust against noise, and is not in ﬂuenced by
changes in amplitude of the seasonal component. This con ﬁrmed that
BFAST can be applied to a large range of time series with varying noise
levels and seasonal amplitudes, representing a wide variety of
ecosystems. BFAST was applied to 16-day MODIS NDVI image time
series (2000 –2008) for a forested study area in south eastern Australia.
This showed that BFAST is able to detect and characterize changes by
estimating time and magnitude of changes occurring in a forested
landscape.
The algorithm can be extended to label changes with information
on the parameters of the ﬁtted piecewise linear models. BFAST can be
used to analyze different types of remotely sensed time series(AVHRR, MODIS, Landsat) and can be applied to other disciplines
dealing with seasonal or non-seasonal time series, such as hydrology,
climatology, and econometrics. The R code ( R Development Core
Team, 2008) developed in this paper is available by contacting the
authors.
Acknowledgements
This work was undertaken within the Cooperative Research Center
for Forestry Program 1.1: Monitoring and Measuring ( www.crcforestry.
com.au ). Thanks to Dr. Achim Zeileis for support with the ‘strucchange
’
package in R, to professor Nicholas Coops, Dr. Geoff Laslett, and the four
anonymous reviewers whose comments greatly improved this paper.
References
Anyamba, A., & Eastman, J. R. (1996). Interannual variability of NDVI over Africa and its
relation to El Nino Southern Oscillation. International Journal of Remote Sensing ,17
(13), 2533 −2548.
Azzali, S., & Menenti, M. (2000). Mapping vegetation –soil–climate complexes in
southern Africa using temporal Fourier analysis of NOAA-AVHRR NDVI data. In-
ternational Journal of Remote Sensing ,21(5), 973 −996.
Bai, J., & Perron, P. (2003). Computation and analysis of multiple structural change
models. Journal of Applied Econometrics ,18(1), 1−22.
Bontemps, S., Bogaert, P., Titeux, N., & Defourny, P. (2008). An object-based change
detection method accounting for temporal dependences in time series with medium
to coarse spatial resolution. Remote Sensing of Environment ,112(6), 3181 −3191.
Cleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. (1990). STL: A seasonal-
trend decomposition procedure based on loess. Journal of Of ﬁcial Statistics ,6,3−73.
Fig. 9. Fitted seasonal, trend and remainder (i.e. estimated noise) components for a 16-day MODIS NDVI time series (data series) of a pine plantation in the no rthern part of the study
area. Three abrupt changes are detected in the trend component of the time series. Time ( ––– ), corresponding con ﬁdence interval (red), direction and magnitude of abrupt change
and slope of the gradual change are shown in the estimated trend component. The solid bars on the right hand side of the plot show the same data range, to ai d comparisons. (For
interpretation of the references to color in this ﬁgure legend, the reader is referred to the web version of this article.)114 J. Verbesselt et al. / Remote Sensing of Environment 114 (2010) 106 –115
Coppin, P., Jonckheere, I., Nackaerts, K., Muys, B., & Lambin, E. (2004). Digital change
detection methods in ecosystem monitoring: A review. International Journal of
Remote Sensing ,25(9), 1565 −1596.
Crist, E. P., & Cicone, R. C. (1984). A physically-based transformation of thematic
mapper data —The TM tasseled cap. IEEE Transactions on Geoscience and Remote
Sensing ,22(3), 256 −263.
de Beurs, K. M., & Henebry, G. M. (2005). A statistical framework for the analysis of long
image time series. International Journal of Remote Sensing ,26(8), 1551 −1573.
DeFries, R. S., Field, C. B., Fung, I., Collatz, G. J., & Bounoua, L. (1999). Combining satellite
data and biogeochemical models to estimate global effects of human-induced land
cover change on carbon emissions and primary productivity. Global Biogeochemical
Cycles ,13(3), 803 −815.
Dixon, R. K., Solomon, A. M., Brown, S., Houghton, R. A., Trexier, M. C., & Wisniewski, J.
(1994). Carbon pools and ﬂux of global forest ecosystems. Science ,263(5144),
185−190.
Fensholt, R., Rasmussen, K., Nielsen, T. T., & Mbow, C. (2009). Evaluation of earth
observation based long term vegetation trends —Intercomparing NDVI time series
trend analysis consistency of Sahel from AVHRR GIMMS, Terra MODIS and SPOTVGT data. Remote Sensing of Environment ,113(9), 1886 −1898.
Hayes, D. J., & Cohen, W. B. (2007). Spatial, spectral and temporal patterns of tropical
forest cover change as observed with multiple scales of optical satellite data. Re-
mote Sensing of Environment ,106(1), 1−16.
Haywood, J., & Randal, J. (2008). Trending seasonal data with multiple structural breaks.
NZ visitor arrivals and the minimal effects of 9/11. Research report 08/10, Victoria
University of Wellington, New Zealand. URL http://msor.victoria.ac.nz/twiki/pub/
Main/ResearchReportSeries/mscs08-10.pdf
Healey, S. P., Cohen, W. B., Zhiqiang, Y., & Krankina, O. N. (2005). Comparison of
Tasseled Cap-based Landsat data structures for use in forest disturbance detection.
Remote Sensing of Environment ,97(3), 301 −310.
Hilker, T., Wulder, M. A., Coops, N. C., Linke, J., McDermid, G., Masek, J. G., Gao, F., &
White, J. C. (2009). A new data fusion model for high spatial- and temporal-
resolution mapping of forest disturbance based on Landsat and MODIS. Remote
Sensing of Environment ,113(8), 1613 −1627.
Huete, A., Didan, K., Miura, T., Rodriguez, E. P., Gao, X., & Ferreira, L. G. (2002). Overview
of the radiometric and biophysical performance of the MODIS vegetation indices.
Remote Sensing of Environment ,83(1–2), 195
−213.
Jin, S. M., & Sader, S. A. (2005). MODIS time-series imagery for forest disturbance
detection and quanti ﬁcation of patch size effects. Remote Sensing of Environment ,
99(4), 462 −470.
Jönsson, P., & Eklundh, L. (2002). Seasonality extraction by function ﬁtting to time-
series of satellite sensor data. IEEE Transactions on Geoscience and Remote Sensing ,
40(8), 1824 −1832.
Kennedy, R. E., Cohen, W. B., & Schroeder, T. A. (2007). Trajectory-based change
detection for automated characterization of forest disturbance dynamics. Remote
Sensing of Environment ,110(3), 370 −386.
Lambin, E. F., & Strahler, A. H. (1994). Change-Vector Analysis in multitemporal space —
A tool to detect and categorize land-cover change processes using high temporal-
resolution satellite data. Remote Sensing of Environment ,48(2), 231 −244.Lhermitte, S., Verbesselt, J., Verstraeten, W.W., Coppin, P., submitted for publication.
Comparison of time series similarity measures for monitoring ecosystem dynamics:
a review of methods for time series clustering and change detection. Remote
Sensing of Environment.
Lu, D., Mausel, P., Brondizio, E., & Moran, E. (2004). Change detection techniques. In-
ternational Journal of Remote Sensing ,25(12), 2365 −2407.
Makridakis, S., Wheelwright, S. C., & Hyndman, R. J. (1998). Forecasting: Methods and
applications , 3rd Edition. New York: John Wiley & Sons.
Myneni, R. B., Hall, F. G., Sellers, P. J., & Marshak, A. L. (1995). The interpretation of
spectral vegetation indexes. IEEE Transactions on Geoscience and Remote Sensing ,33
(2), 481 −486.
R Development Core Team (2008). R: A language and environment for statistical
computing. R Foundation for Statistical Computing, Vienna, Austria. URL www.R-
project.org
Roy, D. P., Borak, J. S., Devadiga, S., Wolfe, R. E., Zheng, M., & Descloitres, J. (2002). The
MODIS land product quality assessment approach. Remote Sensing of Environment ,
83(1–2), 62−76.
Shao, Q., & Campbell, N. A. (2002). Modelling trends in groundwater levels by
segmented regression with constraints. Australian & New Zealand Journal of
Statistics ,44(2), 129 −141.
Stone, C., Turner, R., & Verbesselt, J. (2008). Integrating plantation health surveillance and
wood resource inventory systems using remote sensing. Australian Forestry ,71(3),
245−253.
Townshend, J. R. G., & Justice, C. O. (1988). Selecting the spatial-resolution of satellite
sensors required for global monitoring of land transformations. International
Journal of Remote Sensing ,9(2), 187 −236.
Tucker, C. J. (1979). Red and photographic infrared linear combinations for monitoring
vegetation. Remote Sensing of Environment ,8(2), 127 −150.
Venables, W. N., & Ripley, B. D. (2002). Modern applied statistics with S (pp. 156 −163).,
4th Edition. Springer-Verlag.
Verbesselt, J., Jönsson, P., Lhermitte, S., van Aardt, J., & Coppin, P. (2006). Evaluating satellite
and climate data derived indices as ﬁre risk indicators in savanna ecosystems. IEEE
Transactions on Geoscience and Remote Sensing ,44(6), 1622 −1632.
Verbesselt, J., Robinson, A., Stone, C., Culvenor, D., 2009. Forecasting tree mortality using
change metrics derived from MODIS satellite data. Forest Ecology and Management
258,1 1 6 6−1173 doi:10.1016/j.foreco.2009.06.011
Zeileis, A. (2005). A uni ﬁed approach to structural change tests based on ML scores, F
statistics, and OLS residuals. Econometric Reviews ,24(4), 445 −466.
Zeileis, A., & Kleiber, C. (2005). Validating multiple structural change models —A case
study. Journal of Applied Econometrics ,20(5), 685 −690.
Zeileis, A., Kleiber, C., Krämer, W., & Hornik, K. (2003). Testing and dating of structural
changes in practice. Computational Statistics and Data Analysis ,44, 109−123.
Zeileis, A., Leisch, F., Hornik, K., & Kleiber, C. (2002). strucchange: An R package for
testing for structural change in linear regression models. Journal of Statistical
Software ,7(2), 1−38.
Zhang, X., Friedl, M., & Schaaf, C. (2009). Sensitivity of vegetation phenology detection
to the temporal resolution of satellite data. International Journal of Remote Sensing ,
30(8), 2061 −2074.115 J. Verbesselt et al. / Remote Sensing of Environment 114 (2010) 106 –115
SatlasPretrain: A Large-Scale Dataset for Remote Sensing Image Understanding
Favyen Bastani Piper Wolters Ritwik Gupta Joe Ferdinando Aniruddha Kembhavi
Allen Institute for AI
{favyenb,piperw,ritwikg,joef,anik }@allenai.org
Wind Turbines
Image
Ice Extent20182021
Deforestation
ImageFlood Map
Vessels
Key Statistics
●137 categories
●302 million labels
●7 label types
●17 million image megapixels
●21 million sq km covered
Urban Expansion20122020
Water Resources20182021
Figure 1: S ATLAS PRETRAIN is a large-scale remote sensing dataset. Its labels are relevant to many important planet mon-
itoring applications, including water resource monitoring, tracking deforestation, detecting wind turbines for infrastructure
mapping, tracking glacier loss, detecting floods, tracking urban expansion, and detecting vessels for tackling illegal fishing.
Abstract
Remote sensing images are useful for a wide variety of
planet monitoring applications, from tracking deforestation
to tackling illegal fishing. The Earth is extremely diverse—
the amount of potential tasks in remote sensing images
is massive, and the sizes of features range from several
kilometers to just tens of centimeters. However, creating
generalizable computer vision methods is a challenge in
part due to the lack of a large-scale dataset that captures
these diverse features for many tasks. In this paper, we
present SATLAS PRETRAIN , a remote sensing dataset that
is large in both breadth and scale, combining Sentinel-2
and NAIP images with 302M labels under 137 categories
and seven label types. We evaluate eight baselines and
a proposed method on SATLAS PRETRAIN , and find that
there is substantial room for improvement in addressing re-
search challenges specific to remote sensing, including pro-
cessing image time series that consist of images from very
different types of sensors, and taking advantage of long-
range spatial context. Moreover, we find that pre-training
onSATLAS PRETRAIN substantially improves performance
on downstream tasks, increasing average accuracy by 18%
over ImageNet and 6% over the next best baseline. Thedataset, pre-trained model weights, and code are available
athttps://satlas-pretrain.allen.ai/ .
1. Introduction
Satellite and aerial images provide a diverse range of in-
formation about the physical world. In images of urban ar-
eas, we can identify unmapped roads and buildings and in-
corporate them into digital map datasets, as well as monitor
urban expansion. In images of industrial areas, we can cat-
alogue solar farms and wind turbines to track the progress
of renewable energy deployment. In images of glaciers and
forests, we can monitor slow natural changes like glacier
loss and deforestation. With the availability of global, regu-
larly updated, and public domain sources of remote sensing
images like the EU’s Sentinel missions [4], we can monitor
the Earth for all of these applications and more at a global-
scale, on a monthly or even weekly basis.
Because the immense scale of the Earth makes global
manual analysis of remote sensing images cost-prohibitive,
automatic computer vision methods are crucial for unlock-
ing their full potential. Previous work has proposed apply-
ing computer vision for automatically inferring the posi-
1arXiv:2211.15660v3  [cs.CV]  21 Aug 2023
tions of roads and buildings [10, 13, 33, 37, 60, 61]; moni-
toring changes in land cover and land use such as deforesta-
tion and urban expansion [46, 47]; predicting vessel posi-
tions and types to help tackle illegal fishing [42]; and track-
ing the progress and extent of natural disasters like floods,
wildfires, and tornadoes [8, 23, 44]. However, in practice,
most deployed applications continue to rely on manual or
semi-automated rather than fully automated analysis of re-
mote sensing images [1] for two reasons. First, accuracy
remains a barrier even in major applications like road ex-
traction [12], making full automation impractical. Second,
there is a long tail of remote sensing applications that re-
quire expert annotation but have few labeled examples (e.g.,
a recent New York Times study manually documented ille-
gal airstrips in Brazil using satellite images [9]).
We believe that the lack of a very-large-scale, multi-
task remote sensing dataset is a major impediment for
progress on automated methods for remote sensing tasks
today. First, state-of-the-art architectures such as ViT [26]
and CLIP [43] require huge datasets to achieve peak per-
formance. However, existing remote sensing datasets
for object detection, instance segmentation, and seman-
tic segmentation like DOTA [55], iSAID [58], and Deep-
Globe [24] contain less than 10K images each, compared to
the 328K images in COCO and millions used to train CLIP;
the small size of these datasets means we cannot fully take
advantage of recent architectures. Second, existing remote
sensing benchmarks are fragmented, with individual bench-
marks for categories like roads [41], vessels [42], and crop
types [28], but no benchmark spanning many categories.
The lack of a large-scale, centralized, and accessible bench-
mark prevents transfer learning opportunities across tasks,
and makes it difficult for computer vision researchers to en-
gage in this domain.
We present S ATLAS PRETRAIN , a large-scale dataset
for improving remote sensing image understanding mod-
els. Our goal with S ATLAS PRETRAIN isto label every-
thing that is visible in a satellite image . To this end,
SATLAS PRETRAIN combines Sentinel-2 and NAIP images
with 302M distinct labels under 137 diverse categories and
7 label types: the label types are points like wind tur-
bines and water towers; polygons like buildings and air-
ports; polylines like roads and rivers; segmentation and
regression labels like land cover categories and bathymetry
(water depth); properties of objects like the rotor diameter
of a wind turbine; and patch classification labels like the
presence of smoke in an image. Figure 1 demonstrates the
wide range of categories in S ATLAS PRETRAIN , along with
the diverse applications that they serve.
We find that the huge scale of S ATLAS PRETRAIN en-
ables pre-training to substantially improve downstream per-
formance. We compare S ATLAS PRETRAIN pre-training
against pre-training on other datasets as well as self-supervised learning methods, and find that it improves av-
erage performance across seven downstream tasks by 18%
over ImageNet and 6% over the next best baseline. These
results show that S ATLAS PRETRAIN can readily improve
accuracy on the numerous niche remote sensing tasks that
require costly expert annotation.
Additionally, we believe that S ATLAS PRETRAIN will en-
courage work on computer vision methods that tackle the
unique research challenges in the remote sensing domain.
Compared to general-purpose computer vision methods, re-
mote sensing models require specialized techniques such as
accounting for long-range spatial context, synthesizing in-
formation across images over time captured by diverse sen-
sors like multispectral images and synthetic aperture radar
(SAR), and predicting objects that vary widely in size, from
forests spanning many km2to street lamps. We evalu-
ate eight computer vision baselines on S ATLAS PRETRAIN
and find that no single existing method supports all the
SATLAS PRETRAIN label types; instead, each baseline can
only predict a subset of categories. Thus, inspired by re-
cent work that integrate task-specific output heads [21, 29,
35, 36], we develop a unified model called S ATLAS NET
that incorporates seven such heads so that it can learn from
every category in the dataset. Compared to training sep-
arately on each label type, we find that jointly training
SATLAS NETon all categories and then fine-tuning on each
label type improves average performance by 7.1%, show-
ing that S ATLAS NETis able to leverage transfer learning
opportunities between label types.
In summary, our contributions are:
1. S ATLAS PRETRAIN , a large-scale remote sensing
dataset with 137 categories under seven label types.
2. Demonstrating that pre-training on S ATLAS PRETRAIN
improves average performance on seven downstream
datasets by 6%.
3. S ATLAS NET, a unified model that supports predictions
for all label types in S ATLAS PRETRAIN .
We have released the dataset and code at https://
satlas-pretrain.allen.ai/ . We have also re-
leased model weights pre-trained on S ATLAS PRETRAIN
which can be fine-tuned for downstream tasks.
2. Related Work
Large-Scale Remote Sensing Datasets. Several general-
purpose remote sensing computer vision datasets have been
released. Many of these focus on scene and patch clas-
sification: the UC Merced Land Use (UCM) [57] and
BigEarthNet [51] datasets involve land cover classification
with 21 and 43 categories respectively, while the AID [56],
Million-AID [39], RESISC45 [19], and Functional Map
of the World (FMoW) [22] datasets additionally include
categories corresponding to manmade structures such as
2
Types Classes Labels Pixels km2
SatlasPretrain 7 137 302222K 17003B 21320K
UCM [57] 1 21 2K 1B 1K
BigEarthNet [51] 1 43 1750K 9B 850K
AID [56] 1 30 10K 4B 14K
Million-AID [39] 1 51 37K 4B 18K
RESISC45 [19] 1 45 32K 2B 10K
FMoW [22] 1 63 417K 437B 1748K
DOTA [55] 1 19 99K 9B 38K
iSAID [58] 1 15 355K 9B 38K
Table 1: Comparison of S ATLAS PRETRAIN against existing
remote sensing datasets (K=thousands, B=billions). Types
is number of label types and km2is area covered.
bridges and railway stations, with up to 63 categories. A
few datasets focus on tasks other than scene classification.
DOTA [55] involves detecting objects in 18 categories rang-
ing from helicopter to roundabout. iSAID [58] involves in-
stance segmentation for 15 categories.
All of these datasets involve making predictions for a
single label type, and most involve doing so from a single
image. Thus, they are limited in three ways: the number of
object categories, the diversity of labels, and the opportu-
nities for approaches to learn to synthesize features across
image time series. In contrast, S ATLAS PRETRAIN incorpo-
rates 137 categories under seven label types (see full com-
parison in Table 1), and provides image time series that
methods can leverage to improve prediction accuracy.
A few domain-specific datasets extend beyond these lim-
itations. xView3 [42] involves predicting vessel positions
(object detection) and attributes of those vessels such as ves-
sel type and length (per-object classification and regression)
in SAR images. PASTIS-R [28] involves panoptic segmen-
tation of crop types in crop fields using a time series of SAR
and optical satellite images captured by the Sentinel-1 and
Sentinel-2 constellations. IEEE Data Fusion datasets incor-
porate various aerial and satellite images for tasks like land
cover segmentation [48].
Self-Supervised and Multi-Task Learning for Remote
Sensing. Similar to our work, these approaches share the
goal of improving accuracy on downstream applications
with few labels. Several methods [7, 11, 40, 49, 50, 54] in-
corporate temporal augmentations into a contrastive learn-
ing framework, where images of the same location captured
at different times are encouraged to have closer represen-
tations than images of different locations. They show that
the model improves downstream performance by learning
invariance to transient differences between images of the
same location, such as different lighting and nadir angle
conditions as well as seasonal changes. GPNA proposes
combining self-supervised learning with supervised train-
ing on diverse tasks [45].3. SatlasPretrain
We present S ATLAS PRETRAIN , a very-large-scale
dataset for remote sensing image understanding that im-
proves on existing remote sensing datasets in three key
ways:
1.Scale: SATLAS PRETRAIN contains 40x more image
pixels and 150x more labels than the largest existing
dataset.
2.Label diversity: Existing datasets in Table 1
have unimodal labels, e.g. only classification.
SATLAS PRETRAIN labels span seven label types ; fur-
thermore, they comprise 137 categories, 2x more than
the largest existing dataset.
3.Spatio-temporal images and labels: Rather than being
tied to individual remote sensing images, our labels are
associated with geographic coordinates (i.e., longitude-
latitude positions) and time ranges. This enables meth-
ods to make predictions from multiple images across
time, as well as leverage long-range spatial context from
neighboring images. These features present new re-
search challenges that, if solved, can greatly improve
model performance.
We first provide an overview of the structure of
SATLAS PRETRAIN and detail the imagery that it contains
below. We then describe the labels and how they were col-
lected.
3.1. Structure and Imagery
SATLAS PRETRAIN consists of 856K tiles. These tiles
correspond to Web-Mercator tiles at zoom level 13, i.e., the
world is projected to a 2D plane and divided into a 213×
213grid, with each tile corresponding to a grid cell. Thus,
each S ATLAS PRETRAIN tile covers a disjoint spatial region
spanning up to 25 km2. At each tile, S ATLAS PRETRAIN
includes (1) a time series of remote sensing images of the
tile; and (2) labels drawn from the 137 S ATLAS PRETRAIN
categories. Figure 2 summarizes the dataset, and Figure 3
shows its global geographic coverage.
Existing datasets typically use either high-resolution im-
agery (0.5–2 m/pixel) [19, 22, 39, 58] or low-resolution im-
agery (10 m/pixel) [27, 51]. Although high-resolution im-
agery enables higher prediction accuracy, low-resolution
imagery is often employed in practical applications since
it is available more frequently (weekly vs yearly) and
broadly (globally vs in limited countries). Thus, in
SATLAS PRETRAIN , we incorporate both low- and high-
resolution images, which we will refer to as image modes .
We define separate train and test splits for each image mode,
and compare methods over each mode independently.
In all 856K tiles (828K train and 28K test), we provide
low-resolution 512x512 images. Specifically, we include
3
Sentinel-2 Images
Road type: servicePower plant
type: gas
Property Labels
Segmentation Labels Regression Labels Classification LabelsSnow: none
Smoke: none
Polyline LabelsRiver
Railways
Roads
GrassWaterDevelopedMossShrubTree Cover
High-voltage Towers
Point, Polygon LabelsStorage
tanksPower plant
Buildings
NAIP ImagesHigh-Resolution Low-Resolution
Point LabelsVessels @ 2019-08-22
2022-10-30 2019-08-222019-10-19
Dynamic LabelsSlow-Changing 
LabelsSlow-Changing 
Labels
2022-01-23Figure 2: Overview of the S ATLAS PRETRAIN dataset. S ATLAS PRETRAIN consists of image time series and labels for 856K
Web-Mercator tiles at zoom 13 (left). There are two image modes on which methods are trained and evaluated indepen-
dently: high-resolution NAIP images (top) and low-resolution Sentinel-2 images (bottom). Labels may be slow-changing
(corresponding to the most recent image at a tile) or dynamic (referencing a specific image and time).
Water, not covered
Land, not covered
Water, covered
Land, covered
Figure 3: Geographic coverage of S ATLAS PRETRAIN , with
bright pixels indicating locations covered by images and la-
bels in the dataset. S ATLAS PRETRAIN spans all continents
except Antarctica.
8–12 Sentinel-2 images captured during 2022; this enables
methods to leverage multiple spatially aligned images of a
location to improve prediction accuracy. We also include
historical 2016–2021 images that are relevant for dynamic
labels like floods and ship positions. Sentinel-2 captures 10
m/pixel multispectral images; the European Space Agency
(ESA) releases these images openly. Some categories are
not visible in the low-resolution images, so for this mode
we only evaluate methods on 122 of 137 categories.
In 46K tiles (45.5K train and 512 test), we provide high-
resolution 8192x8192 images. We include 3–5 public do-
main 1 m/pixel aerial images from the US National Agricul-
ture Imagery Program (NAIP) between 2011–2020. These
images are only available in the US, so train and test tiles
for the high-resolution mode are restricted to the US.
We download the images from ESA and USGS, and use
GDAL [6] to process the images into Web-Mercator tiles.
The structure of S ATLAS PRETRAIN enables methods to
leverage both spatial and temporal context. Methods canmake use of long-range spatial context from many neigh-
boring tiles to improve the accuracy of predictions at a tile.
Similarly, methods can learn to synthesize features across
the image time series that we include at each tile in the
dataset to improve prediction accuracy; for example, when
predicting the crop type grown at a crop field, observations
of the crop field at different stages of the agricultural cy-
cle can provide different clues about the type of crop grown
there. In contrast, existing datasets (including all but FMoW
in Table 1) typically associate each label with a single im-
age, and require methods to predict the label with that one
image only.
3.2. Labels
SATLAS PRETRAIN labels span 137 categories, with
seven label types (see examples in Figure 2):
1. Semantic segmentation—e.g., predicting per-pixel land
cover (water vs forest vs developed vs etc.).
2. Regression—e.g., predicting per-pixel bathymetry (wa-
ter depth) or percent tree cover.
3. Points (object detection)—e.g., predicting wind turbines,
oil wells, and vessels.
4. Polygons (instance segmentation)—e.g., predicting
buildings, dams, and aquafarms.
5. Polylines—e.g., predicting roads, rivers, and railways.
6. Properties of points, polygons, and polylines—e.g., the
rotor diameter of a wind turbine.
7. Classification—e.g., whether an image exhibits negligi-
ble, low, or high wildfire smoke density.
Most categories represent slow-changing objects like roads
or wind turbines. During dataset creation, we aim for labels
under these categories to correspond to the most recent im-
age available at each tile. Thus, during inference, if these
4
objects change over the image time series available at a tile,
the model predictions should reflect the last image in the
time series. A few categories represent dynamic objects like
vessels and floods. For labels in these categories, in addi-
tion to specifying the object position, the label specifies the
timestamp of the image that it corresponds to. During in-
ference, for dynamic categories, the model should make a
separate set of predictions for each image in the time series.
We derive S ATLAS PRETRAIN labels from seven sources:
new annotation by domain experts, new annotation by Ama-
zon Mechanical Turk (AMT) workers, and processing five
existing datasets—OpenStreetMap [30], NOAA lidar scans,
WorldCover [53], Microsoft Buildings [3], and C2S [5].
Each category is annotated (valid) in only a subset of
tiles. Thus, in some tiles, a given category may be invalid,
meaning that there is no ground truth for the category in that
tile. In other tiles, a category may be valid but have zero
labels, meaning that there are no instances of that category
in the tile. In supplementary Section A.1, for each category,
we detail the number of tiles where the category is valid, the
number of tiles where the category has at least one label,
and the number of labels under that category; we also detail
the category’s label type and data source.
Labels in S ATLAS PRETRAIN are relevant to numerous
planet and environmental monitoring applications, which
we discuss in supplementary Section A.2.
We summarize the data collection process for each of the
data sources below.
Expert Annotation. Two domain experts annotated 12 cat-
egories: off-shore wind turbines, off-shore platforms, ves-
sels, 6 tree cover categories (e.g. low vs high), and 3 snow
presence categories (none, partial, or full). To facilitate
this process, we built a dedicated annotation tool called Siv
that is customizable for individual categories. For exam-
ple, when annotating marine objects, we found that display-
ing images of the same marine location at different times
was crucial for accurately distinguishing vessels from fixed
infrastructure (generally, a vessel will only appear in one
of the images, while wind turbines and platforms appear in
all images); thus, for these categories, we ensured the do-
main experts could press the arrow keys in Siv to toggle
between different spatially aligned images of the same tile.
Similarly, for tree cover, we found that consulting exter-
nal sources like Google Maps and OpenStreetMap helped
improve accuracy in cases where tree cover was not clear
in NAIP or Sentinel-2 images; thus, when annotating tree
cover, we included links in Siv to these external sources.
AMT. AMT workers annotated 9 categories: coastal land,
coastal water, fire retardant drops, areas burned by wild-
fires, airplanes, rooftop solar panels, and 3 smoke presence
categories (none, low, or high). We reused the Siv annota-
tion tool for AMT annotation, incorporating additional per-
category customizations as needed (which we detail in sup-plementary Section A.3.1).
To maximize annotation quality, for each category, we
first selected AMT workers through a qualification task:
domain experts annotated between 100–400 tiles, and we
asked each candidate AMT worker to annotate the same
tiles; we only asked workers whose labels corresponded
closely with expert labels to continue with further anno-
tation. We also conducted majority voting over multiple
workers; we decided the number of workers needed per tile
on a per-category basis (see Section A.3.2), by first having
one worker annotate each tile, and then analyzing the label
quality. For example, we found that airplanes were unam-
biguous enough that a single worker sufficed, while we had
three workers label each tile for areas burned by wildfires.
OpenStreetMap (OSM). OSM is a collaborative map
dataset built through edits made by contributing users. Ob-
jects in OSM span a wide range of categories, from roads to
power substations. We obtained OSM data as an OSM PBF
file on 9 July 2022 from Geofabrik, and processed it using
the Go osmpbf library to extract 101 categories.
Recall is a key issue for labels derived from OSM. From
initial qualitative analysis, we consistently observed that
OSM objects have high precision but variable recall: the
vast majority of objects were correct, but for some cate-
gories, many objects were visible in satellite imagery but
not mapped in OSM. To mitigate this issue, we employed
heuristics to automatically prune tiles that most likely had
low recall, based on the number of labels and distinct cat-
egories in the tile. For instance, we found that tiles with
many roads but no buildings were likely to have missing
objects in other categories like silo or water tower. We de-
tail these heuristics in supplementary Section A.4.
We found that these heuristics were sufficient to yield
high-quality labels for most categories. However, we iden-
tified 13 remaining low-recall categories, including gas sta-
tions, helipads, and oil wells. From an analysis of 1300
tiles, we determined that recall was still at least 80% for
these categories, which we deemed sufficient for the train-
ing set: there are methods for learning from sparse la-
bels, and large-scale training on noisy labels has produced
models like CLIP that deliver state-of-the-art performance.
However, we deemed that these 13 categories did not have
sufficient recall for the test set. Thus, to ensure a highly ac-
curate test set, for each of these 13 categories, we trained
an initial model on OSM labels and tuned its confidence
threshold for high-recall low-precision detection; we then
hand-labeled its predictions to add missing labels to the test
set. In Section A.4, we detail these categories and the num-
ber of missing labels identified in the test set.
NOAA Lidar Scans. NOAA coastal topobathy maps de-
rived from lidar scans contain elevation data for land and
depth data for water. We download 5,868 such maps from
various NOAA surveys, and process them to derive per-
5
Swin Transformer
Temporal
Max PoolingH/4×W/4×128
H/8×W/8×256
H/16×W/16×512
H/32×W/32×1024U-Net Decoder with Cross
Entropy loss (Segmentation)
2016 2018 2020Swin Transformer
Swin TransformerMulti-Scale Features Water
Developed
Storage Tanks
BuildingsFaster R-CNN Decoder (Point)
U-Net Decoder with
L1 loss (Regression)
U-Net Decoder with Polyline
Extraction (Polyline)
Pooling + Linear (Properties)
Pooling + Linear (Classification)
Mask R-CNN Decoder (Polygon)Figure 4: Model architecture of S ATLAS NET. A separate head is used to predict outputs for each label type. We visualize
example outputs from two such heads (segmentation and polygon).
pixel depth and elevation labels for 5,123 SatlasPretrain
tiles.
WorldCover. WorldCover [53] is a global land cover map
developed by the European Space Agency. We process the
map to derive 11 land cover and land use categories, ranging
from barren land to developed areas.
Microsoft Buildings. We process 70 GeoJSON files from
various Microsoft Buildings datasets [3] to derive building
polygons in S ATLAS PRETRAIN . The data is released under
ODbL.
C2S. C2S [5] consists of flood and cloud labels in Sentinel-
2 images, released under CC-BY-4.0. We warp the labels
to Web-Mercator and include them in S ATLAS PRETRAIN .
We also download and process the Sentinel-2 images that
correspond exactly to the ones used in C2S, so that they
share the same processing as other Sentinel-2 images in
SATLAS PRETRAIN .
Balancing the scale of labels with label quality was a key
consideration in managing new annotation and selecting ex-
isting data sources to process. As we developed the dataset,
we conducted iterative analyses to evaluate the precision
and recall of labels that we collected, and used this infor-
mation to improve later annotation and refine data source
processing. In supplementary Section A.5, we include an
analysis of incorrect and missing labels under every cate-
gory in the final dataset; we find that 116/137 categories
have>99% precision, 15 have 95-99% precision, 4 have
90-95% precision, and 2 have 80-90% precision.
4. SatlasNet
Off-the-shelf computer vision models cannot handle
all the label types in S ATLAS PRETRAIN , e.g., while
Mask2Former [18] can simultaneously perform semantic
and instance segmentation, it is not designed to predict
properties of polygons or classify images. This prevents
these models from leveraging the full set of transfer learning
opportunities present in S ATLAS PRETRAIN ; for example,
detecting building polygons is likely useful for segmenting
images for land cover and land use, since land use includesa human-developed category. We develop a unified model,
SATLAS NET, that is capable of learning from all seven label
types.
Figure 4 shows a schematic of our model. S ATLAS NET
is inspired by recent work that employ task-specific output
heads [21, 29, 35], as well as methods that synthesize fea-
tures across remote sensing image time series [22,27]. It in-
puts a time series of spatially aligned images, and processes
each image (which may contain more than three bands)
through a Swin Transformer [38] backbone (Swin-Base),
which outputs feature maps for each image at four scales.
We apply max temporal pooling at each scale to derive one
set of multi-scale features. We pass the features to seven
output heads (one for each label type) to compute outputs.
For polylines, while specialized polyline extraction archi-
tectures have been shown to improve accuracy [10, 33, 52],
we opt to employ the simpler segmentation approach [60]
where we apply a UNet head to segment images for polyline
categories, and post-process the segmentation probabilities
with binary thresholding, morphological thinning, and line
following and simplification [20] to extract polylines.
5. Evaluation
We first evaluate our method and eight classification, se-
mantic segmentation, and instance segmentation baselines
on the S ATLAS PRETRAIN test split in Section 5.1. We then
evaluate performance on seven downstream tasks in Sec-
tion 5.2, comparing pre-training on S ATLAS PRETRAIN to
pre-training on other remote sensing datasets, as well as
self-supervised learning techniques specialized for remote
sensing.
5.1. Results on SatlasPretrain
Methods. We compare S ATLAS NETagainst eight baselines
on S ATLAS PRETRAIN . We select baselines that are either
standard models or models that provide state-of-the-art per-
formance for subsets of label types in S ATLAS PRETRAIN .
None of the baselines are able to handle all seven
SATLAS PRETRAIN label types. For property prediction
and classification, we compare ResNet [32], ViT [26], and
6
High-Resolution NAIP Images Low-Resolution Sentinel-2 Images
Method Seg↑Reg↓Pt↑Pgon ↑Pline ↑Prop ↑ Seg↑Reg↓Pt↑Pgon ↑Pline ↑Prop ↑Cls↑
PSPNet (ResNext-101) [59] 77.8 15.0 - - 53.2 - 62.1 16.2 - - 30.7 - -
LinkNet (ResNext-101) [15] 77.3 12.9 - - 61.0 - 61.1 14.1 - - 41.4 - -
DeepLabv3 (ResNext-101) [16] 80.1 10.6 - - 59.8 - 61.8 13.9 - - 44.7 - -
ResNet-50 [32] - - - - - 87.6 - - - - - 70.3 97
ViT-Large [26] - - - - - 78.1 - - - - - 66.9 99
Swin-Base [38] - - - - - 87.1 - - - - - 69.4 99
Mask R-CNN (ResNet-50) [31] - -27.6 30.4 - - - -22.0 12.3 - - -
Mask R-CNN (Swin-Base) [31] - -30.4 31.5 - - - -25.6 15.2 - - -
ISTR [34] - - 2.0 4.9 - - - - 1.2 1.4 - - -
SatlasNet (single-image, per-type) 79.4 8.3 28.0 30.4 61.5 86.6 64.8 9.3 25.7 14.8 42.5 67.5 99
SatlasNet (single-image, joint) 74.5 7.4 28.0 31.1 60.9 87.3 55.8 10.6 22.0 10.3 45.5 73.8 99
SatlasNet (single-image, fine-tuned) 79.8 7.2 32.3 33.0 62.4 89.5 65.3 9.0 27.4 16.3 45.9 80.0 99
SatlasNet (multi-image, per-type) 79.4 8.2 25.8 27.5 59.2 77.3 67.2 10.5 31.9 19.0 48.1 67.1 99
SatlasNet (multi-image, joint) 79.2 7.8 31.2 33.8 53.6 87.8 66.7 8.5 31.5 19.5 41.9 78.8 99
SatlasNet (multi-image, fine-tuned) 81.0 7.6 33.2 34.1 61.1 89.2 69.7 7.8 32.0 20.2 50.4 80.0 99
Table 2: Results on the S ATLAS PRETRAIN test set for the high- and low-resolution image modes. We break down results
by label type: segmentation (Seg), regression (Reg), points (Pt), polygons (Pgon), polylines (Pline), properties (Prop), and
classification (Cls). We show absolute error for Reg (lower is better), and accuracy for the others (higher is better).
Swin Transformer [38]. For segmentation, regression, and
polylines, we compare PSPNet [59], LinkNet [15], and
DeepLabv3 [16]. For points and polygons, we compare
Mask R-CNN [31] and ISTR [34].
We train three variants of S ATLAS NET:
• Per-type: train separately on each label type.
• Joint: jointly train across all categories.
• Fine-tuned: fine-tune the jointly trained parameters on
each label type.
All baselines are fine-tuned on each label type (after joint
training on the subset of label types they can handle), which
provides the highest performance.
For each S ATLAS NETvariant, we also evaluate in single-
image and multi-image modes. For all baselines and single-
image S ATLAS NET, we sample training examples by either
(a) sampling a tile, and pairing the most recent image at the
tile with slow-changing labels (with dynamic and other in-
valid categories masked); or (b) sampling a tile and image,
and pairing the image with corresponding dynamic labels.
For multi-image S ATLAS NET, we provide as input a time
series of eight Sentinel-2 images for low-resolution mode
or four NAIP images for high-resolution mode; for slow-
changing labels, the images are ordered by timestamp, but
for dynamic labels, we always order the sampled image at
the end of the time series input. In all cases, we sample
examples based on the maximum inverse frequency of cat-
egories appearing in the example. We use RGB bands only
here, but include results for single-image S ATLAS NETwith
nine Sentinel-2 bands in supplementary Section C.
Across all methods, we input 512x512 images during
both training and inference; for high-resolution inference,
since images covering the tile are 8K by 8K, we indepen-
dently process 256 512x512 windows and merge the model
outputs. We employ random cropping, horizontal and ver-tical flipping, and random resizing augmentations during
training. We initialize models with ImageNet-pretrained
weights. We use the Adam optimizer, and initialize the
learning rate to 10−4, decaying via halving down to 10−6
upon plateaus in the training loss. We train with a batch
size of 32 for 100K batches.
Metrics. We use standard metrics for each label type: ac-
curacy for classification, F1 score for segmentation, mean
absolute error for regression, mAP accuracy for points and
polygons, and GEO accuracy [14] for polylines. We com-
pute metrics per-category, and report the average across cat-
egories under each label type.
Results. We show results on S ATLAS PRETRAIN in Table
2. Across the seven label types, single-image S ATLAS NET
matches or surpasses the performance of state-of-the-art,
purpose-built baselines when trained per-type, validating its
effectiveness as a unified model that can predict diverse re-
mote sensing labels. Jointly training one set of S ATLAS NET
parameters for all categories reduces average performance
on several label types, but S ATLAS NETremains competi-
tive in most cases; this training mode provides large effi-
ciency gains since the backbone features need only be com-
puted once for each image during inference, rather than
once per label type. When fine-tuning S ATLAS NETon each
label type using the parameters derived from joint training,
it provides an average 7.1% relative improvement across the
label types and image modes over per-type training. This
supports our hypothesis that there are transfer learning op-
portunities between the label types, validating the utility of
a unified model for improving performance. Multi-image
SATLAS NETprovides another 5.6% relative improvement
in average performance, showing that it is able to effectively
synthesize information across image time series to produce
better predictions; nevertheless, we believe that there is sub-
7
Image SatlasNet Outputs
Grass
Water
Developed
CropWind turbinesBuildingsWater towers
RailwayRoads
Power
substationParking lots
Tree cover
Oil wellFigure 5: Qualitative results on S ATLAS PRETRAIN . Rightmost: a failure case where S ATLAS NETdetects only 1/5 oil wells.
UCM RESISC45 AID FMoW Mass Roads Mass Buildings Airbus Ships Average
Method 50 All 50 All 50 All 50 All 50 All 50 All 50 All 50 All
Random Initialization 0.26 0.86 0.15 0.77 0.18 0.68 0.03 0.17 0.69 0.80 0.68 0.77 0.31 0.53 0.33 0.65
ImageNet [25] 0.35 0.92 0.17 0.95 0.20 0.81 0.03 0.21 0.77 0.85 0.78 0.83 0.37 0.65 0.38 0.75
BigEarthNet [51] 0.35 0.95 0.20 0.94 0.23 0.78 0.03 0.27 0.78 0.85 0.81 0.85 0.40 0.68 0.40 0.76
MillionAID [39] 0.72 0.97 0.30 0.96 0.30 0.82 0.04 0.35 0.78 0.84 0.82 0.85 0.46 0.67 0.49 0.78
DOTA [55] 0.56 0.99 0.28 0.95 0.33 0.83 0.03 0.30 0.82 0.86 0.84 0.87 0.62 0.75 0.50 0.79
iSAID [58] 0.60 0.97 0.29 0.97 0.34 0.86 0.04 0.30 0.82 0.86 0.84 0.86 0.55 0.73 0.50 0.79
MoCo [17] 0.14 0.14 0.07 0.09 0.05 0.12 0.02 0.03 0.56 0.69 0.62 0.63 0.01 0.21 0.21 0.27
SeCo [40] 0.48 0.95 0.20 0.90 0.27 0.74 0.03 0.26 0.70 0.81 0.71 0.77 0.27 0.54 0.38 0.71
SatlasPretrain 0.83 0.99 0.36 0.98 0.42 0.88 0.06 0.44 0.82 0.87 0.87 0.88 0.56 0.80 0.56 0.83
Table 3: Results on seven downstream tasks when fine-tuned with 50 examples (50) or the entire downstream dataset (All).
Accuracy is reported for UCM, RESISC45, and AID while F1 Score is reported for FMoW, Mass Roads, Mass Buildings, and
Airbus Ships. S ATLAS PRETRAIN pre-training improves average accuracy across the tasks by 6% over the next best baseline.
stantial room for further improvement in methods for pro-
cessing remote sensing image time series.
We show qualitative results in Figure 5, with additional
examples in supplementary Section E. We achieve high ac-
curacy on several categories, such as wind turbines and wa-
ter towers. However, for oil wells, one well is detected but
several others are not. Similarly, for polyline features like
roads and railways, the model produces short noisy seg-
ments, despite ample training data for these categories; we
believe that incorporating and improving models that are
tailored for specialized output types like polylines [33, 52]
has the potential to improve accuracy.
5.2. Downstream Performance
We now evaluate accuracy on seven downstream tasks
when pre-training on S ATLAS PRETRAIN compared to pre-
training on four existing remote sensing datasets, as well
as two self-supervised learning methods. For each down-
stream task, we evaluate accuracy when training on just 50
examples and when training on the whole dataset, to focus
on the challenge of improving performance on niche remote
sensing applications that require expert annotation and thus
have few labeled examples.
Methods. We compare pre-training on high-resolution im-ages in S ATLAS PRETRAIN to pre-training on four exist-
ing remote sensing datasets: BigEarthNet [51], Million-
AID [39], DOTA [55], and iSAID [58]. We use
SATLAS NETin all cases, fine-tuning the pre-trained Swin
backbone on each downstream dataset.
We also compare two self-supervised learning methods,
Momentum Contrast v2 (MoCo) [17] and Seasonal Con-
trast (SeCo) [40]. The latter is a specialized method for
remote sensing that leverages multiple image captures of
the same location to learn invariance to seasonal changes.
For MoCo, we use our S ATLAS NETmodel and train on
SATLAS PRETRAIN images. For SeCo, we evaluate their
original model trained on their dataset. We fine-tune
the weights learned through self-supervision on the down-
stream tasks. We provide results for additional variants in
supplementary Section B.3.
We fine-tune both the pre-training and self-supervised
learning methods by first freezing the backbone and only
training the prediction head for 32K examples, and then
fine-tuning the entire model. We provide additional experi-
ment details in supplementary Section B.1.
Downstream Datasets. The downstream tasks consist of
four existing large-scale remote sensing datasets that in-
volve classification with between 21 and 63 categories:
8
UCM [57], AID [56], RESISC45 [19], and FMoW [22].
The other three are the Massachusetts Buildings and Mas-
sachusetts Roads datasets [41], which involve semantic seg-
mentation, and the Airbus Ships [2] dataset, which involves
instance segmentation.
Results. Table 3 shows downstream performance with
varying training set sizes. S ATLAS PRETRAIN consistently
outperforms the baselines: when training on 50 examples,
we improve average accuracy across the tasks by 18%
over ImageNet pre-training, and by 6% over the next best
baseline. The state-of-the-art performance achieved across
such a wide range of downstream tasks clearly demon-
strates the generalizability of the representations derived
from S ATLAS PRETRAIN pre-training, and the potential of
SATLAS PRETRAIN to improve performance on the numer-
ous niche remote sensing applications. We include results
with more varying training examples in supplementary B.2.
6. Use in AI-Generated Geospatial Data
We have deployed S ATLAS PRETRAIN to develop high-
accuracy models for Satlas ( https://satlas.allen.
ai/), a platform for global geospatial data generated by
AI from satellite imagery. Timely geospatial data, like the
positions of wind turbines and solar farms, is critical for in-
forming decisions in emissions reduction, disaster relief, ur-
ban planning, etc. However, high-quality global geospatial
data products can be hard to find because manual curation
is often cost-prohibitive. Satlas instead applies models fine-
tuned for tasks like wind turbine detection to automatically
extract geospatial data from satellite imagery on a monthly
basis. Satlas currently consists of four geospatial data prod-
ucts: wind turbines, solar farms, offshore platforms, and
tree cover.
7. Conclusion
By improving on existing datasets in both scale and label
diversity, S ATLAS PRETRAIN serves as an effective very-
large-scale dataset for remote sensing methods. Pre-training
on S ATLAS PRETRAIN increases average downstream accu-
racy by 18% over ImageNet and 6% over existing remote
sensing datasets, indicating that it can readily be applied to
the long tail of remote sensing tasks that have few labeled
examples. We have already leveraged models pre-trained
on S ATLAS PRETRAIN to accurately detect wind turbines,
solar farms, offshore platforms, and tree cover in the Satlas
platform at https://satlas.allen.ai/ .
A. Supplementary Material
The supplementary material can be accessed from
https://github.com/allenai/satlas/blob/
main/SatlasPretrain.md .References
[1] The machine vision challenge to better analyze satellite im-
ages of Earth. MIT Technology Review .
[2] Airbus ship detection challenge. https://www.
kaggle.com/c/airbus-ship-detection , 2018.
Airbus.
[3] Microsoft Building Footprints Datasets, 2021. Microsoft.
[4] Copernicus Sentinel Missions. https://sentinel.
esa.int/web/sentinel/home , 2022. European
Space Agency.
[5] A global flood events and cloud cover dataset (version 1.0),
2022. Cloud to Street, Microsoft, Radiant Earth Foundation.
[6] GDAL, 2023. Open Source Geospatial Foundation.
[7] Peri Akiva, Matthew Purri, and Matthew Leotta. Self-
supervised Material and Texture Representation Learning for
Remote Sensing Tasks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 8203–8215, 2022.
[8] Robert S Allison, Joshua M Johnston, Gregory Craig, and
Sion Jennings. Airborne Optical and Thermal Remote
Sensing for Wildfire Detection and Monitoring. Sensors ,
16(8):1310, 2016.
[9] Manuela Andreoni, Blacki Migliozzi, Pablo Robles, and
Denise Lu. The Illegal Airstrips Bringing Toxic Mining to
Brazil’s Indigenous Land. The New York Times .
[10] Favyen Bastani, Songtao He, Sofiane Abbar, Mohammad
Alizadeh, Hari Balakrishnan, Sanjay Chawla, Sam Mad-
den, and David DeWitt. RoadTracer: Automatic Extraction
of Road Networks from Aerial Images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 4720–4728, 2018.
[11] Favyen Bastani, Songtao He, Satvat Jagwani, Mohammad
Alizadeh, Hari Balakrishnan, Sanjay Chawla, Sam Madden,
and Mohammad Amin Sadeghi. Updating Street Maps using
Changes Detected in Satellite Imagery. In Proceedings of the
29th International Conference on Advances in Geographic
Information Systems , pages 53–56, 2021.
[12] Favyen Bastani and Samuel Madden. Beyond Road Extrac-
tion: A Dataset for Map Update using Aerial Images. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 11905–11914, 2021.
[13] Anil Batra, Suriya Singh, Guan Pang, Saikat Basu, CV Jawa-
har, and Manohar Paluri. Improved Road Connectivity by
Joint Learning of Orientation and Segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 10385–10393, 2019.
[14] James Biagioni and Jakob Eriksson. Map Inference in the
Face of Noise and Disparity. In Proceedings of the 20th In-
ternational Conference on Advances in Geographic Informa-
tion Systems , pages 79–88, 2012.
[15] Abhishek Chaurasia and Eugenio Culurciello. LinkNet: Ex-
ploiting encoder representations for efficient semantic seg-
mentation. IEEE Visual Communications and Image Pro-
cessing (VCIP) , pages 1–4, 2017.
[16] Liang-Chieh Chen, George Papandreou, Florian Schroff, and
Hartwig Adam. Rethinking Atrous Convolution for Seman-
tic Image Segmentation. ArXiv , abs/1706.05587, 2017.
9
[17] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
Improved Baselines with Momentum Contrastive Learning.
arXiv preprint arXiv:2003.04297 , 2020.
[18] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention Mask
Transformer for Universal Image Segmentation. 2022.
[19] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote Sens-
ing Image Scene Classification: Benchmark and State of the
Art. In Proceedings of the IEEE , volume 105, pages 1865–
1883, 2017.
[20] Guangliang Cheng, Ying Wang, Shibiao Xu, Hongzhen
Wang, Shiming Xiang, and Chunhong Pan. Automatic Road
Detection and Centerline Extraction via Cascaded End-to-
end Convolutional Neural Network. IEEE Transactions on
Geoscience and Remote Sensing , 55(6):3322–3337, 2017.
[21] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying
Vision-and-Language Tasks via Text Generation. In Interna-
tional Conference on Machine Learning , pages 1931–1942.
PMLR, 2021.
[22] Gordon Christie, Neil Fendley, James Wilson, and Ryan
Mukherjee. Functional Map of the World. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) , 2018.
[23] Annarita D’Addabbo, Alberto Refice, Guido Pasquariello,
Francesco P Lovergine, Domenico Capolongo, and Salvatore
Manfreda. A Bayesian Network for Flood Detection Com-
bining SAR Imagery and Ancillary Data. IEEE Transactions
on Geoscience and Remote Sensing , 54(6):3612–3625, 2016.
[24] Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan
Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia,
and Ramesh Raskar. DeepGlobe 2018: A Challenge to
Parse the Earth through Satellite Images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) Workshops , pages 172–181, 2018.
[25] J. Deng, W. Dong, R. Socher, L. J. Li, Kai Li, and Li Fei-
Fei. ImageNet: A large-scale hierarchical image database.
IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR) , 2009.
[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is
Worth 16x16 Words: Transformers for Image Recognition at
Scale. In International Conference on Learning Representa-
tions , 2021.
[27] Vivien Sainte Fare Garnot and Loic Landrieu. Panoptic
Segmentation of Satellite Image Time Series with Convo-
lutional Temporal Attention Networks. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 4872–4881, 2021.
[28] Vivien Sainte Fare Garnot, Loic Landrieu, and Nesrine
Chehata. Multi-modal Temporal Attention Models for Crop
Mapping from Satellite Time Series. ISPRS Journal of Pho-
togrammetry and Remote Sensing , pages 294–305, 2022.
[29] Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and
Derek Hoiem. Towards General Purpose Vision Systems:
An End-to-End Task-Agnostic Vision-Language Architec-ture. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 16399–
16409, 2022.
[30] Mordechai Haklay and Patrick Weber. OpenStreetMap:
User-Generated Street Maps. IEEE Pervasive computing ,
7(4):12–18, 2008.
[31] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-
shick. Mask r-cnn. In IEEE International Conference on
Computer Vision (ICCV) , pages 2980–2988, 2017.
[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep Residual Learning for Image Recognition. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 770–778, 2016.
[33] Songtao He, Favyen Bastani, Satvat Jagwani, Mohammad
Alizadeh, Hari Balakrishnan, Sanjay Chawla, Mohamed M
Elshrif, Samuel Madden, and Mohammad Amin Sadeghi.
Sat2Graph: Road Graph Extraction through Graph-Tensor
Encoding. European Conference on Computer Vision , pages
51–67, 2020.
[34] Jie Hu, Liujuan Cao, Yao Lu, Shengchuan Zhang, Yan
Wang, Ke Li, Feiyue Huang, Ling Shao, and Rongrong Ji.
ISTR: End-to-End Instance Segmentation with Transform-
ers.ArXiv , abs/2105.00637, 2021.
[35] Ronghang Hu and Amanpreet Singh. Unit: Multimodal Mul-
titask Learning with a Unified Transformer. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 1439–1449, 2021.
[36] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric
Kolve, Derek Hoiem, and Aniruddha Kembhavi. Webly
Supervised Concept Expansion for General Purpose Vision
Models. arXiv preprint arXiv:2202.02317 , 2022.
[37] Zuoyue Li, Jan Dirk Wegner, and Aur ´elien Lucchi. Topolog-
ical Map Extraction from Overhead Images. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 1715–1724, 2019.
[38] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and
B. Guo. Swin Transformer: Hierarchical Vision Transformer
using Shifted Windows. In IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , pages 9992–10002, 2021.
[39] Yang Long, Gui-Song Xia, Shengyang Li, Wen Yang,
Michael Ying Yang, Xiao Xiang Zhu, Liangpei Zhang, and
Deren Li. On Creating Benchmark Dataset for Aerial Im-
age Interpretation: Reviews, Guidances, and Million-AID.
IEEE Journal of Selected Topics in Applied Earth Observa-
tions and Remote Sensing , pages 4205–4230, 2021.
[40] Oscar Manas, Alexandre Lacoste, Xavier Giro i Nieto, David
Vazquez, and Pau Rodriguez. Seasonal Contrast: Unsuper-
vised Pre-Training from Uncurated Remote Sensing Data. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , 2021.
[41] V olodymyr Mnih. Machine Learning for Aerial Image La-
beling . PhD thesis, University of Toronto, 2013.
[42] Fernando Paolo, Tsu ting Tim Lin, Ritwik Gupta, Bryce
Goodman, Nirav Patel, Daniel Kuster, David Kroodsma, and
Jared Dunnmon. xView3-SAR: Detecting Dark Fishing Ac-
tivity Using Synthetic Aperture Radar Imagery. Neural In-
formation Processing Systems , 2022.
10
[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
Transferable Visual Models from Natural Language Supervi-
sion. International Conference on Machine Learning , pages
8748–8763, 2021.
[44] Sudha Radhika, Yukio Tamura, and Masahiro Matsui. Appli-
cation of Remote Sensing Images for Natural Disaster Mit-
igation using Wavelet based Pattern Recognition Analysis.
In2016 IEEE International Geoscience and Remote Sensing
Symposium (IGARSS) , pages 84–87, 2016.
[45] Nasim Rahaman, Martin Weiss, Frederik Tr ¨auble, Francesco
Locatello, Alexandre Lacoste, Yoshua Bengio, Chris Pal,
Li Erran Li, and Bernhard Sch ¨olkopf. A General Purpose
Neural Architecture for Geospatial Systems. HADR Work-
shop at NeurIPS 2022 , 2022.
[46] Caleb Robinson, Le Hou, Kolya Malkin, Rachel Soobitsky,
Jacob Czawlytko, Bistra Dilkina, and Nebojsa Jojic. Large
Scale High-Resolution Land Cover Mapping with Multi-
Resolution Data. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 12726–12735, 2019.
[47] Caleb Robinson, Anthony Ortiz, Kolya Malkin, Blake Elias,
Andi Peng, Dan Morris, Bistra Dilkina, and Nebojsa Jojic.
Human-Machine Collaboration for Fast Land Cover Map-
ping. pages 2509–2517, 2020.
[48] Ronny H ¨ansch; Claudio Persello; Gemine Vivone; Javiera
Castillo Navarro; Alexandre Boulch; Sebastien Lefevre;
Bertrand Le Saux. Data fusion contest 2022 (dfc2022), 2022.
[49] Linus Scheibenreif, Jo ¨elle Hanna, Michael Mommert, and
Damian Borth. Self-Supervised Vision Transformers for
Land-Cover Segmentation and Classification. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 1422–1431, 2022.
[50] Linus Scheibenreif, Michael Mommert, and Damian Borth.
Contrastive Self-Supervised Data Fusion for Satellite Im-
agery. ISPRS Annals of the Photogrammetry, Remote Sens-
ing and Spatial Information Sciences , 3:705–711, 2022.
[51] Gencer Sumbul, Marcela Charfuelan, Begum Demir, and
V olker Markl. BigEarthNet: A Large-Scale Benchmark
Archive for Remote Sensing Image Understanding. In
International Geoscience and Remote Sensing Symposium
(IGARSS) , 2019.
[52] Yong-Qiang Tan, Shang-Hua Gao, Xuan-Yi Li, Ming-Ming
Cheng, and Bo Ren. VecRoad: Point-based Iterative Graph
Exploration for Road Graphs Extraction. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 8910–8918, 2020.
[53] Ruben Van De Kerchove, Daniele Zanaga, Wanda Keers-
maecker, Niels Souverijns, Jan Wevers, Carsten Brockmann,
Alex Grosu, Audrey Paccini, Oliver Cartus, Maurizio San-
toro, et al. ESA WorldCover: Global land cover mapping at
10 m resolution for 2020 based on Sentinel-1 and 2 data. In
AGU Fall Meeting Abstracts , volume 2021, pages GC45I–
0915, 2021.
[54] Yi Wang, Nassim Ait Ali Braham, Zhitong Xiong, Chenying
Liu, Conrad M. Albrecht, and Xiao Xiang Zhu. SSL4EO-
S12: A Large-Scale Multi-Modal, Multi-Temporal Datasetfor Self-Supervised Learning in Earth Observation. ArXiv ,
abs/2211.07044, 2022.
[55] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-
longie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-
pei Zhang. DOTA: A Large-scale Dataset for Object De-
tection in Aerial Images. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , 2018.
[56] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang
Bai, Yanfei Zhong, and Liangpei Zhang. AID: A Benchmark
Dataset for Performance Evaluation of Aerial Scene Classi-
fication. IEEE Journal of Transactions on Geoscience and
Remote Sensing , 55(7):3965–3981, 2017.
[57] Yi Yang and Shawn Newsam. Bag-Of-Visual-Words and
Spatial Extensions for Land-Use Classification. ACM Con-
ference on Spatial Information (SIGSPATIAL) , 2010.
[58] Syed Waqas Zamir, Aditya Arora, Akshita Gupta, Salman
Khan, Guolei Sun, Fahad Shahbaz Khan, Fan Zhu, Ling
Shao, Gui-Song Xia, and Xiang Bai. iSAID: A Large-scale
Dataset for Instance Segmentation in Aerial Images. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) Workshops , 2019.
[59] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid Scene Parsing Network. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , pages 6230–6239,
2017.
[60] Lichen Zhou, Chuang Zhang, and Ming Wu. D-LinkNet:
LinkNet with Pretrained Encoder and Dilated Convolution
for High Resolution Satellite Imagery Road Extraction. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) Workshops , pages
182–186, 2018.
[61] Stefano Zorzi, Shabab Bazrafkan, Stefan Habenschuss, and
Friedrich Fraundorfer. PolyWorld: Polygonal Building Ex-
traction with Graph Neural Networks in Satellite Images.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 1848–1857,
2022.
11
REVIEW SUMMARY◥
SUSTAINABILITY
Using satellite imagery to understand and promote
sustainable development
Marshall Burke *, Anne Driscoll, David B. Lobell, Stefano Ermon
BACKGROUND: Accurate and comprehensive
measurements of a range of sustainable de-
velopment outcomes are fundamental inputs
into both research and policy. For instance,
good measures are needed to monitor prog-
ress toward sustainability goals and evaluateinterventions designed to improve develop-
ment outcomes. Traditional approaches to
measurement of many key outcomes rely on
household surveys that are conducted infre-
quently in many parts of the world and are
often of low accuracy. The paucity of ground
data stands in contrast to the rapidly growing
abundance and quality of satellite imagery.
Multiple public and private sensors launched
in recent years provide temporal, spatial, and
spectral information on changes happening
on Earth ’s surface.
Here we review a rapidly growing scientific
literature that seeks to use this satellite im-
agery to measure and understand various out-
comes related to sustainable development. We
pay particular attention to recent approaches
that use methods from artificial intelligence
to extract information from images, as these
methods typically outperform earlier approaches
and enable new insights. Our focus is on set-
tings and applicatio ns where humans them-
selves, or what they produce, are the outcome
of interest and on where these outcomes are
being measured using satellite imagery.ADVANCES: We describe and synthesize the
variety of approaches that have been used to
extract information from satellite imagery, with
particular attention given to recent machine
learning –based approaches and settings in
which training data are limited or noisy. Wethen quantitatively assess predictive perfor-
mance of these approaches in the domains of
smallholder agriculture, economic livelihoods,
population, and informal settlements. We show
that satellite-based performance in predicting
these outcomes is reasonably strong and im-
proving. Performance improvements have come
through a combination of more numerous and
accurate training data, more abundant and
higher-quality imagery, and creative applica-
tion of advances in computer vision to satellite
inputs and sustainability outcomes. Further,
our analyses suggest that reported model per-
formance likely unders tates true performance
in many settings, given the noisy data on which
predictions are evaluated and the types of noise
typically observed in sustai nability applications.
For multiple outcomes of int erest, satellite-based
estimates can now equal or exceed the accuracy
of traditional approaches to outcome measure-
m e n t .W ed e s c r i b em u l t i p l em e t h o d st h r o u g h
which the true performance of satellite-based
approaches can be better understood.
Integration of satellite -based sustainability
measurements into research has been broad,and we describe applications in agriculture,
fisheries, health, and economics. Documented
uses of these measurements in public-sector
decision-making are rarer, which we attribute
in part to the novelty of the approaches, their
lack of interpretability, and the potential ben-
efits to some policy-makers of not having
certain outcomes be measured.
OUTLOOK: The largest constraint to satellite-
based model performanc e is now training data
rather than imagery. While imagery has be-
come abundant, the scarcity and frequent un-
reliability of ground data make both trainingand validation of satellite-based models diffi-
cult. Expanding the quantity and quality of
such data will quickly accelerate progress in
this field. Other opportunities for advance-
ment include improvements in model inter-
pretability, fusion of satellites with other
nontraditional data that provide complemen-
tary information, and more-rigorous evalua-
tion of satellite-based approaches (relative to
available alternatives) in the context of speci-
fic use cases.
Nevertheless, despite the current and future
promise of satellite-based approaches, we ar-
gue that these approaches will amplify rather
than replace existing ground-based data col-
lection efforts in most settings. Many outcomes
of interest will likely never be accurately es-
timated with satellites; for outcomes where
satellites do have predictive power, high-
quality local training data can nearly always
improve model performance.▪RESEARCH
Burke et al.,Science 371, 1219 (2021) 19 March 2021 1o f1
The list of author affiliations is available in the full article online.
*Corresponding author. Email: mburke@stanford.edu
Cite this article as M. Burke et al .,Science 371, eabe8628
(2021). DOI: 10.1126/science.abe8628
READ THE FULL ARTICLE AT
https://doi.org/10.1126/science.abe8628
Y ears between surveys
0     3     6     9     26    No surveysDigitalGlobe
KOMPSA TPléiades
SPOT 6/7TripleSat
LandSat
MODISSentinelPlanetRapidEyeSkySat10 cm
50 cm
1 m
5 m
10 m
50 m
100 m
500 m
day week month year 10 years
Cloud-free revisit rateSensor resolutionPublic
Private
New since 2010
R21 0.8 0.6 0.4 0.2 0Accuracy1 0.8 0.6 0.4 0.2 0
Smallholder yields, plot levelAsset wealth, village/district levelInformal settlement presenceAverage interval between economic surveys, 1993 to present Satellite resolution and revisit rate, 
Africa 2019Performance of satellite-based
approaches to measurement
Increasing collection of satellite imagery can help measure livelihood out-
comes in areas where ground data are sparse. (Left) Interval between
nationally representative economic surveys over the past three decades shows long
lags in many developing countries. (Middle) Recently added public and privatesatellites have broken the traditional trade-off between te mporal and spatial
resolution. (Right) Performance in measuri ng the presence of informal settlements,
crop yields on smallholder agricultural plots, and village-level asset wealth. R2,
coefficient of determination.
Downloaded from https://www.science.org on March 07, 2024

REVIEW◥
SUSTAINABILITY
Using satellite imagery to understand and promote
sustainable development
Marshall Burke1,2,3*, Anne Driscoll2, David B. Lobell1,2, Stefano Ermon4
Accurate and comprehensive measure ments of a range of sustainable deve lopment outcomes are fundamental
inputs into both research and policy. We synthesize the growing literature that uses satellite imagery to
understand these outcomes, with a fo cus on approaches that combine imagery with machine learning. We
quantify the paucity of ground data on key human-related outcomes and the growing abundance and improving
resolution (spatial, temporal, and spectral) of satellite imagery. We then review recent machine learning
approaches to model-building in the context of scarce and noisy training data, highlighting how this noiseoften leads to incorrect assessment of model performance. We quantify recent model performance across
multiple sustainable development domains, discuss research and policy applications, explore constraints
to future progress, and highlight research directions for the field.
Humans have long sought to image their
habitat from above the ground. Socra-
tes purportedly stated in 500 BCE that
“Man must rise above the earth —to the
top of the atmosphere and beyond —for
only thus will he fully understand the world
in which he lives ”(1). His lofty goal was taken
up in earnest after the advent of photographyin the mid-19th century CE, with earth ob-
servation data collected by strapping cameras
to balloons, kites, airp lanes, and pigeons. The
first known image of Earth from space was
taken nearly a century later (1946) by Am-
erican scientists using a captured Nazi rocket,
revealing blurry expanses of the American
Southwest ( 2). This was followed decades
later by the launch of the first civilian Earth-
observing satellite, Landsat I, in 1972, which
ushered in the modern er a of satellite-based
remote sensing. As of early 2020, there are an
estimated 713 active nonmilitary earth obser-
vation satellites in orbit, 75% of which were
launched within the past five years ( 3). These
satellites are now capturing imagery of Earth
with unprecedented temporal, spatial, and
spectral frequency.
Here we review and synthesize a rapidly
g r o w i n gs c i e n t i f i cl i t e r a t u r et h a ts e e k st ou s e
this satellite imagery to measure and under-
stand various human outcomes, including a
range of outcomes directly linked to the United
Nation ’s Sustainable Development Goals ( 4).
We pay particular attention to recent ap-
proaches that use methods from artificial
intelligence to extract information from im-
ages, as these methods typically outperformearlier approaches, enab ling new insights. Our
focus is on settings and applications where
humans themselves, or what they produce,
are the outcome of interest and where these
outcomes are being predicted using satellite
imagery. We quantify existing performance
in these domains across a large set of studies,
explore key constraints to future progress,and highlight a number of research directions
that we believe are key if these approaches
are going to be improved and adopted by
practitioners.
We do not review and assess the large lit-
erature on using remote sensing for other
Earth observation tasks (e.g., environmental
monitoring) or efforts that use other sources
of nontraditional, unstr uctured data (e.g., data
from social media or cell phones) to measure
human-related outcomes unless these data are
combined with imagery. Our review comple-
ments existing sector-specific reviews, includ-
ing the use of remote sensing in agriculture
(5,6), in economic applications ( 7), and in
the detection of informal settlements ( 8), draw-
ing common lessons across these and other
domains.
We make four main points. First, satellite-
based performance in pre dicting key sustain-
able development outcomes is reasonably
strong and appears to be improving. Indeed,
analyses suggest that reported model per-
formance likely unders tates true performance
in many settings, given the noisy data on
which predictions are e valuated. For multiple
outcomes of interest, satellite-based estimates
can now equal or exceed the accuracy of tra-
ditional approaches to outcome measurement.
Second, perhaps the largest constraint to
model development is now training data
rather than imagery. While imagery has be-
come abundant, the scarcity and, in manysettings, unreliability of ground data makeboth training and validation of satellite-based
models difficult. Third, despite the growing
power of satellite-based approaches, these ap-
proaches will likely amplify rather than fully
replace existing ground-based data collec-
tion efforts, given the necessity of training
data and the likelihood that many outcomes of
interest will likely never be accurately esti-
mated with satellites.
Finally, in the sustainable development do-
mains on which we focus, there remain few doc-
umented cases where satellites have been used
in public-sector decision-making processes —
with applications in population and agricul-tural measurements being the main excep-
tions. Limited adoption is likely driven by a
number of forces, including the recency of the
technology, the lack of accuracy (perceived or
real) of the models, lack of model interpret-
ability, and entrenched interests in maintain-
i n gt h ec u r r e n td a t ar e g i m e .W ed i s c u s sh o w
some of these constraints might be overcome.
The availability and reliability of data
Key data are scarce, and often scarcest in
places where they are most needed
Household- or field-le vel surveys remain the
main data collection tool for key development-
related outcomes. Methodologies for such data
collection are well developed and are imple-
mented by national statistical agencies and
other organizations in nearly all countries of
the world. But their implementation and use
also face a number of important challenges.
First, nationally representative surveys are
expensive and time-consuming to conduct.
Conducting a Demogra phic and Health Sur-
vey (DHS) or Living Standards Measurement
Study (LSMS) in one country for one year
typically costs $1.5 million to $2 million USD
(9), with the entire survey operation taking
multiple years and involving the training and
deployment of enumerators to often remote
and insecure locations. Population censuses
are substantially more expensive, costing tens
to hundreds of millions of US dollars in a typ-
ical African country ( 10).
An implication of this expense is that many
countries conduct surveys infrequently, if at
all. In half of African nations, at least 6.5 years
pass between nationally representative liveli-
hood surveys (Fig. 1A), as compared with sub-
annual frequency in mos tw e a l t h yc o u n t r i e s .
Survey frequency is on average substantially
lower in less wealthy countries (Fig. 1B), mean-
i n gt h a td a t ao nl i v e l i h o o do u t c o m e sa r eo f t e n
lacking where they are arguably most needed.
Surveys are also much less common in less
democratic societies (Fig. 1C), which could at
least partly reflect the desire and ability of
some autocrats to limit awareness of poor eco-
nomic progress ( 11). The frequency of agricul-
tural and population censuses also varies
widely around the world (Fig. 1, D and G).RESEARCH
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 1o f1 2
1Department of Earth System Science, Stanford University,
Stanford, CA, USA.2Center on Food Security and the
Environment, Stanford University, Stanford, CA, USA.
3National Bureau of Economic Research, Cambridge, MA,
USA.4Department of Computer Science, Stanford University,
Stanford, CA, USA.
*Corresponding author. Email: mburke@stanford.edu
Downloaded from https://www.science.org on March 07, 2024

For instance, 24% of the world ’sc o u n t r i e s
(49 out of 206) have gone more than 15 years
since their last agricultural census, and 6%
(13 out of 206) have gone more than 15 years
since their last population census.
A second challenge is that survey samples
are typically only representative at the nation-al or (sometimes) regional level, meaning that
they often cannot be used to generate accurate
summary statistics at a state, county, or more
local level. This represents a challenge for a
range of research or policy applications that
require individual- or lo cal-level information,
for example, targeting an antipoverty program
or studying its impact.
Third, underlying data are not made pub-
licly available in many surveys, including near-
ly all the surveys that contribute to official
poverty statistics (such as those depicted in
Fig. 1A), and no geographic information is
publicly provided on where data were col-
lected. These factors further deepen the chal-
lenge of using such data to conduct local
research or policy evaluation or to train mod-
els to predict local outcomes using these data.
Even when local-level a nonymized g eorefer-
enced data are made public in some form, data
are typically released more than a year after
survey completion, hampering real-time knowl-
edge of livelihood conditions on the ground.Finally, as explored below, ground data can
have multiple sources o f noise or bias, further
limiting their reliability and utility in research
and decision-making. This noise has impor-
tant implications for how satellite-based mod-
els trained on these data are validated and
interpreted.
Existing ground data can be unreliable
Even where ground data are present, several
key sources of error can limit their utility. First,
most outcomes are not measured directly but
rather are inferred from responses to surveys.
These responses can introduce large amounts
of both random and systematic measurement
error. For instance, in household consump-
tion expenditure surveys, changes to the
recall period or the list of items households
are questioned about can lead to household
expenditure estimates that are >25% too low
relative to gold-standard household diaries
(12). In agriculture, the World Bank noted that
the“practice of ‘eye observations ’or‘desk-based
estimation ’is commonly used by agricultural
officers, ”leading to often-conflicting estimates
of key agricultural outcomes by different gov-
ernment ministries and to variation over time
in published statistics that cannot easily
be reconciled with events on the ground
(13). Current practices are likely to have abias toward overestimation, further weaken-
ing the quality of food security assessments
(13,14).
An additional key source of noise comes
from sampling variability. Surveys are typi-
cally designed to be representative at very
large scales (e.g., nationally), and this repre-
sentativeness is typically obtained by taking
small random samples of households or fields
across many cluster locations. Because most
agricultural and economic outcomes of inter-
est often exhibit substantial variation even at
very local levels (e.g., coefficients of variation
>1 at the village level), these small samplesthus represent an unbiased but potentially
very noisy measure of average outcomes in
a given locality.
The combined effects of both measurement
error and sampling variability can be appre-
ciated when comparing two independent mea-
sures of the same outcome for the same
a d m i n i s t r a t i v el e v e l .I nF i g .2 ,w ec o m p a r e
average maize yields at th e first administrative
level (e.g., province or state) as obtained from
household surveys covered by the LSMS –
Integrated Surveys on Agriculture (ISA) pro-
gram versus by official government ministry
estimates in three African countries. This com-
parison reveals both a systematic bias toward
higher yields in official government data than
in household responses and a relatively low
correlation between the two measures, with
the highest observed correlation coefficient
rof 0.39 for Ethiopia.
A third common source of error is noise
purposefully introduced to protect the privacy
of surveyed households. Adding jitter to vil-
l a g ec o o r d i n a t e si sc o m m o np r a c t i c ef o rm o s t
of the publicly released datasets based on
household surveys, for instance with up to
2 km of random jitter added in urban areas
and 5 km in rural areas. Below we explore
the implications of these three sources of error
for model development and evaluation.
Availability of satellite imagery
changing rapidly
Information from satellite imagery has been
used to a limited extent in both agricultural
and socioeconomic applications for decades
(15,16). However, thanks to both public and
private sector investment, recent years have
seen a remarkable increase in the temporal,
spatial, and spectral information available
from satellites and a corresponding use of
this imagery in applications.
To quantify this increase in imagery and
understand how it varies across developing
and developed countries, we randomly sam-
pled 100 locations in Africa and 100 addition-
al locations across the US and EU (sampling
proportional to population) and queried the
availability of cloud-free imagery (defined as
<30% cloud cover) at each location in 2010
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 2o f1 2050100150
100 1000 10000 50000# of surveysR2 = 0.26
050100150
-10 -5 0 5 10# of surveysR2 = 0.38
10152025
100 1000 10000 50000Years sinceR2 = 0.05
5
0
-10 -5 0 5 10Years sinceR2 = 0.03
10152025
5
0
100 1000 10000 50000
Log GDP per capitaYears sinceR2 = 0.00
10152025
5
0
-10 -5 0 5 10
PolityYears sinceR2 = 0.01
10152025
5
0026
10
8
6
3
0
None0
3
6
9
26
No 
surveysYears between surveys Years since censusANationally representative economic surveys
DAgricultural censuses
GPopulation censuses                    B                          C 
                           E                          F
                       H                          I
Fig. 1. Nationally representative economic, agricultural, and population data are collected infre-
quently in much of the world. (A) The average interval between nationally representative economic
surveys (of average or high quality) for the period 1993 –2018 from the UN World Income Inequality
Database ( 109). (B) Relationship between gross domestic product (GDP) per capita ( 110) and number of
surveys in the study period. Nations with higher GDP per capita tend to have more surveys. ( C) Relationship
between the Polity score of each country (+10 is fully democratic, −10 is fully autocratic) ( 111) and the
number of surveys in the study period. ( D) Years since last agricultural census, using data covering
1993 –2018. ( EandF) Relationship between GDP per capita, Polity score, and years since last agricultural
census. ( GtoI) As in (D) to (F), but for population censuses.RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

and 2019 for all available optical sensors,
using multiple online query tools ( 17). We
calculated region- and year-specific average
revisit rates for each sensor and constructed
an imagery-resolution “frontier, ”defined as
the overall revisit rate across sensors at or
below a given spatial resolution.
We found that the addition of many new
sensors has lessened the traditional trade-off
b e t w e e nt e m p o r a la n ds p a t i a lr e s o l u t i o n
(Fig. 3A), particularly at resolutions ≥3m .A l -
though the revisit rate of very-high-resolution
(<1 m) sensors over Africa has seen only slight
improvement over the past decade (Fig. 3B),
and very-high-resolution revisit rates remain
l o w e ri nA f r i c at h a ni nb o t ht h eU Sa n dE U
(Fig. 3C), revisit rates for high-resolution (1 to
5 m) and moderate- to low-resolution sensors
have increased drastically and are globally
equitable.
We sampled and visualized additional im-
ages and sensors across populated African
locations (Fig. 3). Various types of human
activity are readily visible even with moderate-
resolution sensors (5 to 30 m), including
urban infrastructure development, agricul-
tural activity, and moisture availability (Fig.
3 F ) .T h ei n c r e a s i n g l yh i g hr e v i s i tr a t eo f
such imagery also provides key insight into
development-relevant activities that change
seasonally, such as the location and produc-
tivity of croplands (Fig. 3G).
Modeling approaches using satellite imagery
to predict sustainability outcomes
Researchers have taken many different mod-
eling approaches in using this large amount of
new imagery to measure and understand sus-
tainable development. We use “model ”to mean
any function or set of functions mapping inputs
(e.g., satellite images) to outputs (e.g., a wealth
index or crop yield estimates for an area). Suchmodels are often simple, such as linear regres-
sion models that relate satellite-derived vegeta-
tion indices to crop yields ( 18)o rt h a tr e l a t e
nighttime lights (henceforth, “nightlights ”)t o
economic outcomes ( 19). When there is sub-
stantial prior knowledge o f the likely relation-
ship between satellite-derived features and the
o u t c o m eo fi n t e r e s t ,a si nt h ec a s eo fm a n y
agricultural variables, such approaches can
often work well. However, even in these set-
tings, machine learning approaches that seek
to more flexibly learn, rather than specify, the
mapping of inputs to outputs can often im-
prove predictive performance. Here we pro-
vide an overview of the range of modeling
approaches that have been used to relate
satellite images to sustainable development
outcomes.
Shallow models based on
handcrafted features
In some domains, prior knowledge of the phys-
ics, chemistry, or biology of the relevant pro-
cesses suggest that certain functions of the
inputs are likely useful for prediction. This is
the case for numerous vegetation indexes,
which are computed from raw imagery as
simple ratios of reflectances at different wave-
lengths and are known to be related to veg-
etation health. Simple regression models such
as linear regression or random forests can be
used to make pixel-wise predictions directly
from these handcrafted features to the outputs
of interest [see ( 20) for a recent review in the
agricultural domain]. When the input has spa-
tial structure, simple aggregation strategies
can be used to map pixel-wise features to
image-wise features. These include simple
statistics such as taking the mean, quantiles
(e.g., minimum, median, or maximum), or his-
tograms of binned values as inputs to a re-
gression model. For example, Henderson et al.(19) showed how average growth in night-
lights over a country was a strong predictor
of economic growth at the country level in a
linear regression, and our previous work ( 18)
showed how vegetation indices derived from
daytime high-resolution imagery were strong
predictors of smallholder maize yields in a
linear regression.
Models that use spatial structure
in the imagery
In computer vision, spa tial context can often
greatly improve prediction accuracy for image
analysis tasks. Machine learning models withfilters designed to take into account spatial
structure, such as convolutional neural net-
works (CNNs), often perform much better
than handcrafted features and simple aggre-
gation strategies. Deep networks with residual
connections such as DenseNET or ResNet ( 21)
are often used. In this case, features are auto-
matically learned from the data rather than
handcrafted. This is currently the leading ap-
proach in most computer vision applications.
Use of this approach with satellite images in
sustainable development applications has pro-
liferated in recent years, including in the mea-
surement of population ( 22–24), economic
livelihoods ( 25–28), infrastructure quality
(29,30), land use ( 31,32), informal settlements
(33,34), fishing activity ( 35,36), and many
others. In one example, a team hand-annotated
thousands of medium-resolution daytime im-
ages with the location of foreign fishing vessels
and then trained a CNN to predict the presence of
those vessels; predictions were then further hand-
validated using high-resolution imagery ( 36).
Models that use spatial and temporal
structure in the imagery
When available, multiple images of the same
location over time can reduce ambiguity (e.g.,
ambiguity due to cloud cover) and provide
crucial information about changes occurring
on the ground. Such a sequence of images is
similar to a video, and architectures from
video prediction in computer vision can bebrought to bear for prediction and regression
tasks. These include long short-term memory
networks (LSTMs) ( 37), convolutional LSTMs
(38), and three-dimensional (3D) CNNs, where
images are fed in sequence into the model
before it makes a prediction. These models
have been successfully used for crop classi-
fication ( 39–41), crop yield prediction ( 42,43),
predicting landslide susceptibility ( 44), and
assessing building damage after disasters
(45,46), among many other applications. For
example, You et al. (42) assemble near-daily
coarse-resolution multispectral images across
the US Midwest, convert each band in each
image to a histogram of reflectance values,
a n dt h e nt r a i nb o t ha3 DC N Na n da nL S T M
to predict county-level soybean yields from
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 3o f1 2Ethiopia Malawi Nigeria
01 23 01 23 01 230123
LSMS survey–based yield (t/ha)Official govt yield (t/ha)Number of 
households
500
10001500
R2 = 0.15 R2 = 0.11 R2 = 0.00 
Fig. 2. Government and household survey –based data on maize productivity are not well correlated at
the district level. Using government data from eAtlas and household-level yield data from LSMS-ISA
surveys, maize yields (metric tons per hectare) are compared by averaging across all households in a
given district. Data include 2011, 2013, and 2015 data in Ethiopia; 2013 data in Malawi; and 2010 and
2012 data in Nigeria. Comparison is restricted to dist rict-years with at least 30 h ouseholds. Gray line is 1:1
line, while black lines show linear fits within each cou ntry. Points are sized relative to the number of
households contributing to each estimate in the LSMS data.RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

these histograms, outperforming previous
models.
Models that use several modalities
When multiple data modalities are available,such as measurements from different satel-
lites, it is often possible to combine all the
inputs into a single deep learning model.
Approaches include stacking the inputs as ad-
ditional channels of a single network or multi-
branch architectures where data modalities
are processed separately to extract features
that are then concatenated before a final pre-
diction layer. One example of this approachis a model that combined both daytime and
nighttime satellite imag ery to predict village-
level asset wealth in Africa ( 28); separate CNNs
were trained to predict wealth using the two
types of imagery, and then the final layers of
each model were concatenated and used as
predictors in a final ridge regression. Additional
examples include models that combine imagery
with data from weather sensors ( 47), cell phones
(27), Wikipedia ( 48), social media ( 49), street-
level imagery ( 50), or Open Street Map ( 51)t o
predict development-related outcomes.
Model development with limited training data
An additional set of techniques have been de-veloped to utilize the above modeling ap-
proaches in the context of limited training
data —a common problem in sustainability
applications. For instance, standard convolu-
tional neural network architectures contain
millions to tens of millions of trainable pa-
rameters ( 52), whereas training data for spe-
cific sustainability tasks can often number in
the hundreds. This limi ted amount of labeled
data is often insufficient for “end-to-end ”train-
ing of deep networks. Multiple strategies have
been deployed to address this problem.
Using synthetic data
A first approach is to generate and use syn-thetic data to train models. In some cases,
domain knowledge about the relevant physi-
cal process exists in the form of validatedsimulators. These simulators can be used to
provide synthetic training data, i.e., synthetic
inputs of what the process would look like
from space, paired with simulated outputs.
These synthetic pairs can be used to augment
the training data. For example, crop model
simulations can be used to estimate relation-
ships between crop yields and physical param-
eters (e.g., leaf area index, canopy nitrogen)
that have expected relationships with vegeta-
tion indices; model parameters can then be
combined with observed vegetation indices
from satellite imagery to predict yields. This
approach requires no ground data for training
and has been shown to perform as well as or
better than approaches that calibrate directly
to limited field data ( 18,53).
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 4o f1 2DigitalGlobe
KOMPSA TPléiades
SPOT 6/7TripleSat
LandSat
MODISSentinelPlanetRapidEyeSkySat10 cm
50 cm
1 m
5 m
10 m
50 m
100 m
500 m
day week month year 10 years
cloud-free revisit rateSensor resolutionPublic
Private
New since 2010cloud −free revisit ratecloud-free revisit rateSensor resolution10 cm
50 cm
1 m
5 m
10 m
50 m
100 m
500 m
day week month year 10 years
cloud-free revisit rateSensor resolution 2010 Africa
Frontier2019 Africa
Frontier
2019 Africa
Frontier2019 EU + USA
Frontier10 cm
50 cm
1 m
5 m
10 m
50 m
100 m
500 m
day week month year 10 years
day
week
month
year
7 yearsNightlights (500 m)
Landsat (30 m)day
week
month
year
7 years
day
week
month
year
7 yearsSentinel (10 m)
PlanetScope (3 m)day
week
month
year
7 years
YearDigital Globe (<1 m)day
2000 2004 2008 2012 2016week
month
year
7 years
Moisture Index
Shadow Index
Vegetation Index
Built-up Index
Nov - preseason
March - peak
May - harvest
Sept - postseasonA   B      C
D                E           F          G
Fig. 3. Spatial resolution, temporal frequency, and spectral availability of sate llite imagery
have increased substantially since 2000. (A) Average revisit rate and sensor resolution of
cloud-free optical imagery in 2019, averaged acr oss 100 populated African locations (randomly
sampled, proportional to population). ( B) Blue line ( “frontier ”) shows overall revisit rate across
all available sensors at a given spatial resolution in 2010 for same 100 locations (e.g., at 1 m,
the line denotes average cloud-free revisit rate using all sensors ≤1 m); orange line shows same for
2019. Orange area denotes the new combinations of temporal and spatial resolution available
by 2019, which expanded greatly at resolutions >1 m. ( C) Average 2019 coverage in Africa
(orange line) versus 100 locations in US or EU (gray line; locations randomly sampled, proportional
to population). Gray shaded area depicts inequ alities in coverage between US or EU and Africa
in 2019, which are larger for imagery <3 m per pixel. ( D) Calculated revisit periods for several
satellites over 500 randomly selected survey locations in Africa since 2000. Nightlights revisit
rate is set to 1 year given the stable yearly product. ( E) Example imagery corresponding to
each sensor in a single location in central Zambia. Images are real color except for nightlights.
(F) Indices generated from various bands can co nvey different information, as depicted
here using Sentinel 2 data (y ellow colors indicate higher values of the index). ( G) Frequent revisit
rates of new public sensors capture temporal vari ation in human activity, i ncluding rapid changes
throughout the main agricultural season shown here.RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

Transfer learning
A second approach, transfer learning, first
trains a model on a different but related task
for which large amoun ts of labeled data are
available [such as ImageNet in computer vi-
sion or Functional Map of the World ( 54)a n d
WikiSatNet ( 55) for satellite images]. The
model is then “fine-tuned ”on the target task
of interest. For example, Jean et al.(25)f i r s t
trained a neural network to predict night-
lights (a plentiful proxy for economic devel-
opment) from daytime imagery, thus learning
to recognize features in the high-resolution
daytime imagery related to economic activ-ity. Features were then extracted for daytime
images in locations where a very small (<500)
number of observations of economic liveli-
hoods in Africa were available, and a simpler
model (e.g., regularized regression such as
ridge or lasso) used to predict livelihoods
from these features. Another recent approach
applied a trained object identifier to high-
resolution data to identify buildings, vehicles,
and other objects and then used these objects
as features in a regularized regression to pre-
dict economic well-being in Uganda with
high accuracy ( 56).
Transfer learning can also be done spatially,
with models trained in a region with plentiful
data and then fine-tuned to a target geography
where labels are sparse. For example, a model
trained to predict infrastructure quality in
Africa was fine-tuned to a specific country
u s i n go n l yas m a l la m o u n to fl a b e l e dd a t a( 30).
The main challenge with spatial transfer learn-
ing is that changes in the input data distri-
bution from one region to another (e.g., the
appearance of houses or crops) will decrease
predictive performance.
Unsupervised or semi-supervised learning
A third approach uses unsupervised or semi-supervised learning to take advantage of large
amounts satellite imagery for which labels are
not available. Pretrai ning on unlabeled data
has shown great progress in computer vision
(57–59), narrowing the gap with fully super-
vised methods. For instance, two related re-
cent approaches train CNNs to use spatial
similarity between small patches of input im-
ages to derive representations of entire images
without any labeled training data. Represen-
tations learned this way perform well on a
range of tasks, such as crop-type classification
and prediction of wealth and population ( 60,61).
Semi-supervised learning strategies attempt to
improve model performance by additionally
leveraging a small amount of labeled data. This
idea improved performance in predicting eco-
nomic well-being from satellite imagery ( 62).
Model development and evaluation with noisy data
The performance of satellite-based models,
particularly in settings beyond where theywere trained, is perhaps the most important
concern for researcher sa n dp o l i c y - m a k e r s
interested in potential applications in sustain-
able development. Noisy training data can de-
grade model performance in two ways. First, it
can diminish the ability of a model to learn
predictive features. Second, and more subtly,
the model might learn relevant features but
perform poorly in predicting test data, precise-
ly because the test data has noise. This latter
outcome would lead researchers to under-
state the model ’st r u ep e r f o r m a n c e .A sn o i s y
datasets are increasingly used for model de-
velopment, researchers must contend with thedual challenges of not overfitting to noise and
not underestimating model performance. While
existing work mainly highlights the former
challenge ( 63), we believe the latter is perhaps
more fundamental —and underappreciated.
Noisy training versus noisy test data
Studies in the broader deep learning domain
have demonstrated how models trained on
noisy but numerate labels can still perform
well when evaluated on high-quality test data
(64–67). In sustainable development settings,
although noisy training can certainly still de-
grade model performance when the amount
of training data is limited ( 68) or errors are
nonrandom, recent studies in agriculture and
infrastructure highlight how such noise can
be overcome by training on large noisy data-
sets and/or evaluating on high-quality test
data ( 53,69,70). For example, a satellite-based
crop classification model trained on labels
derived from millions of imperfectly geo-
located smartphone photos in India was able
to exceed the performance of benchmark
satellite-based classifiers ( 69).
To further explore this ability to overcome
noise, we used data from an earlier study of
African asset wealth ( 28) to explore the in-
fluence on model performance of three types
of errors common in publicly available train-
ing data: (i) random noise ( “jitter ”)p u r p o s e l y
added to village geocoordinates to protect pri-
vacy, (ii) sampling variability noise due tosmall samples, and (iii) noise from household
misreporting. We trained models with each
type of noise added and evaluated perfor-
mance on the remaining test data that had
either been similarly degraded or unaltered.
When trained and evaluated on noisy data,
model performance de graded with added
noise (Fig. 4, A to C). Yet when evaluated
on undegraded test data , model performance
remained highly stable, even given large
amounts of training noise.
Accurately assessing model performance
Most existing work has focused on techniquesto avoid overstating model performance, in-
cluding strategies discussed above to avoid
overfitting during training and the typicalpractice of testing models on held-out data.
Here we discuss two strategies for dealing
with the opposite problem: understating model
performance resulting from noise in test data.
A first approach is to ensure that a small
amount of very high-quality ground data is
available for model testing. Training is done
on noisier, more numerous data, and testing is
done on the sparser high-quality data. A sec-
ond strategy is to identify an additional var-
iable associated with the outcome of interest,
such as weather, in the case of economic out-
put, or fertilizer, in the case of agricultural
productivity. The strength of association be-tween this variable and model predictions —
as measured, for instance, by correlation —can
then be compared with the association be-
tween the variable and the (noisy) training
data for the model. To illustrate these strat-
egies, Fig. 4, D to F, draws on a recent study of
maize yields in Uganda ( 53). Agreement be-
tween satellite-based yield estimates and noisy
ground data from crop-cuts (i.e., harvests from
small, randomly selected portions of a field)
has a relatively modest explanatory power (co-
efficient of determination R
2=0 . 2 8 )( F i g .4 D ) .
Model performance is much better when pre-
dictions are compared with the gold-standard
measure of full plot harvests, available for a
smaller number of randomly selected fields
(Fig 4E). Similarly, the correlation between
satellite estimates and independent third var-
iables (fertilizer use and soil quality) were the
same as the correlation between crop-cut yields
and these measures, suggesting that the “sig-
nal”in the satellite measures was as strong
as that from the ground measure (Fig. 4F). A
similar finding was obtained in Kenya when
pitting satellite-estimated maize yields against
self-reported yield data ( 18).
Another example of both strategies is given
in (28), where estimates of wealth from sat-
ellites and from ground data are each com-
pared against independent wealth measures
from census data (considered high quality)
and against a measure of annual temperature,
which has been shown to correlate strongly toeconomic outcomes. Ground data and model
predictions showed similar correlation against
the independent wealth measure, and both
uncovered similar nonlinear relationships be-
tween temperature and wealth, suggesting that
the satellite-based wealth measure was roughly
as trustworthy as the original ground data.
Applications
Researchers are actively evaluating the use-fulness of satellite imagery for a range of
sustainable development applications, with
more work thus far focused on whether satel-
lites can be used to make reliable measurements
and less devoted to using derived measures for
downstream research tasks or policy decisions.
We focus on four domains where recent work
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 5o f1 2RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

on satellite-based measurement has been par-
ticularly active and where comparable quanti-
tative results exist across studies. Our goal is to
provide rough performance benchmarks across
these domains and, where possible, diagnose
constraints to further improvement. We in-
clude all published or public preprint studies
where comparable test statistics could be ob-
tained for the outcome of interest in a
developing-world geography. We then review
the more limited set of cases where these and
other satellite-based measurements have been
used for research or policy tasks.
Smallholder agriculture
Roughly 2.5 billion individuals, and over half
of the world ’s poor, are estimated to live in
“smallholder ”households that primarily de-
pend on farming small plots of land for their
livelihoods ( 71). Although remote sensing has
been used in agricultural applications for de-
cades, coarse sensor resolutions and a paucity
of training data had until recently largely pre-
cluded its application in smallholder agricul-
ture, where field sizes are often <0.1 ha (or
roughly one 30-m Landsat pixel).
Here we assemble data from recent studies
attempting to predict yield at the field scale in
smallholder environments (table S1), a capabil-
ity useful for a range of development appli-
cations, including the targeting and evaluation
of agricultural interventions and the rapid mon-
itoring of rural livelihoods [yield prediction
performance at more-aggregate scales is
reviewed in ( 72)]. We found 11 studies with
comparable field-scale performance metrics,
spanning multiple cont inents and seven crops.
All studies used relatively simple models to
relate handcrafted features (typically, vegetation
indices constructed from ratios of reflectances
in the visible and near-infrared wavelengths) to
ground-measured yields, and nearly all eval-
uated models on training rather than held-out
test data. Although predictive performance dif-
fered widely across and within crops (Fig. 5A),
likely owing to the enormous temporal and spa-
tial heterogeneity present in smallholder agricul-ture, our reanalysis of multiple studies for which
replication data were available allowed insight
into the determinants of model performance.
Models trained and evaluated on more “ob-
jective ”ground data (i.e., harvest data col-
lected from crop-cuts or full plot harvests)
performed, on average, substantially better
than models trained on farmer self-reported
data (Fig. 5B), again highlighting the impor-
tance of ground-based measurement error in
model evaluation. Also, model performance
was much higher on larger fields (Fig. 5C),
likely because the same magnitude error can
be more consequential for smaller fields; a
10-m georeferencing error is more consequen-
tial for a 10-m-wide field than it is for a 100-m-
wide field. Finally, additi onal training samplesrapidly improved performance on held-out test
data (as measured by root mean square error)
( F i g .5 D ) ,u pt oa r o u n d3 0t o5 0s a m p l e s .P e r -
formance was largely stable beyond that, sug-
gesting that, at least in the African settings
represented here, adequate performance for
yield prediction could be achieved with only
a few dozen high-quality training samples.
Population
Accurate knowledge of where people are phys-ically located is a critical input into an immense
range of research and policy applications. Be-
cause population censuses are infrequent in many
developing countries and fine-scale data from
existing censuses are of ten not made public,
generating fine-scale es timates of population
locations has been a research focus for decades.
The traditional top-down approach to local-
level population estimation uses satellites
and other inputs to redistribute available ag-
gregate census data down to a finer-scale grid
(1 km or finer), typically using a model trainedon the available coarse-scale data ( 73,74). Anoth-
er approach generates a binary population
mask at fine scale using estimates of building
or settlement locations derived from imagery
and applies this mask to the coarse-scale data
(75). For either approach, predictions can only
be readily evaluated at coarse scale. In the ab-
sence of clear evaluation opportunities, a
consortium of data producers have built use-
ful tools in which different gridded estimates
can be visually compared at local scale ( https://
popgrid.org ).
For additional quantitative comparison, we
studied three commonly u sed population ras-
ters that used satellite data as at least one in-
put in their production: WorldPop ( 74), Global
Human Settlement Layer (GHSL) ( 75), and
LandScan ( 73). We harmonized each to a con-
sistent 1-km grid and compared population
estimates for grid cells with nonzero estimates
across all three rasters. Estimates showed mod-
est agreement ( r= 0.62 to 0.78) when com-
paring across all global pixels (Fig. 6), with
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 6o f1 2AB C
DE F
0.00.51.01.52.0
0.5 1.0 1.5
Satellite yield (Mg/ha)Crop cut yield (Mg/ha)R2 = 0.28
012
0.5 1.0 1.5
Satellite yield (Mg/ha)Full plot yield (Mg/ha)R2 = 0.56unaltered test data noisy test data0.60.70.80.9
0 2.5 5 7.5 10 12.5
average km of jitterR2
0.60.70.80.9
12 8 4
n samples in village
Correlation with inputs
Crop cut yields
Satellite yieldsCorrelation
0.00.20.40.60.81.0
Used
fertilizerSoil Quality 
Index0 0.1 0.2 0.3 0.40.60.70.80.9
added error = N(0, x)
Fig. 4. The role of noise in model performance and evaluation. (AtoC) Performance of wealth prediction
model as noise is added to train and test data. Model trained to predict asset wealth from nightlights imagery
across 4000 African villages, using the dataset from ( 28). Performance is evaluated as three different types of
noise are added to training data: (A) random noise in village geocoordinates (starting from 2.5 km, the actual
noise in the survey data), (B) noise from constructing v illage-level wealth estimates from decreasing numbers of
households within the village to represent sampling variability, and (C) random noise added to village-level wealth
estimates, representing random response error from respondents. Green lines show performance evaluated on
test data where similar noise has been added, blue lines show performance on test data where noise has not been
added. Shaded areas indicate confidence intervals across 200 runs at a given level of added noise. As all types
of training noise increase, model performance degrades when evaluated against similarly noisy test data but does
not degrade when evaluated against unaltered test data. ( DtoF) Example from a study of maize yields in Uganda
(53) in which both ground-based and satellite-based measurements can have noise, and multiple approaches can
help adjudicate which is noisier. (D) Imperfect correlati on between ground- and satellite-based yield measure does
not reveal source of noise. (E) Comparison of satellite measur e with available gold-standard ground measure from full
plot harvest shows higher correlation, indicating ground measure in (D) responsible for at least some of the noise.
(F) Comparison of satellite measure and ground measure with independent thir d measures expected to correlate with
yields (here, fertilizer use and soil quality) suggests tha t the two yield measures in (D) are roughly equally noisy.RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

lowest agreement between LandScan and the
other rasters and lower overall agreement in
Africa ( r= 0.45), perhaps owing to limited
census data on which to train models. Agree-
ment improves when comparisons are made
at increasingly aggregate levels (Fig. 6E), with
correlations approaching r= 1.0 when esti-
mates are aggregated to 100-km pixels.
Validation studies in settings where fine-
scale population data are available found sim-
ilar correlations among datasets, e.g., cell-wise
correlations between the admin data and
GHSL, WorldPop, and LandScan of r=0 . 8 3 ,
0.82, and 0.7, respectively, in Sweden ( 76).
Other studies in China and Europe found
similar or higher performance of individual
gridded datasets evalu ated at somewhat more
aggregate scales but (as in Fig. 6) found that
performance was not uniform and tended to
degrade at finer spatial scales ( 77,78).
Because a standard approach to generating
these estimates is to disaggregate official cen-
sus estimates, final estimates are unavoidably
affected by any inaccuracies in the official cen-
sus data, for instance, owing to the most re-
cent census having occurred a decade or more
prior. An alternative that does not present this
problem is to train bottom-up models to di-
rectly predict local-level population estimates.
These approaches have shown promise in
multiple settings ( 10,24,79), are beginning to
be incorporated into global gridded products
(e.g., WorldPop) for countries where censuses
are particularly out of date ( 80)a n dh a v e
been shown to be a cost-effective way of gen-
erating reliable national-scale population esti-
mates ( 10).
Economic livelihoods
Predicting variation in local-level economic
outcomes is another active domain, motivated
by the paucity of existing data (Fig. 1) and the
broad range of applications for which such
data could be useful. As in the agricultural
setting, existing work spans diverse geogra-
phies and seeks to predict a range of out-
comes, making quantitative comparison ofdifferent models or sensors difficult.
We focused on 12 studies that used imagery —
either alone or in combination with other data —
to predict asset wealth at a local level in the
developing world. Asset wealth is a commonly
used measure of households ’longer-run eco-
nomic well-being and is consistently measured
in a number of georeferenced nationally rep-
resentative household surveys. Figure 6F
shows existing estimates, all of which derive
from studies that applied convolutional neural
networks to imagery to generate features used
to predict wealth and reported evaluation sta-
t i s t i c so nh e l d - o u tt e s td a t a .
While study intercomparison remained chal-
lenging owing to the varied geographic settings
(spanning Africa, Asia, and the Caribbean),spatial scales (from village level to district
level), and varying inclusion of nonsatellite
data, results allowed some generalizations.
First, satellite information could always ex-
plain more than half, and often more than
75%, of the variation in the survey-measured
asset wealth, with performance appearing to
trend upward over time. Again, these esti-
mates likely understate true model perfor-
mance, as test data almost always derive from
public data with known sources of noise. Sec-
ond, studies that made predictions at more-
aggregate spatial scales and studies that
combined satellite information with data from
other sources tended to outperform village-
level satellite-only models. These data fusion
approaches have become increasingly com-
mon, with researchers demonstrating how
combining imagery with data from cell phones(27), Wikipedia ( 48), social media ( 49), or
Open Street Map ( 51) can improve predictions.
Table S2 describes results from additional
studies that looked at other measures of eco-
nomic livelihoods, incl uding consumption ex-
penditure and multidimensional poverty
indices. Prediction performance for consump-
tion expenditure (the measure on which offi-
cial poverty estimates are based) was typically
lower than that for asset wealth, a difference
that has been in part attributed to relatively
higher noise in the consumption data ( 25,28)
and the extreme paucity of public georefer-
enced public consumption data on which to
train models.
Informal settlements
A final related area where there has been muchrecent work is in the detection of informal
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 7o f1 2A  Performance: all smallholder field-scale yield studies     B  Performance by type of training data
C  Performance by field size            D  Performance by number of training examples0.000.250.500.751.00
wheat cotton maize millet sorghum rice beans
croptrain R2
0.10.20.3
0.0 0.1 0.2 0.3 0.4 0.5
minimum field size (ha)train R2
0204060
0 50 100 150
training samples% change in RMSE relative to full modelMedian field size 
Uganda, maize     0.12
Mali, sorghum     1.18
Kenya, maize     0.14India wheat
Mali sorghumUganda maize
all
0.000.250.500.751.00train R2crop cut
full cutself reported
Fig. 5. Performance of satellite-based approaches to measuring smallholder yield at field scale.
(A) Performance across all known published studies where coefficient of determination ( R2) was reported
(32 estimates across 11 studies); R2estimates are “in-sample, ”i.e., for data on which model was trained.
(B) Difference in performance for models trained and evaluated on crop-cut, self-reported, or full-plot harvest
data suggest that more objective crop measures improve performance. First three estimates are for studies
that compared at least two types of ground data in the same setting. “All studies ”estimates pool across
estimates in (A). ( C) Performance generally increases when sample is restricted to larger fields, particularly
in East African settings where field sizes are very small. ( D) Performance on test data improves rapidly
with additional training examples up to ~30 data points, and then improves more gradually thereafter.
Performance measured as average root mean square error between predicted and observed yields in the test
set, averaged over 100 different random subsets of training samples at each size of the training set.RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

settlements (sometimes called slums). Urban
populations are growing rapidly throughout
much of the developing world, and ~30% of
developing-country ur ban populations are es-
timated to live in slums —settled areas where
inhabitants lack access to essential services,
durable housing, and/or tenure security ( 81).
Systematic data on the location and size of
such settlements is lacking, making it difficult
to monitor and target service delivery and toprotect residents against eviction, among other
challenges ( 8).
Because the spatial structure (e.g., density,
size, and type of buildings) can differ substan-
tially between informal settlements and sur-
rounding regions, researchers have sought to
use imagery to measure the location and size
of these settlements [see ( 8) for a recent re-
view]. We focus on 23 studies that used satel-
lite imagery to segment or classify informalsettlements in the developing world. These
studies use a variety of methods, with some
focused on creating rule bases for classifica-
tion and others on directly using machine
learning for classification.
As with the other domains discussed, studies
span diverse geographies where settlements
can be very structurally dissimilar from each
other, making study intercomparison difficult.
However, in 17 studies that reported classi-
fication accuracy (evaluated against typically
small numbers of ground observations), accu-
racy exceeded 80% in most studies and ap-
peared to be improving over time (Fig. 6G).Table S3 shows results from additional studies
that reported alternate performance metrics.
Application in research
Here we highlight a number of settings inwhich measures derived from satellite-based
remote sensing, including those discussed
above, are being used for some downstream
research task in the developing world. The
widest adoption of satellite-derived measures
in research and policy has been in the realm
of population estimates, with existing gridded
population data being used in public health,
disaster response, economic development, and
c l i m a t ec h a n g er e s e a r c h( 10,80,82). Satellite
imagery has also been widely used to better
understand agricultural productivity, includ-
ing why some fields or some regions are more
productive than others ( 6), whether particular
management practices have been adopted
(83), and the impact of infrastructure invest-
ments on productivity ( 84,85). Satellite esti-
mates are also increasingly being used to
identify fields most likely to respond to a
particular input ( 18,53) or new management
practice ( 86).
Fisheries and animal production are addi-
tional food-related domains where satellite
imagery is increasingly used in research and
policy. Recent work shows how multiple satel-
lite sensors and deep learning can shed light
on overall patterns of global fishing activity
(35) as well as on specific activities, such as
illegal fishing ( 36,87).
Researchers in economics also increasingly
use satellite imagery, and particularly night-
lights imagery, for a variety of applications
(7). Nightlights have been used to assess the
validity of official government statistics ( 19,88);
to understand the growth and activity of urban
versus rural areas ( 89,90); and to assess the
role of local and federal institutions, trans-
port costs, and other factors on economic
development ( 91–94). While the use of optical
imagery beyond nightlights remains some-
what more limited, recent papers have shown
how high-resolution optical imagery can be
used to measure compliance with conservation
programs ( 95) and to understand how ethnic
favoritism shapes economic investment ( 96).
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 8o f1 2A  B   C
D          E
F                  GGHSL estimate (1000's)Landscan estimate (1000's)050100150200
05 0 1 0 0 150 200R2=0.38, p<0.01
050100150200
0 50 100 150 200
Worldpop estimate (1000's)Landscan estimate (1000's)R2=0.45, p<0.01
GHSL estimate (1000's)Worldpop estimate (1000's)050100150200
0 50 100 150 200R2=0.61, p<0.01
1102104106108Pixel
count
0.250.500.75Mean
pairwise
correlation
0.50.60.70.80.91.0
2016 2017 2018 2019 2020
Y earR2village level, satellite
village level, satellite+>village level, satellite
>village level, satellite+
0.50.60.70.80.91.0
2016 2017 2018 2019 2020
Ye a rAccuracy
2 class
3 class
4 class
6 class0.60.70.80.91.0
1 km 5 km 10 km 100 km
Raster resolutionCorrelationWorldpop & GHSL
Worldpop & Landscan
GHSL & Landscan
Fig. 6. Performance of satellite-based approaches to measuring population, wealth, and informal
settlements. (AtoC) Comparison of three different satellite-informed global population datasets (Landscan,
WorldPop, and GHSL population) datasets at 1-km resolution globally (colors correspond to scale at right).
(D) Average pairwise correlation within each country at 1-km resolution. Com parisons show modest correlation
between datasets at global scale and often poo r correlation in many developing countries. ( E) Correlations
across datasets improve when data are spatially aggregated. All comparisons are made for pixels that were
not missing and not zero across all three datasets. ( F) Performance in predicting asset wealth in various
developing countries from satellite data (16 estimates from 12 papers), as measured by R2on test data. Filled
markers are estimates that combine satellite informati on with other data (cell phone data, social media data,
or Wikipedia). Circles indicate estimates at the village level, triangles are estimates at a more-aggregate spatial
scale (subdistrict or district). ( G) Performance in predicting the locatio n of informal settlements from imagery
(20 estimates from 17 papers). Colors correspond to the number of categories being predicted.RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

Recent work also shows how satellites can
be useful in the experimental evaluation of
interventions, including measuring the het-
erogeneous impact of new agricultural tech-
nologies on productivity ( 86), measuring the
impacts of cash transfers on household live-
lihoods ( 97), and measuring compliance in a
payment-for-forest-protection program ( 95).
While each of these studies focus on settings
where changes induced by an intervention
are readily apparent in imagery —an aspect
that might not hold in other settings —they
demonstrate the large potential for satellite
imagery to contribute to the quantitative eval-uation of many developm ent interventions.
Use in decision-making
Although satellite-based measures are now
being used in a variety of research applica-
tions, documented examples of their opera-
tional use in public-sector decision-making
and policy in the developing world is much
more limited. Thus, imagery has so far done
more to help understand sustainable devel-
opment, and less to promote it. Systematic in-
formation on operational use in the private or
military sector is even more sparse, although
use is likely widespread and growing. Here we
only consider public-sector nonmilitary use.
As in research, the widest application of
satellite-based measures in public-sector
decision-making is in the population domain.
For instance, the United Nations World Food
Programme and the US government both use
gridded population estimates to inform needs
assessments and target humanitarian response
after natural disasters ( 80). Gridded population
data are also being used to inform sampling
strategies for ground surveys ( 80).
In agriculture, remote-sensed vegetation in-
dices and satellite-derived rainfall estimates
are key inputs into short-term forecasting of
food insecurity, which directly informs food
aid and other humanitarian resource alloca-
tion ( 98). Numerous systems that track agri-
cultural conditions around the world also
make ample use of remote-sensing informa-tion, and output from these systems are used
in a wide array of tasks, including in early
warning alerts, foreign aid decisions, analysis
of commercial trends ,a n dt r a d ep o l i c y( 99).
Data from remote detection of fishing activity
is also being used by numerous governments
and other organizations to manage fisheries
and design protected areas ( 100).
Documented use in other livelihoods mea-
surement appears limited, although anecdot-
ally there is rapidly growing interest in the
policy community in exploring these mea-
sures ( 101). The simplest explanation for lim-
ited adoption is that the combination of
satellite information and machine learning
is still new and decision-makers are unfam-
iliar with these approaches or are not con-vinced that they are “good enough. ”Our view
is that, in many settings, including small-
holder agricultural and livelihood measure-
ment, the true accuracy of satellite-derived
estimates can rival or exceed that of tradi-
tional survey-based measures. It remains the
job of the research community to help make
this clear, and the job of the user community
to transparently define the counterfactual: If
not satellite-based data, what alternative data
w o u l db eu s e dt om a k ead e c i s i o n ,a n dw h a t
do we know about its reliability?
Even if satellite-based measures are accu-
rate, they might not yet be operational. To ourknowledge, there exist no updated, global-scale
estimates of smallholder crop productivity,
economic well-being, or informal settlements
that a decision-maker could immediately use
(estimates are beginning to exist for individual
countries). The research community is arguably
not well positioned to generate and update
such estimates over time, and partnerships
with public-sector institutions or the private
sector to scale and operationalize these esti-
mates could be important in enabling their
sustained use.
Even when models are operational, decision-
makers might be understandably hesitant to
adopt a measure they cannot fully explain.
Deep learning models te nd to sacrifice inter-
pretability for predictive performance, but
understanding why a model makes the pre-
dictions it does can help build trust that pre-
dictions are accurate and fair. Well-publicized
instances of algorithmic bias in other settings
[e.g., predictive policing, sentencing, and hir-
ing decisions ( 102
)] and concerns by civil rights
groups that further deployment of algorithmic
decision-making might worsen racial and
socioeconomic inequalities ( 103,104)u n d e r -
standably amplify worries that predictions
from these new approaches could be either
inaccurate or unfair.
Existing guidelines for Fairness, Account-
ability, and Transparency in Machine Learn-
ing (FAT ML) ( 105), if followed, could help
navigate these issues. These guidelines aim toensure that researchers are aware of potential
discriminatory effects of their algorithms and
are able to investigate and provide redress
should issues arise. While implementation
of the guidelines certainly has its own chal-
lenges ( 106) (e.g., defining “fairness ”), we are
not aware of any of the papers we review
above —our own included —having fully en-
gaged with these guidelines.
A final reason for limited adoption is that
some actors might see benefit in not having
certain outcomes be measured. Autocratic
regimes already collect less data (Fig. 1), and
certain countries have passed laws (since
reversed) that make it a crime to publish in-
dependent estimates of key economic out-
comes ( 107).Conclusions and directions for future work
We draw four main conclusions from the
above analysis and lay out open challenges
and directions for future work. First, satellite-
based performance in pre dicting key sustain-
able development outcomes is reasonably
strong and appears to be improving. Estimates
are being used in a wide variety of research
applications and, in some cases, are already
actively informing decision-making. Indeed,
analyses suggest that reported model perfor-
m a n c el i k e l yu n d e r s t a t es true performance in
many settings, given the noisy data on which
predictions are evaluated, and that satellite-based estimates can equal or exceed the ac-
curacy of traditional approaches to measuring
key outcomes. For certain outcomes, satellite-
based approaches can already add substantial
information at broad scale and low cost com-
pared with what can be collected on the
ground. Numerous quantitative approaches
now exist to assist researchers and practitioners
in better understanding and not underestimat-
ing the performance of satellite-based ap-
proaches relative to tradi tional alternatives.
Second, perhaps the largest constraint to
model development is now training data rath-
er than imagery. While imagery has become
abundant, the scarcity and (in many settings)
unreliability of quality labels make both training
and validation of satellite-based models difficult.
Expanding the quantity and, in particular, the
quality of labels will quickly accelerate progress
in this field and will allow both researchers and
practitioners to measure new outcomes and to
accurately assess model performance.
Third, despite the growing power of satellite-
based approaches, there are many domains
where such approaches are likely to contrib-
ute little in the near term —for instance, in
measuring female empowerment, educational
outcomes, or conflict events. Even in settings
where satellites are likely to be useful, satellite-
based approaches will likely amplify rather
than replace existing ground-based data col-
lection efforts. High-quality local training data
can nearly always improve model performanceand will remain essential for convincing both
researchers and decision-makers that satellite-
based approaches are working.
Finally, there remain limited documented
cases where satellites have been operationalized
into decision-making processes in the sustain-
able development domains where we focus —
with satellite-informed population estimates
being the main exception. Limited adoption is
likely driven by a numbe r of forces, including
the recency of the technology, the lack of ac-
curacy (perceived or real) of the models, lack of
model interpretability, and entrenched interests
in maintaining the current data regime.
Helping to overcome these constraints con-
stitutes a key task for researchers and policy-
makers going forward. We suggest nine specific
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 9o f1 2RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

areas where we believe future work would be
particularly useful:
1) More accurate and more numerous train-
ing data: Many applications of deep learning
outside sustainable development have been
advanced by the curation of public reference
datasets, which lower barriers to entry and
enable comparison of different approaches.
Such datasets are a major public good but are
rare in sustainable development. Particularly
needed are datasets that track outcomes over
time so that models can be optimized to detect
changes. Collecting and publishing location
data from existing or new ground surveys(using appropriate priva cy safeguards already
widely in use) could be mandated by survey
funders.
2) More evaluation in the context of specific
use cases: Most evaluations of satellite estimates
have focused on agreement with a ground-
based measure of a particular outcome. Fewer
studies have then gone the next step to eval-
uate the actual application of the outcome
measure, such as to test the impact of a ran-
domized control trial or target an interven-
tion to a subpopulation. These downstream
tasks often provide a more tangible example
of the utility to potential users and can help
avoid the pitfalls of direct comparisons to
noisy ground measures.
3) Improved model interpretability and
transparency: Interpretable predictions and
transparent decisions based on these predic-
tions are important in settings where people
could be affected. Applying FAT ML or similar
guidelines to research output will also be im-
portant as research is operationalized.
4) Creative data fusion: Combining infor-
mation from optical sensors of different tem-
poral and spatial resolutions, different types of
imagery (e.g., optical and radar), and/or alter-
nate data streams (e.g., from cell phones) ap-
pears to be a particularly promising approach
to improving model performance. As much of
these additional data are collected by the pri-
vate sector, sustained and enforceable data-
sharing agreements between companies andresearchers will be key ( 108).
5) Scaling estimates: Researchers typically
have more incentive to innovate on methods
than to apply validated methods across large
geographies or to update estimates as new
data come in, limiting the utility of method-
ological advances to downstream use. Part-
nerships between academic researchers and
public- or private-sector organizations that
h a v et h es k i l l sa n dr e s o u r c e st od ot h i ss c a l i n g
will be key to operationalizing promising re-
search advances.
6) Measuring changes over time: Much of
the literature reviewed above makes predic-
tions at a given point in time, but many ap-
plications require measuring changes over
time. As few ground datasets repeatedly andreliably measure the sa me locations over time,
curating these datasets and using them to
develop and validate temporal predictions
will be key for tracking the evolution of key
sustainability outcomes.
7) Using imagery to actively guide ground
data collection: Improved satellite predictions
could be used to optimally guide further data
collection on the ground —for instance, to col-
lect data in locations w here model predictions
are least certain. Research should explore
w h e t h e rs u c hs a m p l i n gs t r a t e g i e sc o u l di m -
prove outcome measurement as compared with
traditional sampling approaches.
8) Understanding potential pitfalls in causal
inference applications: For instance, can pov-
erty predictions from a satellite-based model
be used to study the impact of new road con-
struction on poverty, if there is a chance that
the model looks for a road to decide whether a
location is poor? How do we proceed if we are
concerned that image-derived proxies for a
dependent variable of interest are themselves
the independent variable of interest?
9) Improved guidelines for privacy: As pre-
dictions become increasingly granular and ac-
curate, who has access to these data? How can
precisely georeferenced ground data (which is
increasingly collected) be used to train or val-
idate models without undermining privacy?
Guidelines for navigating these issues are in-
creasingly critical as models improve.
REFERENCES AND NOTES
1. G. K. Moore, What is a picture worth? A history of remote
sensing. Hydrol. Sci. Bull. 24, 477 –485 (1979). doi: 10.1080/
02626667909491887
2. O. B. Waxman, “Aerial photography ’s surprising role in
history, ”Time Magazine, 31 May 2018; https://time.com/
longform/aerial-photography-drones-history/ .
3. Union of Concerned Scientists, UCS Satellite Database
(2020); www.ucsusa.org/resources/satellite-database .
4. United Nations General Assembly, “Transforming our world:
The 2030 Agenda for Sustainable Development ”(Division for
Sustainable Development Goals, 2015); https://sdgs.un.org/
2030agenda .
5. D. J. Mulla, Twenty five years of remote sensing in precision
agriculture: Key advances and remaining knowledge gaps.
Biosyst. Eng. 114, 358 –371 (2013). doi: 10.1016/
j.biosystemseng.2012.08.009
6. D. B. Lobell, The use of satellite data for crop yield gap
analysis. Field Crops Res. 143,5 6–64 (2013). doi: 10.1016/
j.fcr.2012.08.008
7. D. Donaldson, A. Storeygard, The view from above:
Applications of satellite data in economics. J. Econ. Perspect.
30, 171–198 (2016). doi: 10.1257/jep.30.4.171
8. M. Kuffer, K. Pfeffer, R. Sliuzas, Slums from space —15 years
of slum mapping using remote sensing. Remote Sens. 8,
455–484 (2016). doi: 10.3390/rs8060455
9. Sustainable Development Solutions Network, “Data for
development: A needs assessment for SDG monitoring and
statistical capacity development ”(UN Sustainable
Development Solutions Network, 2015).
10. N. A. Wardrop et al., Spatially disaggregated population
estimates in the absence of national population and housing
census data. Proc. Natl. Acad. Sci. U.S.A. 115, 3529 –3537
(2018). doi: 10.1073/pnas.1715305115 ; pmid: 29555739
11. S. Devarajan, Africa ’s statistical tragedy. Rev. Income Wealth
59,S 9–S15 (2013). doi: 10.1111/roiw.12013
12. K. Beegle, J. De Weerdt, J. Friedman, J. Gibson, Methods of
household consumption measurement through surveys:
Experimental results from Tanzania. J. Dev. Econ. 98,3–18
(2012). doi: 10.1016/j.jdeveco.2011.11.00113. C. Carletto, D. Jolliffe, R. Banerjee, From tragedy to
renaissance: Improving agricultural data for better policies.
J. Dev. Stud. 51, 133 –148 (2015). doi: 10.1080/
00220388.2014.968140
14. “Capacity needs assessment for improving agricultural
statistics in Kenya, ”(Tech. rep., The World Bank, 2018).
15. R. B. Macdonald, F. G. Hall, Global crop forecasting. Science
208, 670 –679 (1980). doi: 10.1126/science.208.4445.670 ;
pmid: 17771086
16. C. D. Elvidge et al., Relation between satellite observed
visible-near infrared emissions, population, economic activity
and electric power consumption. Int. J. Remote Sens. 18,
1373–1379 (1997). doi: 10.1080/014311697218485
17. See supplementary materials.
18. M. Burke, D. B. Lobell, Satellite-based assessment of yield
variation and its determinants in smallholder Africansystems. Proc. Natl. Acad. Sci. U.S.A. 114, 2189 –2194 (2017).
doi:10.1073/pnas.1616919114 ; pmid: 28202728
19. J. V. Henderson, A. Storeygard, D. N. Weil, Measuring
economic growth from outer space. Am. Econ. Rev. 102,
994–1028 (2012). doi: 10.1257/aer.102.2.994 ;
pmid: 25067841
20. M. Weiss, F. Jacob, G. Duveiller, Remote sensing for
agricultural applications: A meta-review. Remote Sens.
Environ. 236,1 1 1 4 0 2 –111421 (2020). doi: 10.1016/
j.rse.2019.111402
21. K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for
image recognition. Proc. IEEE Comput. Soc. Conf. Comput.
Vis. Pattern Recognit. 2016 , 770 –778 (2016).
22. T. G. Tiecke et al., Mapping the world population one building
at a time. arXiv:1712.05839 [cs.CV] (15 December 2017).
23. Z. Zong, J. Feng, K. Liu, H. Shi, Y. Li, DeepDPM: Dynamic
Population Mapping via Deep Neural Network. Proc. Conf.
AAAI Artif. Intell.
33, 1294 –1301 (2019). doi: 10.1609/
aaai.v33i01.33011294
24. W. Hu et al .,“Mapping missing population in rural India:
A deep learning approach with satellite imagery, ”
Proceedings of the 2019 AAAI/ACM Conference on AI,Ethics, and Society , Honolulu, Hawaii, 27 to 28 January 2019,
pp. 353 –359.
25. N. Jean et al., Combining satellite imagery and machine
learning to predict poverty. Science 353, 790 –794 (2016).
doi:10.1126/science.aaf7894 ; pmid: 27540167
26. A. Head, M. Manguin, N. Tran, J. E. Blumenstock, “Can
human development be measured with satellite imagery? ”
ICTD 2017: Ninth International Conference on Information and
Communication Technologies and Development , Lahore,
Pakistan, 16 to 19 November 2017.
27. J. E. Steele et al ., Mapping poverty using mobile
phone and satellite data. J. R. Soc. Interface 14,
20160690 –20160700 (2017). doi: 10.1098/rsif.2016.0690 ;
pmid: 28148765
28. C. Yeh et al., Using publicly available satellite imagery and
deep learning to understand economic well-being in Africa.
Nat. Commun. 11, 2583 (2020). doi: 10.1038/s41467-020-
16185-w ; pmid: 32444658
29. G. Cadamuro, A. Muhebwa, J. Taneja, Assigning a grade:
Accurate measurement of road quality using satellite
imagery. arXiv:1812.01699 [cs.CV] (6 December 2018).
30. B. Oshri et al.,“Infrastructure Quality Assessment in Africa
using Satellite Imagery and Deep Learning, ”Proceedings of
the 24th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining , London, UK, 19 to
23 August 2018.
31. A. Albert, J. Kaur, M. C. Gonzalez, “Using convolutional
networks and satellite imagery to identify patterns in urban
environments at a large scale, ”Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery andData Mining , Halifax, Nova Scotia, Canada, 13 to 17 August 2017,
pp. 1357 –1366.
32. P. Helber, B. Bischke, A. Dengel, D. Borth, Eurosat: A novel
dataset and deep learning benchmark for land use and
land cover classification. IEEE J. Sel. Top. Appl. Earth Obs.
Remote Sens. 12, 2217
–2226 (2019). doi: 10.1109/
JSTARS.2019.2918242
33. N. Mboga, C. Persello, J. R. Bergado, A. Stein, Detection of
informal settlements from VHR images using convolutional
neural networks. Remote Sens. 9, 1106 –1124 (2017).
doi:10.3390/rs9111106
34. C. Persello, A. Stein, Deep fully convolutional networks for the
detection of informal settlements in VHR images. IEEE
Geosci. Remote Sens. Lett. 14, 2325 –2329 (2017).
doi:10.1109/LGRS.2017.2763738
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 10 of 12RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

35. D. A. Kroodsma et al., Tracking the global footprint of
fisheries. Science 359, 904 –908 (2018). doi: 10.1126/
science.aao5646 ; pmid: 29472481
36. J. Park et al., Illuminating dark fishing fleets in North Korea.
Sci. Adv. 6, eabb1197 (2020). doi: 10.1126/sciadv.abb1197 ;
pmid: 32923605
37. S. Hochreiter, J. Schmidhuber, Long short-term memory.
Neural Comput. 9, 1735 –1780 (1997). doi: 10.1162/
neco.1997.9.8.1735 ; pmid: 9377276
38. X. Shi et al., Convolutional LSTM network: A machine learning
approach for precipitation nowcasting. Adv. Neural Inf.
Process. Syst. 28, 802 –810 (2015).
39. S. Ji, C. Zhang, A. Xu, Y. Shi, Y. Duan, 3D convolutional neural
networks for crop classification with multi-temporal remote
sensing images. Remote Sens. 10,7 5–92 (2018).
doi:10.3390/rs10010075
40. M. Rußwurm, M. Körner, Multi-temporal land cover
classification with long short-term memory neural networks.
Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci 42, 551 –558
(2017). doi: 10.5194/isprs-archives-XLII-1-W1-551-2017
41. R. M. Rustowicz et al., Semantic segmentation of crop type
in Africa: A novel dataset and analysis of deep learning
methods. IEEE Comput. Soc. Conf. Comput. Vis. Pattern
Recognit. 2019 ,7 5–82 (2019).
42. J. You, X. Li, M. Low, D. Lobell, S. Ermon, Deep gaussian
process for crop yield prediction based on remote sensing
data. Proc. Conf. AAAI Artif. Intell 31, 4559 –4566 (2017).
43. J. Sun, L. Di, Z. Sun, Y. Shen, Z. Lai, County-level soybean
yield prediction using deep CNN-LSTM model. Sensors 19,
4363 (2019). doi: 10.3390/s19204363 ; pmid: 31600963
44. L. Xiao, Y. Zhang, G. Peng, Landslide susceptibility
assessment using integrated deep learning algorithm along
the China-Nepal highway. Sensors 18, 4436 (2018).
doi:10.3390/s18124436 ; pmid: 30558225
4 5 . J .Z .X u ,W .L u ,Z .L i ,P .K h a i t a n ,V .Z a y t s e v a ,
Building damage detection in satellite imagery usingconvolutional neural networks. arXiv:1910.06444 [cs.CV]
(14 October 2019).
46. T. Ci, Z. Liu, Y. Wang, Assessment of the degree of building
damage caused by disaster using convolutional neural
networks in combination with ordinal regression. Remote
Sens. 11, 2858 –2877 (2019). doi: 10.3390/rs11232858
47. F. M. Davenport et al ., Using out-of-sample yield forecast
experiments to evaluate which earth observation products
best indicate end of season maize yields. Environ. Res.
Lett. 14,1 2 4 0 9 5 –124109 (2019). doi: 10.1088/1748-9326/
ab5ccd
48. E. Sheehan et al.,“Predicting economic development using
geolocated Wikipedia articles, ”Proceedings of the 25th
ACM SIGKDD International Conference on Knowledge Discoveryand Data Mining , Anchorage, Alaska, 4 to 8 August 2019,
pp. 2698 –2706.
49. M. Fatehkia, B. Coles, F. Ofli, I. Weber, “The Relative Value
of Facebook Advertising Data for Poverty Mapping,
Proceedings of the Fourteenth International AAAI Conference
on Web and Social Media , Atlanta, Georgia, 8 to 11 June 2019,
pp. 934 –938.
50. R. Cao et al., Integrating aerial and street view images for
urban land use classification. Remote Sens. 10, 1553 –1576
(2018). doi: 10.3390/rs10101553
51. I. Tingzon et al.,“Mapping Poverty in the Philippines Using
Machine Learning, Satellite Imagery, and Crowd-sourced
Geospatial Information, ”International Conference on Machine
Learning AI for Social Good Workshop , Long Beach, California,
15 June 2019.
52. G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger,
“Densely connected convolutional networks, ”Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition , Honolulu, Hawaii, 21 to 26 July 2017,
pp. 2261 –2269.
53. D. B. Lobell et al., Eyes in the sky, boots on the ground:
Assessing satellite-and ground-based approaches to crop
yield measurement and analysis. Am. J. Agric. Econ. 102,
202–219 (2020). doi: 10.1093/ajae/aaz051
54. G. Christie, N. Fendley, J. Wilson, R. Mukherjee, Functional
Map of the World. IEEE Comput. Soc. Conf. Comput. Vis.
Pattern Recognit. 2018
, 6172 –6180 (2018).
55. B. Uzkent et al.,“Learning to interpret satellite images using
Wikipedia, ”Proceedings of the Twenty-Eighth International
Joint Conference on Artificial Intelligence , Macao,
10 to 16 August 2019, pp. 3620 –3626.
56. K. Ayush, B. Uzkent, M. Burke, D. Lobell, S. Ermon,
“Generating interpretable poverty maps using objectdetection in satellite images, ”Proceedings of the Twenty-Ninth
International Joint Conference on Artificial Intelligence ,
January 2021, pp. 4410 –4416.
5 7 . K .H e ,H .F a n ,Y .W u ,S .X i e ,R .G i r s h i c k ,M o m e n t u m
contrast for unsupervised visual representation learning.
I E E EC o m p u t .S o c .C o n f .C o m p u t .V i s .P a t t e r nR e c o g n i t .
2020 ,9 7 2 9 –9738 (2020).
58. T. Chen, S. Kornblith, M. Norouzi, G. Hinton, A simple
framework for contrastive learning of visual representations.arXiv:2002.05709 [cs.LG] (1 July 2020).
59. S. Basu et al.,“Deepsat: a learning framework for satellite
imagery, ”Proceedings of the 23rd SIGSPATIAL International
Conference on Advances in Geographic Information Systems ,
Seattle, Washington, 3 to 6 November 2015.
60. N. Jean, S. Wang, G. Azzari, D. Lobell, S. Ermon, Tile2Vec:
Unsupervised representation learning for spatially distributeddata. Proc. Conf. AAAI Artif. Intell. 33, 3967 –3974 (2019).
doi:10.1609/aaai.v33i01.33013967
61. E. Rolf et al.,“A Generalizable and Accessible Approach to
Machine Learning with Global Satellite Imagery ”(NBER
Working Paper 28045, National Bureau of EconomicResearch, 2020).
62. N. Jean, S. M. Xie, S. Ermon, Semi-supervised deep kernel
learning: Regression with unlabeled data by minimizing
predictive variance. Adv. Neural Inf. Process. Syst. 31,
5322 –5333 (2018).
63. A. Elmes et al., Accounting for training data error in machine
learning applied to Earth observations. Remote Sens. 12,
1034 –1073 (2020). doi: 10.3390/rs12061034
64. J. Krause et al. ,“The unreasonable effectiveness of
noisy data for fine-grained recognition, ”14th European
Conference on Computer Vision , Amsterdam, Netherlands,
8 to 16 October 2016.
65. D. Rolnick, A. Veit, S. Belongie, N. Shavit, Deep learning is
robust to massive label noise. arXiv:1705.10694 [cs.LG]
(26 February 2018).
66. N. Natarajan, I. S. Dhillon, P. K. Ravikumar, A. Tewari,
Learning with noisy labels. Adv. Neural Inf. Process. Syst. 26,
1196–1204 (2013).
67. M. Charikar, J. Steinhardt, G. Valiant, “Learning from
untrusted data, ”Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing , Montreal, Quebec,
Canada, 19 to 23 June 2017, pp. 47 –60.
68. A. Paliwal, M. Jain, The accuracy of self-reported crop yield
estimates and their ability to train remote sensing
algorithms. Front. Sustain. Food Syst. 4,2 5–35 (2020).
doi:10.3389/fsufs.2020.00025
69. S. Wang et al., Mapping crop types in southeast India with
smartphone crowdsourcing and deep learning. Remote Sens.
12, 2957 –2999 (2020). doi: 10.3390/rs12182957
70. P. Kaiser et al., Learning aerial image segmentation from
online maps. IEEE Trans. Geosci. Remote Sens. 55,
6054 –6068 (2017). doi: 10.1109/TGRS.2017.2719738
71. R. P. Christen, J. Anderson, “Segmentation of smallholder
households: Meeting the range of financial needs in
agricultural families, ”Focus Note 85, CGAP, Washington,
DC, April 2013.
72. W. Chivasa, O. Mutanga, C. Biradar, Application of remote
sensing in estimating maize grain yield in heterogeneous
African agricultural landscapes: A review. Int. J. Remote
Sens. 38,6 8 1 6 –6845 (2017). doi: 10.1080/
01431161.2017.1365390
73. J. E. Dobson, E. A. Bright, P. R. Coleman, R. C. Durfee,
B. A. Worley, LandScan: A global population database for
estimating populations at risk. Photogramm. Eng. Remote
Sensing 66, 849 –857 (2000).
74. F. R. Stevens, A. E. Gaughan, C. Linard, A. J. Tatem,
Disaggregating census data for population mapping using
random forests with remotely-sensed and ancillary data.
PLOS ONE 10, e0107042 (2015). doi: 10.1371/
journal.pone.0107042 ; pmid: 25689585
75. M. Schiavina, S. Freire, K. MacManus, GHS-POP R2019A -
GHS population grid multitemporal (1975-1990-2000-2015),
Dataset, European Commission, Joint Research Centre (JRC)
(2019); http://data.europa.eu/89h/0c6b9751-a71f-4062-
830b-43c9f432370f .
76. M. F. A. Bustos, O. Hall, T. Niedomysl, U. Ernstson, A pixel level
evaluation of five multitemporal global gridded population
datasets: A case study in Sweden, 1990 –2015. Popul. Environ.
42, 255 –277 (2020). doi: 10.1007/s11111-020-00360-8
77. B. Calka, E. Bielecka, GHS-POP accuracy assessment: Poland
and Portugal case study. Remote Sens. 12, 1105 –1128
(2020). doi: 10.3390/rs1207110578. Z. Bai, J. Wang, M. Wang, M. Gao, J. Sun, Accuracy
assessment of multi-source gridded population distribution
datasets in China. Sustainability 10, 1363 (2018).
doi:10.3390/su10051363
79. R. Engstrom, D. Newhouse, V. Soundararajan, Estimating
small-area population density in Sri Lanka using surveys and
Geo-spatial data. PLOS ONE 15, e0237063 (2020).
doi:10.1371/journal.pone.0237063 ; pmid: 32756580
80. Thematic Research Network on Data and Statistics, “Leaving
no one off the map: A guide for gridded population data for
sustainable development, ”(UN Sustainable Development
Solutions Network, 2020).
81. “Habitat III Issue Paper 22 –Informal Settlements, ”United
Nations Conference on Housing and Sustainable Urban
Development, Quito, Ecuador, 17 to 20 October 2016.
82. S. Leyk et al., The spatial allocation of population: A review of
large-scale gridded population data products and their
fitness for use. Earth Syst. Sci. Data 11, 1385 –1409 (2019).
doi:10.5194/essd-11-1385-2019
83. C. Kubitza, V. V. Krishna, U. Schulthess, M. Jain, Estimating
adoption and impacts of agricultural management practicesin developing countries using satellite data. A scoping
review. Agron. Sustain. Dev. 40, 16 (2020). doi: 10.1007/
s13593-020-0610-2
84. E. Strobl, R. O. Strobl, The distributional impact of large
dams: Evidence from cropland productivity in Africa.J. Dev. Econ. 96, 432 –450 (2011). doi: 10.1016
/j.jdeveco.2010.08.005
85. E. Blanc, E. Strobl, Is small better? A comparison of the
effect of large and small dams on cropland productivity in
South Africa. World Bank Econ. Rev. 28, 545 –576 (2014).
doi:10.1093/wber/lht026
86. M. Jain et al., The impact of agricultural interventions can be
doubled by using satellite data. Nat. Sustain. 2, 931 –934
(2019). doi: 10.1038/s41893-019-0396-x
87. D. Belhabib et al
., Catching industrial fishing incursions into
inshore waters of Africa from space. Fish Fish. 21, 379 –392
(2020). doi: 10.1111/faf.12436
88. M. Pinkovskiy, X. Sala-i-Martin, Lights, camera …income!
Illuminating the national accounts-household surveys debate.
Q. J. Econ. 131, 579 –631 (2016). doi: 10.1093/qje/qjw003
89. V. Henderson, T. Squires, A. Storeygard, D. Weil, The global
distribution of economic activity: Nature, history, and the
role of trade. Q. J. Econ. 133, 357 –406 (2018). doi: 10.1093/
qje/qjx030 ; pmid: 31798191
90. M. Harari, Cities in bad shape: Urban geometry in India. Am.
Econ. Rev. 110, 2377 –2421 (2020). doi: 10.1257/aer.20171673
91. S. Michalopoulos, E. Papaioannou, Pre-colonial ethnic
institutions and contemporary African development.
Econometrica 81, 113–152 (2013). doi: 10.3982/ECTA9613 ;
pmid: 25089052
92. S. Michalopoulos, E. Papaioannou, National institutions and
subnational development in Africa. Q. J. Econ. 129, 151–213
(2013). doi: 10.1093/qje/qjt029 ; pmid: 25802926
93. A. Storeygard, Farther on down the road: Transport costs,
trade and urban growth in sub-Saharan Africa. Rev. Econ.
Stud. 83, 1263 –1295 (2016). doi: 10.1093/restud/rdw020 ;
pmid: 29743731
94. M. L. Pinkovskiy, Growth discontinuities at borders. J. Econ.
Growth 22,1 4 5–192 (2017). doi: 10.1007/s10887-016-9139-2
95. S. Jayachandran et al., Cash for carbon: A randomized trial of
payments for ecosystem services to reduce deforestation.
Science 357, 267 –273 (2017). doi: 10.1126/science.aan0568 ;
pmid: 28729505
96. B. Marx, T. M. Stoker, T. Suri, There is no free house: Ethnic
patronage in a Kenyan slum. Am. Econ. J. Appl. Econ. 11,
36–70 (2019). doi: 10.1257/app.20160484
97. L. Y. Huang, “Measuring the impacts of poverty alleviation
programs with satellite imagery and deep learning ”(2020);
http://luna-yue-huang.com/assets/pdf/jmp.pdf .
98. M. E. Brown, Famine Early Warning Systems and Remote
Sensing Data (Springer Science & Business Media, 2008).
99. S. Fritz et al., A comparison of global agricultural monitoring
systems and current gaps. Agric. Syst. 168, 258 –272 (2019).
doi:10.1016/j.agsy.2018.05.010
100. Global Fishing Watch, Ocean sustainability through
transparency, data-sharing and collaboration (2020);
https://globalfishingwatch.org/wp-content/uploads/GFW-
program-2020.pdf .
101. J. Blumenstock, Machine learning can help get COVID-19 aid
to those who need it most. Nature 10.1038/d41586-020-
01393-7 (2020). doi: 10.1038/d41586-020-01393-7 ;
pmid: 32409767
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 11 of 12RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

102. D. Cossins, “Discriminating algorithms: 5 times AI showed
prejudice, ”New Scientist , 12 April 2018; www.newscientist.
com/article/2166207-discriminating-algorithms-5-times-ai-
showed-prejudice/ .
103. Data for Black Lives, https://d4bl.org .
104. The Leadership Conference on Civil and Human Rights, Civil
rights principles for the era of big data; https://civilrights.
org/2014/02/27/civil-rights-principles-era-big-data/ .
105. N. Diakopoulos et al., Principles for Accountable Algorithms
and a Social Impact Statement for Algorithms; www.fatml.
org/resources/principles-for-accountable-algorithms .
106. P. Gajane, M. Pechenizkiy, On formalizing fairness in
prediction with machine learning. arXiv:1710.03184 [cs.LG]
(28 May 2018).
107. O. Nyeko, “Tanzania drops threat of prison over
publishing independent statistics: Amendment to
statistics act a step in right direction for free expression, ”
Human Rights Watch, 3 July 2019; www.hrw.org/news/2019/07/03/tanzania-drops-threat-prison-over-
publishing-independent-statistics .
108. D. M. J. Lazer et al ., Computational social science:
Obstacles and opportunities. Science 369,
1060 –1062 (2020). doi: 10.1126/science.aaz8170 ;
pmid: 32855329
109. F. Solt, The Standardized World Income Inequality Database,
Version 8, Harvard Dataverse (2019).
110. The World Bank, World Development Indicators (2020);
http://wdi.worldbank.org .
111. M. G. Marshall, T. R. Gurr, POLITY5: Political Regime
Characteristics and Transitions, 1800-2018 (Center for
Systemic Peace, 2020); www.systemicpeace.org/inscr/
p5manualv2018.pdf .
112. A. Driscoll, burke-lab/satellite-review-public: Initial release of
data and code, Version 1, Zenodo (2021); http://doi.org/
10.5281/zenodo.4417632 .ACKNOWLEDGMENTS
W et h a n kJ .X u e ,B .L i n ,a n dZ .T a n gf o re x c e l l e n tr e s e a r c h
assistance. Funding: We thank USAID Bureau for Food Security, the
Global Innovation Fund, Darpa World Modelers program, NSF grants
1651565 and 1522054, and the Stanford King Center on Global
Development for funding. Competing interests: M.B., D.B.L., and S.E.
are co-founders of AtlasAI, a company that uses machine learning tomeasure economic outcomes in the developing world. Data and
materials availability: Data and code for replication of all results are
available at https://github.com/burke-lab/satellite-review and ( 112).
SUPPLEMENTARY MATERIALS
science.sciencemag.org/content/371/6535/eabe8628/suppl/DC1
Materials and Methods
Tables S1 to S3
References ( 113–155)
10.1126/science.abe8628
Burke et al.,Science 371, eabe8628 (2021) 19 March 2021 12 of 12RESEARCH |REVIEW
Downloaded from https://www.science.org on March 07, 2024

Seasonal Contrast:
Unsupervised Pre-Training from Uncurated Remote Sensing Data
Oscar Ma ˜nas1,2Alexandre Lacoste1Xavier Gir ´o-i-Nieto2David Vazquez1Pau Rodriguez1
1Element AI2Universitat Polit `ecnica de Catalunya
oscmansan@gmail.com, pau.rodriguez@servicenow.com
Abstract
Remote sensing and automatic earth monitoring are key
to solve global-scale challenges such as disaster prevention,
land use monitoring, or tackling climate change. Although
there exist vast amounts of remote sensing data, most of
it remains unlabeled and thus inaccessible for supervised
learning algorithms. Transfer learning approaches can re-
duce the data requirements of deep learning algorithms.
However, most of these methods are pre-trained on Ima-
geNet and their generalization to remote sensing imagery
is not guaranteed due to the domain gap. In this work, we
propose Seasonal Contrast (SeCo), an effective pipeline to
leverage unlabeled data for in-domain pre-training of re-
mote sensing representations. The SeCo pipeline is com-
posed of two parts. First, a principled procedure to
gather large-scale, unlabeled and uncurated remote sensing
datasets containing images from multiple Earth locations at
different timestamps. Second, a self-supervised algorithm
that takes advantage of time and position invariance to
learn transferable representations for remote sensing appli-
cations. We empirically show that models trained with SeCo
achieve better performance than their ImageNet pre-trained
counterparts and state-of-the-art self-supervised learning
methods on multiple downstream tasks. The datasets and
models in SeCo will be made public to facilitate transfer
learning and enable rapid progress in remote sensing ap-
plications.1
1. Introduction
Remote sensing is becoming increasingly important to
many applications, including land use monitoring [12], pre-
cision agriculture [29], disaster prevention [37], wildﬁre
detection [11], vector-borne disease surveillance [20], and
tackling climate change [33]. Combined with recent ad-
vances in deep learning and computer vision, there is enor-
1Code, datasets and pre-trained models are available at https://
github.com/ElementAI/seasonal-contrast
Figure 1. Distribution of the Seasonal Contrast (SeCo) dataset.
Each point represents a sampled location. Images are collected
around human settlements to avoid monotonous areas such as
oceans and deserts.
mous potential for monitoring global issues through the au-
tomated analysis of remote sensing and other geospatial
data streams.
Remote sensing provides a vast supply of data. The num-
ber of Earth-observing satellites is continuously growing,
with over 700 satellites currently in orbit generating ter-
abytes of imagery data every day [30]. However, many
downstream tasks of interest are constrained by a lack of
annotations, which are particularly costly to obtain since
they often require expert knowledge, or expensive ground
sensors. In recent years, a number of techniques have been
developed to mitigate the need for labeled data [24, 26, 25],
but their application to remote sensing images is largely un-
derexplored.
Furthermore, existing remote sensing datasets [38, 19,
42] are highly curated to form well-balanced and diversi-
ﬁed classes. Simply discarding the labels does not undo
this careful selection of examples, which also requires con-
siderable human effort. Our goal is to exploit the massive
amount of publicly available remote sensing data for learn-
ing good visual representations in a truly unsupervised way.
To enable this, we construct a remote sensing dataset from
Sentinel-2 [10] tiles without any human supervision, neither
for curating nor annotating the data.arXiv:2103.16607v2  [cs.CV]  3 May 2021
Another characteristic unique to remote sensing data is
satellite revisit, which describes the ability of the system
to make repeated image captures of the same point of the
Earth’s surface over time. For publicly funded satellite con-
stellations such as Sentinel [10] or Landsat [35], the revisit
time is of the order of days. This temporal dimension pro-
vides an additional source of natural variation which com-
plements the artiﬁcial augmentation of images. For in-
stance, no amount of artiﬁcial augmentation can show how
a snowy mountain summit looks like when the snow melts
down, or how the different stages of a crop change through
the seasons.
Self-supervised learning methods have recently emerged
as an effective methodology to learn from vast amounts of
unlabeled data. Contrastive methods push together repre-
sentations of images that are semantically similar (i.e. pos-
itive pairs). Since no labels are available, traditional con-
trastive learning methods that work with natural images use
different artiﬁcial augmentations of the same image (views)
as positive pairs. In the case of remote sensing images, we
propose to leverage the temporal information to obtain pairs
of images from the same location at different points in time,
which we call seasonal positive pairs . We argue that sea-
sonal changes provide more semantically meaningful con-
tent than artiﬁcial transformations, and remote sensing im-
ages provide this natural augmentation for free.
We propose Seasonal Contrast (SeCo), a novel method-
ology for pre-training rich, transferable representations for
remote sensing applications. SeCo consists of two parts,
an unsupervised data acquisition procedure and a self-
supervised learning model to learn from the acquired data.
The self-supervised learning model is designed based on
the observation that encouraging the representation to be
invariant to seasonal changes is a strong inductive bias.
This property can be beneﬁcial for certain downstream tasks
where the prediction will not change with seasonal varia-
tions (e.g. land-cover classiﬁcation, agricultural pattern seg-
mentation, building detection), but harmful for downstream
tasks where seasonal variations are important (e.g. defor-
estation tracking, change detection). We would like to learn
good representations of remote sensing images that are ag-
nostic to the downstream tasks where they could be applied.
To leverage temporal information without limiting the
visual representations to be always invariant to time, we use
the idea of multiple embedding sub-spaces [47]. Instead of
mapping an image to a single embedding space which is in-
variant to all augmentations, we construct separate embed-
ding sub-spaces and optimize them to be variant or invariant
to seasonal changes. We use a multi-head architecture with
a shared backbone which produces a common representa-
tion that encodes the different variances and invariances.
Once the model is trained, this representation can be ap-
plied to a wide range of remote sensing downstream tasks,where the model can selectively utilize the different factors
of variation captured in the representation.
We evaluate SeCo on several remote sensing datasets and
tasks. Our experiments on land-cover classiﬁcation with
BigEarthNet [38] and EuroSAT [19], and change detection
with OSCD [8] demonstrate that SeCo pre-training is more
effective for remote sensing tasks than the common Ima-
geNet [36] and MoCo [18] pre-training.
In summary, our contributions are:
• We describe a general method for collecting uncurated
and unlabeled datasets of remote sensing images. We
use this method to construct a remote sensing dataset
from Sentinel-2 tiles without any human supervision.
• We combine recent contrastive self-supervised learn-
ing methods with the temporal information provided
by satellites to learn good visual representations which
are simultaneously variant and invariant to seasonal
changes.
• We obtain state-of-the-art results on BigEarthNet and
EuroSAT land-cover classiﬁcation, and on OSCD
change detection.
2. Background
Self-supervised learning is the branch of unsupervised
learning where the data itself provides the supervision. The
main idea is to occlude or perturb part of the data and task
the network with predicting it from the visible data. This de-
ﬁnes a pretext task (or proxy loss) and the network is forced
to learn what we care about the data (e.g. a semantic rep-
resentation) in order to solve it. A variety of pretext tasks
have been proposed for images, such as predicting the rel-
ative position of patches [9], solving jigsaw puzzles [31],
predicting rotations [13] or colorization [48].
More recently, contrastive pretext tasks [46, 32, 41, 18,
28, 5, 16, 4] have dominated the subﬁeld of self-supervised
learning, demonstrating superior performance in various
downstream tasks. Intuitively, contrastive learning meth-
ods pull together the representations of similar examples
while pushing apart the representations of dissimilar exam-
ples. Since the examples are not labeled, these methods
make the assumption that each example deﬁnes and belongs
to its own class. Hence, positive pairs are generated by ap-
plying random augmentations to the same example, while
negative pairs come from other instances in the dataset.
Formally, this task can be formulated as a dictionary
look-up problem, where a given example xis augmented
into two views, query xqand keyxk, an encoder network f
maps the examples into an embedding space, and the rep-
resentation of the query q=f(xq)should be closer to
the representation of its designated key k+=f(xk)than
to the representation of any negative key k−coming from
a set of randomly sampled instances different from x. To
this end, a contrastive objective is optimized over a batch of
positive/negative pairs. A common choice is the InfoNCE
loss [32]:
L=−logexp(q·k+/τ)
exp(q·k+/τ) +∑
k−exp(q·k−/τ)(1)
whereτis a temperature hyper-parameter scaling the
distribution of distances.
3. Method
We propose a methodology for pre-training rich, trans-
ferable representations for remote sensing imagery, consist-
ing of a general procedure for collecting an unsupervised
pre-training dataset (Section 3.1) and a self-supervised
learning method (Section 3.2) for leveraging this data.
3.1. Unsupervised Dataset Collection
Remote sensing provides a vast amount of imagery data,
but annotations are usually scarce, and domain expertise or
ground sensors are often required [21]. In order to train on
a large amount of satellite images, we collect a new dataset
of Sentinel-2 [10] patches without any human supervision.
The Sentinel-2 imagery consists of 12 spectral bands (in-
cluding RGB and NIR) at 10 m, 20 m and 60 m resolution,
with a revisit time of around 5 days. We use Google Earth
Engine [15] to process and download image patches from
about 200K locations around the world, where each patch
covers a region of roughly 2.65×2.65km. At each location,
we download 5 images from different dates separated by ap-
proximately 3 months, which capture the seasonal changes
that occurred in the region over a year. To avoid getting im-
ages from the same periods of the year, at each location we
jitter the dates for up to a year. We also ﬁlter out Sentinel-2
tiles with a cloud percentage higher than 10%. In total, we
obtain about 1 million multi-spectral image patches, which
amount to a total of over 387 billion pixels.
Sampling Strategy Our objective is to learn an encoder
that can be used on a wide variety of downstream tasks.
To this end, we need to sample from a wide variety of re-
gions on the Earth. Uniform sampling would lead to a large
amount of redundancy in the types of images. For exam-
ple, oceans cover 71% of the planet, forests cover 31% of
land, and deserts cover 33% of land. To work around this,
we make the assumption that most of the variability can be
observed in the greater areas around cities. The cities them-
selves contain a wide range of constructions, a few kilome-
ters away from cities we often observe a variety of crops and
industrial facilities. Finally, in the range of 50 km-100 km
away from cities, we usually observe natural environments.
Hence, we sample around cities following this heuristic (see
results in Figure 1):1. Sample uniformly among the 10k most populated
cities, and then sample a set of coordinates from the
Gaussian distribution spanning a standard deviation of
50 km around the center of the city.
2. Randomly select a reference date over the past year.
Add periodic increments of 3 months to obtain the
sampling dates.
3. For a 15-day range around each date, check if there ex-
ists a Sentinel-2 tile with less than 10% of cloud cov-
erage that intersects with the coordinates.
4. If there exists a valid Sentinel-2 tile for this location on
all dates, process and download all the image patches.
Otherwise, go to step 1.
We do not perform any additional data cleaning to en-
sure that the obtained images are diverse, informative and
free of clouds. Because our dataset is constructed automati-
cally, we can easily gather more data (more locations, more
dates per location). In this work, however, we limit the scale
to a total of 1M images to make it more comparable to Im-
ageNet [36].
3.2. Seasonal Contrast
Given an unsupervised dataset of remote sensing images
with temporal information, we learn a representation that
takes advantage of the structure of the data. We get inspira-
tion from [47] to develop a multi-augmentation contrastive
learning method. This approach can selectively prevent in-
formation loss incurred by artiﬁcial augmentations, and ex-
tend it with natural augmentations provided by the seasonal
changes on remote sensing images. Instead of projecting
every view to a common embedding space which is invari-
ant to all augmentations, a common representation is pro-
jected into several embedding sub-spaces which are vari-
ant or invariant to time (see Figure 2). Hence, the shared
representation will contain both time-varying and invariant
features, which will transfer efﬁciently to remote sensing
downstream regardless of whether they involve temporal
variation.
3.2.1 Views Generation
Given a reference image (query), we produce multiple pos-
itive pairs (keys) with seasonal and artiﬁcial augmenta-
tions. LetTbe a set of commonly used artiﬁcial augmen-
tations [18], such as random cropping, color jittering, and
random ﬂipping. We ﬁrst obtain 3 images from the same
location at different times, xt0,xt1andxt2, which are ran-
domly selected among all the available ones for that loca-
tion. No additional transformations are applied to the query
image, i.e.xq=xt0. Hence,xt1andxt2can be considered
seasonal augmentations (or temporal views) of xq. The ﬁrst
key view contains both seasonal and artiﬁcial transforma-
tions,xk0=T(xt1), the second key view contains only
q
k0
k1
k2h0
 f
f
f
fh1
h2shared weights
positives
negativesT0
T1
T2Z0
Z1
Z2Figure 2. Diagram of the Seasonal Contrast method. A query image ( q) is augmented with temporal ( k0, k1) and synthetic ( k0, k2)
transformations T. Image embeddings produced by the encoder fare projected into three different sub-spaces by heads h0, h1, h2. Green
boxes represent positive pairs while red boxes represent negative pairs (i.e. including images from other locations). Sub-space Z0is
invariant to all transformations, thus all keys belong to the same class as the query. Z1is invariant to seasonal augmentations, while Z2is
invariant to synthetic augmentations.
seasonal augmentations, xk1=xt2, and the third view con-
tains only artiﬁcial augmentations, xk2=T(xt0).
3.2.2 Multiple Embedding Sub-spaces
The query and key views are encoded by a neural network f
into representations vq,vk0,vk1,vk2in a common embed-
ding spaceV∈Rd. Next, each intermediate representation
is projected into 3 different sub-spaces Z0,Z1,Z2∈Rd′by
non-linear projection heads h0,h1,h2, wherehi:V↦→Z i.
Following recent literature [44], the embedding sub-spaces
arel2-normalized, effectively restricting them to the unit
hypersphere.
The embedding sub-space Z0is invariant to all augmen-
tations,Z1is invariant to seasonal augmentations but vari-
ant to artiﬁcial augmentations, and Z2is invariant to arti-
ﬁcial augmentations but variant to seasonal augmentations.
Namely, inZ0all embeddings zi
0should be pulled together,
inZ1onlyzq
1andzk1
1should be pulled together and pushed
apart fromzk0
1andzk2
1, and inZ2onlyzq
2andzk2
2should
be pulled together and pushed apart from zk0
2andzk1
2. This
is represented visually in Figure 2.
A contrastive learning objective is optimized on each
embedding sub-space based on Equation 1, where the def-
inition of positive (and negative) pairs depends on the in-
variances (and variances) that are encoded. In Z0, the pos-
itive pair for the query zq
0iszk0
0, and the negative pairs areembeddings of other instances in this embedding sub-space.
For embedding sub-space Z1, the positive pair for the query
zq
1iszk1
1, while the negative pairs are embeddings of other
instances in this embedding sub-space, plus zk0
1andzk2
1.
Note thatzk0
1andzk2
1are harder negative pairs for zq
1as
they come from the same instance but have a different arti-
ﬁcial augmentation. Positive and negative pairs in embed-
ding spaceZ2are analogous toZ1.
The ﬁnal learning objective is the sum of all the embed-
ding sub-space losses. The encoder fmust preserve time-
varying and invariant information in the general embedding
spaceVin order to optimize the combined contrastive learn-
ing objectives of all normalized embedding sub-spaces Zi.
Note that the original contrastive learning objective [32] is
a particular case of multi-augmentation contrastive learning
when only the embedding sub-space Z0is used.
The representation for transfer learning is taken from the
general embedding space V, since we do not assume any a
priori knowledge about the downstream tasks. In case the
right invariances for downstream tasks were known, the rep-
resentation could be extracted from a particular embedding
sub-spaceZi.
4. Experiments
In this study, we evaluate the learned representations on
three downstream tasks: two land-cover classiﬁcation tasks,
Pre-training Backbone100k images 1M images
Linear probing Fine-tuning Linear probing Fine-tuning
10% 100% 10% 100% 10% 100% 10% 100%
Random init.ResNet-1843.05 45.95 68.11 79.80 43.05 45.95 68.11 79.80
ImageNet (sup.) 65.69 66.40 78.76 85.90 65.69 66.40 78.76 85.90
MoCo-v2
ResNet-1869.70 70.90 78.76 85.17 69.28 70.79 78.33 85.23
MoCo-v2+TP 70.20 71.08 79.80 85.71 72.58 73.60 80.68 86.59
SeCo (ours) 74.67 75.52 81.49 87.04 76.05 77.00 81.86 87.27
Random init.ResNet-5043.95 46.92 69.49 78.98 43.95 46.92 69.49 78.98
ImageNet (sup.) 70.46 71.82 80.04 86.74 70.46 71.82 80.04 86.74
MoCo-v2
ResNet-5071.85 73.27 79.23 85.79 73.71 75.65 80.08 86.05
MoCo-v2+TP 72.61 73.91 79.04 85.35 74.50 76.32 80.20 86.11
SeCo (ours) 77.49 79.13 81.72 87.12 78.56 80.35 82.62 87.81
Table 1. Mean average precision on the BigEarthNet land-cover classiﬁcation task. Results cover different pre-training approaches and
different ResNet backbones. We also explore the effect of the unlabeled pre-training set size between 100k and 1M images, and the size of
the BigEarthNet training set between 10% and 100%.
where the representation should be invariant to seasonal
changes, and a change detection task, where the representa-
tion should be variant to seasonal changes.
Pre-training Implementation Details We adopt Mo-
mentum Contrast (MoCo-v2) [6] as the backbone for our
method due to its combination of state-of-the-art perfor-
mance and memory efﬁciency. We apply the same artiﬁ-
cial augmentations as MoCo-v2, i.e. color jittering, random
grayscale, Gaussian blur, horizontal ﬂipping, and random-
resized cropping. We use a ResNet [17] architecture as the
feature extractor, and a 2-layer MLP head with a ReLU
activation and 128-dimensional output for each embedding
sub-space. We also use separate queues [18] for each em-
bedding sub-space, containing 16,384 negative embeddings
at a time. We pre-train the network for 200 epochs with a
batch size of 256. We use an SGD optimizer with a mo-
mentum of 0.9 and a weight decay of 1e-4. We set an initial
learning rate of 0.03 and divide it by 10 at 60% and 80% of
the epochs. A temperature scaling τof 0.07 is used in the
contrastive loss. Although the collected dataset contains up
to 12 spectral bands, in this work we focus on the RGB
channels since it is a more general modality.
Methods We compare our unsupervised learning ap-
proach against several baselines, including random ini-
tialization, ImageNet supervised pre-training, and self-
supervised pre-training. For the latter, we provide results
for MoCo-v2 pre-training on our unsupervised dataset with-
out exploiting the temporal information. In this case, the
length of the dataset depends on the total number of images
and not the number of geographical locations, so we divide
the number of pre-training epochs by the number of images
per location. We also provide results for MoCo-v2 pre-
training on our dataset leveraging the temporal information
for generating positive pairs (MoCo-v2+TP), i.e. positiveimage pairs come from the same location at different times,
and MoCo-v2 artiﬁcial augmentations are then applied to
the spatially aligned image pairs (similar to Ayush et al.
[1]). We evaluate all methods with linear probing (freezing
the encoder and training only the classiﬁer) and ﬁne-tuning
(updating the parameters of both the encoder and the clas-
siﬁer).
4.1. Land-Cover Classiﬁcation on BigEarthNet
BigEarthNet [38] is a challenging large-scale multi-
spectral dataset of Sentinel-2 [10] images, captured with
similar sensors to the ones in our unsupervised dataset, i.e.
12 frequency channels (including RGB) are provided. It
consists of 125 Sentinel-2 tiles acquired between June 2017
and May 2018 over 10 European countries, which are di-
vided into 590,326 non-overlapping image patches, each
covering an area of 1.2×1.2km with resolutions of 10 m,
20 m, and 60 m per pixel. We discard about 12% of the
patches which are fully covered by seasonal snow, clouds
or cloud shadows. This is a multi-label dataset where each
image is annotated by multiple land-cover classes, so we
measure the downstream performance in terms of mean av-
erage precision (mAP). We adopt the new class nomencla-
ture introduced in [39], and we use the same train/val splits
proposed in [30].
Implementation Details We evaluate the learned repre-
sentations by training a linear classiﬁcation layer with su-
pervised learning. We initialize the ResNet backbone with a
pre-trained representation and add a single fully-connected
layer which maps from the intermediate representation to
class logits. We ﬁne-tune the network for 100 epochs with a
batch size of 1024, and report the best validation results for
each run. We use an Adam optimizer with default hyper-
parameters. For linear probing, we set the initial learning
rate to 1e-3; for full ﬁne-tuning, we set the initial learning
1
2
5
10
20
50
100
Percentage of labeled data
40
50
60
70mean Average Precision
Linear probing
1
2
5
10
20
50
100
Percentage of labeled data
60
70
80mean Average Precision
Fine-tuning
Random init. ImageNet MoCo-v2 MoCo-v2+TP SeCoFigure 3. Label-efﬁcient land-cover classiﬁcation on BigEarthNet.
We use a ResNet-18 backbone pre-trained on 1M images.
rate to 1e-5. During training, the learning rate is divided by
10 at 60% and 80% of the epochs.
Quantitative Results Table 1 compares the accuracy of
SeCo pre-training on BigEarthNet with other pre-training
methods. The comparison is done by linear probing or ﬁne-
tuning with different backbones, number of pre-training im-
ages, and percentage of BigEarthNet labeled data available.
For linear probing, we observe that SeCo consistently out-
performs MoCo-v2+TP. We also observe that temporal pos-
itives (TP) improve the performance of MoCo-v2 by a nar-
row margin. Moreover, we ﬁnd that SeCo features signif-
icantly improve over ImageNet pre-trained features, which
conﬁrms our hypothesis that there is a gap between remote
sensing and natural image domains. We also ﬁnd that this
gap decreases when ﬁne-tuning an ImageNet pre-trained
feature extractor on the whole BigEarthNet training set.
Nonetheless, with 1M images and a ResNet-50 backbone,
SeCo features achieve 1.1%higher accuracy than ImageNet
features. To the best of our knowledge, this is the ﬁrst time
an unsupervised method obtains higher accuracy than Ima-
geNet pre-training on BigEarthNet with 100% of the labels.
Regarding the backbone size, we observe a wider perfor-
mance gap between ResNet-18 and ResNet-50 when linear
probing than when ﬁne-tuning the whole network. We also
ﬁnd that pre-training with 1M images yields better perfor-
mance regardless of the backbone used. In all cases, we ﬁnd
that SeCo is more efﬁcient than the baselines when only us-
ing10% of BigEarthNet’s labeled data; we provide more
details in the next section.
Study on Label-Efﬁcient Transfer Learning Figure 3
shows the linear probing and ﬁne-tuning performance of
SeCo and the different baselines for different percentages
of labeled data on BigEarthNet. For linear probing, we ob-
serve that, with only 1%of the BigEarthNet labels, SeCo
outperforms ImageNet pre-training with 100% of the labels
and matches MoCo-v2 with 20% of the labels. We also ob-
serve that the gap between ImageNet pre-training and self-SamplingLinear probing Fine-tuning
10% 100% 10% 100%
Gaussian 74.67 75.52 81.49 87.04
Uniform 71.63 72.59 79.65 85.75
Table 2. Comparison of SeCo dataset sampling strategies. We use
a ResNet-18 backbone pre-trained on 100k images.
supervised learning increases with the amount of labeled
data, while the gap between self-supervised methods does
not change signiﬁcantly. For all percentages of labeled data,
SeCo achieves a constant ∼4%improvement gap with re-
spect to MoCo-v2+TP. When ﬁne-tuning, the performance
gap between self-supervised methods and ImageNet nar-
rows down when increasing the percentage of labeled data.
Nevertheless, SeCo is more label-efﬁcient than all the base-
lines, matching the performance of ImageNet pre-training
using all the available labels with only 50% of the labels.
Ablation on the Locations Sampling Strategy In order
to evaluate the effectiveness of our sampling strategy to
collect uncurated images for pre-training remote sensing
representations, we download an alternative version of the
SeCo dataset where the Earth locations are sampled uni-
formly from within the continents. We download 100k im-
ages following this approach and pre-train a ResNet-18 with
the SeCo method. Table 2 compares transfer learning per-
formance on the BigEarthNet downstream task when using
each sampling scheme. We observe that a SeCo representa-
tion pre-trained on images sampled from a mixture of Gaus-
sians centered around human settlements (see section 3.1)
provides better downstream performance than sampling the
images uniformly. We argue this is because populated re-
gions tend to be more diverse due to human activity and
thus collected images contain more information for learn-
ing good representations.
4.2. Land-Cover Classiﬁcation on EuroSAT
EuroSAT [19] also addresses the challenge of land use
and land cover classiﬁcation using Sentinel-2 satellite im-
ages. The images correspond to 34 European countries, and
they consist of 10 classes corresponding to different land
uses. Each of the classes is composed of 2,000 to 3,000 im-
ages, making a total of 27,000 labeled images. The images
size is 64×64pixels, covering an area of 640×640m.
All 13 Sentinel-2 spectral bands are included. We adopt the
same train/val splits proposed in [30].
Implementation Details On this task, we also evaluate
the learned representations by learning a linear classiﬁer
with supervised learning. We initialize a ResNet-18 back-
bone with a pre-trained representation and add a single
fully-connected layer on top. In this case, we initialize
the backbone with representations pre-trained on 1M satel-
lite images (except when using random weights or loading
Pre-training Accuracy
Random init. 63.21
Imagenet (sup.) 86.44
MoCo-v2 83.72
MoCo-v2+TP 89.51
SeCo (ours) 93.14
Table 3. Fine-tuning accuracy on the EuroSAT land-cover classi-
ﬁcation task. We use a ResNet-18 backbone pre-trained on 1M
images.
an ImageNet pre-trained model). We freeze the backbone
weights and train the classiﬁer for 100 epochs with a batch
size of 32, reporting the best validation accuracy for each
run. We use an Adam optimizer with default hyperparam-
eters, setting the initial learning rate to 1e-3 and dividing it
by 10 at 60% and 80% of the epochs.
Quantitative Results Table 3 compares the linear prob-
ing accuracy of SeCo representations against the different
baselines. We see that SeCo achieves 6.7%higher accuracy
than ImageNet pre-training and 3.6%higher accuracy than
MoCo-v2+TP. These results conﬁrm that the learned repre-
sentation is not only effective on BigEarthNet, but also gen-
eralizes to other remote sensing datasets such as EuroSAT.
4.3. Change Detection on Onera Satellite
The Onera Satellite Change Detection (OSCD)
dataset [8] is composed of 24 pairs of multispectral images
from Sentinel-2. The images were recorded between
2015 and 2018 from locations all over the world with
various levels of urbanization, where urban changes were
visible. Each location contains aligned pairs covering all
13 Sentinel-2 spectral bands. Images vary in spatial reso-
lution between 10 m, 20 m and 60 m, with approximately
600×600pixels at 10 m resolution. The goal is to detect
changes between satellite images from different dates.
Pixel-level change ground truth is provided for all training
and validation image pairs. We use the same train/val splits
proposed by Daudt et al. [8]: 14 images for training and
10 images for validation. We measure the the downstream
performance in terms of F1 score, as it is common in the
image segmentation literature.
Implementation Details For every pair of images from a
given location at two different timestamps, we produce seg-
mentation masks by following a procedure similar to Daudt
et al. [7]. First, a ResNet-18 backbone extracts features
from each image. We keep the features after each down-
sampling operation in the backbone network. Then, we
compute the absolute value of the difference between the
two sets of features in each pair, and use the feature differ-
ences as input to a U-Net [34] in order to generate binary
segmentation masks. The backbone network is initialized
with representations pre-trained on 1M satellite images. ToPre-training Precision Recall F1
Random init. 70.53 19.17 29.44
Imagenet (sup.) 70.42 25.12 36.20
MoCo-v2 64.49 30.94 40.71
MoCo-v2+TP 69.14 29.66 40.12
SeCo (ours) 65.47 38.06 46.94
Table 4. Fine-tuning results on the Onera Satellite change detec-
tion task. We use a ResNet-18 pre-trained on 1M images.
avoid overﬁtting, we freeze the backbone and only train the
weights of the U-Net, add a 0.3dropout rate after each
upsampling layer in the U-Net, and augment the training
images with random horizontal ﬂips and 90◦rotations. In
addition, since the images in the OSCD dataset have vari-
able size, we split them into non-overlapping patches of
96×96pixels. We train the decoder for 100 epochs with a
batch size of 32, and report results on the validation set from
the point of view of the ”change” class. We use an Adam
optimizer with a weight decay of 1e-4. We set the initial
learning rate to 1e-3 and decrease it exponentially with a
multiplicative factor of 0.95 at each epoch.
Quantitative Results Table 4 compares SeCo with ran-
dom initialization, ImageNet pre-training, MoCO-v2, and
MoCo-v2+TP. We observe that SeCo initialization achieves
higher recall and F1 score than all the baselines. In par-
ticular, SeCo outperforms MoCo-v2+TP by 6.8% F1 score.
This might be due to MoCo-v2+TP representations being
invariant to temporal variations, which is not a desirable
property in a change detection task. Interestingly, although
both SeCo and MoCo-v2 consider image patches from the
same location at different timestamps as negative pairs (i.e.
their learned representations are variant to time), SeCo at-
tains a 6.2% higher F1 score. This indicates that the multi-
ple embedding sub-spaces make SeCo more effective at de-
tecting temporal changes by disentangling image augmen-
tations from temporal variations.
Qualitative Results Figure 4 compares the change detec-
tion masks produced by our method and all the baselines
on two samples from the OSCD validation set. We ob-
serve that SeCo pre-training produces higher quality masks
which cover more of the changed pixels without excessive
false negatives. We also notice some discrepancies in the
performance of MoCo-v2 with and without leveraging tem-
poral information (TP). We hypothesize these might be due
to the different treatment of temporal invariance by each ap-
proach, and the image differences resembling more artiﬁcial
augmentations or temporal changes. SeCo overcomes this
problem by learning a representation that preserves time-
varying and invariant factors.
Random init. 
(F1 = 26.62)
ImageNet
(F1 = 45.81)
MoCo-v2
(F1 = 79.00)
MoCo-v2+TP
(F1 = 61.45)
SeCo
(F1 = 84.25)
Random init.
(F1 = 1.58)
ImageNet
(F1 = 19.71)
MoCo-v2
(F1 = 2.62)
MoCo-v2+TP
(F1 = 34.69)
SeCo
(F1 = 61.89)Image 1 Image 2 Ground truth
Image 1 Image 2 Ground truthFigure 4. Comparison of qualitative results on the Onera Satellite change detection task. Each row contains the input images, the ground
truth mask, and the generated change detection masks for a validation sample.
5. Related Work
Learning from Uncurated Data Recent efforts in unsu-
pervised feature learning have focused on either small or
highly curated datasets like ImageNet, whereas using uncu-
rated raw datasets was found to decrease the feature quality
when evaluated on a transfer task [9, 2]. Caron et al. [3]
propose a self-supervised approach which leverages clus-
tering to improve the performance of unsupervised meth-
ods trained on uncurated data. Other methods use metadata
such as hashtags [23, 40, 27], geolocation [45] or the video
structure [14] as a source of noisy supervision. In our work,
we leverage the geographical and temporal information of
remote sensing data to learn unsupervised representations
from uncurated datasets.
Multi-augmentation Contrastive Learning Recent self-
supervised contrastive learning methods have been able
to produce impressive transferable visual representations
by learning to be invariant to different image augmenta-
tions. However, these methods implicitly assume a par-
ticular set of representational invariances, and can per-
form poorly when a downstream task violates this assump-
tion. Xiao et al. [47] propose Leave-one-out Contrastive
Learning (LooC), a multi-augmentation contrastive learn-
ing framework that produces visual representations able to
capture varying and invariant factors by constructing sepa-
rate embedding spaces, each of which is invariant to all but
one augmentation. In our work, we use a similar approach
to learn representations that are variant and invariant to the
seasonal changes present in remote sensing images.
Unsupervised Learning in Remote Sensing While un-
supervised learning has been extensively studied on natural
image datasets (e.g. ImageNet), this subﬁeld remains un-
derexplored on the remote sensing domain. This is quite
surprising given the importance of remote sensing for Earth
observation, the vast amount of readily available data, and
the many opportunities for self-supervision from the uniquecharacteristics of satellite images. For instance, Jean et al.
[22] use the geographical information of images to sample
positive and negative pairs and build a pretext task based
on the triplet loss. Uzkent et al. [42] pair georeferenced
Wikipedia articles with satellite images of the correspond-
ing locations, and learn representations by predicting prop-
erties of the articles from the images. Vincenzi et al. [43]
leverage the multi-spectrality of remote sensing images to
build a colorization task, where they reconstruct the vis-
ible colors from the other spectral bands. More similar
to our work, Ayush et al. [1] also propose to exploit the
temporal information in satellite imagery to generate posi-
tive pairs and train a contrastive objective. However, their
representations are always invariant to temporal changes,
which might be detrimental for downstream tasks involv-
ing temporal variation. We overcome this problem by using
multi-augmentation contrastive learning, where the repre-
sentations preserve time-varying and invariant information.
6. Conclusions
We presented Seasonal Contrast (SeCo), a new transfer
learning pipeline for remote sensing imagery. SeCo consists
of a data collection strategy and a self-supervised learning
algorithm that leverages this data. First, we sample loca-
tions around populated regions over multiple timestamps,
which provides a diverse set of satellite images. Then, we
extend multi-augmentation contrastive learning methods to
take into account the seasonal changes and learn rich and
transferable remote sensing representations.
We compared SeCo with the common ImageNet pre-
training and MoCo pre-training on the collected data using
different backbones and dataset sizes. We found that SeCo
outperforms the considered baselines on BigEarthNet, Eu-
roSAT and OSCD tasks. Thus, we conclude that domain-
speciﬁc unsupervised pre-training is more effective for re-
mote sensing applications than pre-training with standard
datasets such as ImageNet or algorithms such as MoCo.
References
[1] K. Ayush, B. Uzkent, C. Meng, M. Burke, D. Lobell, and
S. Ermon. Geography-aware self-supervised learning. arXiv
preprint arXiv:2011.09980 , 2020. 5, 8
[2] M. Caron, P. Bojanowski, A. Joulin, and M. Douze. Deep
clustering for unsupervised learning of visual features. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV) , pages 132–149, 2018. 8
[3] M. Caron, P. Bojanowski, J. Mairal, and A. Joulin. Unsuper-
vised pre-training of image features on non-curated data. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 2959–2968, 2019. 8
[4] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski,
and A. Joulin. Unsupervised learning of visual fea-
tures by contrasting cluster assignments. arXiv preprint
arXiv:2006.09882 , 2020. 2
[5] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A sim-
ple framework for contrastive learning of visual represen-
tations. In International conference on machine learning ,
pages 1597–1607. PMLR, 2020. 2
[6] X. Chen, H. Fan, R. Girshick, and K. He. Improved base-
lines with momentum contrastive learning. arXiv preprint
arXiv:2003.04297 , 2020. 5
[7] R. C. Daudt, B. Le Saux, and A. Boulch. Fully convolutional
siamese networks for change detection. In 2018 25th IEEE
International Conference on Image Processing (ICIP) , pages
4063–4067. IEEE, 2018. 7
[8] R. C. Daudt, B. Le Saux, A. Boulch, and Y . Gousseau. Ur-
ban change detection for multispectral earth observation us-
ing convolutional neural networks. In IGARSS 2018-2018
IEEE International Geoscience and Remote Sensing Sympo-
sium , pages 2115–2118. IEEE, 2018. 2, 7
[9] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised vi-
sual representation learning by context prediction. In Pro-
ceedings of the IEEE international conference on computer
vision , pages 1422–1430, 2015. 2, 8
[10] M. Drusch, U. Del Bello, S. Carlier, O. Colin, V . Fernandez,
F. Gascon, B. Hoersch, C. Isola, P. Laberinti, P. Martimort,
et al. Sentinel-2: Esa’s optical high-resolution mission for
gmes operational services. Remote sensing of Environment ,
120:25–36, 2012. 1, 2, 3, 5
[11] F. Filipponi. Exploitation of sentinel-2 time series to map
burned areas at the national level: A case study on the 2017
italy wildﬁres. Remote Sensing , 11(6):622, 2019. 1
[12] G. M. Foody. Remote sensing of tropical forest environ-
ments: towards the monitoring of environmental resources
for sustainable development. International journal of remote
sensing , 24(20):4035–4046, 2003. 1[13] S. Gidaris, P. Singh, and N. Komodakis. Unsupervised rep-
resentation learning by predicting image rotations. arXiv
preprint arXiv:1803.07728 , 2018. 2
[14] D. Gordon, K. Ehsani, D. Fox, and A. Farhadi. Watching the
world go by: Representation learning from unlabeled videos.
arXiv preprint arXiv:2003.07990 , 2020. 8
[15] N. Gorelick, M. Hancher, M. Dixon, S. Ilyushchenko,
D. Thau, and R. Moore. Google earth engine: Planetary-
scale geospatial analysis for everyone. Remote sensing of
Environment , 202:18–27, 2017. 3
[16] J.-B. Grill, F. Strub, F. Altch ´e, C. Tallec, P. H. Richemond,
E. Buchatskaya, C. Doersch, B. A. Pires, Z. D. Guo, M. G.
Azar, et al. Bootstrap your own latent: A new approach to
self-supervised learning. arXiv preprint arXiv:2006.07733 ,
2020. 2
[17] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
770–778, 2016. 5
[18] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick. Momentum
contrast for unsupervised visual representation learning. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 9729–9738, 2020. 2, 3,
5
[19] P. Helber, B. Bischke, A. Dengel, and D. Borth. Eurosat: A
novel dataset and deep learning benchmark for land use and
land cover classiﬁcation. IEEE Journal of Selected Topics in
Applied Earth Observations and Remote Sensing , 2019. 1,
2, 6
[20] C. Ippoliti, L. Candeloro, M. Gilbert, M. Goffredo,
G. Mancini, G. Curci, S. Falasca, S. Tora, A. Di Lorenzo,
M. Quaglia, et al. Deﬁning ecological regions in italy based
on a multivariate clustering approach: A ﬁrst step towards a
targeted vector borne disease surveillance. PloS one , 14(7):
e0219072, 2019. 1
[21] N. Jean, M. Burke, M. Xie, W. M. Davis, D. B. Lobell, and
S. Ermon. Combining satellite imagery and machine learn-
ing to predict poverty. Science , 353(6301):790–794, 2016.
3
[22] N. Jean, S. Wang, A. Samar, G. Azzari, D. Lobell, and S. Er-
mon. Tile2vec: Unsupervised representation learning for
spatially distributed data. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , volume 33, pages 3967–
3974, 2019. 8
[23] A. Joulin, L. Van Der Maaten, A. Jabri, and N. Vasilache.
Learning visual features from large weakly supervised data.
InEuropean Conference on Computer Vision , pages 67–84.
Springer, 2016. 8
[24] I. Laradji, P. Rodriguez, O. Manas, K. Lensink,
M. Law, L. Kurzman, W. Parker, D. Vazquez, and
D. Nowrouzezahrai. A weakly supervised consistency-
based learning method for covid-19 segmentation in ct
images. 2021. 1
[25] I. H. Laradji, D. Vazquez, and M. Schmidt. Where are the
masks: Instance segmentation with image-level supervision.
2019. 1
[26] I. H. Laradji, R. Pardinas, P. Rodriguez, and D. Vazquez.
Looc: Localize overlapping objects with count supervision.
2020. 1
[27] D. Mahajan, R. Girshick, V . Ramanathan, K. He, M. Paluri,
Y . Li, A. Bharambe, and L. Van Der Maaten. Exploring
the limits of weakly supervised pretraining. In Proceedings
of the European Conference on Computer Vision (ECCV) ,
pages 181–196, 2018. 8
[28] I. Misra and L. v. d. Maaten. Self-supervised learning
of pretext-invariant representations. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 6707–6717, 2020. 2
[29] D. J. Mulla. Twenty ﬁve years of remote sensing in precision
agriculture: Key advances and remaining knowledge gaps.
Biosystems engineering , 114(4):358–371, 2013. 1
[30] M. Neumann, A. S. Pinto, X. Zhai, and N. Houlsby. In-
domain representation learning for remote sensing. arXiv
preprint arXiv:1911.06721 , 2019. 1, 5, 6
[31] M. Noroozi and P. Favaro. Unsupervised learning of visual
representations by solving jigsaw puzzles. In European con-
ference on computer vision , pages 69–84. Springer, 2016. 2
[32] A. v. d. Oord, Y . Li, and O. Vinyals. Representation
learning with contrastive predictive coding. arXiv preprint
arXiv:1807.03748 , 2018. 2, 3, 4
[33] D. Rolnick, P. L. Donti, L. H. Kaack, K. Kochanski,
A. Lacoste, K. Sankaran, A. S. Ross, N. Milojevic-
Dupont, N. Jaques, A. Waldman-Brown, et al. Tack-
ling climate change with machine learning. arXiv preprint
arXiv:1906.05433 , 2019. 1
[34] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convo-
lutional networks for biomedical image segmentation. In
International Conference on Medical image computing and
computer-assisted intervention , pages 234–241. Springer,
2015. 7
[35] D. P. Roy, M. A. Wulder, T. R. Loveland, C. E. Woodcock,
R. G. Allen, M. C. Anderson, D. Helder, J. R. Irons, D. M.
Johnson, R. Kennedy, et al. Landsat-8: Science and product
vision for terrestrial global change research. Remote sensing
of Environment , 145:154–172, 2014. 2
[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,
S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,
et al. Imagenet large scale visual recognition challenge
(2014). arXiv preprint arXiv:1409.0575 , 2014. 2, 3[37] G. J. Schumann, G. R. Brakenridge, A. J. Kettner, R. Kashif,
and E. Niebuhr. Assisting ﬂood disaster response with earth
observation data and products: A critical assessment. Remote
Sensing , 10(8):1230, 2018. 1
[38] G. Sumbul, M. Charfuelan, B. Demir, and V . Markl.
Bigearthnet: A large-scale benchmark archive for remote
sensing image understanding. In IGARSS 2019-2019 IEEE
International Geoscience and Remote Sensing Symposium ,
pages 5901–5904. IEEE, 2019. 1, 2, 5
[39] G. Sumbul, J. Kang, T. Kreuziger, F. Marcelino, H. Costa,
P. Benevides, M. Caetano, and B. Demir. Bigearthnet dataset
with a new class-nomenclature for remote sensing image un-
derstanding. arXiv preprint arXiv:2001.06372 , 2020. 5
[40] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting
unreasonable effectiveness of data in deep learning era. In
Proceedings of the IEEE international conference on com-
puter vision , pages 843–852, 2017. 8
[41] Y . Tian, D. Krishnan, and P. Isola. Contrastive multiview
coding. arXiv preprint arXiv:1906.05849 , 2019. 2
[42] B. Uzkent, E. Sheehan, C. Meng, Z. Tang, M. Burke,
D. Lobell, and S. Ermon. Learning to interpret satellite
images in global scale using wikipedia. arXiv preprint
arXiv:1905.02506 , 2019. 1, 8
[43] S. Vincenzi, A. Porrello, P. Buzzega, M. Cipriano, P. Fronte,
R. Cuccu, C. Ippoliti, A. Conte, and S. Calderara. The color
out of space: learning self-supervised representations for
earth observation imagery. arXiv preprint arXiv:2006.12119 ,
2020. 8
[44] T. Wang and P. Isola. Understanding contrastive representa-
tion learning through alignment and uniformity on the hyper-
sphere. In International Conference on Machine Learning ,
pages 9929–9939. PMLR, 2020. 4
[45] T. Weyand, I. Kostrikov, and J. Philbin. Planet-photo ge-
olocation with convolutional neural networks. In European
Conference on Computer Vision , pages 37–55. Springer,
2016. 8
[46] Z. Wu, Y . Xiong, S. X. Yu, and D. Lin. Unsupervised feature
learning via non-parametric instance discrimination. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 3733–3742, 2018. 2
[47] T. Xiao, X. Wang, A. A. Efros, and T. Darrell. What should
not be contrastive in contrastive learning. arXiv preprint
arXiv:2008.05659 , 2020. 2, 3, 8
[48] R. Zhang, P. Isola, and A. A. Efros. Colorful image col-
orization. In European conference on computer vision , pages
649–666. Springer, 2016. 2
1 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdataDynamic World, Near real-time 
global 10 m land use land cover 
mapping
Christopher F. Brown  1 ✉, Steven P . Brumby2, Brookie Guzder-Williams  3, Tanya Birch  1, 
Samantha Brooks Hyde2, Joseph Mazzariello2, Wanda Czerwinski1, Valerie J. Pasquarella4, 
Robert Haertel1, Simon Ilyushchenko1, Kurt Schwehr  1, Mikaela Weisse3, Fred Stolle3, 
Craig Hanson3, Oliver Guinan  1, Rebecca Moore1 & Alexander M. Tait2
Unlike satellite images, which are typically acquired and processed in near-real-time, global land 
cover products have historically been produced on an annual basis, often with substantial lag times between image processing and dataset release. We developed a new automated approach for globally 
consistent, high resolution, near real-time (NRT) land use land cover (LULC) classification leveraging 
deep learning on 10 m Sentinel-2 imagery. We utilize a highly scalable cloud-based system to apply 
this approach and provide an open, continuous feed of LULC predictions in parallel with Sentinel-2 acquisitions. This first-of-its-kind NRT product, which we collectively refer to as Dynamic World, 
accommodates a variety of user needs ranging from extremely up-to-date LULC data to custom 
global composites representing user-specified date ranges. Furthermore, the continuous nature of the product’s outputs enables refinement, extension, and even redefinition of the LULC classification. In combination, these unique attributes enable unprecedented flexibility for a diverse community of users across a variety of disciplines.
Background & Summary
Regularly updated global land use land cover (LULC) datasets provide the basis for understanding the sta -
tus, trends, and pressures of human activity on carbon cycles, biodiversity, and other natural and anthropo -
genic processes1–3. Annual maps of global LULC have been developed by many groups. These maps include 
the National Aeronautics and Space Administration (NASA) MCD12Q1 500 m resolution dataset4,5 (2001–
2018), the European Space Agency (ESA) Climate Change Initiative (CCI) 300 m dataset6 (1992–2018), and 
Copernicus Global Land Service (CGLS) Land Cover 100  m dataset7,8 (2015–2019). While widely used, many 
important LULC change processes are difficult or impossible to observe at a spatial resolution greater than 
100 m and annual temporal resolution9, such as emerging settlements and small-scale agriculture (prevalent in 
the developing world) and early stages of deforestation and wetland/grassland conversion. Inability to resolve these processes introduces significant errors in our understanding of ecological dynamics and carbon budgets. Thus, there is a critical need for spatially explicit, moderate resolution (10–30 m/pixel) LULC products that are 
updated with greater temporal frequency.
Currently, almost all moderate resolution LULC products are available with only limited spatial and/or tem
-
poral coverage (e.g., USGS NLCD10 and LCMAP11) or via proprietary and/or closed products (e.g., BaseVue12, 
GlobeLand3013, GlobeLand1014) that are generally not available to support monitoring, forecasting, and decision 
making in the public sphere. A noteworthy exception is the recent iMap 1.015 series of products available globally 
at a seasonal cadence with a 30 m resolution. Nonetheless, globally consistent, near real-time (NRT) mapping 
of LULC remains an ongoing challenge due to the tremendous computational and data storage requirements.
Simultaneous advances in large-scale cloud computing and machine learning algorithms in 
high-performance open source software frameworks (e.g., TensorFlow16) as well as increased access to satellite 
1Google, LLC, 1600 Amphitheatre Pkwy., Mountain View, CA, 94043, USA. 2National Geographic Society, 1145 17th 
St NW, Washington, DC, 20036, USA. 3World Resources Institute, 10  G St NE #800, Washington, DC, 20002, USA. 
4Department of Earth & Environment, Boston University, 685 Commonwealth Avenue, Boston, MA, 02215, USA. 
✉e-mail: cfb@google.comDATA  DeSCRIPTOROPeN

2 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/image collections through platforms such as Google Earth Engine17 have opened new opportunities to create 
global LULC datasets at higher spatial resolutions and greater temporal cadence than ever before. In this paper, 
we introduce a new NRT LULC dataset produced using a deep-learning modeling approach. Our model, which was trained using a combination of hand-annotated imagery and unsupervised methods, is used to operation
-
ally generate NRT predictions of LULC class probabilities for new and historic Sentinel-2 imagery using cloud computing on Earth Engine and Google Cloud AI Platform. These products, which we refer to collectively as Dynamic World, are available as a continuously updating Earth Engine Image Collection that enables users to leverage both class probabilities and multi-temporal results to track LULC dynamics in NRT and create custom products suited to their specific needs. We find that our model exhibits strong agreement with expert annota
-
tions for an unseen validation dataset, and though difficult to compare with existing products due to differences in temporal resolution and classification schemes, achieves better or comparable performance relative to other state-of-the-art global and regional products when compared to the same reference dataset.
Methods
Land Use Land Cover taxonomy. The classification schema or “taxonomy” for Dynamic World, shown 
in Table  1, was determined after a review of global LULC maps, including the USGS Anderson classification 
system18, ESA Land Use and Coverage Area frame Survey (LUCAS) land cover modalities19, MapBiomas classi -
fication20, and GlobeLand30 land cover types13. The Dynamic World taxonomy maintains a close semblance to 
the land use classes presented in the IPCC Good Practice Guidance (forest land, grassland, cropland, wetland, 
settlement, and other)21 to ensure easier application of the resulting data for estimating carbon stocks and green-
house gas emissions. Unlike single-pixel labels, which are usually defined in terms of percent cover thresholds, the Dynamic World taxonomy was applied using “dense” polygon-based annotations such that LULC labels are applied to areas of relatively homogenous cover types with similar colors and textures.
Training dataset collection. Our modeling approach relies on semi-supervised deep learning and requires 
spatially dense (i.e., ideally wall-to-wall) annotations. To collect a diverse set of training and evaluation data, we divided the world into three regions: the Western Hemisphere (160°W to 20°W), Eastern Hemisphere-1 (20°W to 100°E), and Eastern Hemisphere-2 (100°E to 160°W). We further divided each region by the 14 RESOLVE Ecoregions biomes
22. We collected a stratified sample of sites for each biome per region based on NASA 
MCD12Q1 land cover for 20174. Given the availability of higher-resolution LULC maps in the United States and 
Brazil, we used the NLCD 201610 and MapBiomas 201720 LULC products respectively in place of MODIS prod-
ucts for stratification in these two countries.
At each sample location, we performed an initial selection of Sentinel-2 images from 2019 scenes based on 
image cloudiness metadata reported in the Sentinel-2 tile’s QA60 band. We further filtered scenes to remove 
images with many masked pixels. We finally extracted individual tiles of 510 × 510 pixels centered on the sample 
sites from random dates in 2019. Tiles were sampled in the UTM projection of the source image and we selected one tile corresponding to a single Sentinel-2 ID number and single date.
Further steps were then taken to obtain an “as balanced as possible” training dataset with respect to the 
LULC classifications from the respective LULC products. In particular, for each Dynamic World LULC category contained within a tile, the tile was labeled to be high, medium, or low in that category. We then selected an approximately equal number of tiles with high, medium or low category labels for each category.
To achieve a large dataset of labeled Sentinel-2 scenes, we worked with two groups of annotators. The first 
group included 25 annotators with previous photo-interpretation and/or remote sensing experience. The expert group labeled approximately 4,000 image tiles (Fig.  1a), which were then used to train and measure the per
-
formance and accuracy of a second “non-expert” group of 45 additional annotators who labeled a second set of approximately 20,000 image tiles (Fig.  1b). A final validation set of 409 image tiles were held back from 
the modeling effort and used for evaluation as described in the Technical Validation section. Each image tile in the validation set was annotated by three experts and one non-expert to facilitate cross-expert and expert/non-expert QA comparisons.
All Dynamic World annotators used the Labelbox platform
23, which provides a vector drawing tool to 
mark the boundaries of feature classes directly over the Sentinel-2 tile (Fig.  2). We instructed both expert and 
non-expert annotators to use dense markup instead of single pixel labels with a minimum mapping unit of 50 × 50 m (5 × 5 pixels). For water, trees, crops, built area, bare ground, snow & ice, and cloud, this was a fairly 
straightforward procedure at the Sentinel-2 10  m resolution since these feature classes tend to appear in fairly 
homogenous agglomerations. Shrub & scrub and flooded vegetation classes proved to be more challenging as they tended not to appear as homogenous features (e.g. mix of vegetation types) and have variable appearance. Annotators used their best discretion in these situations based on the guidance provided in our training material (i.e. descriptions and examples in Table  1). In addition to the Sentinel-2 tile, annotators had access to a match
-
ing high-resolution satellite image via Google Maps and ground photography via Google Street View from the image center point. We also provided the date and center point coordinates for each annotation. All annotators were asked to label at least 70% of a tile within 20 to 60 minutes and were allowed to skip some tiles to best bal
-
ance their labeling accuracy with their efficiency.
Image preprocessing.  We prepared Sentinel-2 imagery in a number of ways to accommodate both annota-
tion and training workflows. An overview of the preprocessing workflow is shown in Fig.  3.
For training data collection, we used the Sentinel-2 Level-2A (L2A) product, which provides radiometri -
cally calibrated surface reflectance (SR) processed using the Sen2Cor software package24. This advanced level 
of processing was advantageous for annotation, as it attempts to remove inter-scene variability due to solar dis -
tance, zenith angle, and atmospheric conditions. However, systematically produced Sentinel-2 SR products are 
3 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/currently only available from 2017 onwards. Therefore, for our modeling approach, we used the Level-1C (L1C) 
product, which has been generated since the beginning of the Sentinel-2 program in 2015. The L1C product rep -
resents Top-of-Atmosphere (TOA) reflectance measurements and is not subject to a change in processing algo -
rithm in the future. We note that for any L2A image, there is a corresponding L1C image, allowing us to directly map annotations performed using L2A imagery to the L1C imagery used in model training. All bands except for B1, B8A, B9, and B10 were kept, with all bands bilinearly upsampled to 10 m for both training and inference.
In addition to our preliminary cloud filtering in training image selection, we adopted and applied a novel 
masking solution that combines several existing products and techniques. Our procedure is to first take the 
10 m Sentinel-2 Cloud Probability (S2C) product available in Earth Engine
25 and join it to our working set of 
Sentinel-2 scenes such that each image is paired with the corresponding mask. We compute a cloud mask by 
thresholding S2C using a cloud probability of 65% to identify pixels that are likely obscured by cloud cover. We then apply the Cloud Displacement Index (CDI) algorithm
26 and threshold the result to produce a second cloud Class ID LULC Type Description Examples
0 Water• Water is present in the image.
•  Contains little-to-no sparse vegetation, no rock outcrop, and no built-up features like docks.
•  Does not include land that can or has previously been covered by water.• Rivers• Ponds & Lakes• Ocean
• Flooded Salt Pans
1 Trees•  Any significant clustering of dense vegetation, 
typically with a closed or dense canopy.
•  Taller and darker than surrounding vegetation (if surrounded by other vegetation).• Wooded vegetation• Dense green shrubs• Cluster of dense, tall vegetation within savannas•  Plantations such as apples, bananas, citrus, and rubber
•  Swamp (dense/tall vegetation with no obvious 
water)
• Any mix of the above
• Any burned areas of the above
2 Grass•  Open areas covered in homogenous grasses with little to no taller vegetation.
•  Other homogenous areas of grass-like vegetation (blade-type leaves) that appear different from trees and shrubland.
•  Wild cereals and grasses with no obvious human plotting (i.e. not a structured field).•  Natural meadows and fields with sparse or no 
tree cover
• Open savanna with little to no tree cover
•  Parks, golf courses, human manicured lawns, including large fields in urban settings like soccer and baseball.
• Tree cut-throughs for power lines, gas etc.• Pastures
• Reeds and marshes with no obvious flooding
3 Flooded vegetation•  Areas of any type of vegetation with obvious 
intermixing of water.
•  Do not assume an area is flooded if flooding is 
observed in another image.
•  Seasonally flooded areas that are a mix of grass/
shrub/trees/bare ground.• Flooded mangroves
• Emergent vegetation
4 Crops • Human planted/plotted cereals, grasses, and crops.• Corn, wheat, soy, etc.• Hay and fallow plots of structured land
5 Shrub & Scrub•  Mix of small clusters of plants or individual plants 
dispersed on a landscape that shows exposed soil and rock.
•  Scrub-filled clearings within dense forests that are clearly not taller than trees. Appear grayer/browner due to less dense leaf cover.•  Moderate to sparse cover of bushes, shrubs, and tufts of grass
•  Savannas with very sparse grasses, trees, or other plants
6 Built area•  Clusters of human-made structures or individual 
very large human-made structures.
•  Contained industrial, commercial, and private 
building, and the associated parking lots.
•  A mixture of residential buildings, streets, lawns, trees, isolated residential structures or buildings surrounded by vegetative land covers.
•  Major road and rail networks outside of the 
predominant residential areas.
•  Large homogeneous impervious surfaces, including 
parking structures, large office buildings, and residential housing developments containing clusters of cul-de-sacs.•  Cluster of houses, can include smalls lawns or 
small patches of trees can be included
•  Dense villages, town, and cityscape (buildings 
and roads together)
• Clusters of paved roads and large highways• Asphalt and other human-made surfaces
7 Bare ground•  Areas of rock or soil containing very sparse to no vegetation.
•  Large areas of sand and deserts with no to little vegetation.
•  Large individual or dense networks of dirt roads.• Exposed rock• Exposed soil• Desert and sand dunes• Dry salt flats and salt pans• Dried lake bottoms• Mines
• Large empty lots in urban areas
8 Snow & Ice•  Large homogenous areas of thick snow or ice, 
typically only in mountain areas or highest latitudes.
• Large homogenous areas of snowfall.• Glaciers• Permanent snowpack• Snowfall
Table 1 . Dynamic World Land Use Land Cover (LULC) classification taxonomy. Definitions and examples 
were provided as part of annotator reference materials, along with descriptions of colors and patterns typically 
associated with each LULC type.
4 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/mask, which is intersected with the S2C mask to reduce errors of commission by removing bright non-cloud 
targets based on Sentinel-2 parallax effects. We finally intersect this sub-cirrus mask with a threshold on the Sentinel-2 cirrus band (B10) using the thresholding constants proposed for the CDI algorithm
26, and take a 
morphological opening of this as our cloudy pixel mask. This mask is computed at 20 m resolution.
In order to remove cloud shadows, we extend the cloudy pixel mask 5 km in the direction opposite the solar 
azimuthal angle using the scene level metadata “SOLAR_AZIMUTH_ANGLE” and a directional distance trans -
form (DDT) operation in Earth Engine. The final cloud and shadow mask is resampled to 100 m to decrease both 
the data volume and processing time. The resulting mask is applied to Sentinel-2 images used for training and inference such that unmasked pixels represent observations that are likely to be cloud- and shadow-free.
The distribution of Sentinel-2 reflectance values are highly compressed towards the low end of the sensor 
range, with the remainder mostly occupied by high return phenomena like snow and ice, bare ground, and 
specular reflection. To combat this imbalance, we introduce a normalization scheme that better utilizes the 
useful range of Sentinel-2 reflectance values for each band. We first log-transform the raw reflectance values to 
Fig. 1 Global distribution of annotated Sentinel-2 image tiles used for model training and periodic testing 
(neither including 409 validation tiles). (a) 4,000 tiles interpreted by a group of 25 experts (b) 20,000 tiles 
interpreted by a group of 45 non-experts. Hexagons represent approximately 58,500 km2 areas and shading 
corresponds to the count of annotated tile centroids per hexagon.
Fig. 2 Sentinel-2 tile and example reference annotation provided as part of interpreter training. This example 
was used to illustrate the Flooded vegetation class, which is distinguished by small “mottled” areas of water mixed with vegetation near a riverbed. Also note that some areas of the tile are left unlabeled.
5 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/equalize the long tail of highly reflective surfaces, then remap percentiles of the log-transformed values to points 
on a sigmoid function. The latter is done to bound on (0, 1) without truncation, and condenses the extreme end members of reflectances to a smaller range.
To account for an annotation skill differential between the non-expert and expert groups, we one-hot encode 
the labeled pixels, and smooth them according to the confidence in a binary label of the individual annota
-
tor (expert/non-expert): this is effectively linearly interpolating the distributions per-pixel from their one-hot encoding (i.e. a vector of binary variables for each class label) to uniform probability. We used 0.2 for experts, and 0.3 for non-experts (i.e. ~82% confidence on the true class for experts and ~73% confidence on the true class for the non-expert. We note that these values approximately mirror the Non-Expert to Expert Consensus agreement as discussed in the Technical Validation section). This is akin to standard label-smoothing
27,28, with 
the addition that the degree of smoothing is associated with annotation confidence.
We generate a pair of weights for each pixel in an augmented example designed to compensate for class 
imbalance across the training set and weight high-frequency spatial features at the inputs during “synthesis” (discussed further in the following section). We also include a weight per pixel designed to attenuate labels in 
the center of labeled polygons where human annotators often missed small details using a simple edge finding 
kernel.
We finally perform a series of augmentations (random rotation and random per-band contrasting) to our 
input data to improve generalizability and performance of our model. These augmentations are applied four times to each example to yield our final training dataset of examples paired with class distributions, masks, and weights (Fig.  3).
Model training. Our broad approach to transferring the supervised label data to a system that could be applied 
globally was to train a Fully Convolutional Neural Network (FCNN)29. Conceptually, this approach transforms 
pre-processed Sentinel-2 optical bands to a discrete probability distribution of the classes in our taxonomy on the 
basis of spatial context. This is done per-image with the assumption that sufficient spatial and spectral context is available to recover one of our taxonomic labels at a pixel. There are a few notable benefits to such an approach: namely that given the generalizability of modern deep neural networks, it is possible, as we will show, to produce 
a single model that achieves acceptable agreement with hand-digitized expert annotations globally. Furthermore, 
Synthesis 
weight sSynthesis 
weightsCloud & 
Shadow Mask
Kernel
AugmentationNormaliz e
LabelsLabel 
weightsFeaturesKernel
S2 L2A
(SR)
S2C
CDI
DDTS2 L1C
(TOA)
TRAINAnnotate
Fig. 3 Training inputs workflow. Annotations created using Sentinel-2 Level 2 A Surface Reflectance imagery 
are paired with masked and normalized Sentinel-2 Level 1 C Top of Atmosphere imagery, and inputs are 
augmented to create training inputs used for modeling. Cloud and shadow masking involves a three-step 
process that combines the Sentinel-2 Cloud Probability (S2C) product with the Cloud Displacement Index (CDI), which is used to correct over-masking of bright non-cloud targets” and directional distance transform 
(DDT), which is used to remove the expected path of shadows based on sun-sensor geometry.
6 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/
Fig. 4 Training protocol used to recover the labeling model. The bottom row shows the progression from a 
normalized Sentinel-2 L1C image, to class probabilities, to synthesized Sentinel-2. The dashed red and blue 
arrows show how the labeling model is optimized with respect to both the class probability and synthesis pathway, and the synthesis model is optimized only with respect to the synthesized imagery. The example image is retrieved from Earth Engine using ee.Image(‘GOOGLE/DYNAMICWORLD/V1/20190517T083601_201905
17T083604_T37UET’).
Normaliz e
ee.Model.predictImageS2C
CDI
DDT
XSentinel-2 
L1C image
Dynamic 
World 
outputCloud & 
Shadow 
mask
Fig. 5 Near-Real-Time (NRT) prediction workflow. Input imagery is normalized following the same protocol 
used in training and the trained model is applied to generate land cover predictions. Predicted results are 
masked to remove cloud and cloud shadow artifacts using Sentinel-2 cloud probabilities (S2C), the Cloud Displacement Index (CDI) and a directional distance transform (DDT), then added to the Dynamic World image collection.
7 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/since model outputs are generated from a single image and a single model, it is straightforward to scale as each 
Sentinel-2 L1C image need only be observed once.
Although applying CNN modeling, including FCNN, to recover LULC is not a new idea30–32, we intro -
duce a number of novel innovations that achieve state-of-the-art performance on LULC globally with a neu -
ral network architecture almost 100x smaller than architectures used for semantic segmentation or regression 
of ground-level camera imagery (specifically compared to U-Net33 and DeepLab v3+34 architectures). Our 
approach also leverages weak supervision by way of a synthesis pathway: this pathway includes a replica of the labeling model architecture that learns a mapping from estimated probabilities back to the input reflectances, in a way, a reverse LULC classifier that offers both multi-tasking and a constraint to overcome deficiencies in human labeling (Fig.  4).
Fig. 6 Examples of Sentinel-2 imagery (RGB) and corresponding Dynamic World NRT products for April 
2021. Location coordinates reported for image centroid. (a) Brazil, ee.Image(‘GOOGLE/DYNAMICWORLD/
V1/20210405T134209_20210405T134208_T22KCA ’) and corresponding Dynamic World labels. (b) Poland, zoomed view of ee.Image(‘GOOGLE/DYNAMICWORLD/V1/20210402T095029_20210402T095027_T34UDD’) and corresponding Dynamic World product with a hillshade on the Top-1 confidence class applied 
to the categorical labels, revealing features not normally visible with discrete valued LULC maps.
Index Band Name Description Data Type Range
0 water Estimated probability of complete coverage by water. double (0, 1)
1 trees Estimated probability of complete coverage by trees. double (0, 1)
2 grass Estimated probability of complete coverage by grass. double (0, 1)
3 flooded_vegetation Estimated probability of complete coverage by flooded vegetation. double (0, 1)
4 crops Estimated probability of complete coverage by crops. double (0, 1)
5 shrub_and_scrub Estimated probability of complete coverage by shrub and scrub. double (0, 1)
6 built Estimated probability of complete coverage by built area. double (0, 1)
7 bare Estimated probability of complete coverage by bare ground. double (0, 1)
8 snow_and_ice Estimated probability of complete coverage by snow and ice. double (0, 1)
9 label Index of the band with the highest estimated probability. unsigned byte [0, 8]
Table 2 . Bands of the images in the “GOOGLE/DYNAMICWORLD/V1” collection.
8 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/Near real-time inference. Using Earth Engine in combination with Cloud AI Platform, it is possible to 
handle enormous quantities of satellite data and apply custom image processing and classification methods using 
a simple scaling paradigm (Fig.  5). To generate our NRT products, we apply the normalization described earlier to 
the raw Sentinel-2 L1C imagery and pass all normalized bands except B1, B8A, B9 and B10 after bilinear upscal-ing to ee.Model.predictImage. This output is then masked using our cloud mask derived from the unnormalized L1C image. Creation of these images is triggered automatically when new Sentinel-2 L1C and S2C images are available. The NRT collection is continuously updated with new results. For a full Sentinel-2 tile (roughly 100 km 
x 100 km), predictions are completed on the order of 45 minutes. In total, we evaluate ~12,000 Sentinel-2 scenes 
per day, processing half on average due to a filter criteria on the CLOUDY_PIXEL_PERCENTAGE metadata of 35%. A new Dynamic World LULC image is processed approximately every 14.4 s.
Data Records
The Dynamic World NRT product is available for the full Sentinel-2 L1C collection from 2015-06-27 to pres -
ent. The revisit frequency of Sentinel-2 is between 2–5 days depending on latitude, though Dynamic World 
imagery is produced at about half this frequency (across all latitudes) given the aforementioned 35% filter on the CLOUDY_PIXEL_PERCENTAGE Sentinel-2 L1C metadata.
The NRT product is hosted as an Earth Engine Image Collection under the collection ID “GOOGLE/
DYNAMICWORLD/V1” . This is referenced in either the Earth Engine Python or JavaScript client library 
with ee.ImageCollection('GOOGLE/DYNAMICWORLD/V1') and in the Earth Engine data catalog 
at https://developers.google.com/earth-engine/datasets/catalog/GOOGLE_DYNAMICWORLD_V1
35. The 
images in this collection have names matching the individual Sentinel-2 L1C asset IDs from which they were Name Description Data Type
system:indexThe part of the path of the image in the collection following the final forward slash. 
This matches the system:index of the Sentinel-2 L1C image from which this image was derived.string
system:time_startThe average acquisition time of pixels in this image in milliseconds since the Unix epoch. 
This matches the system:time_start of the Sentinel-2 L1C image from which this image 
was derived.long integer
system:footprint A geometry bounding the image data. geometry
system:asset_size The size in bytes of the image data as stored. long integer
dynamicworld_algorithm_versionThe version string uniquely identifying the Dynamic World model and inference process 
used to produce the image.string
qa_algorithm_versionThe version string uniquely identifying the cloud masking process used to produce the 
image.string
Table 3 . Metadata of the images in the “GOOGLE/DYNAMICWORLD/V1” collection.
Three Expert StrictExpert ConsensusExpert MajoritySimple Expert Majority
Non-Expert Agreement 91.5% 77.8% 75.2% 81.4%
Table 4 . Agreement between non-experts and expert voting schemes.
Fig. 7 409 annotated Sentinel-2 tile centers in the test dataset, shown as white points overlaid on a 2019 MODIS 
NDVI composite to show global distribution of vegetated areas.
9 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/derived, e.g. a Sentinel-2 L1C image accessed in Earth Engine with ee.Image('COPERNICUS/S2/2016
0711T084022_20160711T084751_T35PKT') has a matching Dynamic World LULC product in ee.Image('GOOGLE/DYNAMICWORLD/V1/20160711T084022_20160711T084751_T35PKT') as in 
Fig. 6. Each image in the collection has bands corresponding to Table  2. Probability bands (all except the “label” 
band) sum to 1. Each image in the collection has additional metadata corresponding to Table  3.
Our 409-tile test dataset, including expert consensus annotations and corresponding Dynamic World esti
-
mated probabilities and class labels for each 5100 m ×  5100 m tile are archived in Zenodo at the following 
https://doi.org/10.5281/zenodo.476650836. The training dataset has been archived in PANGAEA in a separate 
repository: https://doi.org/10.1594/PANGAEA.93347537. The training and test data collected for Dynamic 
World are also available as Earth Engine Image Collection and can be accessed with:
ee.ImageCollection(‘projects/wri-datalab/dynamic_world/v1/DW_LABELS’).
technical Validation
We used several different approaches to characterize the quality of our NRT products. We first compared expert and non-expert annotations to establish baseline agreement across human interpreters. This is particularly rel
-
evant in understanding the quality of 20,000 training tiles that were annotated by non-experts. We then com -
pared expert reference annotations with Dynamic World products and to existing national and global products produced at an annual time step. We note that, for all comparisons with Dynamic World products, we ran the trained Dynamic World model directly on the Sentinel-2 imagery in the test tile and applied our cloud mask in order to benchmark the NRT results for the reference image date.
To create a balanced validation set, we randomly extracted ten image-markup pairs per biome per hemi
-
sphere from the existing markups: 140 from the 14 biomes in the Western Hemisphere, 130 from the 13 biomes in Eastern Hemisphere-1, and another 140 from the 14 biomes in Eastern Hemisphere-2. Each tile was inde
-
pendently labeled by three annotators from the expert group and by a member of the non-expert group such that we had four different sets of annotations for each validation tile. In total, this process produced 1636 tile annotations over 409 Sentinel-2 tiles (Fig.  7), and these tiles were excluded from training and online validation.Expert Consensus
Water Trees GrassFlooded 
Vegetation CropsShrub & ScrubBuilt AreaBare GroundSnow & Ice CloudPrecision/User’s
Non-ExpertsWater 7814103 36668 21205 58174 193344 18921 17533 68692 3688 3002 94.90%
Trees 237858 16664442 499046 301831 657282 1834363 133325 71652 18987 19774 81.50%
Grass 9921 60697 540597 55648 265055 113899 14984 18240 550 1407 50.00%
Flooded Vegetation 255910 107576 23721 879482 40311 33552 9357 23270 0 0 64.00%
Crops 13392 150401 258153 7920 12567377 223368 92983 88258 0 5141 93.70%
Shrub & Scrub 72746 1748087 1008622 519267 1656386 5143322 245498 1996929 137415 12010 41.00%
Built Area 10716 222831 40154 2028 324127 59196 6224787 35812 866 1510 89.90%
Bare Ground 22661 25522 23480 4482 268845 257830 22460 1036373 49328 4090 60.40%
Snow & Ice 2644 59334 539 687 73 21734 0 15917 1388854 11 93.20%
Cloud 3033 8988 198 264 19887 244 3740 393 3955 161082 79.80%
Recall/Producer’s 92.60% 87.30% 22.40% 48.10% 78.60% 66.70% 92.00% 30.90% 86.60% 80.20% 77.80%
Table 5 . Per-pixel confusion matrix of Non-Experts to Expert Consensus. Note that cloud is included as both 
sets of annotations include this label (n = 58,963,662).
Three Expert Strict
Water Trees GrassFlooded Vegetation CropsShrub & Scrub Built AreaBare Ground Snow/IcePrecision/User’s:
Dynamic WorldWater 5964550 1450 0 17674 51640 125 5212 7713 2 98.60%
Trees 18510 7966289 118152 97159 656797 238465 8677 668 0 87.50%
Grass 24 4641 223688 8704 478891 38111 1046 20504 0 28.80%
Flooded Veg. 1680 1600 2185 275367 36005 3283 32 79 0 86.00%
Crops 262 11326 8093 426 5316019 126244 7389 5527 0 97.10%
Shrub & Scrub 408 121316 16676 497 309762 913085 14460 30428 0 64.90%
Built Area 63 4416 521 0 73663 3385 2506552 2171 0 96.70%
Bare Ground 126219 392 103 0 171023 155783 77606 901851 0 62.90%
Snow & Ice 49541 55081 0 0 25229 466 10457 8959 537301 78.20%
Recal/Producer’sl: 96.80% 97.50% 60.60% 68.90% 74.70% 61.70% 95.30% 92.20% 100.00% 88.4%
Table 6 . Confusion matrix of Dynamic World to Three Expert Strict, i.e. valid where all three experts labeled 
and all agreed (n = 27,841,623).
10 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/Because new Dynamic World classifications are generated for each individual Sentinel-2 image and the qual -
ity of these classifications is expected to vary spatially and temporally as a function of image quality, it is difficult 
to provide design-based measures of accuracy that are representative of the full (and continuously updating) collection. Therefore, we focus instead on using the multiple annotations for each validation tile as a means to characterize both the quality and agreement of annotations themselves, as well as the ability of our NRT model to generalize to new (unseen) images at inference time.
Annotations were combined in three different ways to measure (1) agreement between expert and non-expert 
labels, (2) expert-to-expert consistency, and (3) agreement between machine labeling and multi-expert consen
-
sus under several expert voting schemes.The four voting schemes considered were Three Expert Strict agreement, where all three experts had an opinion and all three agreed on feature class; Expert Consensus, where all three experts agreed, or where two experts agreed and the third had no opinion, or where one expert had an opinion and the other two did not; Expert Majority , where at least two experts agreed on feature class, or where one 
expert had an opinion and the other two did not; Expert Simple Majority, where at least two experts agreed and 
at least two agreed on feature class.
Comparison of expert and non-expert annotations.  To assess the quality of non-expert annotations, 
which comprise the majority of our training dataset, we directly compared rasterized versions of hand-digitized 
expert and non-expert annotations for our validation sample. Though these validation images were not used as part of model training, this comparison highlights strengths and potential weaknesses of the training set. We summarize the agreement between non-experts and experts for different voting schemes in Table  4 and show the 
full confusion matrix of Non-Experts to Expert Consensus in Table  5.
Agreement for all comparisons was greater than 75%, suggesting fairly consistent labeling across differ -
ent levels of expertise. As would be expected, the Three Expert Strict set shows the highest overlap with the 
Non-Expert set (91.5%), as only the pixel labels with the highest confidence amongst expert annotators remain.
Comparison of Dynamic World predictions with expert annotations. To assess the model’s ability 
to generalize to new images, the trained Dynamic World model was applied to the 409 test tiles and the class with Expert Consensus
Water Trees GrassFlooded 
Vegetation CropsShrub & Scrub Built AreaBare Ground Snow/IcePrecision/User’s:
Dynamic WorldWater 7664249 47476 34405 160034 333689 54613 45573 112658 4178 90.60%
Trees 121205 17522174 1019096 803380 2565217 2529992 281318 120507 8921 70.20%
Grass 5956 83205 876142 149792 1343601 311448 39657 101129 695 30.10%
Flooded Veg. 51371 68818 45450 722106 120045 56370 6860 35856 6 65.20%
Crops 21083 93924 139766 35422 9841373 574660 126895 241771 38 88.90%
Shrub & Scrub 17666 628594 380724 75929 1220212 3552589 151919 440744 29373 54.70%
Built Area 10375 146794 55121 3930 610401 94431 6489015 75899 744 86.70%
Bare Ground 171029 15374 28976 8811 313838 661030 183342 2214615 42538 60.80%
Snow & Ice 68277 195648 8649 550 59474 104295 14122 122907 1417512 71.20%
Recall/Producer’s: 94.30% 93.20% 33.80% 36.80% 60.00% 44.70% 88.40% 63.90% 94.20% 73.80%
Table 7 . Confusion matrix of Dynamic World to Expert Consensus, i.e. valid where at least two experts labeled 
and all agreed in any case (n = 68,137,571).
Expert Majority
Water Trees GrassFlooded Vegetation CropsShrub & Scrub Built AreaBare Ground Snow/IcePrecision/User’s:
Dynamic WorldWater 7801513 49529 39511 187484 511166 59061 46461 197843 4178 87.70%
Trees 127510 20225220 1280463 963384 2835774 3215029 293635 150630 8956 69.50%
Grass 6465 135888 1415436 170294 1917310 405560 41812 151648 695 33.30%
Flooded Veg. 54365 73337 56852 764482 133162 72474 6878 40949 6 63.60%
Crops 23790 144438 261750 36363 10821384 707154 134486 328916 38 86.90%
Shrub & Scrub 18649 1034643 551074 99947 1476117 4498630 157166 708463 32079 52.50%
Built Area 11187 156117 57647 3935 666712 104009 6620303 82974 744 85.90%
Bare Ground 178304 16203 40275 8835 400529 1022049 198274 2722023 48658 58.70%
Snow & Ice 68319 199018 8656 550 59786 109192 14527 214993 1422556 67.80%
Recall/Producer’s: 94.10% 91.80% 38.10% 34.20% 57.50% 44.10% 88.10% 59.20% 93.70% 71.30%
Table 8 . Confusion matrix of Dynamic World to Expert Majority, i.e. valid where, amongst labels, there was 
consensus or only one expert labeled (n = 78,916,422).
11 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/the highest probability (or “Top-1” label) was compared to the four expert voting schemes. Neither the validation 
images, nor other images from the same locations were available to the model during training. Thus, this assess-ment quantifies how well the model performs when applied outside the training domain. The results of these comparisons are shown in Tables  6–9.
We considered the Expert Consensus scheme to best balance “easy” labels (where many experts would agree) 
and “hard” labels (where labels would be arguably more ambiguous) and used this as our primary performance 
metric. Overall agreement between these single-image Dynamic World model outputs and the expert labels was observed to be 73.8%. Comparing this 73.8% to the non-expert to expert agreement of 77.8% in Table  5, we 
note the similarity of the predictions to the level of agreement amongst the labels themselves. Unsurprisingly the 
model achieved the highest agreement for classes where annotators were confident (water, trees, built area, snow 
& ice) but had greater difficulty for classes where the annotators were less confident (grass, flooded vegetation, shrub & scrub, and bare ground).
Comparison of Dynamic World and other LULC datasets. As a third point of comparison, we contex-
tualize our results in terms of existing products. We qualitatively and quantitatively compared Dynamic World 
with other publicly available global and regional LULC datasets (Table  10). For each Dynamic World valida-
tion tile, we reprojected the compared dataset to the UTM zone of the tile, upsampled the data to 10 m using 
nearest-neighbor resampling, and extracted a tile matching the extent of the labeled validation tile. For regional 
LULC datasets, such as LCMAP , NLCD, and MapBiomas, we were limited to tiles located within the regional 
boundary (e.g., only 42 validation tiles are within the spatial coverage of MapBiomas). We note that in every case, some cross-walking was necessary to match the taxonomies to the Dynamic World LULC classification scheme. We show a visual comparison of Dynamic World to other products in Fig.  8.
Measured against the expert consensus of annotations for the 409 global tiles, Dynamic World exceeded 
the agreement of all other LULC datasets except for the regional product LCMAP 2017 (Table  10). For the best 
global LULC product in our comparison study (ESA CGLS ProbaV 2019), Dynamic World achieved agreement 
at a higher spatial resolution (10 m vs 100 m) and improved agreement by 7.5%. For the current best regional 
product (LCMAP 2017), Dynamic World agreed 1.2% less with our expert consensus. We note that to per -
form the LCMAP comparison, we had to reduce our number of classes by combining grass and shrub & scrub as LCMAP does not separate these classes. When combining the Dynamic World grass and shrub & scrub classes, the agreement rises slightly to 74.2%, though LCMAP agreement was only validated against 11.7% of the tiles in a regional sample, and is an annual product not NRT. Further, direct comparison to ESA datasets are Expert Simple Majority
Water Trees GrassFlooded 
Vegetation CropsShrub & Scrub Built AreaBare Ground Snow/IcePrecision/User’s:
Dynamic WorldWater 7203733 12969 9542 82175 422487 16660 15451 116578 57 91.40%
Trees 51183 15906784 596718 512895 1728587 1589558 80497 50509 492 77.50%
Grass 1066 77515 1030113 72589 1451071 213904 13709 91378 0 34.90%
Flooded Veg. 12885 15142 18924 521553 91294 33735 600 10042 0 74.10%
Crops 3830 78317 153125 12494 9204745 469898 37157 127148 0 91.30%
Shrub & Scrub 3231 663092 248003 36709 958322 2969428 64467 389238 5807 55.60%
Built Area 2150 36312 6089 68 325235 24489 4931216 22913 0 92.20%
Bare Ground 142656 2294 15237 203 338099 732591 132422 2124769 9991 60.70%
Snow & Ice 53479 128014 9 0 49884 26204 12566 134024 978892 70.80%
Recall/Producer’s: 96.40% 94.00% 49.60% 42.10% 63.20% 48.90% 93.30% 69.30% 98.40% 77.8%
Table 9 . Confusion matrix of Dynamic World to Expert Simple Majority, i.e. valid where at least one expert 
labeled and all agreed in any case (n = 57,707,212).
Dataset NRT Global Agreement Scale (m) Tiles
Mapbiomas Amazonia 2018 (N.Brazil, Venezuela, Peru, Bolivia) No No 54.8% 30 11
ESA S2GLC Europe 2019 No No 59.2% 10 45
ESA CCI 2018 No Ye s 61.6% 300 409
ESA CGLS ProbaV 2019 No Ye s 66.3% 100 409
NLCD 2016 (30 m, CONUS + Alaska) No No 66.7% 30 56
Mapbiomas Brazil 2019 No No 67.4% 30 20
LCMAP 2017 (30 m, CONUS only) No No 75.0% 30 48
Dynamic World (NRT) Ye s Ye s 73.8% 10 409
Table 10. Comparison of Dynamic World to other LULC datasets in terms of temporal frequency, global 
coverage, agreement with our Expert Consensus test dataset, scale and Sentinel-2 tiles mapped. Bold values 
indicate top qualitative performance in each comparison category.
12 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/difficult due to the resolution differences, with 300 m more spatially generalized than 10 m. It is also important 
to note that the Dynamic World comparison to the annotated validation tile is for the same image date, while 
there may be a mismatch in dates when comparing to other LULC datasets. Thus, by characterizing the relative agreement of different datasets with hand-annotated labels for a specific Sentinel-2 image, these comparisons provide important insights into the value of NRT classification for capturing fine-grained spatial and temporal 
variability in LULC.
Fig. 8 Visual comparison of Dynamic World (DW) to other global and regional LULC datasets for validation 
tile locations in (A) Brazil (−11.437°, −61.460°), (B) Norway, (61.724°, 6.484°), and (C) the United States 
(39.973°, −123.441°). Datasets used for comparison include 300 m European Space Agency (ESA) Climate 
Change Initiative (CCI); 100 m Copernicus Global Land Service (CGLS) ProbaV Land Cover dataset; 10 m ESA 
Sentinel-2 Global Land Cover (S2GLC) Europe 2019; 30 m MapBiomas Brazil dataset; and 30 m USGS National 
Land Cover Dataset (NLCD). Each map chip represents a 5.1 km by 5.1 km area with corresponding true-color 
(RGB) Sentinel-2 image shown in the first column. All products have been standardized to the same legend 
used for DW . Note differences in resolution as well as differences in the spatial distribution and coverage of land use land cover classes.
Fig. 9 Example of Dynamic World mode composite (February - September 2021), time series of class 
probabilities for single pixel (location indicated by circled white point), and select Dynamic World predictions with corresponding single-date Sentinel-2 images for temperate deciduous forest in Massachusetts, USA (centered on latitude: 42.491°, longitude: −72.275°).
13 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/
Fig. 10 Demonstration of relative weakness exhibited in Dynamic World in separating arid shrubland from 
crops. (a) An oil field in Texas, USA; (b) Agricultural mosaic in Florida, USA. High resolution image shown for 
reference. Estimated class prediction probabilities scaled from [0, 1] with red corresponding to the maximum probability of the crops class and blue corresponding to the maximum probability of the shrub & scrub class. In arid shrubland, the estimated probabilities for shrub and crops are more similar (purple) than in temperate 
or other biomes. The probabilities were averaged per-pixel over July 2021 and the reference imagery was taken 
from the Google Maps Satellite layer.
Fig. 11 Mode composite of all Dynamic World NRT products from 2021-04-01 to 2021-05-01. Areas of black 
correspond to no data over land (due to cloud cover) with white corresponding to no data over water.
14 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/Usage Notes
Extensions of the Dynamic World NRT collection offer new opportunities to create global analysis products at 
a speed, cost, and performance that is appropriate for a broad range of stakeholders, e.g. national or regional governments, civil society, and national and international research and policy organizations. It is our hope that Dynamic World and spatially consistent products like it can begin to make LULC and derived analysis globally equitable.
Time series of class probabilities.  Though we used Top-1 labels for validation and cross-dataset com-
parisons, Dynamic World includes class probabilities in addition to a single “best” label for each pixel (Table  2). 
While inclusion of class probabilities and other continuous metrics that characterize uncertainties in LULC clas-
sifications are becoming increasingly common (i.e. LCMAP cover confidence attributes11), Dynamic World is 
distinct in providing dense time series of class probabilities updated with a similar cadence to the acquisition of the source imagery itself.
Rather than provide LULC labels that are intended to represent a multi-date time period, Dynamic World 
provides single-date snapshots that reflect the highly transitional and temporally dynamic nature of cover type 
probabilities. For example, in temperate regions that experience seasonal snow cover, a mode composite of Dynamic World labels reflects dominant tree and water cover types from February through September (Fig.  9). 
However, a time series of class probabilities for a pixel in an area of deciduous forest that is classified as “Trees” 
in the mode composite and during leaf-on conditions (e.g. June 6) is also classified as Snow & Ice when the 
ground is snow-covered (February 21) and has an increased Shrub & Scrub probability during early spring before leaf-out (March 13). This example illustrates the advantages of an instantaneous and probabilistic NRT classification approach, while also highlighting the challenges of standardizing validation metrics for a dynamic LULC dataset.
Uncertainties. We find single-date Dynamic World classifications agree with the annotators nearly as well as 
the annotators agree amongst each other. The Dynamic World NRT product also achieves performance near, or 
exceeding many popular regional and global annual LULC products when compared to annotations for the same validation tiles. However, we have observed that performance varies spatially and temporally as a function of both the quality of S2 cloud masking and variability in land cover and condition.
Dynamic World tends to perform most strongly in temperate and tree-dominated biomes. Arid shrublands 
and rangelands were observed to present the greatest source of confusion specifically between crops and shrub. 
In Fig.  10, we demonstrate this phenomenon by observing that the maximum of estimated probabilities between 
crops and shrubs tends towards 0.5 in a sample of arid shrubland in Texas (seen by the low contrast purple color -
ing) even though this region does not contain cultivated land. By visual qualitative inspection, Dynamic World identifies grasslands better than the generally low agreement suggested by our Expert Consensus (30.1% for Dynamic World to 50% by non-experts, a 19.9% delta), and identifies crops more poorly than the generally high agreement suggested by our Expert consensus (88.9% by Dynamic World to 93.7% by non-experts, a 4.8% delta).
We also note that single-date classifications are highly dependent on accurate cloud and cloud shadow mask
-
ing. Though we have implemented a fairly conservative masking process that includes several existing products and algorithms, missed clouds are typically misclassified as Snow & Ice and missed shadows as Water. However, because Dynamic World predictions are directly linked to individual Sentinel-2 acquisitions, these misclas
-
sifications can be identified by inspecting source imagery and resolved through additional filtering or other post-processing.
Creating new products from the Dynamic World collection.  As a fundamentally NRT and continu-
ous product, Dynamic World allows users to constrain observed data ranges and leverage the continuous nature 
of the outputs to characterize land conditions as needed for their specific interests and tasks. For example, we do not expect the prescriptiveness of the “label” band to be appropriate for all user needs. By applying a desired threshold or more advanced decision framework to the estimated probabilities, it is possible to customize a dis-
crete classification as is appropriate for a user’s unique definitions or downstream task. Furthermore, users can 
aggregate NRT results to represent longer time periods. For example, one could create a monthly product as seen in Fig.  11 by mode-compositing the highest probability label over a one month period using a simple filterDate 
and mode in Earth Engine. It is also straightforward to generate a more traditional annual product by aggregating the estimated distributions for a given year or between the spring and autumn equinoxes to represent growing season cover only. Thus, unlike conventional map products, Dynamic World enables a greater degree of flexibility 
for users to generate custom aggregations and derivative products uniquely tailored to their needs and study 
areas.
Quantifying accuracy of derived products. Rigorous assessment of map accuracy and good practices 
in estimating areas of mapped classes require probability sampling design that supports design-based inference of population-level parameters such as overall accuracy
38. However, one of the fundamental requirements of 
design-based inference is a real, explicitly defined population, and in the case of map accuracy assessment, this population typically refers to a population of pixels included in a map and assigned different class labels
39. Given 
that Dynamic World is a continuously updating image collection that can be post-processed into any number of different map products, the construction of a design-based sample would be dependent on the specific temporal aggregations and/or reclassifications performed by end-users.
In the assessments performed as part of our Technical Validation, we focus on agreement between reference 
annotations and our Top-1 NRT labels as our primary validation metric. While these agreement assessments 
support the general quality and utility of the Dynamic World dataset from the perspective of benchmarking, 
15 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/we note that our confusion matrices are not population confusion matrices and thus cannot be used to estimate 
population parameters. These matrices also do not account for model-based estimates of uncertainty, specifically class probability bands that characterize uncertainty in model predictions. While more rigorous characterization of model uncertainty could be achieved using model-based inference techniques
38, we argue that this is less 
appropriate for products like Dynamic World that are intended to be further refined into more traditional map products that can be assessed using design-based methods.
As an example, a Dynamic World derived product was generated by simply averaging class probabilities 
and a proof-of-concept assessment was performed by the University of Maryland Global Land Analysis and 
Discovery Laboratory (UMD-GLAD) using a stratified random sampling strategy with a total of 19 strata based 
on a prototype 30 m UMD-GLAD LULC map. Fifty sampling units were randomly selected from each of the 19 
strata. Reference data for interpretation and class assignment consisted of high resolution data from the Google Maps Satellite layer viewed in Google Earth and MODIS time-series NDVI. Each interpreted sampling unit was re-labeled with one of the eight DynamicWorld classes and all results were compared to the temporally aggre
-
gated DynamicWorld product. Results generally indicated higher accuracies in terms of precision/user’s accu -
racy and recall/producer’s accuracy for relatively stable LULC classes such as water and trees. However, mixed classes such as built area and shrub & scrub and classes such as bare ground, crop, grass, and flooded vegetation that represent transient states or exhibit greater temporal dynamics tended to show much lower accuracies. Some of these lower levels of agreement also reflect potential mismatches in class definitions that arise from the NRT nature of the Dynamic World classes, i.e. “Flooded vegetation” may characterize an ephemeral state that is 
different from a more traditional “wetland” categorization.
While this example provides one possible derived product and assessment useful for demonstration pur
-
poses, we intentionally do not provide a standard derivative map product of the Dynamic World dataset and 
instead encourage users, as is standard practice, to develop assessments of their unique derivative map products using tools such as Collect Earth
40 designed for reference data collection and community standard guidance41–43. 
Reference sample design should reflect user-specified temporal aggregation (i.e., monthly, annual, multi-year) as well as any post-classification modifications to the original Dynamic World legend. There may also be interest
-
ing opportunities to compare Dynamic World NRT and derived products with existing reference samples (e.g., LCMAP), in which case accuracy results and area estimates should be computed using estimators that account for differences between the map used for sample stratification and the Dynamic World product being assessed.
Code availability
The Dynamic World NRT dataset has been made available as an Earth Engine Image Collection under 
“GOOGLE/DYNAMICWORLD/V1” . This is referenced in either the Earth Engine Python or JavaScript client library with: ee.ImageCollection(‘GOOGLE/DYNAMICWORLD/V1’).
We provide a public web interface for rapid exploration of the dataset at: https://sites.google.com/view/
dynamic-world/home .
We also provide an example of accessing Dynamic World using the Earth Engine Code Editor in the following 
code snippet: https://code.earthengine.google.com/710e2ae9d03cd994c6e8dc9213257cbc.
The Dynamic World model has been run for historic Sentinel-2 imagery and is being run for newly acquired 
Sentinel-2 imagery; users are therefore encouraged to work with outputs available in the NRT Image Collection available on Earth Engine. Nonetheless, to ensure reproducibility, we have archived the trained model, exam-ple code for running inference, and additional information on the model architecture in Zenodo at https://doi.
org/10.5281/zenodo.5602141
44.
Received: 15 July 2021; Accepted: 4 April 2022;
Published: xx xx xxxx
References
 1. Feddema, J. J. The Importance of Land-Cover Change in Simulating Future Climates. Science  310, 1674–1678 (2005).
 2. Sterling, S. M., Ducharne, A. & Polcher, J. The impact of global land-cover change on the terrestrial water cycle. Nature Clim. Change  
3, 385–390 (2012).
 3. Luyssaert, S. et al . Land management and land-cover change have impacts of similar magnitude on surface temperature. Nature 
Clim. Change 4, 389–393 (2014).
 4. Friedl, M & Sulla-Menashe, D. MCD12Q1 MODIS/Terra+ Aqua Land Cover Type Y early L3 Global 500m SIN Grid V006. NASA 
EOSDIS Land Processes DAAC https://doi.org/10.5067/MODIS/MCD12Q1.006 (2019)
 5. Sulla-Menashe, D., Gray, J. M., Abercrombie, S. P . & Friedl, M. A. Hierarchical mapping of annual global land cover 2001 to present: 
The MODIS Collection 6 Land Cover product. Remote Sens. Environ. 222, 183–194 (2019).
 6. European Space Agency Climate Change Initiative, Land Cover maps.elie.ucl.ac.be/CCI/viewer/download/ESACCI-LC-Ph2-
PUGv2_2.0.pdf (2017).
 7. Buchhorn, M. et al . Copernicus Global Land Service: Land Cover 100m: collection 3: epoch 2019: Globe. zenodo  https://doi.
org/10.5281/ZENODO.3939050 (2020).
 8. Buchhorn, M. et al . Copernicus Global Land Cover Layers—Collection 2. Remote Sens. 12, 1044 (2020).
 9. Kennedy, R. E. et al . Bringing an ecological view of change to Landsat-based remote sensing. Front. Ecol. Environ. 12, 339–346 
(2014).
 10. Jin, S. et al. Overall Methodology Design for the United States National Land Cover Database 2016 Products. Remote Sens. 11, 2971 
(2019).
 11. Brown, J. F. et al . Lessons learned implementing an operational continuous United States national land change monitoring capability: 
The Land Change Monitoring, Assessment, and Projection (LCMAP) approach. Remote Sens. Environ. 238, 111356 (2020).
 12. Frye, C., Nordstrand, E., Wright, D. J., Terborgh, C. & Foust, J. Using Classified and Unclassified Land Cover Data to Estimate the 
Footprint of Human Settlement. Data Sci. J. 17, 1–12 (2018).
 13. Chen, J. et al. Global land cover mapping at 30m resolution: A POK-based operational approach. ISPRS J. Photogramm. Remote Sens.  
103, 7–27 (2015).

16 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/ 14. Gong, P . et al. Stable classification with limited sample: transferring a 30-m resolution sample set collected in 2015 to mapping 10-m 
resolution global land cover in 2017. Sci. Bull. 64, 370–373 (2019).
 15. Liu, H. et al. Production of global daily seamless data cubes and quantification of global land cover change from 1985 to 2020 - iMap 
World 1.0. Remote Sens. Environ. 258, 112364 (2021).
 16. Abadi, M. et al. TensorFlow: A system for large-scale machine learning. OSDI  (2016).
 17. Gorelick, N. et al. Google Earth Engine: Planetary-scale geospatial analysis for everyone. Remote Sens. Environ. 202, 18–27 (2017).
 18. Anderson, J. R. et al . A Land Use and Land Cover Classification System for Use with Remote Sensor Data. Report No. 964 (USGS 
1976).
 19. European Commission. Joint Research Centre. LUCAS 2015 topsoil survey: presentation of dataset and results. https://doi.
org/10.2760/616084 (Publications Office, 2020).
 20. Souza, C. M. Jr. et al . Reconstructing Three Decades of Land Use and Land Cover Changes in Brazilian Biomes with Landsat Archive 
and Earth Engine. Remote Sens. 12, 2735 (2020).
 21. Penman, J. et. al . in Good Practice Guidance For Land Use, Land-use Change And Forestry (Institute for Global Environmental 
Strategies, 2003).
 22. Dinerstein, E. et al. An Ecoregion-Based Approach to Protecting Half the Terrestrial Realm. BioScience 67, 534–545 (2017).
 23. Labelbox, San Francisco, CA, USA. Available online: https://labelbox.com/.
 24. Main-Knorn, M. et al. Sen2Cor for Sentinel-2. in Image and Signal Processing for Remote Sensing XXIII (eds. Bruzzone, L., Bovolo, 
F. & Benediktsson, J. A.) https://doi.org/10.1117/12.2278218 (SPIE, 2017).
 25. Sentinel-2: Cloud Probability https://developers.google.com/earth-engine/datasets/catalog/COPERNICUS_S2_CLOUD_
PROBABILITY (2021).
 26. Frantz, D., Haß, E., Uhl, A., Stoffels, J. & Hill, J. Improvement of the Fmask algorithm for Sentinel-2 images: Separating clouds from bright surfaces based on parallax effects. Remote Sens. Environ. 215, 471–481 (2018).
 27. Müller, R., Kornblith, S., & Hinton, G. When does label smoothing help? Preprint at https://arxiv.org/abs/1906.02629 (2019).
 28. Xu, Y ., Xu, Y ., Qian, Q., Li, H., & Jin, R. Towards understanding label smoothing. Preprint at https://arxiv.org/abs/2006.11653 (2020).
 29. Long, J., Shelhamer, E. & Darrell, T. Fully convolutional networks for semantic segmentation. in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) https://doi.org/10.1109/cvpr.2015.7298965 (IEEE, 2015).
 30. Phiri, D. et al. Sentinel-2 Data for Land Cover/Use Mapping: A Review. Remote Sens. 12, 2291 (2020).
 31. Sumbul, G., Charfuelan, M., Demir, B. & Markl, V . Bigearthnet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding. in IGARSS 2019 - 2019 IEEE Geosci. Remote Sens. Symposium. https://doi.org/10.1109/igarss.2019.8900532 (IEEE, 
2019).
 32. Ienco, D., Interdonato, R., Gaetano, R. & Ho Tong Minh, D. Combining Sentinel-1 and Sentinel-2 Satellite Image Time Series for land cover mapping via a multi-source deep learning architecture. ISPRS J. Photogramm. Remote Sens. 158, 11–22 (2019).
 33. Ronneberger, O., Fischer, P . & Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation. in Lect. Notes Comput. Sci. 234–241 https://doi.org/10.1007/978-3-319-24574-4_28 (Springer International Publishing, 2015).
 34. Chen, L.-C., Zhu, Y ., Papandreou, G., Schroff, F. & Adam, H. Encoder-Decoder with Atrous Separable Convolution for Semantic 
Image Segmentation. in Computer Vision – ECCV 2018 833– 851. https://doi.org/10.1007/978-3-030-01234-2_49 (Springer 
International Publishing, 2018).
 35. World Resources Institute, Google. Dynamic World V1. Earth Engine Data Catalog https://developers.google.com/earth-engine/
datasets/catalog/GOOGLE_DYNAMICWORLD_V1 (2022).
 36. Brown, C. F. et al . Dynamic World Test Tiles. zenodo  https://doi.org/10.5281/ZENODO.4766508 (2021).
 37. Tait, A. M., Brumby, S. P ., Hyde, S. B., Mazzariello, J. & Corcoran, M. Dynamic World training dataset for global land use and land cover categorization of satellite imagery. PANGAEA https://doi.org/10.1594/PANGAEA.933475 (2021).
 38. Stehman, S. V . & Foody, G. M. Key issues in rigorous accuracy assessment of land cover products. Remote Sensing of Environment  
231, 111199 (2019).
 39. Stehman, S. V . Practical Implications of Design-Based Sampling Inference for Thematic Map Accuracy Assessment. Remote Sensing 
of Environment 72, 35–45 (2000).
 40. Saah, D. et al. Collect Earth: An online tool for systematic reference data collection in land cover and use applications. Environmental 
Modelling & Software  118, 166–171 (2019).
 41. Olofsson, P ., Foody, G. M., Stehman, S. V . & Woodcock, C. E. Making better use of accuracy data in land change studies: Estimating accuracy and area and quantifying uncertainty using stratified estimation. Remote Sensing of Environment 129, 122–131 (2013).
 42. Olofsson, P . et al. Good practices for estimating area and assessing accuracy of land change. Remote Sensing of Environment 148, 
42–57 (2014).
 43. Stehman, S. V . & Foody, G. M. Key issues in rigorous accuracy assessment of land cover products. Remote Sensing of Environment  
231, 111199 (2019).
 44. Brown, C. google/dynamicworld: v1.0.0. zenodo  https://doi.org/10.5281/zenodo.5602141 (2021).
Acknowledgements
We thank Tyler A. Erickson at Google for assistance with the previous version of our dataset explorer app. We 
thank Matt Hansen and the University of Maryland Global Land Analysis and Discovery lab for contributions to external validation. Development of the Dynamic World training data was funded in part by the Gordon and Betty Moore Foundation.
Author contributions
C.F.B. was the primary author and developed and implemented the modeling, cloud masking, and inference methods. S.P .B., S.B.H., J.M., and A.M.T. oversaw the training data collection and technical validation. B.G-W ., M.W ., F.S., and C.H. assisted with training data collection, developed the taxonomy and use-case guidance, and supported the additional validation. T.B., W .C., R.H., S.I., K.S., O.G., and R.M. supported and contributed to the inference and data pipelines used in production of Dynamic World. V .J.P . contributed writing, the time-series 
explorer app, and Top-1 probability hillshade visualization.
Competing interests
The authors declare no competing interests.
Additional information
Correspondence and requests for materials should be addressed to C.F.B.
Reprints and permissions information is available at www.nature.com/reprints.
17 Scientific  Data  |           (2022) 9:251  | https://doi.org/10.1038/s41597-022-01307-4
www.nature.com/scientificdata www.nature.com/scientificdata/Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and 
institutional affiliations.
Open Access This article is licensed under a Creative Commons Attribution 4.0 International 
License, which permits use, sharing, adaptation, distribution and reproduction in any medium or 
format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-ative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the 
material. If material is not included in the article’s Creative Commons license and your intended use is not per-
mitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/. © The Author(s) 2022
Land cover mapping at very high resolution with rotation equivariant
CNNs: Towards small yet accurate models
Diego Marcosa,b,⇑, Michele Volpia, Benjamin Kellenbergerb, Devis Tuiaa,b
aMultiModal Remote Sensing, University of Zurich, Switzerland1
bLaboratory of GeoInformation Science and Remote Sensing, Wageningen University and Research, The Netherlands2
article info
Article history:
Received 11 June 2017Received in revised form 25 November 2017Accepted 27 January 2018
Available online 19 February 2018
Keywords:
Semantic labeling
Deep learning
Rotation invarianceSub-decimeter resolutionabstract
In remote sensing images, the absolute orientation of objects is arbitrary. Depending on an object’s ori-
entation and on a sensor’s ﬂight path, objects of the same semantic class can be observed in different ori-
entations in the same image. Equivariance to rotation, in this context understood as responding with a
rotated semantic label map when subject to a rotation of the input image, is therefore a very desirablefeature, in particular for high capacity models, such as Convolutional Neural Networks (CNNs). If rotation
equivariance is encoded in the network, the model is confronted with a simpler task and does not need to
learn speciﬁc (and redundant) weights to address rotated versions of the same object class. In this workwe propose a CNN architecture called Rotation Equivariant Vector Field Network (RotEqNet) to encode
rotation equivariance in the network itself. By using rotating convolutions as building blocks and passing
only the values corresponding to the maximally activating orientation throughout the network in theform of orientation encoding vector ﬁelds, RotEqNet treats rotated versions of the same object withthe same ﬁlter bank and therefore achieves state-of-the-art performances even when using very small
architectures trained from scratch. We test RotEqNet in two challenging sub-decimeter resolution
semantic labeling problems, and show that we can perform better than a standard CNN while requiringone order of magnitude less parameters.
/C2112018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier
B.V. All rights reserved.
1. Introduction
In this paper we consider the task of semantic labeling , which
corresponds to the automatic assignment of each pixel to a set of
predeﬁned land-cover or land-use classes. The classes are selected
speciﬁcally for the task to be solved and deﬁne the learning prob-
lem for the model.
When using low- to mid-resolution multispectral imagery (e.g.
Landsat), it is customary to assume that the spectral information
carried by a pixel is sufﬁcient to classify it into one of the semantic
classes, thus reducing the need for modeling spatial dependencies.
However, when dealing with very-high spatial resolution (VHR)
imagery, i.e. imagery in the meter to sub-decimeter resolution
range, the sensor trades off spectral resolution to gain spatial
details. Such data is commonly composed of red-green-blue (RGB)color channels, occasionally with an extra near infrared (NIR) band.
Due to this trade-off, single pixels tend not to contain sufﬁcient
information to be assigned with high conﬁdence to the correct
semantic class, when relying on spectral characteristics only. More-
over, depending on the task, some classes can be semantically
ambiguous: a typical example is land use mapping, where objects
belonging to different classes can be composed of the same material
(e.g. road and parking lots), thus making analysis based on spectra
of single pixels not suitable. To resolve both problems, spatial con-
text needs to be taken into account, for example via the extraction
and use of textural ( Regniers et al., 2016 ), morphological ( Dalla
Mura et al., 2010; Tuia et al., 2015 ), tree-based ( Gueguen and
Hamid, 2015 ) or other types ( Malek et al., 2014 ) of spatial features.
These features consider the neighborhood around a pixel as part of
its own characteristics, and allow to place spectral signatures in
context and solve ambiguities at the pixel level ( Fauvel et al.,
2013 ). The diverse and extensive pool of possible features led to a
surge in works focusing on the automatic generation and selection
of discriminant features ( Harvey et al., 2002; Glocer et al., 2005;
Tuia et al., 2015 ), aimed at preventing to compute and store fea-
tures that are redundant or not suited for a particular task.
https://doi.org/10.1016/j.isprsjprs.2018.01.021
0924-2716/ /C2112018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.1www.geo.uzh.ch/en/units/multimodal-remote-sensing .
2www.geo-informatie.nl .
⇑Corresponding author at: Laboratory of Geo-information Science and Remote
Sensing, PO Box 47 6700 AA Wageningen, Netherlands
E-mail address: diego.marcos@wur.nl (D. Marcos).ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107
Contents lists available at ScienceDirect
ISPRS Journal of Photogrammetry and Remote Sensing
journal homepage: www.elsevier.com/locate/isprsjprs

Another common approach to reduce the computational burden
while enforcing spatial reasoning is to extract local features from a
support deﬁned by unsupervised segmentation. Also, spatial rules
can be encoded by Markov random ﬁelds, where spatial consis-
tency is usually enforced by minimizing a neighborhood-aware
energy function ( Moser et al., 2013 ) or speciﬁc spatial relationships
between the classes ( Volpi and Ferrari, 2015 ).
In the situations described above, a successful solution comes at
the cost of having to manually engineer a high-dimensional set of
features potentially covering all the local variations of the data in
order to encode robust and discriminative information. In this set-
ting, there is no guarantee that the features employed are optimal
for a given semantic labeling problem. These problems raised the
interest of the community in solutions avoiding to manually engi-
neer the feature space, solutions that are extensively studied underthedeep learning paradigm. The aim of deep learning is to train a
parametric system learning feature extraction jointly with a classi-
ﬁer ( Goodfellow et al., 2016 ), in an end-to-end manner. When
focusing on image data, Convolutional Neural Networks (CNNs,
LeCun et al. (1998) ) are state-of-the-art. Their recognized success
follows from new ground-breaking results in many computer
vision problems. CNNs stand out thanks to their ability to learn
complex problem-speciﬁc features, while jointly optimizing a loss
(e.g. a classiﬁer, a regressor, etc.). Thanks to recent hardware
advances accelerating CNN training consistently, as well as the
existence of pre-trained models to get started, CNNs have become
one of the most studied models in recent remote sensing research
dealing with VHR imagery, as we brieﬂy review below.
The ﬁrst models proposed studied the effectiveness of translat-
ing computer vision architectures directly to aerial data for tile clas-
siﬁcation. In that sense, a single label was retrieved per image tile,
thus tackling what in computer vision is called the image classiﬁca-
tion problem
3: authors in Castelluccio et al. (2015) and Penatti et al.
(2015) studied the effect of ﬁne-tuning models trained on natural
image classiﬁcation problems, in order to adapt them quickly to
above-head image classiﬁcation. Their results suggested that such a
strategy is relevant for image classiﬁcation and can be used to reuse
models trained on a different modality. Transposing these model in
the semantic labeling problem is also possible, typically applying
the models using a sliding window centered at each location of the
image, as tested in Campos-Taberner et al. (2016) . However, the
authors also came to three important conclusions: (i) models trained
from scratch (in opposition to ﬁne-tuned models from vision) tend to
provide better results on speciﬁc labeling tasks; (ii) by predicting a
single label per patch, the one corresponding to the pixel on which
the patch is centered, these models are not able to encode explicit
label dependencies in the output space and (iii) the computational
overhead of the sliding window approach is extremely large. Suchconclusions support the use of network architectures that have been
developed speciﬁcally for semantic labeling problems: recent efforts
tend to consider fully convolutional approaches ( Long et al., 2015 ),
where the CNN does not only predict a single label per patch, but
actually provides directly the label map for all the pixels that com-
pose the input tile. The approaches proposed vary from spatial inter-
polation ( Maggiori et al., 2017 ), fully convolutional models ( Audebert
et al., 2016 ), deconvolutions ( Volpi and Tuia, 2017 ), stacking activa-
tions ( Maggiori et al., 2016 ) to hybridization with other classiﬁers
(Liu et al., 2017 ), but they all are consistent in one observation: fully
convolutional architectures drastically reduce the inference time and
naturally encode some aspect of output dependencies, in particular
learning dependent ﬁlters at different scales, thus reducing the need
of cumbersome postprocessing of the prediction map.While these works open endless opportunities for remote sens-
ing image processing with CNNs, they also showed one of the big-
gest downsides of these models: CNNs tend to need large amounts
of ground truth to be trained, and setting up the architecture, as
well as selecting hyperparameters, can be troublesome, since
cross-validation is often prohibitive in terms of processing time.
Note that it is often that case when the number of parameters is
larger than the number of training samples, which makes regular-
ization techniques and data augmentation a must-do, at the cost of
signiﬁcantly slowing model training. Our contribution aims at
addressing this drawback of CNNs, i.e.the large model sizes and
need for labels when there is a limited availability of ground truth.
In this paper, we propose to tackle the problem by exploiting a
property of objects and features in remote sensing images: their
orientation is arbitrary .
Overhead imagery differs from natural images in that the abso-
luteorientation of objects and features within the images tends to
be irrelevant for most tasks, including semantic labeling. This is
because the orientation of the camera in nadir-looking imagery is
most often arbitrary. As a consequence, the label assigned to an
element in the image should not change if the image is taken with
a different camera orientation. We call this property equivariance ,
and it is a property that recently attracted a lot of interest in image
analysis ( Lei et al., 2012; Cheng et al., 2016 ).
Given a rotation operator, gað/C1Þ, we say that a function fð/C1Þis
equivariant to rotations if fðgað/C1ÞÞ ¼ gaðfð/C1ÞÞ, invariant to rotations
iffðgað/C1ÞÞ ¼ fð/C1Þand, more generally, covariant to rotations if
fðgað/C1ÞÞ ¼ hðfð/C1ÞÞ, with hð/C1Þbeing some function other than gað/C1Þ.
Note that, in the case of semantic labeling, the property we are
interested in is equivariance, although it becomes invariance ifwe consider a single pixel at a time. We will therefore use the
terms equivariance and invariance interchangeably in this paper.
With CNNs, equivariance to the rotation of inputs can be
approximated by randomly rotating the input images during train-
ing, a technique known as data augmentation orjittering (Leen,
1995 ). If the CNN has enough capacity and has seen the training
samples in sufﬁcient number of orientations, it will learn to be
invariant to rotations ( Lenc and Vedaldi, 2015 ). While this kind
of data augmentation greatly increases the generalization accu-
racy, it does not offer any advantage in terms of model compact-
ness, since similar ﬁlters, but with different orientations, need to
be learned independently. A different approach, hard coding such
invariances within the model, has the two main beneﬁcial effects:
ﬁrst, the model becomes robust to variations which are not dis-
criminative, as a standard CNN with enough ﬁlters would learn;
and second, model-based invariance can be interpreted as some
form of regularization ( Leen, 1995 ). This added robustness ulti-
mately lead to models which have high capacity (as high as a stan-
dard CNN) but with lower sample complexity.
There has been a recent surge in works that explore ways of
encoding model-based rotation invariance in CNNs. Laptev et al.
(2016) perform a rotation of the input image in order to reduce
the sample complexity of the problem and Jaderberg et al. (2015)
extend this to afﬁne transformations. These approaches provide
invariance to a global rotation of the input image and not to local
relative rotations, and are therefore not very well suited for seg-
mentation tasks. Cohen and Welling (2016) encode equivariance
to shifts and to rotations by multiples of 90
/C14by tying ﬁlter weights,
while Zhou et al. (2017) use linearly interpolated ﬁlters. These two
methods are in principle suited for segmentation tasks. The former
is limited to invariance to 90/C14rotations and the latter, although
offering more ﬂexibility, has the drawback of requiring a trade-
off between the number of rotations and the memory require-
ments, bringing the authors to use 8 orientations, at multiples of
45/C14.Worrall et al. (2016) reduce the space of possible ﬁlters to3This is not to be confused with the semantic labeling problem we address in this
paper, which is the task of attributing a label to every pixel in the tile.D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107 97
combinations of complex harmonic wavelets, thusachieving per-
fect equivariance to rotations. By doing so, they obtain a more
compact internal representation by encoding oriented activations
as complex valued feature maps, but at the cost of reducing the
expressiveness of each ﬁlter.
In this paper, we consider a solution that combines the advan-
tages of these two last methods. Our model applies an arbitrary
number of rotated instances of each ﬁlter at every location, in a
way that each ﬁlter activation is composed by a vector of activa-
tions (as opposed to a scalar in standard CNN), thus representing
the activation of each rotated ﬁlter. We then propose to max-
pool these activations, compressing the information in a simple
2D vector that represents the magnitude and orientation of the
maximally activating ﬁlter. This allows us to encode ﬁne-grained
rotation invariance ( i.e.very small angles) and, at the same time,
to avoid constraining the ﬁlters to any particular class, thus
enabling more expressive ﬁlters. The proposed Rotation Equivari-
ant Vector Field Network (RotEqNet, Marcos et al. (2016) ) achieves
model-based invariance while reducing the number of required
parameters by around one order of magnitude. This is done by
sharing the same convolutional ﬁlter wights across all angles, thus
providing regularization to irrelevant modes of variations ( Leen,
1995 ).
In addition, the decrease in sample complexity allows models to
be trained more efﬁciently. In this paper, we also analyze the effect
that the amount of available ground truth has on the performance
of CNNs learning for semantic labeling of overhead imagery based
on two public datasets at submetric resolution: the Vaihingen and
the Zeebruges benchmarks (see Section 3).
In Section 2we brieﬂy present the intuition behind RotEqNet, as
well as its main components. In Section 3we present the data and
the setup of the experiments presented and discussed in Section 4.
2. Rotation Equivariant Vector Field Networks (RotEqNet)
In this paper, we propose to make a CNN equivariant to rota-
tions by rotating ﬁlters and considering only the maximal activa-
tion across rotations. This section ﬁrst recalls basics about CNNs
and then presents the RotEqNet modules as a way of extending
any CNN architecture into a rotation equivariant one. For more
details about RotEqNet, the reader can refer to Marcos et al.
(2016) , where the theory was originally presented by the authors.2.1. Convolutional neural networks for semantic labeling
In this section we brieﬂy present the building blocks of CNNs, as
well as an example of a fully convolutional architecture to perform
semantic labeling.
2.1.1. CNN building blocks
CNNs consist of a cascade of operations applied to an input
image x2RM/C2N/C2dsuch that it can be nonlinearly transformed in
the desired output. The number of such operations deﬁnes the
depth of the network. CNNs are often organized in convolutional
blocks as the one depicted in Fig. 1 .
The convolution operator
y¼x/C3wþb; ð1Þ
between xand a ﬁlter w2Rm/C2m/C2d, where b2Ris the bias, produces
a feature map y2RM/C0mþ1/C2N/C0mþ1by applying locally a scalar product
operation between wand every patch in xof size m/C2min a sliding
window manner. A convolution block in a CNN corresponds to the
convolution of the image with a series of ﬁlters, which are repre-
sented in different colors in Fig. 1 .
The dimensionality of the activations equals the number of ﬁl-
ters in the layer. To control the spatial extent of the activations
after convolutions, it is common to apply zero-padding, which does
not inﬂuence the value of the activations, but does compensate for
the amount of pixels lost at the borders of the image. In order to
obtain an advantage from the depth of the model, in terms of
expressive power, it is necessary to apply some non-linear opera-
tion to the output of each convolution. The most common is the
rectiﬁed linear unit (ReLU), which clips the negative values to zero,
asy¼maxð0;xÞ.
Once the activations are obtained, they are often pooled in the
spatial domain, for example by taking the maximum value occur-
ring in a very small (usually 2 /C22) local window. This operation,
called max-pooling is represented in Fig. 1 by the red squares
and, besides the obvious effect of reducing the amount of data, also
allows the ﬁlters in the next layer to ‘see’ more context of the orig-
inal image: looking again at the schematic in Fig. 1 , if the ﬁrst ﬁl-
ters see a 3 /C23 region, those of the second layer (orange cube on
the right) will see a 3 /C23 region in the reduced activations map,
which coresponds to a 7 /C27 region in the original image. By cas-
cading several convolutional and max-pooling blocks, the network
Fig. 1. Schematic of the ﬁrst convolutional layer of a CNN. This layer learns Nf = 5 ﬁlters of size 3 /C23/C23 and applies a spatial pooling halving the size of the activation map
(only two out of the ﬁve activation maps are shown for clarity). In the activation maps, the colored pixels (in green or blue) correspond to those receiv ing information from
the receptive ﬁeld marked in orange in the input image (left). (For interpretation of the references to colour in this ﬁgure legend, the reader is refer red to the web version of
this article.)98 D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107
actually becomes aware of a wide context around the pixel being
considered, while reducing the number of required learnable
parameters, and provides invariance to local (at each layer) and
global (at the network level) translations. The latter is evident in
image classiﬁcation problems: an image contains a cat indepen-
dently of where it is located. For semantic labeling tasks, max-
poolings have the effect of learning locally consistent and multi-
scale ﬁlters.
Two other operators are often used to improve the learning pro-
cess: batch normalization and dropout. The former normalizes
each feature map within a batch to have zero mean and unit stan-
dard deviation. The latter sets a certain proportion of randomly
selected feature maps to zero during training, thus preventing
the ﬁlters from depending too much on one another.
2.1.2. From patch classiﬁcation to (dense) semantic labeling
Early CNN models in vision were designed for tile (or image)
classiﬁcation, i.e. to provide a single label for an image. In semantic
labeling, we are interested in obtaining a dense prediction, i.e. a
map where each pixel is assigned to the most likely label. As stated
in the introduction, this can be achieved in a number of ways,
including fully convolutional models ( Long et al., 2015 ). A very
simple way to perform dense predictions is to use the activation
maps themselves as features to train a classiﬁer predicting the
label of every pixel. If max-pooling operations have been per-
formed, spatial upsampling, e.g. by interpolation, is required to
bring all activations to the same spatial resolution. One of these
approaches, known as ‘‘hypercolumns” ( Hariharan et al., 2015 ),
using ﬁxed upsamplings, is represented in Fig. 2 . It follows the
intuition that the different activation maps contain information
about speciﬁc features extracted at different scales, from low-
level ones (ﬁrst layers react to corners, gradients, etc.) to more
semantic and contextual ones (last layers activate to class-
speciﬁc features). Therefore, a stack of such features can be used
to learn an effective classiﬁer for dense semantic labeling tasks,
where spatial information is crucial. In remote sensing, the idea
of hypercolumns was used in the architecture proposed by
Maggiori et al. (2016) . In the experiments, we will use this archi-tecture for dense semantic labeling, using two fully connected lay-
ers as classiﬁer (see Section 3.2.1 for details), and train the model
end-to-end.
2.2. From translation to rotation invariance
As mentioned above, CNNs are translation equivariant by
design. To understand why it is more complex to achieve natural
equivariance to rotations by means of convolutions, we will brieﬂy
summarize how translation equivariance is obtained by standard
CNNs before moving to rotation equivariance and our proposed
solution, RotEqNet.
2.2.1. Translation equivariance in CNNs
The convolution of an image xwith a ﬁlter w(Eq. (1)) is com-
puted by applying the same dot product operation over all overlap-
ping m/C2mwindows on x.I fxis shifted by an integer translation in
the horizontal and vertical directions, given a reference location,
the same neighborhoods in xwill exist in the translated x. The cor-
responding convolution output, except for some possible border
effects, are exactly the same up to some global translation con-
stant. For this reason, neighborhood-based operations are transla-
tion equivariant when applied to images. The fact that the
operation is local and produces a single scalar per neighborhood
has another advantageous effect: the output can be effortlessly
re-arranged in useful ways. Typically, the spatial structure of theactivations is set to match the one of the input (as in CNNs, see
Fig. 1 ).
2.2.2. Rotation equivariance in RotEqNet
If we want the operator to be equivariant to rotation, the struc-
ture of the layer activations becomes more complex. One possibil-
ity would be to return a series of values corresponding to the
convolution of xwith rotated versions of the canonical ﬁlter w.
In this case, the activations ywould be a 3D tensor where a trans-
lation in the 3rd dimension corresponds to a rotation of w. The
covariance achieved in this way could easily be transformed into
equivariance by means of pooling across orientations, since the
Fig. 2. Schematic of the model considered for dense semantic labeling. Each activation map in the CNN (top part) is upsampled at the original image resolution (blue arrows),
concatenated to the original image (red arrow) and fed to a local fully connected layer (1 /C21 convolutions), in this example using 18 features. (For interpretation of the
references to colour in this ﬁgure legend, the reader is referred to the web version of this article.)D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107 99
value returned at each image location will remain constant when
the image is rotated and thus a rotation of the input image will
result in the same rotation of the output feature map.
In particular, we propose to perform a single-binned max-
pooling operation across the newly added orientation dimension.
At each location, it ﬁres on the largest activation across orienta-
tions, returning its value (magnitude) and the angle at which it
occurred. This way, we are able to keep the 2D arrangement of
the image and activations throughout the CNN layers, while
achieving rotation equivariance as provided by this pooling strat-
egy. Furthermore, this strategy allows the network to make use
of the information about the orientation of feature activations
observed in previous layers. Similar to spatial max-pooling, this
orientation pooling propagates only information about the maximal
activation, discarding all information about non-maximal activa-tions. This has the drawback of potentially loosing useful informa-
tion (e.g. when two orientations are equally discriminant), but
offers the advantage of reducing the memory requirements of both
the model and the feature maps along the network, making them
independent of the number of rotations used. Since the result of
such pooling is no longer a scalar as in conventional CNNs, but a
2D vector (magnitude and angle), each activation map can now
be treated as a vector ﬁeld. Fig. 3 schematizes this intuition and
shows a RotEqNet convolutional block in comparison to the stan-
dard CNN convolutional block of Fig. 1 .
2.3. RotEqNet modules
RotEqNet essentially involves rotating CNN ﬁlters and pooling
across the orientation space to retrieve the maximal activations
and their angle observed at each location and per ﬁlter. To achieve
such behavior, several building blocks of CNNs must be re-
designed in order to accommodate vector ﬁeld inputs/outputs. In
this section, we brieﬂy summarize how the convolution and pool-
ing operators have been modiﬁed. Modiﬁcations of spatial pooling
and batch normalization are straightforward and we invite the
interested reader to consult Marcos et al. (2016) for more details.
2.3.1. Rotating convolution
As introduced above, rotation equivariance can be achieved by
computing the activations on a series of rotated versions of the ﬁl-
ters being learned. This boils down to calculate rotated versions ofeach main (or canonical ) ﬁlter at Rorientations
a¼½a1;... ;aR/C138.I n
case of remote sensing images, for which the orientation might
be completely arbitrary, acan span the entire 360/C14rotation space,
while in other applications with a clear top-down relations, one
could limit the angles to a smaller range (e.g. it is unlikely that a
tree depicted in a ground level image is oriented in the left-right
direction, but some tilts due to camera shake could be present).
The rotation of the ﬁlter is obtained by resampling wwith bilin-
ear interpolation, after rotation of ardegrees around the ﬁlter cen-
ter. The position ½i0;j0/C138after rotation of a speciﬁc ﬁlter weight,
originally located at ½i;j/C138in the canonical form, is
½i0;j0/C138¼½i;j/C138cosðarÞsinðarÞ
/C0sinðarÞcosðarÞ/C20/C21
: ð2Þ
Coordinates are relative to the center of the ﬁlter. Since the rotation
can force weights near the corners of the ﬁlter to be relocated out-
side of its spatial support, only the weights within a circle of diam-
eter mpixels, the ﬁlter size, are used to compute the convolutions.
The output tensor for ﬁlter w, of size y2RM/C2N/C2R, consists of Rfea-
ture maps (see the center part of Fig. 3 ), each one computed as
yðrÞ¼x/C3wrþb8r¼1;2...R; ð3Þ
where ð/C3Þis a standard convolution operator, and bis a shared bias
across all rotations. The tensor yencodes the roto-translation out-
put space such that rotation of the input corresponds to a transla-
tion across the feature maps. Only the canonical, non rotated,
version of wis actually stored in the model. During backpropaga-
tion, gradients ﬂow through the ﬁlter with maximal activation, very
similarly to the max-pooling case. Consequently, the gradients have
to be aligned to the rotation of the canonical ﬁlter, which is recov-
ered thanks to the angle as given by the orientation pooling. Thus,
ﬁlters are updated as:
rw¼X
rrotate ðrwr;/C0arÞ; ð4Þ
2.3.2. Orientation pooling
The rotating convolution above outputs a set of Ractivations
per canonical ﬁlter, each one corresponding to one rotated version
ofw. To avoid the explosion of dimensionality related to the prop-
agation of all these activations to the next layer, we perform a
pooling across the space of orientation aiming as pushing forward
Fig. 3. Schematic of the ﬁrst convolutional layer of RotEqNet. This layer learns Nf = 5 ﬁlters of size 3 /C23/C23. Each ﬁlter is rotated to a pre-deﬁned range of angles and the
activation at each orientation is computed. Then an orientation pooling retains only the maximal activation and the angle that generated it, thus pro viding a vector activation
per pixel (represented by colored arrows). This vector map is pooled spatially as for conventional CNNs. The output is a stack of vector activations. I n the activation maps, the
colored pixels (in green or blue) correspond to those receiving information form the receptive ﬁeld marked in orange in the input image (left). For cla rity, only two out of the
ﬁve activation maps are shown. (For interpretation of the references to colour in this ﬁgure legend, the reader is referred to the web version of this ar ticle.)100 D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107
only the information relative to the direction of maximal activa-
tion. In order to preserve as much information as possible, we keep
two kinds of information: the magnitude of activation and the orien-
tation that generated it .
To do so, we extract a 2D map of the largest activations
q2RM/C2Nand their corresponding orientations h2RM/C2N. Speciﬁ-
cally, for activations located at ½i;j/C138we get:
q½i;j/C138¼max
ry½i;j;r/C138; ð5Þ
h½i;j/C138¼360
Rarg max
ry½i;j;r/C138: ð6Þ
This can be treated as a polar representation of a 2D vector ﬁeld
as long as q½i;j/C138P08i;j. This condition is met when using a func-
tion on ythat returns non-negative values: we therefore employ
the Rectiﬁed Linear Unit (ReLu) operation, deﬁned as
ReLu ðqÞ¼maxðq;0Þ. In the backward pass the magnitude of the
incoming 2D vector gradient is passed to the corresponding maxi-
mally activated position, y½i;j;rmax/C138, as is done with standard max-
pooling.
2.3.3. Dealing with vector inputs
Note that the orientation pooling block outputs vector ﬁelds,
where each location in the activation carries both the maximal
magnitude and its orientation observed in polar representation
(see the rightmost matrix in Fig. 3 ). This means that the output
of such pooling is vectorial and cannot be used anymore in a tradi-
tional convolutional layer. However, if we convert this polar repre-
sentation into Cartesian coordinates, each ﬁlter wproduces a
vector ﬁeld feature map z2RM/C2N/C22, where the output of each loca-
tion consists of two values ½u;v/C1382R2encoding the same
information.
u¼ReLu ðqÞcosðhÞð 7Þ
v¼ReLu ðqÞsinðhÞð 8Þ
Since the horizontal and vertical components ½u;v/C138are orthogo-
nal, the convolution of two vector ﬁelds can be computed summing
standard convolutions calculated separately in each component:
ðz/C3wÞ¼ðzu/C3wuÞþðzv/C3wvÞ; ð9Þ
By using this trick, we can now calculate convolutions between
vector ﬁelds and design deep architectures which are rotation
equivariant.3. Data and setup
3.1. Datasets
We test the proposed system on two recent benchmarks that
raised signiﬁcant interest thanks to the dense ground truth pro-
vided over a set of sub-decimeter resolution image tiles: the Vai-
hingen and Zeebruges data, which are brieﬂy described below.
Both datasets consist of three optical bands and a Digital Surface
Model (DSM). Since using the DSM has been shown to improve
the segmentation results ( Audebert et al., 2017; Marmanis et al.,
2016; Volpi and Tuia, 2017 ) we use it in all of our experiments.
We do this by stacking the DSM with the optical data and treating
it as an additional band, as in Volpi and Tuia (2017) , since this has
almost no impact in the total number of model parameters (see
Fig. 4 ).
3.1.1. Vaihingen benchmark
The Vaihingen benchmark dataset has been provided to the
community as a challenge organized by the International Society
for Photogrammetry and Remote Sensing (ISPRS) Commission
III.4, the ‘‘2D semantic labeling contest”.4The dataset is composed
of 33 orthorectiﬁed image tiles acquired over the town of Vaihingen
(Germany), with an average size of 2494 /C22064 and a spatial reso-
lution of 9 cm. Among the 33 frames, 16 are fully annotated and dis-
tributed to participants, while the remaining ones compose the test
set and their ground truth is not distributed. Images are composed
by 3 channels: near infrared (NIR), red (R) and green (G). The chal-
lenge also provides a DSM coregistered to the image tiles. We use
a normalized version of the DSM (nDSM), where the heights are rel-
ative to the nearest ground pixel, redistributed by Gerke (2015) . One
of the training tiles is illustrated in Fig. 5 .
The task involves 6 land-cover/land-use classiﬁcation classes:
‘‘impervious surfaces” (roads, concrete ﬂat surfaces), ‘‘buildings”,
‘‘low vegetation”, ‘‘trees”, ‘‘cars” and a class of ‘‘clutter” to group
uncategorized surfaces and noisy structures. Classes are highly
imbalanced, with the classes ‘‘buildings” and ‘‘impervious sur-
faces” accounting for roughly 50% of the data, while classes such
as ‘‘car” and ‘‘clutter” account only for 2% of the total labels.
In our setup, 11 of the 16 fully annotated image tiles are used
for training, and the remaining ones (tile ID 11, 15, 28, 30, 34)
Fig. 4. one of the training tiles of the Vaihingen dataset: left: image; center: nDSM; right: ground truth.
4http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html .D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107 101
for testing, as in Sherrah (2016), Volpi and Tuia (2017), Maggiori
et al. (2017) .
3.1.2. Zeebruges benchmark
This benchmark has been acquired in 2011 over the city of Zee-
bruges (Belgium) and it is has been provided as part of the IEEE
GRSS Data Fusion Contest in 2015 ( Campos-Taberner et al.,
2016 ).5It is composed by seven tiles of 10 ;000/C210 ;000 pixels.
The tiles have a spatial-resolution of 5 cm and represent RGB chan-
nels only. Five of the seven images are released with labels ( Lagrange
et al., 2015 ) and used for training, while the remaining two are kept
for testing the generalization accuracy, accordingly to the challenge
guidelines. This dataset also comes with a Lidar point cloud, that we
processed into a DSM by averaging point clouds locally and interpo-
lating where necessary.
The semantic labeling problem involves 8 classes, as proposed
inLagrange et al. (2015) : the same six as in the Vaihingen bench-
mark, plus a ‘‘water” and ‘‘boats” class. It is worth mentioning that,since a large portion of the are is covered by a harbour, most of the
structures and cargo containers are labeled as ‘‘clutter”. Another
major difference to the Vaihingen dataset is that the ‘‘Water” class
is predominant, as it represents 30% of the training data, while cars
and boats together count just a mere 1%. Also, 1% of the data
belongs to an ‘‘uncategorized” class, which is not accounted for
in the labeling problem. The lack of a NIR channel and a higher
sample diversity make this benchmark more challenging than
the previous one, as we will see in Section 4.
3.2. Experimental setup
3.2.1. CNN architecture
We use a RotEqNet architecture based on hypercolumns
(Hariharan et al., 2015 ), in which every convolutional layer before
the concatenation is a rotating convolution. After the concatena-
tion, only standard fully connected layers are used, since 1 /C21 con-
volutions are inherently rotation equivariant. See Fig. 6 for a
schematic of the full architecture used. In both experiments we
build the baseline CNN with the exact same architecture as its
RotEqNet counterpart, but with four fold more ﬁlters in each layer,
Fig. 5. One of the training tiles of the Zeebruges dataset: left: image; center: nDSM; right: ground truth.
5http://www.grss-ieee.org/community/tec hnical-committees/data-fusion/2015-
ieee-grss-data-fusion-contest/ .
Fig. 6. Hypercolumn based architecture used in all our experiments. Note that all the layers are rotation equivariant, since all the convolutional layers ar e either RotEqNet
convolutions or fully connected (1 /C21) standard convolutions, which are also rotation equivariant by construction.102 D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107
resulting in approximately 10-fold more parameters. At this model
size the performance started to saturate.
We use an architecture with six convolutional layers, each with
downsampling by a factor of 2 using max-pooling. The number of
ﬁlters per layer in each of the convolutional layers is set as
½2;2;3;4;4;4/C138/C1Nf, where the ith element of the vector represents
the number of ﬁlters in the ith layer, such that Nf is the only
parameter used to change the size of the models. All the convolu-tional layers use 7 /C27 ﬁlters. This size allows to capture oriented
patterns in the corresponding image or feature map, as seen in
Fig. 7 . After applying a linear rectiﬁcation (ReLU) and batch nor-
malization, each activation map is then upsampled to the size of
the original image using bilinear interpolation and concatenated,
together with the raw image (see bottom part of Fig. 2 ), and pro-
cessed with three layers of 1 /C21 convolutions with
½50/C1Nf ;50/C1Nf ;C/C138ﬁlters, where Cis the number of classes. This is
followed by a softmax normalization. The 1 /C21 convolutions
implement a local fully connected layer, or, in other terms, per-
forms local classiﬁcation by a multi-layer perceptron (MLP). These
1/C21 convolutions are inherently rotation equivariant, so we use
standard convolutions as in Long et al. (2015) . The whole pipeline
is learned jointly end-to-end.
Our model performs arbitrarily dense prediction, i.e. given an
arbitrarily sized input patch the output will always be a prediction
map with the same size. As a result, the CNN architecture is ﬁxed
and the dataset reshaped to a series of ﬁxed-sized patches. For
the Vaihingen dataset, the spatial extent of the inputs is
512/C2512, while for Zeebruges is 500 /C2500. Note that the input
size does not inﬂuence the results.
All models are trained with stochastic gradient descent with
momentum (ﬁxed to 0.9), while other hyperparameters are tuned
by minimizing validation errors on 30 samples randomly selected
from the training set. The batch size is 4 in RotEqNet and 2 in
the standard CNN, because of the latter’s higher memory require-
ments. In both benchmarks, we perform data augmentation con-
sisting of random rotations, uniformly sampled between 0
/C14and
360/C14, and randomly ﬂipping the tiles in the vertical or horizontal
dimension. Note that performing full 360/C14rotations for data aug-
mentation is not strictly necessary when using RotEqNet, but it
has been shown to be additionally improve the performance (seeresults in Marcos et al. (2016) ) and doing so makes a comparison
with standard CNNs easier, since they are trained under more sim-
ilar conditions. As will be discussed below, data augmentation is
required by standard CNNs in order to be able learn rotation invari-
ance from examples, given that enough ﬁlters can be learned.
Regarding RotEqNet, rotating inputs does not have a direct effect
on learning diverse ﬁlters, but rather on data interpolation making
a same input tile looking different numerically (an effect also
improving training for standard CNNs).
All models are trained from scratch and ﬁlters are initialized
using the improved Xavier method.
The hardware used in all experiments consists of a single desk-
top with 32 GB of RAM and a 12 GB Nvidia Titan X GPU.
3.3. Experimental setup
3.3.1. Vaihingen
In the case of Vaihingen, we report a comparison between
RotEqNet and standard CNNs trained without rotating convolu-
tions. In order to test the sensitivity to the amount of ground truth,
we train three models per architecture, using respectively 4%, 12%
and 100% of the available training set. We compare architectures
with the same structure and number of layers, only varying the
number of ﬁlters. We compare a small RotEqNet model with a
CNN of larger capacity (but no built-in rotation equivariance).
The size of both models was chosen to be the smallest that would
obtain over 87 %overall accuracy on the validation set, which is in
line with the results published in Volpi and Tuia (2017) . The ﬁnal
number of ﬁlters deﬁned in this way was found to be Nf = 3 for
RotEqNet ( /C25105parameters) and Nf = 12 for the standard CNN
(/C25106parameters). The models using the full dataset were trained
for 22 epochs. In the RotEqNet models the learning rate was
2/C110/C02in the ﬁrst 11 epochs, followed by six epochs at 4 /C110/C03
and ﬁve at 8 /C110/C04, while the weight decay was 4 /C110/C02;4/C110/C03
and 8 /C110/C04respectively. In the standard models those values were
halved. This difference is due to a larger number of gradient update
iterations in the standard CNN caused by the need to use a smaller
mini-batch due to the larger memory requirements. For the exper-
iments with a reduced training set the number of epochs was
Fig. 7. Visualization of all the ﬁlters learned on the ﬁrst layer of the RotEqNet model on Vaihingen, (a) on the optical channels, (b) on the NDSM, and of the Stan dard CNN
model, (c) on the optical channels, (d) on the NDSM. The ﬁlters are not normalized to appreciate the relative importance of each channel.D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107 103
increased such that all the models would see the same number of
iterations ( i.e.mini-batches).
3.4. Zeebruges
In the case of Zeebruges, we compare RotEqNet with results
from the literature ( Campos-Taberner et al., 2016 ), as they report
results obtained with much larger architectures, both pre-trained
or learned from scratch. Since this dataset is more complex than
the previous one, we increased the model size and trained three
RotEqNet models with Nf = ½4;5;7/C138. The training schedule consisted
of 34 epochs, with the ﬁrst 12 epochs using a learning rate of
1/C110/C02, 12 more with 2 /C110/C03and 10 at 4 /C110/C04. The weight decay
for the same segments was set to 6 /C110/C02;1:2/C110/C02and 2 :4/C110/C03
respectively.
4. Results and discussion
4.1. Vaihingen
Table 1 shows the results in terms of the per class F1 scores, the
overall accuracy (OA) and average accuracy (AA) for the experi-
ments on the Vaihingen dataset. We observe that both models
reach over 87% OA when using the whole dataset, in line with
recent publications and with the accuracy obtained by RotEqNet
in the withhold test set, evaluated as 87.6% by the benchmark ser-
ver. This only drops to around 84.7% when just 4% of the trainingset is used, suggesting that this dataset is highly redundant.
The advantage of using RotEqNet becomes more apparent when
measuring the AA, mostly because of an improved accuracy detect-
ing the car class. We hypothesize that RotEqNet might be better
suited to detect objects with clear and consistent boundaries, such
as cars, because it is being forced to learn oriented ﬁlters, better
adapted to detect edge-like features. Surprisingly, RotEqNet
improves its performance gap with respect to the standard CNN
when the amount of available ground truth increases. This sug-
gests that encoding rotation invariance allows the model to con-
centrate more on solving the semantic labeling task, rather than
having to learn to be invariant to rotations.
As a comparison, we show the results recently published by
Volpi and Tuia (2017) using a much larger model and those
obtained by applying the method by Zhou et al. (2017) (ORN) with
a model of the same architecture and size as ours.
In order to glimpse at what is being learned by both models, in
Fig. 7 we show all the 7 /C27 ﬁlters learned in the ﬁrst convolutionallayer in each model. Note that the values near the corner of the ﬁl-
ters in the RotEqNet model are zero because the support of the ﬁl-
ter is a disk circumscribed in the 7 /C27 grid. Out of the six ﬁlters
learned by RotEqNet in the ﬁrst layer, three seem to have special-
ized in learning corner features involving vegetation (the reddish
tones means high response to the near infrared channel), one on
low lying impervious surfaces and two in high impervious surfaces,
which could be interpreted as rooftops. On the other hand, a
majority of the standard ﬁlters seem to be relatively less structured
and respond to some combination of color and height. We can also
see a few instances of edge detectors that have been learned in dif-
ferent orientations. Note that the particular orientation of theRotEqNet ﬁlters is arbitrary and any other rotated version could
have been learned as the canonical ﬁlter.
RotEqNet does not need to learn ﬁlters that are rotated versions
of each other because all of these versions are explored by applying
each ﬁlter at many different orientations. This means that, while
standard CNNs require data augmentation to perform well in a
rotation equivariant setting, RotEqNet extracts features at different
orientations and keeps the largest activations, effectively analyzing
the input at different orientations without rotating it explicitly.
Fig. 8 shows a few examples of the obtained classiﬁcation maps.
We see how RotEqNet performs better on smaller objects, such as
cars or the grass path in the second image, but generates less
smooth edges. The latter is possibly due to different orientation
for certain features being chosen in contiguous pixels.
4.2. Zeebruges
The results in Table 2 show the performance of the proposed
method on the Zeebruges dataset compared to the last published
results in Campos-Taberner et al. (2016) . Although the authors of
Campos-Taberner et al. (2016) were not aiming at obtaining light-
weight models, the two best results they report are obtained by
CNNs containing of the order of 10
7parameters, while RotEqNet
achieves comparable results with a mere 105parameters, two
orders of magnitude less. In particular, out the models used by
Campos-Taberner et al. (2016) , the VGG/SVM model consists of a
linear SVM classiﬁer trained on features extracted by a VGG net-
work with 2 :5/C1107parameters, while the AlexNet model is a pre-
trained CNN that has been ﬁne tuned end-to-end on the
benchmark’s training set. It has around 6 /C1107parameters. A
RotEqNet network with 1 :4/C1105parameters, with Nf = 4, was
enough to obtain better results than the VGG/SVM model, andTable 1
Results on the Vaihingen validation set. F1 scores per class and global average (AA) and overall accuracies (OA). Best result perrow is in dark gray, second in light gray.104 D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107
one with 4 :3/C1105parameters, Nf = 7, was enough to close the
accuracy gap with the ﬁne tuned AlexNet. These results highlight
the advantage in terms of model size reduction that can be
obtained by sparing the model from having to learn to be equivari-
ant to rotations. In this dataset we see again that RotEqNet per-
forms particularly well on the car and the building classes, both
associated with strong edge features, while it lags behind with
respect to both competing models in the tree class, which contains
rather anisotropic features.
4.3. Computational time
On the one hand, due to the additional burden of requiring to
interpolate the rotated ﬁlters and the linear dependency betweenthe number of orientations Rand the number of convolutions to
compute, RotEqNet can potentially increase the computational
time required with respect to a standard CNN. On the other hand,
the reduction in the number of feature maps, which is independent
ofR, can compensate for this if Ris small enough. As we can see in
Table 3 , the RotEqNet model tested on the Vaihingen validation set
and trained with R¼16 outperforms the standard CNN in terms of
speed up to R¼64. Note that all tests are performed on a single
CPU to make the results more comparable.
5. Conclusion
Deep learning models, and in particular convolutional neural
networks, have shown their potential for remote sensing image
Fig. 8. Examples of classiﬁcation maps obtained in the Vaihingen validation images with the RotEqNet and the standard CNN models.
Table 2
Results on Zeebruges. F1 scores per class and global average (AA) and overall accuracies (OA) and Cohen’s Kappa. Best result per row is in dark gray, sec ond in light gray.D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107 105
analysis. By learning ﬁlters directly from data, they allow to learn
and encode spatial information without engineering the feature
space in a problem-dependent way. But if these models have
potential, they still suffer from the need for an extensive (and com-
prehensive) training set, cumbersome hyperparameter tunning
and considerable computing resources, both in terms of memory
and operations. In this paper, we have explored the possibility ofreducing such requirements by encoding one prior information
about the images: the fact that their orientation, as well as that
of objects it contains, is often arbitrary. Such prior can be exploited
by making the CNN model rotation equivariant, i.e. by forcing the
network to react in the same way each time it encountered the
same semantic class, independently from the spatial orientation
of the features. We achieved this behavior by applying rotating
convolutions, where a canonical ﬁlter is applied at many orienta-
tions and the maximal activation is propagated through the CNN.
The proposed RotEqnNet therefore has minimal memory and stor-
age requirements, since it does not need to learn ﬁlters which
respond to each speciﬁc orientation and thus generates less inter-
mediate feature maps at runtime. Rotation equivariance is encoded
within the model itself (similarly to how CNNs achieve translation
equivariance) and propagating only maximal activations reduces
the model size and runtime memory requirements while keeping
most of the orientation information.
We applied the proposed framework to two subdecimeter land
cover semantic labeling benchmarks. The results show two main
tendencies: on one hand, that explicitly encoding rotation equiv-
ariance in deep learning dense semantic labeling models allows
for much smaller models, between one and two orders of magni-
tude compared to traditional CNNs. On the other hand, they also
show that a CNN encoding equivariance in its structure – rather
than through data augmentation – also provides robustness
against varying amounts of training data, allowing to train efﬁ-
ciently and perform well in modern remote sensing tasks. This last
point is of particular importance when considering that the
amount of available labels can vary enormously in remote sensing
depending on the mode of acquisition and the problem at hand.
RotEqNet is not limited to semantic labeling tasks. Its logic can
be applied to any deep model involving convolutions where a pre-
deﬁned behavior with respect to rotations is expected. As shown inMarcos et al. (2016) , it can be applied to various applications
requiring rotation invariance, equivariance or even covariance,
which opens doors for the application of RotEqNet to tackle prob-
lems of detection (cars, airplanes, trees) or regression (super-
resolution, biophysical parameters) when only limited labeled
instances are at hand.
Acknowledgements
This work has been partly supported by the Swiss National
Science Foundation (grant PZ00P2-136827, http://p3.snf.ch/pro-
ject-136827 ). The authors would like to thank the Belgian Royal
Military Academy for acquiring and providing the 2015 Contest
data, and the ONERA – The French Aerospace Lab – for providingthe corresponding ground-truth. They also would like to thank
the ISPRS working group II/4 for providing the data of the 2D
semantic labeling challenge over Vaihingen.
References
Audebert, N., Le Saux, B., Lefèvre, S., 2016. Semantic segmentation of earth
observation data using multimodal and multi-scale deep networks. In: Proc.
Asian Conference on Computer Vision.
Audebert, N., Le Saux, B., Lefèvrey, S., 2017. Fusion of heterogeneous data in
convolutional networks for urban semantic labeling. In: Urban Remote Sensing
Event (JURSE), 2017 Joint. IEEE, pp. 1–4 .
Campos-Taberner, M., Romero-Soriano, A., Gatta, C., Camps-Valls, G., Lagrange, A.,
Saux, B.L., Beaupère, A., Boulch, A., Chan-Hon-Tong, A., Herbin, S., Randrianarivo,
H., Ferecatu, M., Shimoni, M., Moser, G., Tuia, D., 2016. Processing of extremely
high resolution LiDAR and RGB data: outcome of the 2015 IEEE GRSS data fusion
contest. Part A: 2D contest. IEEE J. Sel. Top. Appl. Earth Obs. Rem. Sens. 9, 5547–
5559 .
Castelluccio, M.P.G., Sansone, C., Verdoliva, L., 2015. Land Use Classiﬁcation in
Remote Sensing Images by Convolutional Neural Networks, 1–11. Available
from: <arXiv 1508>.
Cheng, G., Zhou, P., Han, J., 2016. Learning rotation-invariant convolutional neural
networks for object detection in VHR optical remote sensing images. IEEE Trans.
Geosci. Rem. Sens. 54, 7405–7415 .
Cohen, T.S., Welling, M., 2016. Group Equivariant Convolutional Networks.
Available from: <arXiv:1602.07576>.
Dalla Mura, M., Atli Benediktsson, J., Waske, B., Bruzzone, L., 2010. Morphological
attribute proﬁles for the analysis of very high resolution images. IEEE Trans.
Geosci. Rem. Sens. 48, 3747–3762 .
Fauvel, M., Tarabalka, Y., Benediktsson, J., Chanussot, J., Tilton, J., 2013. Advances in
spectral-spatial classiﬁcation of hyperspectral images. Proc. IEEE 101, 652–675.https://doi.org/10.1109/JPROC.2012.2197589 .
Gerke, M., 2015. Use of the Stair Vision Library within the ISPRS 2D Semantic
Labeling Benchmark (Vaihingen). Technical Report. ITC, Univ. of Twente.
Glocer, K., Eads, D., Theiler, J., 2005. Online feature selection for pixel classiﬁcation.
In: Int. Conf. Machine Learning, Bonn, Germany.
Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep Learning. URL: < http://goodfeli.
github.io/dlbook/ > (Book in preparation for MIT Press).
Gueguen, L., Hamid, R., 2015. Large-scale damage detection using satellite imagery.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1321–1328.
Hariharan, B., Arbelaez, P., Girshick, R., Malik, J., 2015. Hypercolumns for object
segmentation and ﬁne-grained localization. In: The IEEE Conference onComputer Vision and Pattern Recognition (CVPR).
Harvey, N.R., Theiler, J., Brumby, S.P., Perkins, S., Szymanski, J.J., Bloch, J.J., Porter, R.
B., Galassi, M., Young, A.C., 2002. Comparison of GENIE and conventional
supervised classiﬁers for multispectral image feature extraction. IEEE Trans.
Geosci. Rem. Sens. 40, 393–404 .
Jaderberg, M., Simonyan, K., Zisserman, A., et al., 2015. Spatial transformer
networks. In: Advances in Neural Information Processing Systems, pp. 2017–
2025.
Lagrange, A., Le Saux, B., Beaupere, A., Boulch, A., Chan-Hon-Tong, A., Herbin, S.,
Randrianarivo, H., Ferecatu, M., 2015. Benchmarking classiﬁcation of earth-
observation data: From learning explicit features to convolutional networks. In:
Proc. IGARSS, Milan, Italy, pp. 4173–4176.
Laptev, D., Savinov, N., Buhmann, J.M., Pollefeys, M., 2016. TI-pooling:
transformation-invariant pooling for feature learning in convolutional neural
networks. Available from: <arXiv:1604.06318>.
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. Gradient-based learning applied to
document recognition. Proc. IEEE. 86 (11), 2278–2324. https://doi.org/10.1109/
5.726791 .
Leen, T.K., 1995. From data distributions to regularization in invariant learning.
Neural Comp. 7, 974–981 .
Lei, Z., Fang, T., Huo, H., Li, D., 2012. Rotation-invariant object detection of remotely
sensed images based on texton forest and hough voting. IEEE Trans. Geosci.Rem. Sens. 50, 1206–1217
.
Lenc, K., Vedaldi, A., 2015. Understanding image representations by measuring their
equivariance and equivalence. In: Proceedings of the IEEE Conf. on Computer
Vision and Pattern Recognition (CVPR), pp. 991–999.Table 3
Computational time of the forward pass in a single CPU and accuracy on the validation set of the Vaihingen dataset. The RotEqNet models are tested with d ifferent values of the
number of orientations, R.
Model RotEqNet CNN
R 8 16 32 64 128 –
OA 86.90 87.89 87.81 87.71 87.65 87.47
AA 80.69 84.34 85.18 85.33 85.51 78.18
Kappa 0.82 0.84 0.84 0.83 0.83 0.83
Time per tile (s) 1.4 1.7 2.3 3.1 5.0 4.2106 D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107
Liu, Y., Piramanayagam, S., Monteiro, S.T., Saber, E., 2017. Dense semantic labeling of
very-high-resolution aerial imagery and lidar with fully-convolutional neural
networks and higher-order CRFs. In: IEEE/CVF CVPRW Earthvision. IEEE, pp.
1561–1570 .
Long, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for semantic
segmentation. In: IEEE/CVF International Conference on Computer Vision andPattern Recognition.
Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., 2016. High-resolution aerial image
labeling with convolutional neural networks. Available from: <arXiv:1611.
01962>.
Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., 2017. Convolutional neural
networks for large-scale remote-sensing image classiﬁcation. IEEE Trans.
Geosci. Rem. Sens. 55, 645–657 .
Malek, S., Bazi, Y., Alajlan, N., AlHichri, H., Melgani, F., 2014. Efﬁcient framework for
palm tree detection in UAV images. IEEE J. Sel. Top. Appl. Earth Obs. Rem. Sens.
7, 4692–4703 .
Marcos, D., Volpi, M., Komodakis, N., Tuia, D., 2016. Rotation equivariant vector ﬁeld
networks. Available from: <arXiv:1612.09346>.
Marmanis, D., Wegner, J.D., Galliani, S., Schindler, K., Datcu, M., Stilla, U., 2016.
Semantic segmentation of aerial images with an ensemble of CNSS. ISPRS Ann.
Photogram. Rem. Sens. Spatial Inform. Sci. 3, 473–480 .
Moser, G., Serpico, S.B., Benediktsson, J.A., 2013. Land-cover mapping by Markov
modeling of spatial-contextual information. Proc. IEEE 101, 631–651 .Penatti, O., Nogueira, K., dos Santos, J.A., 2015. Do deep features generalize from
everyday objects to remote sensing and aerial scenes domains? In: IEEE/CVF
Computer Vision and Pattern Recognition Workshops, Earthvision.
Regniers, O., Bombrun, L., Lafon, V., Germain, C., 2016. Supervised classiﬁcation of
very high resolution optical images using wavelet-based textural features. IEEE
Trans. Geosci. Rem. Sens. 54, 3722–3735 .
Sherrah, J., 2016. Fully convolutional networks for dense semantic labelling of high-
resolution aerial imagery. Available from: <arXiv:1606.02585>.
Tuia, D., Courty, N., Flamary, R., 2015. Multiclass feature learning for hyperspectral
image classiﬁcation: sparse and hierarchical solutions. ISPRS J. Int. Soc. Photo.
Rem. Sens. 105, 272–285 .
Volpi, M., Ferrari, V., 2015. Semantic segmentation of urban scenes by learning local
class interactions. In: IEEE/CVF CVPRW Earthvision.
Volpi, M., Tuia, D., 2017. Dense semantic labeling of subdecimeter resolution images
with convolutional neural networks. IEEE Trans. Geosci. Rem. Sens. 55, 881–
893.
Worrall, D.E., Garbin, S.J., Turmukhambetov, D., Brostow, G.J., 2016. Harmonic
Networks: Deep Translation and Rotation Equivariance. Available from: <arXiv:1612.04642>.
Zhou, Y., Ye, Q., Qiu, Q., Jiao, J., 2017. Oriented Response Networks. Available from:
<arXiv:1701.01833>.D. Marcos et al. / ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018) 96–107 107
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789 1778
nature ecology & evolution
Articlehttps://doi.org/10.1038/s41559-023-02206-6
A high-resolution canopy height model of  
the Earth
Nico Lang    1,2 , Walter Jetz    3, Konrad Schindler    1 & Jan Dirk Wegner    1,4 
The worldwide variation in vegetation height is fundamental to the global 
carbon cycle and central to the functioning of ecosystems and their biodiversity. Geospatially explicit and, ideally, highly resolved information is required to manage terrestrial ecosystems, mitigate climate change and prevent biodiversity loss. Here we present a comprehensive global canopy height map at 10 m ground sampling distance for the year 2020. We have developed a probabilistic deep learning model that fuses sparse height data from the Global Ecosystem Dynamics Investigation (GEDI) space-borne LiDAR mission with dense optical satellite images from Sentinel-2. This model retrieves canopy-top height from Sentinel-2 images anywhere on Earth and quantifies the uncertainty in these estimates. Our approach improves the retrieval of tall canopies with typically high carbon stocks. According to our map, only 5% of the global landmass is covered by trees taller than 30 m. Further, we find that only 34% of these tall canopies are located within protected areas. Thus, the approach can serve ongoing efforts in forest conservation and has the potential to foster advances in climate, carbon and biodiversity modelling.
As our society depends on a multitude of terrestrial ecosystem ser -
vices1, the conservation of Earth’s forests has become a priority on the 
global political agenda2. To ensure sustainable development through 
biodiversity conservation and climate change mitigation, the United 
Nations have formulated global forest goals that include maintaining 
and enhancing global carbon stocks and increasing forest cover by 3% 
between 2017 and 20302. Yet global demand for commodities is driv -
ing deforestation, impeding progress towards these ambitious goals3. 
Earth observation and satellite remote sensing play a key role in this 
context, as they provide the data to monitor the quality of forested 
area at global scale4. However, to measure progress in terms of carbon 
and biodiversity conservation, novel approaches are needed that go 
beyond detecting forest cover and can provide consistent information 
about morphological traits predictive of carbon stock and biodiversity5 
at global scale. One key vegetation characteristic is canopy height5,6.
Mapping canopy height in a consistent fashion at global scale is 
key to understand terrestrial ecosystem functions, which are domi -
nated by vegetation height and vegetation structure7. Canopy-top height is an important indicator of biomass and the associated, global  
aboveground carbon stock8. At high spatial resolution, canopy height 
models (CHMs) directly characterize habitat heterogeneity9, which 
is why canopy height has been ranked as a high-priority biodiversity 
variable to be observed from space5. Furthermore, forests buffer micro -
climate temperatures under the canopy10. While it has been shown that 
in the tropics, higher canopies provide a stronger dampening effect 
on microclimate extremes11, targeted studies are needed to see if such 
relationships also hold true at global scale10. Thus a homogeneous 
high-resolution CHM has the potential to advance the modelling of 
climate impact on terrestrial ecosystems and may assist forest man -
agement to bolster microclimate buffering as a mitigation service to protect biodiversity under a warmer climate
10.
Given forests’ central relevance to life on our planet, several new 
space missions have been developed to measure vegetation struc -
ture and biomass. A key mission is the Global Ecosystem Dynamics 
Investigation (GEDI) operated by NASA (National Aeronautics and 
Space Administration), which has been collecting full-waveform LiDAR Received: 9 June 2023
Accepted: 24 August 2023
Published online: 28 September 2023
 Check for updates
1EcoVision Lab, Photogrammetry and Remote Sensing, ETH Zürich, Zürich, Switzerland. 2Department of Computer Science, University of Copenhagen, 
Copenhagen, Denmark. 3Department of Ecology and Evolutionary Biology, Yale University, New Haven, CT, USA. 4Institute for Computational Science, 
University of Zurich, Zürich, Switzerland.  e-mail: nila@di.ku.dk; jandirk.wegner@uzh.ch
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789
 1779
Article https://doi.org/10.1038/s41559-023-02206-6nearby reference data27,28. Technically, existing mapping schemes 
aggregate reflectance data over time but perform pure pixel-to-pixel 
mapping without regard to local context and image texture.
Going global . Here we extend previous regional deep learning 
methods16–18 to a global scale. These methods have been shown to 
mitigate the saturation of tall canopies by exploiting texture while not 
depending on temporal features16. Previous work has demonstrated 
that without the ability to learn spatial features, the performance 
drops substantially especially for the tall canopies16. In more detail, 
our approach employs an ensemble29 of deep, (fully) convolutional 
neural networks (CNN), each of which takes as input a Sentinel-2  
optical image and transforms it into a dense canopy height map with the 
same GSD of 10 m (ref. 16 ) (Fig. 1a). Our unified, global model is trained 
with sparse supervision, using reference heights at globally distrib -
uted GEDI footprints derived from the raw waveforms30. A dataset of  
600 million samples is constructed by extracting Sentinel-2 image 
patches of 15 × 15 pixels around every GEDI footprint acquired between 
April and August in 2019 or 2020. The sparse GEDI data is rasterized to 
the Sentinel-2 10-m grid by setting the pixel corresponding to the centre 
of each GEDI footprint to the associated footprint height. In this way, 
during model training, one can optimize the loss function with respect 
to (w.r.t.) the model parameters only at valid reference pixels (Fig. 1a ); 
whereas during map generation, the CNN model will nevertheless 
output a height prediction for every input pixel. To evaluate the model 
globally, we split the collected dataset at the level of Sentinel-2 tiles. 
Of the 100 km × 100 km regions defined by the Sentinel-2 tiling, 20% 
are held out for validation and the remaining 80% are used to train the 
model (the validation regions and the associated estimation errors are 
shown in Extended Data Fig. 1).
An important goal of our work is low estimation error for tall  
vegetation because it indicates potentially high carbon stocks. To that 
end, we extend the CNN model in three ways (Fig. 1b). First, we equip 
the model with the ability to learn geographical priors31 by feeding it 
geographical coordinates (in a suitable cyclic encoding) as additional 
input channels (Fig. 1a). Second, we employ a fine tuning strategy 
where the sample loss is re-weighted inversely proportional to the 
sample frequency per 1-m height interval so as to counter the bias in 
the reference data towards low canopies (which reflects the long-tailed 
worldwide height distribution, where low vegetation dominates and 
high values are comparatively rare). Finally, we train an ensemble of 
CNNs and aggregate estimates from repeated Sentinel-2 observa -
tions of the same location, which reduces the underestimation of tall 
canopies even further. The combination of all three measures yields 
the best performance. The average root mean square error (aRMSE) of 
the height estimates, balanced across all 5-m height intervals, is 7.3 m, 
and the average mean error (aME, that is, bias) is −1.8 m (Fig. 1b ). The 
root mean square error (RMSE) over all validation samples (without 
height balancing) is 6.0 m, with a bias of 1.3 m (Fig. 1c ). The latter is due 
to a slight overestimation of low canopy heights and is the price we pay 
for improving the performance on tall canopies (Fig. 1b).
A biome-level analysis based on the 14 biomes defined by The 
Nature Conservancy ( www.nature.org) shows how the bias varies across 
biomes (Fig. 1d and Extended Data Fig. 2). The model is able to cor -
rectly identify bare soil in deserts with zero height, with marginal error 
and no bias. The bias is also low in montane, temperate and tropical 
grasslands and in Mediterranean and tropical dry broadleaf forest, 
but higher in flooded grasslands. The most severe overestimation, on 
average ≈ 2.5 m, is observed for mangroves, tundra and tropical conifer -
ous forests. The highest spread of residuals is observed in tropical and 
temperate coniferous forests and in the tundra, where we note that the 
latter is substantially underrepresented in the dataset, as GEDI’s range 
does not extend beyond 51.6° N. Furthermore, the GEDI reference data 
in these tundra regions (southern part of Kamchatka Mountain and 
Forest Tundra and Trans-Baikal Bald Mountain Tundra) appears rather data explicitly for the purpose of measuring vertical forest structure globally, between 51.6° N and S (ref. 12). GEDI has unique potential to 
advance our understanding of the global carbon stock, but its geo -
graphical range, and also its spatial and temporal resolutions, are 
limited. The mission, initially planned to last for two years, collected four years of data from April 2019 to March 2023. The instrument will 
be stored on the International Space Station and is expected to con -
tinue collecting data in fall 2024. Independent of these interruptions, GEDI is a sampling mission expected to cover, at most, 4% of the land 
surface. By design, the collected samples sparsely cover the surface of 
the Earth, which restricts the resolution of gridded mission products to 
1 km cells12. In contrast, satellite missions such as Sentinel-2 or Landsat, 
which have been designed for a broader range of Earth observation 
needs, deliver freely accessible archives of optical images that are not as 
tailored to vegetation structure but offer longer-term global coverage 
at high spatial and temporal resolution. Sensor fusion between GEDI and multi-spectral optical imagery has the potential to overcome the limitations of each individual data source
13.
In this work, we describe a deep learning approach to map canopy-  
top height globally with high resolution, using publicly available optical 
satellite images as input. We deploy that approach to compute a global 
canopy-top height product with 10-m ground sampling distance (GSD), 
based on Sentinel-2 optical images for the year 2020. That global map 
and the underlying source code and trained models are made publicly 
available to support conservation efforts and science in disciplines 
such as climate, carbon and biodiversity modelling. The map can be 
explored interactively in this browser application: nlang.users.earth-
engine.app/view/global-canopy-height-2020 .
However, estimating forest characteristics such as canopy height 
or biomass from optical images is a challenging task14, as the physical 
relationships between spectral signatures and vertical forest struc -
ture are complex and not well understood15. Given the vast amount of 
data collected by the GEDI mission, we circumvent this lack of mecha -
nistic understanding by harnessing supervised machine learning, in  
particular end-to-end deep learning. From millions of data examples, 
our model learns to extract patterns and features from raw satellite 
images that are predictive of high-resolution vegetation structure. By 
fusing GEDI observations (that is, RH98, the relative height at which 98% 
of the energy has been returned) with Sentinel-2 images, our approach 
enhances the spatial and temporal resolution of the CHM and extends 
its geographic range to the sub-Arctic and Arctic regions outside of 
GEDI’s coverage. While retrieval of vegetation parameters with deep 
learning has been demonstrated regionally and up to country scale16–19, 
we scale it up and process the entire global landmass. This step presents 
a technical challenge but is crucial to enable operational deployment and ensure consistent, globally homogeneous data.
Results and discussion
Deep learning approach
Deep learning is revolutionizing fields ranging from medicine20 to 
weather forecasting21 and has great potential to advance environmental 
monitoring22,23, but its application to global remote sensing is techni-
cally challenging due to the large data volume22, 24. Cloud platforms 
such as Google Earth Engine25 simplify the analysis of satellite data 
but provide a limited set of traditional machine learning tools that 
depend on manual feature design. To use them, one must sacrifice some 
flexibility in terms of methods in return for easy access to large data 
archives and compute power. In particular, canopy height estimation 
with existing standard tools tends to struggle with the underestima -
tion of tall canopies, as the height estimates saturate around 25 to 30 m  
(refs. 26–28). This is a fairly severe limitation in regions dominated by 
tall canopies, such as tropical forests, and deteriorates downstream car -
bon stock estimation, because tall trees have especially high biomass6.  
A further restriction of prior large-scale CHM projects is that they rely 
on local calibration, which hampers their use in locations without 
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789 1780
Article https://doi.org/10.1038/s41559-023-02206-6noisy and contaminated with a notable number of outliers (Extended 
Data Fig. 2).
Comparison to existing canopy height estimates. We compare our 
estimates with a state-of-the-art global-scale canopy-top height map (henceforth referred to as UMD) that has been derived by combining 
GEDI data (RH95) with Landsat image composites27. This UMD map relies on local model fitting and is created by combining the results 
of multiple regional models, which means it is not available beyond 
the GEDI coverage north of 51.6° latitude. For a fair comparison the 
UMD map with 30 m GSD is re-projected to the same Sentinel-2 10-m grid. Overall, our map reduces the underestimation bias from − 7.1 m to − 1.7 m w.r.t. the hold-out GEDI validation data (in total, 87 million 
footprints) when averaging the bias across height intervals (Extended 
ca
bInput
Sentinel-2 + geo coordinatesCanopy-top height
Target
GEDI reference
Sparse
supervision
Variance
RMSE: 6.0, MAE: 4.0, ME: 1.3Number of samples
GEDI reference height (m)1 × 1 convFeature representation
1 × 1 convCNN
25
S2 only (aRMSE: 8.3, aMAE: 7.1, aME: –5.4)
(aRMSE: 7.6, aMAE: 6.3, aME: –4.3)
(aRMSE: 7.3, aMAE: 5.8, aME: –3.1)(aRMSE: 7.3, aMAE: 5.5, aME: –1.8)S2 + geoS2 + geo balancedFinal ensembleResiduals (m)
GEDI reference height (m)
Mangroves
Deserts
Mediterranean forest
Tundra
Montane grassland
Flooded grassland
Temperate grassland
Tropical grassland
Boreal Biome
Temperate coniferous
Temperate broadleaf
Tropical coniferous
Tropical dry broadleaf
Tropical moist broadleaf20
15
10
5
0
–5
–10
–15
–20
–2570 105
104
103
102
101
100Estimated height from Sentinel-2 (m)60
50
40
30
20
10
0
0 10 20 30 40 50 60 70 0 5 10 15 20 25 30 35 40 45
GEDI reference height (m)10 15 20 25
Residuals (m)–5 0 5
Number of samples10510610710 15 30 35 0 550lat
sin(π lon/180)
cos(π lon/180)
c
d
Fig. 1 | Model overview and global model evaluation on held-out GEDI 
reference data. a, Illustration of the model training process with sparse 
supervision from GEDI LiDAR. The CNN takes the Sentinel-2 (S2) image and encoded geographical coordinates (lat, lon) as an input to estimate dense 
canopy-top height and its predictive uncertainty (variance). The two model 
outputs are estimated from the shared feature representation with separate convolutional layers (conv). b , Residual analysis w.r.t. canopy height intervals and 
ablation study of model components. Negative residuals indicate that estimates are lower than reference values. The boxplot shows the median, the quartiles and the 10th and 90th percentiles (n  = 88, 332, 537). RMSE, root mean square error; 
MAE, mean absolute error; ME, mean error (i.e. bias). aRMSE, aMAE and aME are the balanced versions of these metrics, where the metric is computed separately in each 5-m height interval and then averaged across all intervals. c , Confusion 
plot for the final model ensemble, showing good agreement between predictions from Sentinel-2 and GEDI reference. d , Biome-level analysis of final ensemble 
estimates: GEDI reference height, residuals and number of samples per biome. The boxplots show the median, the quartiles and the 10th and 90th percentiles (n = 88, 332, 537).
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789
 1781
Article https://doi.org/10.1038/s41559-023-02206-6Data Fig. 3a). The UMD map underestimates the reference data over 
the entire height range starting from 5 m canopy height, whereas the 
underestimation increases for canopy heights >30 m. While the UMD 
map has negligible bias for low vegetation <5 m, our map tends to overestimate some of the vegetation <5 m. Furthermore, our map 
has an overestimation bias of ≈ 2 m for heights ranging from 5 to 20 m 
and a bias of <1 m from 20 to 35 m. Starting from vegetation 35 m 
tall upwards, the negative bias grows with canopy-top height but is  
substantially lower compared to the UMD map. The bias also varies 
across biomes (Extended Data Fig. 3b). In most of the cases where the 
UMD map tends to underestimate the GEDI reference height, our map 
tends to overestimate. Exceptions where our map has a low bias of <1 m 
are: tropical dry broadleaf, tropical grassland, temperate grassland, 
flooded grasslands, montane grassland and Mediterranean forests. 
It is worth noting that our global model did not see these validation 
regions (each 100 × 100 km) during training; in contrast, the UMD 
approach fits a local model for each region.
Evaluation with independent airborne LiDAR. In addition, we com-
pare our final map (and the UMD map) with independent reference 
data from two airborne LiDAR sources: (1) NASA’s Land, Vegetation, 
and Ice Sensor (LVIS) airborne LiDAR campaigns32, which were designed 
to deliver canopy-top heights comparable to GEDI12 and (2) GEDI-like 
canopy-top height retrieved from high-resolution canopy height 
models derived from small-footprint airborne laser scanning (ALS) 
campaigns in Europe33. We report error metrics within 24 Sentinel-2 tile 
regions (12 each for LVIS and ALS) from 11 countries in North and Central 
America and Europe (Extended Data Table 1) covering a diverse range 
of vegetation heights and biomes (Extended Data Fig. 4). In nine out of 
the 12 regions with UMD map data, our map yields lower random error 
(RMSE and mean absolute error (MAE)) and bias (mean error). While 
the UMD map underestimates the airborne LiDAR data in all regions, our map tends to overestimate the reference data in most, but not all, 
cases. The strongest differences are observed in regions with high 
average canopy height (that is 28–36 m in the United States (Oregon) and Gabon) where the UMD map has a bias ranging from −19 to −23% w.r.t. the average height, and our map yields a bias of −4% to −13% and 
7% to 10%. In the rare cases where the underestimation bias of UMD 
map is lower than the overestimation bias of ours, we observe qualita -
tively that our map captures structure within high vegetation, where 
the UMD map saturates (for example, Netherlands; Extended Data  
Fig. 7c). Further qualitative comparisons against LVIS and ALS data are 
presented in Extended Data Fig. 7a–f. Comparing the mean error over 
regions within the GEDI coverage reveals that our map outperforms the 
UMD map on all error metrics. The UMD map yields an RMSE of 9.1 m and a bias of −4.5 m (−25.2%) and our map an RMSE of 7.9 m and a bias of 1.7 m (16.2%) (Extended Data Table 2).
Our approach allows us to map beyond the northernmost latitude 
with GEDI data for which we have 12 regions with independent airborne 
LiDAR data (Extended Data Table 1). Here we find a mean RMSE of 
5.3 m and a bias of 0.5 m (19.4%) (Extended Data Table 2). In the north-ernmost region in Finland at 70° N latitude, the dominant vegetation 
structure is captured in our map with an RMSE of 3.0 m and a bias of 
0.5 m (17.1%) (for example, Extended Data Fig. 8a). In other words, our 
estimates agree well with independent LVIS and ALS data, even outside 
the geographic range of the GEDI training data (qualitative examples in Extended Data Fig. 8a–f).
Modelling predictive uncertainty
Whereas deep learning models often exhibit high predictive skill and 
produce estimates with low overall error, the uncertainty of those esti -
mates is typically not known or unrealistically low (that is, the model 
is over-confident)34. But reliable uncertainty quantification is crucial 
to inform downstream investigations and decisions based on the map 
content8; for example, it can indicate which estimates are too uncertain and should be disregarded30. To afford users of our CHM a trustworthy,  
spatially explicit estimate of the map’s uncertainty, we integrate 
probabilistic deep learning techniques. These methods are specifi -
cally designed to quantify also the predictive uncertainty, taking into 
account, on the one hand, the inevitable noise in the input data and, 
on the other hand, the uncertainty of the model parameters resulting 
from the lack of reference data for certain conditions35. In particular, we 
independently train five deep CNNs that have identical structure but are 
initialized with different random weights. The spread of the predictions 
made by such a model ensemble29 for the same pixel is an effective way 
to estimate model uncertainty (also known as epistemic uncertainty), 
even with small ensemble size36. Each individual CNN is trained by maxi -
mizing the Gaussian likelihood rather than minimizing the more widely 
used squared error. Consequently, each CNN outputs not only a point 
estimate per pixel but also an associated variance that quantifies the 
uncertainty of that estimate (a.k.a. its aleatoric uncertainty)35.
During inference, we process images from ten different dates  
(satellite overpasses) within a year at every location to obtain full 
coverage and exploit redundancy for pixels with multiple cloud-free observations. Each image is processed with a randomly selected CNN 
within the ensemble, which reduces computational overhead and can 
be interpreted as natural test-time augmentation, known to improve the calibration of uncertainty estimates with deep ensembles
37.
Finally, we use the estimated aleatoric uncertainties to merge 
redundant predictions from different imaging dates by weighted  
averaging proportional to the inverse variance. While inverse-variance 
weighting is known to yield the lowest expected error38, we observe 
a deterioration of the uncertainty calibration for low values (<4 m  
standard deviation in Extended Data Fig. 5a). We also note that uncer-
tainty calibration varies per biome (Extended Data Fig. 5c), so it may be 
advisable to re-calibrate in post-processing depending on the intended 
application and region of interest. Despite these observations, the 
estimated predictive uncertainty correlates well with the empirical 
estimation error and can therefore be used to filter out inaccurate 
predictions, thus lowering the overall error at the cost of reduced com -
pleteness (Extended Data Fig. 5b). For example, by filtering out the 20% 
most uncertain canopy height estimates, overall RMSE is reduced by 13% 
(from 6.0 m to 5.2 m) and the bias is reduced by 23% (from 1.3 m to 1.0 m).
Interestingly, the tendency to overestimate some of the low veg -
etation <5 m can also be removed by using our estimated uncertainty 
to filter out, for example, the 20% of validation points with the high -
est relative standard deviation (‘ETH (ours) 80%’ in Extended Data 
Fig. 3a). Here we follow the filtering protocol proposed in previous 
work using an adaptive threshold depending on the predicted canopy 
height to preserve the full canopy height range30. The ability to identify 
erroneous estimates based on the predictive uncertainty is a unique 
characteristic of our proposed methodology and allows us to reduce random errors and biases (Extended Data Fig. 3).
Global canopy height map
The model has been deployed globally on the Sentinel-2 image archive 
for the year 2020 to produce a global map of canopy-top height. To cover 
the global landmass ( ≈ 1.3 × 1012 pixels at the GSD of Sentinel-2), a total 
of ≈ 160 terabytes of Sentinel-2 image data are selected for processing. 
This required ≈ 27,000 graphics processing unit (GPU) hours ( ≈ 3 GPU 
years) of computation time, parallelized on a high-performance cluster 
to obtain the global map in ten days real time.
The new dense canopy height product at 10-m GSD makes it  
possible to gain insights into the fine-grained distribution of canopy 
heights anywhere on Earth and the associated uncertainty (Fig. 2). Three 
example locations (A–C in Fig. 2 ) demonstrate the level of canopy detail 
that the map reveals, ranging from harvesting patterns from forestry 
in Washington state, United States (A), through gallery forests along 
permanent rivers and ground water in the forest–savannah of northern 
Cameroon (B), to dense tropical broadleaf forest in Borneo, Malaysia (C).
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789 1782
Article https://doi.org/10.1038/s41559-023-02206-6Also at large scale, the predictive uncertainty is positively  
correlated with the estimated canopy height (Fig. 2b ). Still, some 
regions such as Alaska, Yukon (northwestern Canada) and Tibet exhibit 
high predictive uncertainty, which cannot be explained only by the 
canopy height itself. The two former lie outside of the GEDI coverage, 
so the higher uncertainty is probably due to local characteristics that 
the model has not encountered during training. The latter indicates 
that also within the GEDI range, there are environments that are more  
challenging for the model, for example, due to globally rare ecosystem 
characteristics not encountered elsewhere. Ultimately, all three regions 
might be affected by frequent cloud cover (and snow cover), limiting 
the number of repeated observations. Qualitative examples with high 
uncertainty, but reasonable canopy-top height estimates, for Alaska 
are presented in Extended Data Fig. 8e,f.
Our new dataset enables a full, worldwide assessment of coverage 
of the global landmass with vegetation. Doing this for a range of thresh -
olds recovers an estimate of the global canopy height distribution (for the year 2020; Fig. 3a and Extended Data Fig. 6a). We find that an area 
of 53.6 × 106 km2 (41% of the global landmass) is covered by vegetation 
with >5 m canopy height, 39.6 × 106 km2 (30%) by vegetation >10 m and 
6.7 × 106 km2 (5%) by vegetation >30 m (Fig. 3b ).
We see that protected areas (according to the World Database 
on Protected Areas, WDPA39) tend to contain higher vegetation com-
pared to the global average (Fig. 3a ). Furthermore, 34% of all canopies 
>30 m fall into protected areas (Fig. 3a ). Extended Data Fig. 6b provides 
examples of protected areas that show good agreement with mapped 
canopy height patterns. This analysis highlights the relevance of the 
new dataset for ecological and conservation purposes. For instance, 
canopy height and its spatial homogeneity can serve as an ecological 
indicator to identify forest areas with high integrity and conservation 
value. That task requires both dense area coverage at reasonable resolu -
tion and a high saturation level to locate very tall vegetation.
Finally, our new map makes it possible to analyse the exhaus -
tive distributions of canopy heights at the biome level, revealing 
Canopy-top height (m)
Standard deviation (m)
A
BCA
BC012 km
0 1 2 km
012 km
0 1 2 km
00.5 1.0 km00.5 1 km
>15
0>50
0A
Ba
bC
A
B
C
Fig. 2 | Global canopy height map for the year 2020. The underlying data 
product, estimated from Sentinel-2 imagery, has 10-m ground sampling distance and is visualized in Equal Earth projection. a , Canopy-top height. b , Predictive 
standard deviation of canopy-top height estimates. Location A details harvesting patterns in Washington state, United States. Location B shows gallery forests along permanent rivers and ground water in the forest–savannah of northern Cameroon. Location C shows tropical broadleaf forest in Borneo, Malaysia.
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789
 1783
Article https://doi.org/10.1038/s41559-023-02206-6characteristic frequency distributions and trends within different 
types of terrestrial ecosystem (Fig. 3b ). While, for instance, the canopy 
heights of tropical moist broadleaf forests follow a bimodal distribu-
tion with a mode >30 m, mangroves have a unimodal distribution with 
a large spread and heights ranging up to >40 m. Notably, our model has 
learned to predict reasonable canopy heights in tundra regions, despite 
scarce and noisy reference data for that ecosystem.
Discussion
The evaluation with hold-out GEDI reference data and the compari -
son with independent airborne LiDAR data show that the presented 
approach yields a new, carefully quality-assessed state-of-the-art global 
map product that includes quantitative uncertainty estimates. But 
the retrieval of canopy height from optical satellite images remains a 
challenging task and has its limitations.
Limitations and map quality. We note that despite the nominal 10-m 
ground sampling distance of our global map, the effective spatial  
resolution at which individual vegetation features can be identified is 
lower. As a consequence of the GEDI reference data used to train the 
model, each map pixel effectively indicates the largest canopy-top 
height within a GEDI footprint ( ≈ 25 m diameter) centred at the 
pixel. Two subtle reasons further impact the effective resolution,  
compared to a map learned from dense, local reference data (for example,  
airborne LiDAR16): the sparse supervision means that the model never 
explicitly sees high-frequency variations of the canopy height between 
adjacent pixels, and misalignments caused by the geolocation uncer-
tainty (15–20 m) of the GEDI version 1 data12,40,41 introduce further noise. 
While at present, we do not see this as severely limiting the utility of 
the map, in the future one could consider extending the method with 
techniques for guided super resolution42 to better preserve small  
features visible in the raw Sentinel-2 images, such as canopy gaps. Further -
more, using the latest GEDI release version with improved geolocation  
accuracy41 may improve the measured performance.
Regarding map quality, besides minor artefacts in regions with 
persistent cloud cover, we observe tiling artifacts at high latitudes in the northern hemisphere. The systematic inconsistencies at tile bor-
ders point at degradation of the absolute height estimates, possibly 
caused by a lack of training data for particular, locally unique vegetation 
structures. Interestingly, it appears that a notable part of these errors are constant offsets between the tiles.Whereas our approach substantially reduces the error for tall 
canopies representing potentially high carbon stocks, we observe a  
tendency to overestimate some areas with very low canopy heights 
(<5 m). However, future downstream applications may use additional land cover masks or predictive uncertainty to identify and filter these 
biased estimates. The modelled spatially explicit uncertainty can be 
used to identify erroneous estimates. On a global scale, we observe 
that some regions are subject to overall high predictive uncertainty 
including tropical regions such as Papua New Guinea, but also regions 
in northern latitudes, for example, Alaska. This shows the importance 
of modelling the predictive uncertainty to allow transparent and 
informed use of the canopy height map. It also indicates that there 
are limits when using optical images as the only predictor for canopy 
height estimation and that future work could explore the combination 
with additional predictive environmental data to resolve ambiguities in visual observations.
Potential applications.  In the context of the GEDI mission goals12, 
our presented canopy height map may be used to fill the gaps in the 
gridded products at 1-km resolution where no GEDI tracks are avail-
able43, 44. According to the Global Forest Resources Assessment 202045, 
31% of the global land area is covered by forests. Whereas our new 
global canopy height map can contribute to global forest cover esti -
mates, such a forest definition relies not solely on canopy height, 
but also on connectivity and includes areas with trees likely to reach 
a certain height. Therefore, these derivations require more in situ 
data and threshold calibration. Furthermore, there are at least two 
major downstream applications that the new high-resolution canopy 
height dataset can help to advance at global scale, namely biomass 
and biodiversity modelling. Furthermore, our model can support 
monitoring of forest disturbances. Canopy-top height is a key indica-tor to study the global aboveground carbon stock stored in the form 
of biomass8. On a local scale, we compare our canopy height map with 
dense aboveground carbon density data46 that was produced by a tar -
geted airborne LiDAR campaign in Sabah, northern Borneo47 (Fig. 4a). 
We observe that for natural tropical forests, the spatial patterns agree 
well and that our canopy height estimates from Sentinel-2 are predictive 
of carbon density even in tropical regions, with canopy heights up to 65 m. Notably, the relationship between carbon and canopy height is 
sensitive up to ≈ 60 m. We note that although it is technically possible 
to map biomass at 10-m ground sampling distance, this may not be 
40 Total landmass
Protected areas
Protected fraction 20
04050Canopy height from Sentinel-2 (m)
Canopy height from Sentinel-2 (m)
Frequency Cumulative rel. freq.Trop. moist
broadleafTrop. dry
broadleafTemp.
broadleafTrop.
coniferousTemp.
coniferous BorealTrop.
grassland
Tundra
FrequencyMediterranean
forest Deserts MangrovesFlooded
grasslandMontane
grasslandTemp.
grasslandaa b
30
10
100%20
040
20
0
Fig. 3 | Global canopy height distributions of the entire landmass, protected 
areas and biomes. a, Frequency distribution and cumulative distribution 
(relative frequency) for the entire global landmass and within protected areas (according to WDPA
39) and fraction of vegetation above a certain height that is 
protected. b, Biome-level frequency distribution of canopy heights according to 14 terrestrial ecosystems defined by The Nature Conservancy. Urban areas and croplands (based on ESA WorldCover
58) have been excluded. Abbreviations are 
used for tropical (trop.) and temperate (temp.) biomes. Supplementary Fig. 1 shows the distributions for canopy heights >1 m.
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789 1784
Article https://doi.org/10.1038/s41559-023-02206-6meaningful in regions such as the tropics, with dense vegetation where 
single tree crowns may exceed a 10-m pixel. It is rather recommended 
to model biomass at coarser spatial resolutions (for example 0.25 ha) suitable to capture the variation of dense vegetation areas. Neverthe-
less, high-resolution canopy height data has great potential to improve 
global biomass estimates by providing descriptive statistics of the 
vegetation structure within local neighbourhoods.
We further demonstrate that our model can be deployed annually  
to map canopy height change over time, for example, to derive 
changes in carbon stock and estimate carbon emissions caused by 
global land-use changes, at present mainly deforestation48. Annual 
canopy height maps are computed for a region in northern Califor -
nia where wildfires have destroyed large areas in 2020 (Fig. 4b ). Our 
automated change map corresponds well with the mapped fire extent 
from the California Department of Forestry and Fire Protection ( www.
fire.ca.gov ), while at the same time the annual maps are consistent 
in areas not affected by the fires, where no changes are expected. 
While the sensitivity of detectable changes such as annual growth 
might be limited by the model accuracy and remains to be evaluated 
(for example, with repeated GEDI shots or LiDAR campaigns), such 
high-resolution change data may potentially help to reduce the high uncertainty of emissions estimates that are reported in the annual 
Global Carbon Budget48. It is worth mentioning that the presented 
approach yields reliable estimates based on single cloud-free Sentinel-2 
images. Thus, its potential for monitoring changes in canopy height 
is not limited to annual maps but to the availability of cloud-free 
images that are taken at least every five days globally. This high update  
frequency makes it relevant for, for example, real-time deforestation alert systems, even in regions with frequent cloud cover.
A second line of potential applications includes biodiversity  
modelling as the high spatial resolution of the canopy height model 
brings about the possibility to characterize habitat structure and  
vegetation heterogeneity, known to promote a number of ecosys -
tem services49 and to be predictive of biodiversity50,51. The relation -
ship between heterogeneity and species diversity is founded in niche 
theory50,51, which suggests that heterogeneous areas provide more 
ecological niches for different species to co-exist. Our dense map 
makes it possible to study second-order homogeneity9 (which is not 
easily possible with sparse data like GEDI) and down to a length scale of 10 to 20 m.
Technically, our dense high-resolution map makes it a lot  
easier for scientists to intersect sparse sample data, for example, field 
Canopy  height from Sentinel-2
Canopy 
height (m)
>50
0
>50
030
–30>250
0
050100150200250300Aboveground carbon density (Mg C ha−1)
0 10
Canopy height from Sentinel-2 (m)20 30 40 50 60Aboveground carbon density
ACD 
(Mg C ha−1)
20190 20 40 km
0 10 20 kmCanopy height (m)2021
Wildfire extent 2020‘August Complex’Heightchange (m)Diﬀerence 2021–2019a
b
Fig. 4 | Examples for potential applications. a, Biomass and carbon stock 
mapping. In Sabah, northern Borneo, canopy height estimated from Sentinel-2 
optical images correlates strongly with aboveground carbon density (ACD) from a targeted airborne LiDAR campaign
47 (5.3812° N, 117.0748° E). The boxplot shows 
the median, the quartiles and the 10th and 90th percentiles (n  = 339, 835, 325). b, Monitoring environmental damages. In 2020, wildfires destroyed large areas 
of forests in northern California. The difference between annual canopy height maps is in good agreement with the wildfire extent mapped by the California Department of Forestry and Fire Protection (40.1342° N, 123.5201° W).
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789
 1785
Article https://doi.org/10.1038/s41559-023-02206-6plots, with canopy height. To make full use of scarce field data in bio -
mass or biodiversity research, dense complementary maps are a lot 
more useful: when pairing sparse field samples with other sparsely  
sampled data, the chances of finding enough overlap are exceedingly 
low; whereas pairing them with low-resolution maps risks biases due to the large-scale difference and associated spatial averaging.
Conclusion
We have developed a deep learning method for canopy height retrieval 
from Sentinel-2 optical images. That method has made it possible to 
produce a globally consistent, dense canopy-top height map of the 
entire Earth, with 10-m ground sampling distance. Besides Sentinel-2, 
the GEDI LiDAR mission also plays a key role as the source of sparse but 
uniquely well-distributed reference data at global scale. Compared to 
previous work that maps canopy height at global scales27, our model 
substantially reduces the overall underestimation bias for tall cano -
pies. Our model does not require local calibration and can therefore 
be deployed anywhere on Earth, including regions outside the GEDI 
coverage. Moreover, it also delivers spatially explicit estimates for 
the predictive uncertainties of the retrieved canopy heights. As our 
method, once trained, needs only image data, maps can be updated 
annually, opening up the possibility to track the progress of commit-ments made under the United Nation’s global forest goals to enhance 
carbon stock and forest cover by 20302. At the same time, the longer 
the GEDI mission will collect on-orbit data, the denser the reference 
data for our approach will become, which can be expected to dimin -
ish the predictive uncertainty and improve the effective resolution of its estimates.
As a possible future extension, our model could be extended to 
map other vegetation characteristics17 at global scale. In particular, 
it appears feasible to densely map biomass by retraining with GEDI 
L4A biomass data8 or by adding additional data from planned future 
space missions52.
Whereas deep learning technology for remote sensing is  
continuously being refined by focusing on improved performance at 
regional scale, its operational utility has been limited by the fact that 
it often could not be scaled up to global coverage. Our work demon -
strates that if one has a way of collecting globally well-distributed 
reference data, modern deep learning can be scaled up and employed 
for global vegetation analysis from satellite images. We hope that 
our example may serve as a foundation on which new, scalable 
approaches can be built that harness the potential of deep learning 
for global environmental monitoring and that it inspires machine 
learning researchers to contribute to environmental and conserva-tion challenges.
Methods
Data
This work builds on data from two ongoing space missions: the  
Copernicus Sentinel-2 mission operated by the European Space Agency 
(ESA) and NASA’s Global Ecosystem Dynamics Investigation (GEDI). The 
Sentinel-2 multi-spectral sensor delivers optical images covering the global landmass with a revisit time of, at most, five days on the equa-tor. We use the atmospherically corrected L2A product, consisting of 12 bands ranging from the visible and near infrared to the short wave 
infrared. While the visible and near infrared bands have 10-m GSD, 
the other bands have a 20-m or 60-m GSD. For our purposes, all bands 
are upsampled with cubic interpolation to obtain a data cube with 
10-m ground sampling distance. The GEDI mission is a space-based 
full-waveform LiDAR mounted on the International Space Station 
and measures vertical forest structure at sample locations with a 25-m 
footprint, distributed between 51.6° N and S. We use footprint-level 
canopy-top height data derived from these waveforms as sparse refer -
ence data30,53. The canopy-top height is defined as RH98, the relative 
height at which 98% of the energy has been returned, and was derived from GEDI L1B version 1 waveforms54 collected between April and 
August in the years 2019 and 2020.
To train the deep learning model, a global training dataset has been 
constructed within the GEDI range by combining the GEDI data and the 
Sentinel-2 imagery. For every Sentinel-2 tile, we select the image with 
the least cloud coverage between May and September 2020. Thus, the 
model is trained to be invariant against phenological changes within this period but is not designed to be robust outside of this period for 
regions experiencing high seasonality. Ultimately, the annual maps are 
computed on Sentinel-2 images from the same period. Image patches 
of 15 × 15 pixels (that is, 150 m × 150 m on the ground) are extracted 
from these images at every GEDI footprint location. Therefore, the 
GEDI data are rastered to the Sentinel-2 pixel grid by setting the canopy 
height reference value of the pixel that corresponds to the centre of the 
GEDI footprint. Locations for which the image patch is cloudy or snow 
covered are filtered out from the dataset. To correct noise injected by 
the geolocation uncertainty of the GEDI version 1 data41, we use the 
Sentinel-2 L2A scene classification and assign 0 m canopy height to 
footprints located in the categories ‘not vegetated’ or ‘water’ . This 
procedure also addresses the slight positive bias due to slope in the 
GEDI reference data30. Overall, the resulting dataset contains 600 × 106 
samples globally distributed within the GEDI range. All samples located 
within 20% of the Sentinel-2 tiles in that range (each 100 × 100 km) are 
set aside as validation data (Extended Data Fig. 1).
A second evaluation is carried out w.r.t. canopy-top heights inde -
pendently derived from airborne LiDAR data from two sources. This 
includes NASA’s LVIS. LVIS is a large-footprint full-waveform LiDAR 
from which the LVIS L2 height metric RH98 is rastered to the Sentinel-2 
10-m grid. The second source is high-resolution canopy height models  
(1-m GSD) derived from small-footprint ALS campaigns33. To derive 
a comparable ‘GEDI-like’ canopy-top height metric within the 25-m 
footprint, we first run a circular max-pooling filter with a 12-m radius 
at the 1-m GSD resolution with a stride of 1 pixel (that is 1 m) before 
we resample the canopy height models to the Sentinel-2 10-m GSD  
(Supplementary Fig. 5 illustration). This processing is necessary as 
the maximum canopy height depends on the footprint size and avoids 
the comparison of systematically different canopy height metrics.  
Locations of the LVIS and ALS data are visualized in Extended Data Fig. 4.
Deep fully convolutional neural network
Our model is based on the fully convolutional neural network  
architecture proposed in prior work16. The architecture employs a 
series of residual blocks with separable convolutions55 without any 
downsampling within the network. The sequence of learnable 3 × 3 
convolutional filters is able to extract not only spectral but also textural 
features. To speed up the model for deployment at global scale, we 
reduce its size, setting the number of blocks to eight and the number of filters per block to 256. This speeds up the forward pass by a factor 
of ≈ 17 compared to the original, larger model. In our tests, the smaller 
version did not cause higher errors in an early phase of training. When 
trained long enough, a larger model with higher capacity may be able 
to reach lower prediction errors, but the higher computational cost 
of inference would limit its utility for repeated, operational use. The 
model takes the 12 bands from Sentinel-2 L2A product and the cyclic 
encoded geographical coordinates per pixel as input for a total of 15 
input channels. Its outputs are two channels with the same spatial 
dimension as the input, one for the mean height and one for its variance 
(Fig. 1). Because the architecture is fully convolutional, it can process 
arbitrarily sized input image patches, which is useful when deploying at large scale.
Model training with sparse supervision
Formally, canopy height retrieval is a pixel-wise regression task. We train 
the regression model end to end in supervised fashion, which means that 
the model learns to transform raw image data into spectral and textural 
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789 1786
Article https://doi.org/10.1038/s41559-023-02206-6features predictive of canopy height, and there is no need to manually 
design feature extractors (Supplementary Fig. 3). We train the convo -
lutional neural network with sparse supervision, that is, by selectively 
minimizing the loss (equation (1 )) only at pixel locations for which there 
is a GEDI reference value. Before feeding the 15-channel data cube to 
the CNN, each channel is normalized to be standard normal, using the 
channel statistics from the training set. The reference canopy heights are 
normalized in the same way, a common pre-processing step to improve 
the numerical stability of the training. Each neural network is trained for 
5,000,000 iterations with a batch size of 64, using the Adam optimizer56. 
The base learning rate is initially set to 0.0001 and then reduced by factor 
0.1 after 2,000,000 iterations and again after 3,500,000 iterations. This 
schedule was found to stabilize the uncertainty estimation.
Modelling the predictive uncertainty
Modelling uncertainty in deep neural networks is challenging due to 
their strong nonlinearity but crucial to build trustworthy models. The 
approach followed in this work accounts for two sources of uncertainty, 
namely the data (aleatoric) and the model (epistemic) uncertainty35. The 
uncertainty in the data, resulting from noise in the input and reference 
data, is modelled by minimizing the Gaussian negative log likelihood 
(equation (1 )) as a loss function35. This corresponds to independently 
representing the model output at every pixel i  as a conditional Gaussian 
probability distribution over possible canopy heights, given the input 
data, and estimating the mean ̂μ and variance ̂σ2 of that distribution.
ℒNLL=1
NN
∑
i=1(̂μ(xi)−yi)2
2̂σ2(xi)+1
2loĝσ2(xi). (1)
To account for the model uncertainty, which in high-capacity 
neural network models can be interpreted as the model’s lack of knowl -
edge about patterns not adequately represented in the training data, 
we train an ensemble29 of five CNNs from scratch, that is, each time 
starting the training from a different randomly initialized set of model 
weights (learnable parameters). At inference time, we process images 
from T different acquisition dates (here T  = 10) for every location to 
obtain full coverage and to exploit redundancy in the case of repeated 
cloud-free observations of a pixel. Each image is processed with one 
CNN picked randomly from the ensemble. This procedure incurs no 
additional computational cost compared to processing all images with 
the same CNN. It can be interpreted as a natural variant of test-time 
augmentation, which has been demonstrated to improve the calibra-
tion of uncertainty estimates from deep ensembles in the domain of 
computer vision37. Finally, the per-image estimates are merged into a 
final map by averaging with inverse-variance weighting (equation ( 3)). 
If the variance estimates of all ensemble members are well calibrated, 
this results in the lowest expected error38. Thus the variance of the final 
per-pixel estimate is computed with the weighted version of the law of 
total variance (equation ( 4))35. For readability we omit the pixel index i .
̂pt=1/̂σt2
∑j=1T1/̂σj2, (2)
̂y=T
∑
t=1̂pt̂μt, (3)
Var(̂y)=T
∑
t=1̂pt̂μt2−(T
∑
t=1̂pt̂μt)2
+T
∑
t=1̂pt̂σt2, (4)
Correction for imbalanced height distribution
We find that the underestimation bias on tall canopies is partially 
due to the imbalanced distribution of reference labels (and canopy heights overall), where large height values occur rarely. To mitigate 
it, we fine tune the converged model with a cost-sensitive version of 
the loss function. A softened version of inverse sample-frequency 
weighting is used to re-weight the influence of individual samples on 
the loss (equation (5 )). To establish the frequency distribution of the 
continuous canopy height values in the training, we bin them into 1-m 
height intervals and in each of the resulting K  bins count the number 
of samples Nk. Empirically, for our task, the moderated reweighting 
with the square root of the inverse frequency works better (leaving all 
other hyper-parameters unchanged). Moreover, we do not fine tune 
all model parameters but only the final regression layer that computes 
mean height (Fig. 1a). We observe that the uncertainty calibration is 
preserved when fine tuning only the regression weights for the mean 
(‘S2 + geo balanced: mean’ in Extended Data Fig. 5a), whereas fine 
tuning also the regression of the variance decalibrates the uncertainty 
estimation (‘S2 + geo balanced: mean&var’). The fine tuning is run for 
750,000 iterations per network.
qi=√1/Nk,i∈k
∑K
j=1√1/Nj(5)
Evaluation metrics
Several metrics are employed to measure prediction performance: the RMSE (equation (6 )) of the predicted heights, their MAE (equa-
tion ( 7)) and their mean error (ME, equation (8 )). The latter quantifies 
systematic height bias, where a negative ME indicates underesti -
mation, that is, predictions that are systematically lower than the 
reference values.
RMSE=√√√
√1
NN
∑
i=1(̂yi−yi)2(6)
MAE=1
NN
∑
i=1|̂yi−yi| (7)
ME=1
NN
∑
i=1(̂yi−yi) (8)
We also report balanced versions of these metrics, where the 
respective error is computed separately in each 5-m height interval 
and then averaged across all intervals. They are abbreviated as aRMSE, 
aMAE and aME (Fig. 1b ).
The bias analyses with the independent airborne LiDAR data include 
the normalized mean error (NME, equation ( 7)) in percentage, where the 
mean error is divided by the average of the reference values ̄y:
NME=100
̄yNN
∑
i=1(̂yi−yi) (9)
For the estimated predictive uncertainties, there are, by definition, 
no reference values. A common scheme to evaluate their calibration 
is to produce calibration plots34,57 that show how well the uncertain -
ties correlate with the empirical error. As this correlation holds only 
in expectation, both the uncertainties and the empirical errors at the 
test samples must be binned into K  equally sized intervals. In each 
bin Bk, the average of the predicted uncertainties is then compared 
against the actual average deviation between the predicted height and 
the reference data. On the basis of the calibration plots, it is further  
possible to derive a scalar error metric for the uncertainty calibra -
tion, the uncertainty calibration error (UCE) (equation (10))57. Again, 
we additionally report a balanced version, the average uncertainty 
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789
 1787
Article https://doi.org/10.1038/s41559-023-02206-6calibration error (AUCE) (equation (11 )), where each bin has the same 
weight independent of the number Nk of samples in it.
UCE=K
∑
k=1Nk
N|err(Bk)−uncert(Bk)| (10)
AUCE=1
KK
∑
k=1|err(Bk)−uncert(Bk)| (11)
In our case err(Bk) represents the RMSE of the samples falling into 
bin Bk, and the bin uncertainty uncert(Bk) is defined as the root mean 
variance (RMV):
RMV=√√
√1
Nk∑
i∈Bk̂ui, (12)
with ̂ui=̂σi2 when evaluating the calibration of a single CNN and 
̂ui=Var(̂yi) when evaluating the calibration of the ensemble. We refer 
to the RMV as the predictive standard deviation in our calibration plots 
(Extended Data Fig. 5a,c).
Global map computation
Sentinel-2 imagery is organized in 100 km × 100 km tiles; a total of 
18,011 tiles cover the entire landmass of the Earth, excluding Antarc -
tica. However, depending on the ground tracks of the satellites, some 
tiles are covered by multiple orbits, whereas, in general, no more than 
two orbits are needed to get full coverage. To optimize computational 
overhead, we select the relevant orbits per tile by using those with the 
smallest number of empty pixels, according to the metadata. For every 
tile and relevant orbit, the ten images with least cloud cover between 
May and September 2020 are selected for processing.
While it only takes ≈ 2 minutes to process a single image tile with 
the CNN on a GeForce RTX 2080 Ti GPU, downloading the image from 
the Amazon Web Service S3 bucket takes about 1 minute, and loading 
the data into memory takes about 4 minutes. To process a full tile with 
all ten images per orbit takes between 1 and 2.5 hours, depending on the number of relevant orbits (one or two).
We apply only minimal post-processing and mask out built-up 
areas, snow, ice and permanent water bodies according to the ESA 
WorldCover classification58, setting their canopy height to ‘no data’ 
(value: 255). The canopy height product is released in the form of 3° × 3° 
tiles in geographic longitude/latitude, following the format of the 
recent ESA WorldCover product. This choice shall simplify the inte -
gration of our map into existing workflows and the intersection of the 
two products. Note that the statistics in the present paper were not 
computed from those tiles but in Gall–Peters orthographic equal-area 
projection with 10-m GSD for exact correspondence between pixel 
counts and surface areas.
Energy and carbon emissions footprint
The presented map has been computed on a GPU cluster located in 
Switzerland. Carbon accounting for electricity is a complex endeavour, 
due to differences in how electricity is produced and distributed. To 
put the power consumption needed to produce global maps with our 
method into context, we estimate carbon emissions for two scenarios, 
where the computation is run on Amazon Web Services (AWS) in two 
different locations: European Union (Stockholm) and United States East 
(Ohio). With ≈ 250 W to power one of our GPUs, we get a total energy 
consumption of 250 W × 27,000 h = 6,750 kWh for the global map. The 
conversion to emissions highly depends on the carbon efficiency of 
the local power grid. For European Union (Stockholm), we obtain an  
estimated 338 kg CO2-equivalent, whereas for United States East 
(Ohio), we obtain 3,848 kg CO2-equivalent, a difference by a factor >10. 
Whereas the former is comparable to driving an average car from Los Angeles to San Francisco and back (1,360 km), the latter corresponds 
to a round trip from Seattle (United States) to San José, Costa Rica 
(15,500 km). These estimates were conducted using the Machine Learn-
ing Impact calculator (ref. 59 ). For the carbon footprint of the current 
map (not produced with AWS), we estimate ≈ 729 kg CO2-equivalent, 
using an average of 108 g CO2-equivalent kWh−1 for Switzerland, as 
reported for the year 201760.
Reporting summary
Further information on research design is available in the Nature  
Portfolio Reporting Summary linked to this article.
Data availability
A summary of all links to data, browser application and code can be 
found on the project page at langnico.github.io/globalcanopyheight. 
Global map: the global canopy height map for 2020 is available for 
download at https://doi.org/10.3929/ethz-b-000609802. Individual 
tiles can be downloaded at langnico.github.io/globalcanopyheight/
assets/tile_index.html. The map is also available on the Google Earth Engine (GEE assets on project page). The global map can be explored 
interactively in this browser application: nlang.users.earthengine.
app/view/global-canopy-height-2020 . GEDI footprint data: sparse 
footprint-level RH98 estimates used as reference data for developing 
the presented model are available on Zenodo at https://doi.org/10.5281/
zenodo.7737946  (ref. 53), which is the filtered version of the full orbit 
predictions for 2019, https://doi.org/10.5281/zenodo.5704852 (ref. 61), 
and 2020, https://doi.org/10.5281/zenodo.7737869 (ref. 62). Training 
and validation datasets: the global training and validation dataset with 
image patches and rastered reference data is available at https://doi.
org/10.3929/ethz-b-000609845 . The rastered airborne LiDAR canopy 
height models from LVIS and ALS used for validation are available at 
https://doi.org/10.5281/zenodo.7885699 . ESA WorldCover: a derived 
version of the original ESA WorldCover 10-m 2020 v10058 re-projected 
to the Sentinel-2 UTM Tiling Grid for the global land surface is available 
at https://doi.org/10.5281/zenodo.7888150 (ref. 63).
Code availability
The source code and the trained models are available via Github, github.
com/langnico/global-canopy-height-model.
References
1. Manning, P. et al. Redefining ecosystem multifunctionality. Nat. 
Ecol. Evol. 2, 427–436 (2018).
2. United Nations Strategic Plan for Forests 2017–2030. (United Nations); https://www.un.org/esa/forests/documents/
un-strategic-plan-for-forests-2030/index.html (2017).
3. Hoang, N. T. & Kanemoto, K. Mapping the deforestation footprint of nations reveals growing threat to tropical forests. Nat. Ecol. Evol.  5, 845–853 (2021).
4. Hansen, M. C. et al. High-resolution global maps of 21st-century forest cover change. Science 342, 850–853 (2013).
5. Skidmore, A. K. et al. Priority list of biodiversity metrics to observe from space. Nat. Ecol. Evol. 5, 896–906 (2021).
6. Jucker, T. et al. Allometric equations for integrating remote sensing imagery into forest monitoring programmes. Glob. 
Change Biol. 23, 177–190 (2017).
7. Migliavacca, M. et al. The three major axes of terrestrial ecosystem function. Nature 598, 468–472 (2021).
8. Duncanson, L. et al. Aboveground biomass density models for NASA’s Global Ecosystem Dynamics Investigation (GEDI) lidar mission. Remote Sens. Environ. 270, 112845 (2022).
9. Tuanmu, M.-N. & Jetz, W. A global, remote sensing-based characterization of terrestrial habitat heterogeneity for biodiversity and ecosystem modelling. Glob. Ecol. Biogeogr. 24, 
1329–1339 (2015).
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789 1788
Article https://doi.org/10.1038/s41559-023-02206-610. De Frenne, P. et al. Global buffering of temperatures under forest 
canopies. Nat. Ecol. Evol. 3, 744–749 (2019).
11. Jucker, T. et al. Canopy structure and topography jointly constrain the microclimate of human-modified tropical landscapes. Glob. Change Biol. 24, 5243–5258 (2018).
12. Dubayah, R. et al. The global ecosystem dynamics investigation: high-resolution laser ranging of the Earth’s forests and topography. Sci. Remote Sens. 1, 100002 (2020).
13. Valbuena, R. et al. Standardizing ecosystem morphological traits from 3D information sources. Trends Ecol. Evol. 35, 656–667 
(2020).
14. Gibbs, H. K., Brown, S., Niles, J. O. & Foley, J. A. Monitoring and 
estimating tropical forest carbon stocks: making redd a reality. 
Environ. Res. Lett. 2, 045023 (2007).
15. Rodríguez-Veiga, P., Wheeler, J., Louis, V., Tansey, K. & Balzter, H. Quantifying forest biomass carbon stocks from space. Curr. For. Rep.  3, 1–18 (2017).
16. Lang, N., Schindler, K. & Wegner, J. D. Country-wide high-resolution vegetation height mapping with Sentinel-2. Remote Sens. Environ. 233, 111347 (2019).
17. Becker, A. et al. Country-wide retrieval of forest structure from 
optical and SAR satellite imagery with deep ensembles. Preprint 
at https://doi.org/10.48550/arXiv.2111.13154 (2021).
18. Lang, N., Schindler, K. & Wegner, J. D. High carbon stock mapping at large scale with optical satellite imagery and spaceborne LIDAR. Preprint at https://doi.org/10.48550/arXiv.2107.07431 
(2021).
19. Rodríguez, A. C., D’Aronco, S., Schindler, K. & Wegner, J. D. Mapping oil palm density at country scale: an active learning approach. Remote Sens. Environ. 261, 112479 (2021).
20. Jumper, J. et al. Highly accurate protein structure prediction with alphafold. Nature 596, 583–589 (2021).
21. Ravuri, S. et al. Skilful precipitation nowcasting using deep generative models of radar. Nature 597, 672–677 (2021).
22. Reichstein, M. et al. Deep learning and process understanding for data-driven earth system science. Nature 566, 195–204 (2019).
23. Tuia, D. et al. Perspectives in machine learning for wildlife conservation. Nat. Commun. 13, 792 (2022).
24. Kattenborn, T., Leitloff, J., Schiefer, F. & Hinz, S. Review on convolutional neural networks (CNN) in vegetation remote sensing. ISPRS J. Photogramm. Remote Sens. 173, 24–49 (2021).
25. Gorelick, N. et al. Google Earth Engine: planetary-scale geospatial analysis for everyone. Remote Sens. Environ. 202, 18–27 (2017).
26. Hansen, M. C. et al. Mapping tree height distributions in sub-Saharan Africa using Landsat 7 and 8 data. Remote Sens. Environ. 185, 221–232 (2016).
27. Potapov, P. et al. Mapping global forest canopy height through integration of GEDI and Landsat data. Remote Sens. Environ. 253, 
112165 (2021).
28. Healey, S. P., Yang, Z., Gorelick, N. & Ilyushchenko, S. Highly local model calibration with a new GEDI LiDAR asset on Google Earth Engine reduces Landsat forest height signal saturation. Remote Sens. 12, 2840 (2020).
29. Lakshminarayanan, B., Pritzel, A. & Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. In Proc. 31st International Conference on Neural Information 
Processing Systems 6405–6416 (Curran Associates, Inc., Red Hook, 2017).
30. Lang, N. et al. Global canopy height regression and uncertainty estimation from GEDI LIDAR waveforms with deep ensembles. Remote Sens. Environ. 268, 112760 (2022).
31. Tang, K., Paluri, M., Fei-Fei, L., Fergus, R. & Bourdev, L. Improving image classification with location context. In Proc. IEEE International Conference on Computer Vision 1008–1016 (IEEE Computer Society, Los Alamitos, 2015).32. Blair, J. Processing of NASA LVIS elevation and canopy (LGE, LCE and LGW) data products, version 1.0. NASA https://lvis.gsfc.nasa.gov (2018).
33. Liu, S. et al. The overlooked contribution of trees outside forests to tree cover and woody biomass across Europe. Preprint at Research Square https://doi.org/10.21203/rs.3.rs-2573442/v1 
(2023).
34. Guo, C., Pleiss, G., Sun, Y. & Weinberger, K.Q. On calibration of modern neural networks. In Proc. 34th International Conference on Machine Learning 1321–1330 (ML Research Press, 2017).
35. Kendall, A. & Gal, Y. What uncertainties do we need in Bayesian deep learning for computer vision? In Proc. 31st International 
Conference on Neural Information Processing Systems 5580–5590 
(Curran Associates, Inc., Red Hook, 2017).
36. Ovadia, Y. et al. Can you trust your model’s uncertainty? Evaluating predictive uncertainty under dataset shift. In Proc. 33rd Conference on Neural Information Processing Systems  
13991–14002 (Curran Associates, Inc., Red Hook, 2019).
37. Ashukha, A., Lyzhov, A., Molchanov, D. & Vetrov, D. Pitfalls of in-domain uncertainty estimation and ensembling in deep learning. In Proc. 8th International Conference on Learning 
Representations (Curran Associates, Inc., Red Hook, 2020); 
https://openreview.net/forum?id=BJxI5gHKDr , https://dblp.org/
rec/conf/iclr/AshukhaLMV20.bib
38. Strutz, T. Data Fitting and Uncertainty: A Practical Introduction to 
Weighted Least Squares and Beyond (Vieweg and Teubner, 2010).
39. Protected Planet: The World Database on Protected Areas (WDPA)  
(UNEP-WCMC & IUCN, 2021); https://www.protectedplanet.net/en
40. Roy, D. P., Kashongwe, H. B. & Armston, J. The impact of geolocation uncertainty on GEDI tropical forest canopy height estimation and change monitoring. Sci. Remote Sens. 4, 100024 
(2021).
41. Tang, H. et al. Evaluating and mitigating the impact of systematic geolocation error on canopy height measurement performance of gedi. Remote Sens. Environ. 291, 113571 (2023).
42. De Lutio, R., D’Aronco, S., Wegner, J. D. & Schindler, K. Guided super-resolution as pixel-to-pixel transformation. In Proc. IEEE/CVF International Conference on Computer Vision 8828–8836 (IEEE Computer Society, Los Alamitos, 2019).
43. Dubayah, R. et al. GEDI launches a new era of biomass inference from space. Environ. Res. Lett. 17, 095001 (2022).
44. Dubayah, R. et al. GEDI L3 Gridded Land Surface Metrics Version 1 (ORNL DAAC, 2021); https://doi.org/10.3334/ORNLDAAC/1865
45. Global Forest Resources Assessment 2020: Main Report (FAO, 
2020); https://doi.org/10.4060/ca9825en
46. Asner, G. P., Brodrick, P. G. & Heckler, J. Global airborne observatory: forest canopy height and carbon stocks for Sabah, Borneo Malaysia. Zenodo https://doi.org/10.5281/zenodo.4549461 (2021).
47. Asner, G. P. et al. Mapped aboveground carbon stocks to advance forest conservation and recovery in Malaysian Borneo. Biol. Conserv. 217, 289–310 (2018).
48. Friedlingstein, P. et al. Global carbon budget 2021. Earth Syst. Sci. Data Discuss. 14, 1917–2005 (2022).
49. Felipe-Lucia, M. R. et al. Multiple forest attributes underpin the supply of multiple ecosystem services. Nat. Commun. 9,  
4839 (2018).
50. MacArthur, R. H. & MacArthur, J. W. On bird species diversity. Ecology 42, 594–598 (1961).
51. Tews, J. et al. Animal species diversity driven by habitat heterogeneity/diversity: the importance of keystone structures.  
J. Biogeogr. 31, 79–92 (2004).
52. Le Toan, T. et al. The biomass mission: objectives and requirements. In IGARSS 2018–2018 IEEE International Geoscience and Remote Sensing Symposium 8563–8566 (IEEE, Piscataway, 2018).
Nature Ecology & Evolution | Volume 7 | November 2023 | 1778–1789
 1789
Article https://doi.org/10.1038/s41559-023-02206-653. Lang, N. et al. Filtered canopy top height estimates from GEDI 
LIDAR waveforms for 2019 and 2020. Zenodo https://doi.org/  
10.5281/zenodo.7737946 (2023).
54. Dubayah, R. et al. GEDI L1B Geolocated Waveform Data Global Footprint Level V001 (NASA, 2020).
55. Chollet, F. Xception: deep learning with depthwise separable convolutions. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition 1800–1807 (IEEE Computer Society, Los Alamitos, 2017).
56. Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. In Proc. 3rd International Conference on Learning Representations  
(Eds. Bengio, Y. & LeCun, Y.) (Curran Associates, Inc., Red Hook, 
2015).
57. Laves, M.-H., Ihler, S., Fast, J. F., Kahrs, L. A. & Ortmaier, T. 
Well-calibrated regression uncertainty in medical imaging with deep learning. In Proc. 3rd Conference on Medical Imaging with Deep Learning 393–412 (ML Research Press, 2020).
58. Zanaga, D. et al. ESA WorldCover 10 m 2020 v100. Zenodo  
https://doi.org/10.5281/zenodo.5571936 (2021).
59. Lacoste, A., Luccioni, A., Schmidt, V. & Dandres, T. Quantifying  
the carbon emissions of machine learning. Preprint at  
https://doi.org/10.48550/arXiv.1910.09700 (2019).
60. Rüdisüli, M., Romano, E., Eggimann, S. & Patel, M. K. Decarbonization strategies for Switzerland considering embedded greenhouse gas emissions in electricity imports. Energy Policy 162, 112794 (2022).
61. Lang, N. et al. Global canopy top height estimates from GEDI LIDAR waveforms for 2019. Zenodo https://doi.org/10.5281/zenodo.5704852 (2021).
62. Lang, N. et al. Global canopy top height estimates from GEDI LIDAR waveforms for 2020. Zenodo https://doi.org/10.5281/zenodo.7737869 (2023).
63. Lang, N., Schindler, K. & Wegner, J. D. ESA WorldCover 10 m 2020 v100 reprojected to the Sentinel-2 UTM tiling grid. Zenodo  
https://doi.org/10.5281/zenodo.7888150 (2023).
Acknowledgements
The project received funding from Barry Callebaut Sourcing AG as part of a research project agreement. The Sentinel-2 data access for the computation of the global map was funded by AWS Cloud credits for research, courtesy of P. Gehler. We thank Y. Sica for sharing a rastered version of the WDPA data and S. Liu and M. Brandt for sharing the rastered ALS canopy height models in Europe. We thank N. Gorelick and S. Ilyushchenko for providing the resources to make our global map available on the Google Earth Engine. We greatly appreciate the open data policies of the ESA Copernicus program, the NASA GEDI mission and the LVIS campaigns. N.L. acknowledges support by the research grant DeReEco (grant number 34306) from Villum Foundation and by the Pioneer Centre for AI, Danish National Research Foundation grant number P1.Author contributions
N.L. developed the code and carried out the experiments under the guidance of J.D.W. and K.S. W.J. gave suggestions and feedback on the global analyses. All authors contributed to the article and the analyses of the results.
FundingInformation
Open access funding provided by Swiss Federal Institute of Technology Zurich.
Competing interests
The authors declare no competing interests.
Additional information
Extended data is available for this paper at  
https://doi.org/10.1038/s41559-023-02206-6 .
Supplementary information The online version  
contains supplementary material available at  
https://doi.org/10.1038/s41559-023-02206-6 .
Correspondence and requests for materials should be addressed to 
Nico Lang or Jan Dirk Wegner.
Peer review information Nature Ecology & Evolution thanks the 
anonymous reviewers for their contribution to the peer review of  
this work.Reprints and permissions information is available at  
www.nature.com/reprints.Publisher’s note Springer Nature remains neutral with regard  
to jurisdictional claims in published maps and institutional  
affiliations.Open Access This article is licensed under a Creative Commons 
Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
© The Author(s) 2023, corrected publication 2024
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6
Extended Data Fig. 1 | Geographical error analysis on held-out GEDI validation data at 1 degree resolution ( ≈ 111 km on the equator). a) Root mean square 
error (RMSE). b) Mean absolute error (MAE). c) Mean error (ME), where negative ME means an underestimation bias when the predictions are lower than the reference values.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6
Extended Data Fig. 2 | Biome-level evaluation with GEDI reference data. Biome-level confusion plots showing the relationship between GEDI reference data and the 
estimated canopy top height from Sentinel-2.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6ba
Extended Data Fig. 3 | See next page for caption.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6Extended Data Fig. 3 | Comparison of canopy top height estimates from ETH 
(ours) and UMD against the hold-out GEDI validation data. a) Residual analysis w.r.t. GEDI reference canopy height. The boxplot shows the median, the quartiles, and the 10th and 90th percentiles (n=87,406,779 “UMD" and “ETH (ours)"; 
n=69,925,423 “ETH (ours) 80%"). b) Residual analysis w.r.t. biomes defined by The 
Nature Conservancy. Negative residuals indicate that estimates are lower than reference values. “ETH (ours) 80%" is a filtered version of our estimates where the 20% of estimates with the highest relative standard deviation are removed. This filtering follows the protocol proposed in previous work using an adaptive threshold depending on the predicted canopy height to preserve the full canopy height range
30. The distributions over the full height range are compared in 
Supplementary Information Fig. S2. The boxplot shows the median,  
the quartiles, and the 10th and 90th percentiles (n=87,406,779 “UMD" and  
“ETH (ours)"; n=69,925,423 “ETH (ours) 80%").
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6
Extended Data Fig. 4 | Locations of independent airborne LIDAR campaigns. Independent airborne LIDAR data from 24 regions including 12 regions with LVIS 
LIDAR and 12 regions with small-footprint airborne laser scanning (ALS) campaigns.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6cb a
Extended Data Fig. 5 | See next page for caption.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6Extended Data Fig. 5 | Evaluation of the estimated uncertainty. a) Calibration 
plot showing the relationship between the estimated predictive uncertainty and the empirical error. b) Improvement of overall error metrics when filtering out the most uncertain canopy height predictions with the help of the estimated predictive uncertainty. c) Biome-level calibration plots, showing the relationship between the estimated predictive uncertainty and the empirical error w.r.t. GEDI reference data.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6
Extended Data Fig. 6 | Protected area analysis according to the World 
Database on Protected Areas (WDPA)39. a) Cumulative area covered by 
vegetation above a given height in protected areas and unprotected areas. The sum at height 0 equals the area of the global landmass (excluding Antarctica). b) Examples where the dense canopy height map reveals the spatial patterns of protected areas. Left: “Devil’s Staircase Wilderness" containing federally protected old-growth forest stands in the Oregon Coast Range. Center: “Ulu Temburong National Park" in Brunei, Borneo, established in 1991. Right: Protected areas in Ghana that indicate the strong impact of protection measures on the growing vegetation.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6
Extended Data Fig. 7 | See next page for caption.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6Extended Data Fig. 7 | Qualitative examples comparing UMD and ETH (ours) 
against GEDI-like canopy top height derived from small-footprint airborne laser scanning (ALS) campaigns (a-c) and L VIS LIDAR (d-f ). a) Switzerland (from tile region 32TMT with average height 14.1m). Our map with RMSE: 8.0m, 
bias: 1.5 m (10.5%) outperforms the UMD map with RMSE: 10.1m, bias: -5.4 m 
(-37.4%). b) Spain (from tile region 30SWH with low average height 6.0m). Our map with RMSE: 4.7m, bias: 0.5 m (7.7%) outperforms the UMD map with RMSE: 5.2m, bias: -3.3 m (-54.9%). c) Netherlands (from tile region 31UFT with average height 6.6m). Our map with RMSE: 6.4m, bias: 3.9 m (59.9%) yields higher error than the UMD map with RMSE: 5.9m, bias: -2.2 m (-37.5%). In both b) and c), the UMD map underestimates high vegetation and misses small structures in low vegetation areas. d) Gabon, Lopé National Park (from the tile region 32MQE with the highest average height of 36.2 m). Our map with RMSE: 8.9m, bias: -4.8 m (-13.3%) outperforms the UMD map with RMSE: 11.4m, bias: -7.9 m (-21.8%). e) US, 
Oregon (from tile region 10TER with an average height of 28.6m). Our map with 
RMSE: 9.6m, bias: 2.9m, (10.0%) outperforms the UMD map with RMSE: 12.0 m, bias: -5.4 m (-19.3%). f) Costa Rica (from tile region 16PHS with average height 16.7m). Our map with RMSE: 9.2m, bias: 5.9 m (35.4%) yields a higher error than the UMD map with RMSE: 8.1m, bias: -3.0 m (-18.0%).
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6
Extended Data Fig. 8 | See next page for caption.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6Extended Data Fig. 8 | Qualitative examples beyond GEDI coverage (that is 
north of 51.6∘ latitude) against GEDI-like canopy top height derived from 
small-footprint airborne laser scanning (ALS) campaigns (a-c) and L VIS LIDAR (d-f ). a) Finland (from tile region 35WNT with average height 3.0m). Our map yields an RMSE: 3.0 m and bias: 0.5 m (17.1%). b) Finland (from tile region 
35VNL with average height 15.3m). Our map yields an RMSE: 5.7 m and bias:  
-2.6 m (-17.2%). c) Wales (from tile region 30UVD with average height 3.3m). An 
error case with an RMSE: 7.3 m and bias: 5.7 m (171.1%). Our map overestimates the ALS reference data, especially in low vegetation areas. d) Canada (from tile region 08WNA with average height 4.3m). Our map yields an RMSE: 2.8 m and bias:  
0.7 m (16.4%). e) US, Alaska (from tile region 06VXR with average height 9.3m). Our map yields an RMSE: 6.4 m and bias: 2.7 m (29.2%). f) US, Alaska (from tile region 06WVT with average height 6.8m). Our map yields an RMSE: 5.3 m and 
bias: 0.6 m (8.7%). Both e) and f) show high predictive uncertainty, yet the canopy 
top height estimates are reasonable.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6Extended Data Table 1 | Tile-level evaluation with independent airborne LIDAR data
LVIS airborne LIDAR (Canada, US, Costa Rica, and Gabon) and GEDI-like canopy top height derived from small-footprint airborne laser scanning (ALS) campaigns in Europe (Finland, Estonia, 
Denmark, Wales, Netherlands, Switzerland, and Spain). Error metrics are given per Sentinel-2 tile for a total of 24 tiles, 12 tiles for LVIS (top rows) and 12 tiles for ALS (bottom rows). The best 
results are highlighted in bold.
Nature Ecology & Evolution
Article https://doi.org/10.1038/s41559-023-02206-6Extended Data Table 2 | Averaged evaluation with independent airborne LIDAR data
Comparison with independent airborne LIDAR data (LVIS and ALS from Extended Data Table 1) averaged over all 24 tiles (top), tiles within the GEDI coverage (center), and tiles beyond the 
GEDI coverage (bottom).
1 nature portfolio  |  reporting summary March 2021
Corresponding author(s): Nico Lang, Jan Dirk Wegner
Last updated by author(s): May 11, 2022
Reporting Summary
Nature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistenc y and transparency 
in reporting. For further information on Nature Portfolio policies, see our Editorial Policies  and the Editorial Policy Checklist .
Statistics
For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Me thods section.
n/a Confirmed
The exact sample size ( n) for each experimental group/condition, given as a discrete number and unit of measurement
A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly
The statistical test(s) used AND whether they are one- or two-sided 
Only common tests should be described solely by name; describe more complex techniques in the Methods section.
A description of all covariates tested
A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons
A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regress ion coefficient) 
AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)
For null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted 
Give P values as exact values whenever suitable.
For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings
For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes
Estimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated
Our web collection on statistics for biologists  contains articles on many of the points above.
Software and code
Policy information about availability of computer code
Data collection The data were provided by Copernicus/ESA and NASA's GEDI mission. Sentinel-2 images were accessed using the sentinelsat and sen tinelhub 
API from Scihub and AWS S3. GEDI-derived data were accessed from zenodo.
Data analysis Only free and open source software was used for data analysis: python 3.7, pytorch '1.8.0+cu101', QGIS 3.20, GDAL 3.2.0
For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published lit erature, software must be made available to editors and 
reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio  guidelines for submitting code & software  for further information.
Data
Policy information about availability of data
All manuscripts must include a data availability statement . This statement should provide the following information, where applicable: 
- Accession codes, unique identifiers, or web links for publicly available datasets 
- A description of any restrictions on data availability 
- For clinical datasets or third party data, please ensure that the statement adheres to our policy  
 
The global canopy height map for 2020 is accessible for download and available in the Google Earth Engine. All links, source co de, and the trained models used 
to generate the map will be released via the project page: https://langnico.github.io/globalcanopyheight/. The global map can b e explored interactively in this 
browser application: https://nlang.users.earthengine.app/view/global-canopy-height-2020 
2 nature portfolio  |  reporting summary March 2021Field-specific reporting
Please select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before m aking your selection.
Life sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences
For a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf
Ecological, evolutionary & environmental sciences study design
All studies must disclose on these points even when the disclosure is negative.
Study description A deep learning model was trained on ~600,000 GEDI footprints paired with the corresponding Sentinel-2 image patches. 
The global canopy height map is based on 160 TB of Sentinel-2 image data from the year 2020.
Research sample The study is a wall-to-wall assessment of the global landmass at 10-meter ground sampling distance.
Sampling strategy No sampling was necessary as the entire landmass is part of the study. For the evaluation of the model performance, the samplin g is 
randomized at the Sentinel-2 tile level (100 km x 100 km  regions). The sampling with the tiles is given by the GEDI sampling p attern 
along the ground tracks of the International Space Station.
Data collection Sentinel-2 images were downloaded from the AWS S3 bucket. GEDI-derived data were accessed from zenodo. LVIS airborne LIDAR 
data is downloaded from the National Snow & Ice Data Center (NSIDC).
Timing and spatial scale The global canopy height map is based on images from the year 2020. For every location, we processed the 10 image tiles with th e 
least cloud coverage between May and September.
Data exclusions We have used the ESA World Cover Map to mask urban and water areas. In the biome-level distribution analyses, we have exlcuded crop lands based on the ESA World Cover Map to characterize the height distribution of natural ecosystems.
Reproducibility The final map is the result of an ensemble of five models deployed on 10 repeated image observations. As the optimization of de ep 
neural networks is based on stochastic algorithms, slight variations in the output are to be expected.
Randomization NA. To evaluate the model globally, we split the collected dataset at the level of Sentinel-2 tiles. I.e., of the 100 km×100 km  regions 
defined by the Sentinel-2 tiling 20% are held out for validation, the remaining 80% are used to train the model.
Blinding NA. We additionally report an evaluation of our final model against independent reference data from NASA’s LVIS airborne LIDAR campaigns.
Did the study involve field work? Yes No
Reporting for specific materials, systems and methods
We require information from authors about some types of materials, experimental systems and methods used in many studies. Here,  indicate whether each material, 
system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the approp riate section before selecting a response. 
Materials & experimental systems
n/a Involved in the study
Antibodies
Eukaryotic cell lines
Palaeontology and archaeology
Animals and other organisms
Human research participants
Clinical data
Dual use research of concernMethods
n/a Involved in the study
ChIP-seq
Flow cytometry
MRI-based neuroimaging
